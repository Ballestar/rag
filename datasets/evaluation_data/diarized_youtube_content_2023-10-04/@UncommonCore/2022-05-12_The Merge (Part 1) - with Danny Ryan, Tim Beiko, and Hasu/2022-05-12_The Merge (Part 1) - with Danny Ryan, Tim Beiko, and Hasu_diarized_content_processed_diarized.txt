00:00:03.370 - 00:00:20.110, Speaker A: Welcome to Uncommon Core, where we explore the big ideas in crypto from first principles. This show is hosted by Su Zu, the CEO and Chief Investment Officer of Three Arrows Capital. And Me Hasu, a crypto researcher and writer.
00:00:21.570 - 00:00:21.934, Speaker B: Hey.
00:00:21.972 - 00:00:34.982, Speaker A: Welcome tim Becko and danny ryan to uncommon core. More we are here today to talk about Ethereum's huge upgrade, the transition to proof of stake, also colloquially termed the merge. Yeah.
00:00:34.982 - 00:00:48.250, Speaker A: Let's start by introducing yourselves, starting with you, Danny, and then Tim. So please introduce yourself, where you work, what you have done so far in crypto and from a high level. Why are you qualified to speak about the merge?
00:00:49.390 - 00:00:58.234, Speaker C: Yeah. So my name is Danny Ryan. I work with the Ethereum Foundation, technically on the research team, so I do some research around protocol upgrades.
00:00:58.234 - 00:01:20.790, Speaker C: I also do a lot of spec writing, I do some testing, and I do a lot of communication and coordination with engineers working on protocol upgrades as well as people beyond. I've been doing this for a while. I got involved in 2017 because proof of stake was just around the corner, and I thought there might be some interesting opportunities there.
00:01:20.790 - 00:01:38.902, Speaker C: I began to work on testing and doing various things related to proof of stake to help it get over the edge, and I'm still doing that today. Honestly, it was a bigger project than we expected. A lot of meandering paths and turns.
00:01:38.902 - 00:01:58.274, Speaker C: I understand the system, I helped design the system, so I guess that's why I'm qualified. And the system is kind of in production today in some capacity, so I've helped steward that into production and now I'm helping steward kind of its final phase. Well, its most important phase coming.
00:01:58.274 - 00:01:58.786, Speaker C: Yeah.
00:01:58.808 - 00:02:14.550, Speaker B: So I'm Tim Baco, also at the Ethereum Foundation. I joined the Ethereum Foundation late 2020, but prior to that, I spent a couple of years at Consensus on one of their Ethereum client teams. And I got interested in a different way at Ethereum.
00:02:14.550 - 00:02:31.558, Speaker B: I've been following it since about the Dow, but in 2017, there was this ICO kind of craze on Ethereum. And I remember as a user there, there was a crazy ICO and the network would get congested for days like the mempool would just not clear. It was absolutely terrible.
00:02:31.558 - 00:02:54.978, Speaker B: And that's kind of what made me want to work on the protocol. It felt like this was clearly there was demand for this thing, but it could not meet it, and the base layer was not in a good spot enough to meet all the demand. And so over the next couple of years, I joined Consensus as they were starting up a protocol team, helped build one of their Ethereum clients there, and then gradually got more and more involved in protocol work.
00:02:54.978 - 00:03:03.698, Speaker B: And now I lead the core developer calls, working on what we now call the execution layer, but basically the proof of work Ethereum chain.
00:03:03.874 - 00:03:14.502, Speaker A: Well, amazing. I already have two things that I want to dig into. So Danny, you said you joined in 2017 when Proof of Stake was just around the corner.
00:03:14.502 - 00:03:25.760, Speaker A: So what happened since then? What paths of the game tree, quote unquote, did you explore that turned out to be unworkable on the road to proof of Stake? And what makes you confident that this time is different?
00:03:26.610 - 00:03:53.030, Speaker C: So in 2017, there were a couple of different so ethereum proof of Stake had been in a very much a research phase and not a lot of production engineering at that time because there were fundamentally hard problems to solve. So there are a couple of different parallel research tracks in which Casper FFG kind of coalesced into a formalized algorithm and paper in 2017. And that began to be kind of what we were marching towards.
00:03:53.030 - 00:04:22.154, Speaker C: FFG is like a Proof of Stake finality gadget that can layer on top of some block proposal mechanism. So at the time we thought we might layer that on top of proof of Work as a tried and true block proposal mechanism and then eventually swap that out for an underlying proof of Stake block proposal mechanism. So that was the path that was being followed in 2017 that coalesced into an EIP called EIP 1011 hybrid Casper proof of work, proof of Stake.
00:04:22.154 - 00:04:35.406, Speaker C: And that's kind of what people were working on at the time. Some of the engineering teams at the time were doing some kind of early testing and testnets of that. A lot of the logic was inside of a contract.
00:04:35.406 - 00:05:03.294, Speaker C: So actually it would be kind of like a system level contract of those finality rules and the voting that validators do and stuff. But at the same time, we were also trying to solve some other problems. So Sharding and other scalability upgrades, but kind of in a parallel track, it became clear in 2018 that these are not disparate problems and trying to solve them in two places does not really get us where we're going and makes ultimately a very complicated system.
00:05:03.294 - 00:05:14.798, Speaker C: And by having two call them two validator games in two different places to stake in two different places to be doing things for the protocol, having them in competition was also kind of bad and weird.
00:05:14.894 - 00:05:18.702, Speaker A: And the two things are sharding and block production.
00:05:18.766 - 00:05:27.230, Speaker C: Oh yeah, define these things. So in any consensus algorithm, we need to produce blocks. So there's consensus participants that produce blocks.
00:05:27.230 - 00:05:46.662, Speaker C: Miners in Proof of Work validators improve of stake. These people generally have some amount of capital that they've dedicated to the system, either in the form of mining hardware and energy consumption and Proof of work or the in protocol asset itself, ether in Proof of Stake that gives them the right to play this game and produce blocks. And then the other one was osharding.
00:05:46.662 - 00:06:03.450, Speaker C: Essentially, we're beyond trying to make the protocol more secure and more sustainable in this Proof of Stake migration. We also would like to make it more scalable and designs that try to get more out of the consensus mechanism. We often in the Ethereum landscape called Sharding.
00:06:03.450 - 00:06:13.746, Speaker C: There's lots of different flavors and explorations in this domain. But essentially can we take the same amount of crypto economic capital and the same consensus participants who come to consensus on more?
00:06:13.928 - 00:06:29.654, Speaker A: Interesting. Yeah, I personally have never looked too deeply into Sharding and I would assume that most of my listeners haven't either. So I was interested in double clicking on this idea that Sharding is something that the block producers do because I don't think most people have this mental model.
00:06:29.772 - 00:06:48.234, Speaker C: Right. And you could imagine having a different set of participants like the block producers of the kind of the L One chain and then some other consensus participants that go and do this other thing to try to scale out in these other scalable zones. But those designs, kind of having them disparate in the designs does not make for elegant designs.
00:06:48.234 - 00:07:08.782, Speaker C: So that's something that we were doing in 2018 is we kind of had these two different in protocol games, the Proof of Stake game and then kind of like the Sharding game. And that wasn't very nice. As well as EIP 1011, the minimum stake to be involved was 1500 E, which was not really great from a decentralization perspective.
00:07:08.782 - 00:07:46.850, Speaker C: From allowing hobbyist home stakers to get involved and with some insight from Justin Drake and some others being able to leverage BLS aggregate signatures, we were able to kind of go into a different design landscape that was a bit more radical than EIP 1011 to reduce the participation threshold to 32 ETH. And we went into a totally different design landscape where we were kind of rearchitecting the ethereum consensus not as a layer on top of proof of work, but as this totally separate thing that we're operating in parallel. That then happened for a while.
00:07:46.850 - 00:08:12.646, Speaker C: That consensus mechanism, the Beacon Chain, was released at the end of 2020 with a number of new teams working on it. So the classic teams, Gaff, Parity, they kept kind of the charge on that proof of work chain, kept refining it and optimizing it, whereas some new teams came in and built out this consensus mechanism called the Beacon Chain, launched at the end of 2020. A meandering path of refining the specifications and getting this thing into production and testing.
00:08:12.646 - 00:08:18.922, Speaker C: That took a couple of years. That's been in production in parallel with the proof of work chain and now we're bringing it together.
00:08:19.056 - 00:08:41.170, Speaker A: Yeah, you touched a little bit on the process and both of you are working for the Ethereum Foundation. So maybe, Tim, can you give us some insight on what is Ethereum Foundation's role in protocol development? And for a complex proposal like Proof of Stake, how does it go from idea to production? What parties are in charge here and working together?
00:08:41.320 - 00:09:03.062, Speaker B: Right. It's funny to think of proof of Stake as a proposal in a way because it was always part of the Ethereum Roadmap, right? And the reason I say this is there are things that are actual proposals like say EIP 1559, where there's a big part of the process that's like getting community consensus around this and making sure that this is actually a change that people want. And I think with proof of stake.
00:09:03.062 - 00:09:35.522, Speaker B: We haven't really had to do this because it's been part of the roadmap and I think also because Ethereum Classic exists and I think that it was much earlier on, but it did give kind of an off ramp for people who were not on board with the whole migration to move away. So for basically everything else, I think making sure that the client developers and researchers get the right inputs from the community is a big part of the job. I think in this case, not as much.
00:09:35.522 - 00:09:58.582, Speaker B: The thing that EF ends up doing a ton of is like cross client testing and infrastructure. So, as Danny kind of mentioned, and again, if your viewers are not familiar with Ethereum, this might be relevant. Ethereum does not have a single implementation like Bitcoin, right? Like in Bitcoin you have Bitcoin core and this is basically the protocol.
00:09:58.582 - 00:10:35.958, Speaker B: Ethereum instead has written specification which is the yellow paper on the proof of work chain and there's an executable spec on the beacon chain. So we do a lot of work maintaining those and then providing testing infrastructure for the different client implementation teams to then write their implementation of the spec. And so I think the other thing we do is just generally kind of provide momentum in a way where we get all these teams together in calls every two weeks, both Danny and myself, and kind of make sure that we're making progress, that we're finding the biggest issues and blocking those.
00:10:35.958 - 00:10:59.118, Speaker B: We'll organize some research workshops, get everybody together in person to do different things. And historically the EF did a lot more like hands on kind of development, the Get team being like the most kind of notable example where they are still kind of developing their client today. But on the Proof of Stake side, for example, there is no Ethereum Foundation client, right? Like they guessed code base.
00:10:59.118 - 00:11:19.978, Speaker B: So I think over time, yeah, it's definitely moved much more from having to write all the code because literally no one else was doing it, to then having to maintain kind of this core infrastructure that the different teams contributing to the protocol use. And then ideally, even over time we'd like to get more and more of those teams to contribute back to this common infrastructure.
00:11:20.174 - 00:11:42.406, Speaker C: We spend a lot of time probably coordinating and helping people kind of see the options available and come to decisions and things like that. But the EF rarely is even in the position to be able to make a decision when it comes to what gets to production. Obviously, I think the EF has a number of strong engineering talent or strong researcher talent.
00:11:42.406 - 00:11:56.846, Speaker C: And so they write good specifications, they have good ideas. But at the end of the day, because there's so many different teams involved and because of the way the kind of community works is if the EF tomorrow said we must do X, it wouldn't necessarily happen.
00:11:57.028 - 00:12:22.070, Speaker B: Yeah, and one note on that, vitalik is always complaining he's had EIPS impending for like two years to remove self destruct and stuff. So I think this is like the one part where from the outside people think maybe Vitalik can single handedly put a change in. And if you want evidence that he cannot, you just go to the EIP's website, look at the draft ones and how many have Vitalik's name in it and are not into production.
00:12:22.070 - 00:12:24.490, Speaker B: That'll give you a good gauge.
00:12:24.910 - 00:12:39.422, Speaker A: Yeah. So, Danny, can you describe what blockchains need a consensus engine for and what is sort of the difference here between how it's going to work in proof of stake versus proof of work from a high level.
00:12:39.556 - 00:13:00.486, Speaker C: Right. So something has to tell us what the state of reality is in a blockchain. And that is something has to order our interactions, usually called transactions, and allow us to agree on what the end result of that is.
00:13:00.486 - 00:13:37.490, Speaker C: So, abstractly, if we're just in kind of accounts and balances, I want to know that I have two Ether and you have one Ether and not that you think you have two Ether and I have one Ether because of different perspectives on reality. And so a consensus mechanism ultimately, in these decentralized environments allow us to do that without Visa or a bank or someone sitting in the middle of it. And what I like to call these proof of work, proof of stake and maybe proof of some other kind of stuff, they're crypto economic consensus mechanisms and they allow us to agree on the state of the world.
00:13:37.490 - 00:14:12.400, Speaker C: Assuming there's not some large enough economic actor attacking this thing. In proof of work, we make a claim that if not more than 50% of anyone on the network has more than 50% of the mining assets and power and burning of energy, then we can generally, assuming that we've seen all the same types of blocks, agree on what the state of the world is. So more concretely, in proof of work, we have people that want to show up, they want to make some money, they want to play an economic game.
00:14:12.400 - 00:14:29.470, Speaker C: They take capital and they want to participate in this crypto economic consensus mechanism. So they convert that capital into the asset that allows them to play that game. In mining, that is, mining hardware in Ethereum that is generally done on GPUs.
00:14:29.470 - 00:14:42.662, Speaker C: In other mining algorithms like Bitcoin and others, there's different types of hardware that you might invest in to do so. So you need hardware and you need the ability to kind of continually demonstrate that that hardware is dedicated to the network. And so you do that.
00:14:42.662 - 00:14:58.502, Speaker C: You solve these kind of hashing algorithm puzzles through the burning of energy. And so you're kind of constantly dedicating economic resources to the protocol. In doing so, you get the right randomly every once in a while to make blocks.
00:14:58.502 - 00:15:25.858, Speaker C: So the whole point of me doing that and dedicating all these economic resources to the protocol is to make blocks. And the protocol incentivizes those that get to make a block. They get some sort of issuance, so they get new assets, so new ether in the form of ethereum, and they also get to get transaction fees or other types of privileged things related to making blocks, maybe mev and others, but we don't have to get in that now.
00:15:25.858 - 00:15:50.178, Speaker C: So, crypto economic consensus algorithm, I post up a bunch of economic resources in the form of mining and proof of work and I get to make blocks, I get to get some value. And the byproduct of me doing that is to make the network more secure under certain assumptions. So the more crypto economic capital that shows up in mines, the harder it is for anyone actor to get those amount of assets to kind of take over the network.
00:15:50.178 - 00:16:03.822, Speaker C: So this is mining crypto economic consensus protocol. I have capital, I buy mining power, and I run computationally hard puzzles in proof of stake. It's a very similar idea.
00:16:03.822 - 00:16:23.910, Speaker C: I like to call it proof of dedication of scarce resource to the protocol. It turns out proving that you're burning a bunch of energy on mining is an easy thing to demonstrate to the protocol through this kind of ace symmetric of hashing. But there's another thing that's easy to prove that you've dedicated to the protocol and that's the in protocol asset itself, ether in our case.
00:16:23.910 - 00:16:50.000, Speaker C: So instead of taking my capital and converting it into mining hardware, I take this capital that I want to participate in this game with, I convert it into ether, the base asset of the platform. And instead of having to burn energy constantly to prove that I've dedicated it to the protocol, I can just lock it. So I can have an in protocol mechanism to lock it, to deposit it into the consensus mechanism to pretty much elect, hey, I'm playing this game now.
00:16:50.000 - 00:17:11.554, Speaker C: I can now either maybe get some reward if I do well in the game, or maybe I can lose some money if I don't do well in the game. And so now instead of burning energy and kind of proving with hashes that I'm dedicated to the protocol, the protocol itself can say, okay, you're in the game. So every once in a while the protocol will randomly say, hey, you, you validator, you can produce a block.
00:17:11.554 - 00:17:24.678, Speaker C: And so instead of this kind of random extra protocol process, we have this pseudorandom in protocol process and similarly I get to make blocks. And the reason that I want to do that is I get some reward. I get to order transactions and get transaction fees.
00:17:24.678 - 00:17:48.446, Speaker C: Additionally, in proof of stake, there are other kind of responsibilities given to you in the ethereum proof of stake protocol. So not only do I make blocks every once in a while, which are made every 12 seconds, very frequently I'm called upon to just say, hey, what's the state of the world? What do you see? As the head of the blockchain, we call this an attestation. Attestations kind of get to give continually crypto economic weight to blocks.
00:17:48.446 - 00:18:07.046, Speaker C: I think every like six minutes, each validator is called upon once to make one of these attestations to kind of give weight to the chain. The summation of these attestations also allow us to finalize so crypto economically finalize portions of the chain. And you don't really get this in proof of work.
00:18:07.046 - 00:18:43.694, Speaker C: In proof of work, you kind of get this like the deeper the block is, the more likely it's not to be reorged because it's very costly to do so. Whereas in proof of stake we get these points at which we can make stronger claims that are unless one third of all of the capital is willing to be burned, all the capital that is staking at the time, then there won't be a conflicting chain with this chain. So similarly, we get to make kind of these arguments about certain size attackers and the ability for them to kind of rewrite history or double spend, but in slightly different ways, but with similar goals.
00:18:43.694 - 00:18:50.638, Speaker C: So we want to make blocks and we want to make claims about whether those blocks are true and canonical in both, I think.
00:18:50.664 - 00:19:09.466, Speaker B: Yeah, one way I've also thought about it much simpler is you're kind of figuring out what's a way to decide who the next block producer is without like a central party. Right. And you need a way to decide between various entities who gets that right.
00:19:09.466 - 00:19:29.714, Speaker B: And proof of work, you get this as a proxy of your hash rate, right, where you can just increase the difficulty based on the hash rate. And then you kind of get this fair lottery across all the participants. And then proof of stake, because we can shuffle the validators in the protocol means we can then just ask them for kind of a fixed ticket size and shuffle that.
00:19:29.714 - 00:19:56.746, Speaker B: And it's like a lottery as well. I think that for me, again, if your listeners are not like protocol researchers, is kind of the thing that helped get it where, yeah, you can will allow a centralized third party shuffle the block producer set and get kind of a fair weighting based on your stake or your hash rate. And then obviously you want to have a high amount of resources on both sides to make sure that it's secure against attackers coming in, going to the.
00:19:56.768 - 00:20:08.270, Speaker C: Economic resource allows us to make it less gameable. Right? Like the right to make a block is really valuable. Ideally I can game that lottery and be able to make blocks disproportionate to other people.
00:20:08.270 - 00:20:23.678, Speaker C: But the way we bound that to a scarce resource in proof of work and in proof of stake makes it so that it is not gameable. I mean, it's only to the extent that you can apply more resources and so it is kind of bound in reality.
00:20:23.854 - 00:20:46.374, Speaker A: Yeah, I think in particular I like this framing of blockchain needs a next leader who gets to propose the next block. How do we elect them? Both systems prefer to run a lottery instead of just saying the wealthiest party gets to attend it or whatever. And proof of work, the lotteries run via hashing and that provides sort of the civil resistance and proof of stake.
00:20:46.374 - 00:20:58.622, Speaker A: Sort of. You could also say, oh, everybody who participates in this lottery gets to this randomly drawn via some on chain algorithm. But this is just another way of saying like proof of x.
00:20:58.622 - 00:21:17.026, Speaker A: Right? Because people will do whatever it takes to provide more tickets. And so you want to create a way to prove costliness and to create civil resistance that doesn't result in infinite negative externalities and is as fair as possible and equitable as possible and access to the blockchain's native asset.
00:21:17.058 - 00:21:23.490, Speaker C: I guess, yeah, you could do proof of car, but that's really hard to demonstrate to the protocol.
00:21:23.570 - 00:21:24.374, Speaker A: That's right.
00:21:24.492 - 00:21:38.474, Speaker C: Or proof of personhood might actually make a reasonable consensus protocol, but that's not, like, an easy problem to solve. You can't necessarily prove cryptographically that you are a unique person and not gaming it as being another unique person.
00:21:38.592 - 00:21:54.578, Speaker A: Yeah, I think it's a good exercise, I think to understand to new people coming into the space. I always recommend try to reverse engineer what these different parts of a blockchain are actually supposed to do and then just try to think of a better way. Try to replace them.
00:21:54.578 - 00:22:22.082, Speaker A: Replace proof of work with proof of personhood or with proof of car or whatever right. Or proof of bank account. And then tell me does this achieve the goals better or does it have any drawbacks? And then I think it's very easy to see why a lot of blockchains have converged on either proof of work or proof of stake and why other proof of x algorithms are usually like they have very large drawbacks and are not really on par with those two.
00:22:22.082 - 00:22:28.906, Speaker A: Tim, why do we like the proof of stake? What do you think is the main reasons why we want to switch in the first place?
00:22:29.088 - 00:22:39.246, Speaker B: I think this has changed over time. Right now. If you take like a non blockchain native view, people talk about the environmental impact, and I think that's obviously a big one.
00:22:39.246 - 00:22:50.478, Speaker B: But it's kind of like something that we see now based on the scale of things that I don't think was kind of a huge motivation, like almost eight years ago when these things were much smaller.
00:22:50.574 - 00:23:01.638, Speaker C: I think some very early blog posts mention it as a motivating factor. It's been known that there's this thing that scales with the value of the platform. That's not very nice.
00:23:01.724 - 00:23:04.774, Speaker B: Right? Yeah. So that's definitely a big part of it. Right.
00:23:04.774 - 00:23:33.774, Speaker B: And again, it's like people try to quantify this, but I almost feel like it's more of a qualitative difference. You go from literally raising the amount of computers you need to secure the network as forever to not requiring any kind of excess computing resource beyond just running servers, which any single website or web application does. So that is clearly a massive difference given the scale of mining today.
00:23:33.774 - 00:23:58.566, Speaker B: I think the other thing it gives us is resilience against large attacks in an Iterated way, which you don't really get in proof of work. So in proof of work, if somebody picks up 51% of the mining power and starts attacking the chain, you have two options. One is like, you create more mining hardware to then get the good guys 51% again.
00:23:58.566 - 00:24:20.734, Speaker B: But then that's very kind of weak if someone already has half the hash rate. So the other option you have is you can change the hashing algorithm because both Ethereum and Bitcoin have kind of hashing algorithms which are optimized for certain types of computers. So if you change this hashing algorithm to something else, you can make all the current kind of computers useless and then move to another type of computer.
00:24:20.734 - 00:24:43.334, Speaker B: The challenge with that is that at Bitcoin and Ethereum scale, there's like a limited number of computing devices that already exist in the world at the required scale to provide the security that these systems have. So Bitcoin is a bit easier to think about because it's mostly ASIC mining now. But imagine someone gets 51% of ASIC miners, and Clearity is going to maintain that.
00:24:43.334 - 00:25:03.818, Speaker B: Then you can either keep the chain operating under this weird sensor condition, or you can choose to change the mining algorithm. So if you're Bitcoin, the only other type of mining algorithm you could consider is something where there's general purpose kind of computers available to mine this. So this would probably be like a GPU intensive mining algorithm.
00:25:03.818 - 00:25:11.678, Speaker B: So you do this. You have this very controversial hard fork in Bitcoin. You've moved from Asics to GPU mining in that process.
00:25:11.678 - 00:25:21.630, Speaker B: By the way, you also burn all of the honest miners because you can't the good guys. Yeah, you can't discriminate between them at the protocol. So now you've moved to GPU miners.
00:25:21.630 - 00:25:50.678, Speaker B: But what if your attacker had taught a step ahead of you, and they also have 51% of mineable GPUs under their control? There's just literally no other kind of computing device you can move to. And you're kind of stuck in that world where the attacker has maintained control over proof of work. Ethereum would basically be the same thing but reverse because we start with a GPU friendly algorithm and we probably even less credibly could quickly move to an ASIC friendly algorithm because asics needs to be produced.
00:25:50.678 - 00:26:19.218, Speaker B: But there's maybe some ways where we can roughly kind of damage most eGPUs and there are ethereum asics and kind of nudge it towards them, but it wouldn't be as clean. But then again, attacker thinks two steps ahead, they already have half the ethereum asics and there's nothing we can do there. So this is really bad because there's a pretty kind of short steps that some actor can take to make the consensus algorithm just not stable under proof of stake.
00:26:19.218 - 00:27:04.786, Speaker B: Like Danny mentioned earlier, once you get a third of the stake kind of committed to attacking the chain, they might be able to do things like revert previously finalized blocks or propose an alternate history for those finalized blocks. And that's again, also very bad for the stability of the chain. But the thing that we have is those validator IDs are basically encoded in the protocol, right? So we can uniquely identify kind of the colluding validators directly within the protocol and we can choose to apply as a socially coordinated hard fork, some penalties to those validator IDs, right? And we could under most kind of benign of cases, you could just forcefully exit them from the validator set.
00:27:04.786 - 00:27:17.826, Speaker B: And this is maybe a world where imagine you have strong evidence that those validators are just offline. Maybe they're just like not attacking the chain. So you could choose to just have a hard fork where we say, well, these validators are just forcefully withdrawn.
00:27:17.826 - 00:27:38.958, Speaker B: And then you could go all the way to the more extreme case where if this was an attack on the protocol, you could remove those validators from the validator set and basically kind of delete their ether. And again, this would be like something we have to socially coordinate. So it would be probably very contentious, but not more contentious than moving bitcoin from ASIC mining to GPU mining, right? It's on that level.
00:27:39.124 - 00:28:13.286, Speaker C: There are versions of attacks where you don't even have to socially coordinate, right? There's an in protocol mechanism called slashing where there are cryptographically provable nefarious things where I essentially contradict myself and I can lose my capital by contradicting myself. In the extreme where an attacker is not only strong enough to create contradictory histories, but is strong enough to censor, then you might need to do kind of these social coordination. But in either of the events, I would argue that proof of stake has better recovery modes in these extreme scenarios.
00:28:13.286 - 00:28:30.526, Speaker C: These are not scenarios that you want to be in in proof of stake or proof of work. These are disaster scenarios. But there ends up being a very concrete cost and ability for the network, which is the network community applications to kind of pick things up.
00:28:30.526 - 00:28:37.350, Speaker C: And move forward. Whereas in proof of work it's not so clear that there are good recovery mechanisms in the event of disaster.
00:28:37.450 - 00:29:04.454, Speaker B: Right? And I think that the mitigation on proof of stake is also stronger. Because if you did end up in this absolute worst case scenario, where there's some harm being done to the protocol, which you can't identify in protocol, and you need to socially coordinate to resolve, then say you go all out, you literally remove that attacker's funds. They would have to then buy up a third of kind of the staking supply again or acquire.
00:29:04.454 - 00:29:23.834, Speaker B: Right. I think there is also a case where the attacker can just steal the supply or whatnot but they would need to then do that twice and put all those new validators through the staking queue, have them activate on the network, have them become validators and then kind of do their hostile takeover again. But then at that point we can just repeat the same exercise.
00:29:23.834 - 00:29:32.478, Speaker B: We can socially coordinate another hard, fork those validators again and we can do this over and over and over. So there's no kind of end to the iteration.
00:29:32.574 - 00:29:34.370, Speaker C: Everybody loves burned ETH.
00:29:35.270 - 00:30:01.450, Speaker B: Right? But just the fact that we can do that over and over I think is like a much stronger deterrent to an attacker because also you would think those hard forks get socially easier to coordinate. Like the first one will be extremely hard and whatnot. But then if the same attacker does the same thing three months later it's like we already have the playbook to run through this and yeah, there is no limit to how frequent you can do that versus in proof of work after like two iterations of this game.
00:30:01.450 - 00:30:07.678, Speaker B: If you're a large market cap asset you're basically over. So I think that is a very nice property.
00:30:07.764 - 00:30:08.158, Speaker A: Yeah.
00:30:08.244 - 00:30:20.286, Speaker C: And again you don't want to be in the scenario where you're doing this. You don't want to be in the scenario where you have to socially have consensus to burn a bunch of capital because there's an attack. So it's best as a discouragement.
00:30:20.286 - 00:30:41.434, Speaker C: It might have to happen at some point in the future of this consensus mechanism but it's not something to be done lightly and it's not something to be done in light circumstances because you also don't want to kind of degrade the mechanism. You don't want the pitchforks to be able to come out too easily and just be burning capital willy nilly because that's the degradation I think in the model.
00:30:41.552 - 00:31:14.318, Speaker A: Okay, so TiDR basically in proof of stake, if fraud happens and the attacker isn't bigger than x percent then the protocol can automatically cryptographically prove that Byzantine behavior happened and slash them which means to confiscate their funds and kick them from a validator set above x percent. The same entity can also censor the fraud proof if you will and then you need to socially coordinate. But both are possible because validators can be individually identified and targeted in protocol.
00:31:14.318 - 00:31:23.398, Speaker A: Unlike in proof of work where you have this amorphous mass of hash power that's sitting off chain you can either target all of them or no one.
00:31:23.564 - 00:31:51.038, Speaker B: And then the last thing I'll add is proof of stake is also cheaper than proof of work from a protocol issuance perspective. So in proof of work you need to basically issue enough coins to pay for people's high fixed costs of buying these computers and expanding this energy. Whereas proof of stake we basically provide kind of a reward on people's capital and we don't need to kind of have that reward be high enough to cover their fixed cost.
00:31:51.038 - 00:32:09.570, Speaker B: And on ethereum you see this where the issuance of the proof of stake chain is about one 10th of the issuance of the proof of work chain, right? So it's like not only is it stronger from a security perspective but then it's also cheaper from a protocol expenditure perspective.
00:32:09.650 - 00:32:28.090, Speaker A: I think you can argue it is cheaper because it is more secure but it cannot be more secure and cheaper like those things being sort of orthogonal to each other, right? Like proof of work has no fixed costs. Miners only spend as much money as they can earn and they have no real fixed costs either, right?
00:32:28.240 - 00:32:51.250, Speaker C: Miners have a couple of things that they're paying for, right? They're paying to burn energy, right? So they need to get some margin on top of paying for that energy consumption rate. So it might be 5% or 10% is their expected kind of equilibrium margin, whereas the protocol has to pay for that energy to be burned and then some margin on top of it, whereas in proof of stake, the asset doesn't have an ongoing cost to be sitting there.
00:32:51.320 - 00:33:21.306, Speaker A: Well, I don't think that's really true. I think in both cases you start from the incentive that the protocol creates and then in both cases you have miners or stakers chasing that incentive and spending as much money as possible. So I've long said that people sort of think that stakers have no costs, no ongoing costs but if you think all the way through the end this couldn't be true because if it was actually free then stakers would just continually acquire more capital and stake more and more until some equilibrium point is reached.
00:33:21.306 - 00:33:45.246, Speaker A: And I think we see that today with lido where leveraged staking is extremely popular and stakers start to have spent all of their income on borrowing more money. So they have these costs of capital, basically these financial costs to acquire more capital until the point where it is not actually cheaper to borrow and stake more money than the income that they already make from staking.
00:33:45.358 - 00:34:09.706, Speaker C: And I agree with that but I would argue that assuming that there's x percent x ether issued then y capital is going to show up to get that ether. Whereas in proof of work y capital less some margin, some fixed cost is going to show up. So the amount of mining hardware is going to be for that same amount reduced because there's also expenditures in the energy.
00:34:09.888 - 00:34:18.282, Speaker A: Well, I think in either case the amount of hardware that is produced and purchased is in response to how big the incentive is in the protocol.
00:34:18.426 - 00:34:49.702, Speaker B: Yeah, and I think this is maybe also an ethereum specific thing. I think I see your point has to but in ethereum's world it's almost like we have clearly this demand like the incentives are so high that people are kind of expending all this energy to come in mine. And because of this move to proof of stake it's like we have this one shot phase transition where we can go from suppliers who have a high cost basis to suppliers who don't.
00:34:49.702 - 00:35:05.578, Speaker B: And because the incentives are still like ETH denominated we can drastically reduce kind of the total amount paid by the protocol. But I'm not sure this would hold in a world where you're starting from proof of stake from scratch versus starting from proof of work from scratch.
00:35:05.674 - 00:35:26.366, Speaker A: I think I may be seeing where actually you're going with this which is in proof of work it's not actually possible to mine profitably unless you're really competitive. But in proof of stake if you own ether then you can mine even if you're like half as if you're way less profitable than the best staker.
00:35:26.398 - 00:35:54.314, Speaker B: Yeah, or maybe another way to put this is like say I don't know what mining margins are but say they're 10%, right? It's like the proof of work incentives on ethereum today are able to pay for that 90% fixed cost plus 10% reward and that's like whatever the markets agreed is the right amount. And obviously if some miners come in and they have lower fixed cost then they'll outcompete the others. But in practice we're pretty mature on ethereum because we have GPUs and ASIC markets so say we've settled in this equilibrium.
00:35:54.314 - 00:36:25.526, Speaker B: Well, if we move to proof of stake it's like we no longer have to pay that 90% of fixed hardware, right? We can just as the protocol kind of remove that portion of issuance which we basically have and then because kind of the network value is already kind of established and the incentives are already kind of worth something which is different than a much more nascent proof of work or proof of stake chain. I think we get to do this. Protocol level cost reductions in terms of security spend.
00:36:25.526 - 00:36:35.980, Speaker B: But I think I agree with you that it's not clear to me that would be true if you started a new network on proof of work versus proof of stake that you would get this cost saving.
00:36:36.670 - 00:37:01.282, Speaker A: Well, the way I always visualize it for myself is that issuance can be reduced because proof of stake has these better security properties where you can actually identify and slash individual attackers. And so we can set the threshold for when they get slashed much lower than we could in proof of work because we can do it without disrupting the protocol and hence the incentive for misbehavior is lower.
00:37:01.336 - 00:37:01.698, Speaker C: Right.
00:37:01.784 - 00:37:16.866, Speaker A: Let's actually get out of this rabbit hole. I want to hear maybe from you Danny, or also from you, Tim. How will the merge actually happen as an event? Like how do we go from the last block on proof of work to the first block on proof of stake?
00:37:16.978 - 00:37:42.766, Speaker C: I like to think about the existing proof of work network as two things, kind of the valuable bits that users care about and interact with. We might call it the execution layer or the application layer. This is the state, this is the payload of transactions, the state route, the various things that go into a block and then we have this kind of outer shell of that execution layer and today it is proof of work.
00:37:42.766 - 00:37:57.970, Speaker C: It's kind of this proof of work seal. It's this crypto economic kind of like home and carriage for the valuable bits of ethereum in parallel to that proof of work chain. Today we have a proof of state consensus mechanism called the beacon chain operating in parallel.
00:37:57.970 - 00:38:25.706, Speaker C: And what is it? It is a consensus mechanism. What is a consensus mechanism good for? For coming to consensus on stuff. So from a high level the merge is at point A, the valuable bits of ethereum, the execution layer being inside of that proof of work carriage and at point B it in a kind of a continuous way moving into the beacon chain for its kind of new outer shell of proof of stake.
00:38:25.706 - 00:38:48.050, Speaker C: And the nice thing here is that that execution layer kind of self references and backlinks to itself as it moves through proof of work and then moves into proof of stake and still continues and has this backlink. So it has kind of like a continuous nature into the proof of stake. Another nice thing there is that kind of the application layer is undisrupted so it becomes very transparent to users.
00:38:48.050 - 00:39:21.434, Speaker C: So what is actually happening there is that the proof of Stake Validators are watching the proof of work chain and waiting for some terminal condition to agree upon and begin in the next beacon block to kind of reference whatever the terminal proof of work block was as kind of the parent of its execution layer. And then begin to put the valuable bits of ethereum to begin to pack transactions and reference the state route and other things like that. In our case, that terminal condition is a total difficulty.
00:39:21.434 - 00:39:46.806, Speaker C: So we call it the TTD, the terminal total difficulty. And that's essentially the validator is looking for some end crypto economic weight on proof of work to kind of do the transition and take over the fork choice and take over the valuable bits of ethereum. And anyone watching for this transition is watching for hitting TTV and watching for a proof of stake chain taking over from that point.
00:39:46.806 - 00:40:09.034, Speaker C: And the thing is we want the validators to kind of strip the power from the miners as soon as possible. And so any proof of stake chain that references a valid terminal total difficulty and is valid in its construction has a higher weight. So I'm going to follow any such chain higher than any proof of work chain.
00:40:09.034 - 00:40:22.862, Speaker C: And so that's pretty much what happens. I think one interesting thing to note here is that this happens singularly. It happens on one proof of work chain, not on all proof of work branches simultaneously.
00:40:22.862 - 00:40:40.870, Speaker C: Like most upgrades for ethereum and other chains happen at a block height. So adding an opcode happens at block in but it happens at block in simultaneously on all potential branches, on all different forks. Which is nice.
00:40:40.870 - 00:40:47.318, Speaker C: It's cool. On the canonical chain I get this opcode. If there happens to be a competing branch, I also get the opcode.
00:40:47.318 - 00:41:22.134, Speaker C: And so we just kind of have this signal where when you hit block in, you get the opcode on all possible chains, all possible futures, whereas because we want the validators to take over, we want them to take over just one thing. We want them, whatever they say to be canonical. So we want them to take over the canonical thing and not to take over many simultaneous branches or to accidentally take over a low weight branch because it's pretty easy to kind of construct a false but not very cryptoeconomically heavy chain in proof of work.
00:41:22.134 - 00:41:38.566, Speaker C: So I could get to block in maybe faster than the canonical chain is going to get to block in. And I don't want that to trigger my validators to take over because I want the validators to take over only once and I want them to take over what is canonical or at least really close to canonical because it has this heavy weight.
00:41:38.758 - 00:41:55.170, Speaker A: I see. So you were thinking about different activation mechanisms and something like block height is too easily manipulated because it doesn't say anything about how valuable the actual fork was to how costly it was to produce, right?
00:41:55.240 - 00:42:06.270, Speaker C: Yeah. And in a normal fork, that's fine. Somebody makes some really unvaluable chain that gets to block in before the canonical chain and it gets the new opcode but nobody really cares.
00:42:06.270 - 00:42:12.450, Speaker C: Right. So we want to make sure we're doing something on kind of the heavy, valuable and secure canonical chain.
00:42:12.530 - 00:42:24.634, Speaker A: Is there any sort of code that's already for switching that's already deployed in the clients that miners run today or is there no need for anything to happen on the minor side? Ideally not only on the vendor side? Yeah.
00:42:24.672 - 00:42:50.882, Speaker B: So there is a world where this is kind of based on the difficulty bomb. Assuming we don't have to touch the difficulty bomb, we can not have miners upgrade their existing code. And what happens from everyone else running a node is that all of the Validators or also all of the non mining or non staking node today's they add kind of the missing part, right? So today if you're a miner, you just run, say geth or besu or something.
00:42:50.882 - 00:43:14.538, Speaker B: And then at the merge, if you're running one of those kind of execution layer nodes, you also need to run a consensus layer node like Prism or Lighthouse in parallel to it. And so that means your post merge Ethereum node is really the combination of these two pieces of software. And because everyone else say except miners kind of adds this consensus layer node on top, they all agree on what the TTD is.
00:43:14.538 - 00:43:41.714, Speaker B: And then at that point they kind of choose to listen to each other to get information about the head of the chain and just not listen to miners anymore. The one thing that's also kind of neat here is using this TTD is also robust to short attacks from miners. So because we are using the heaviest chain, any competing block which exceeds the terminal total difficulty but whose parents does not.
00:43:41.714 - 00:44:04.758, Speaker B: So the first block past that is like a Valid last proof of work chain. And then the Validator who's set to produce the next block can just choose from any of those and kind of go on and build on that block. So it's kind of neat where miners can it's possible both through malice but also through regular operations, that we do get two competing proof of work blocks.
00:44:04.758 - 00:44:24.970, Speaker B: Like we get uncles on Ethereum, right? But because basically we have a clear boundary condition for that, then the Validator is free to choose from that small set of block which would each have kind of the same, not necessarily even the same height, but they would each be a block which exactly exceeds or directly exceeds TTD.
00:44:25.050 - 00:44:29.330, Speaker A: What if I run a non mining, non validating node? What do I need to do then?
00:44:29.480 - 00:44:39.470, Speaker B: Non mining, non validating node. So it depends on which layer. So if you're running a non mining, non validating node on the beacon chain today, you need to add basically an execution layer node.
00:44:39.470 - 00:44:59.622, Speaker B: And the reason for that is, imagine you're just running a node on the beacon chain today. You can verify everything, but after the merge you're going to have these blocks which contain transactions as well. And you need to have an execution layer node to send the transactions, to run them through the EVM and ensure that they're executed as per the protocol rules, right? And vice versa.
00:44:59.622 - 00:45:14.778, Speaker B: So if you're running your node on the execution layer after the merge, you don't know what the head of the chain is, right? So you need a consensus error node to tell you this is actually the latest valid block and then you can verify its transactions.
00:45:14.874 - 00:45:24.578, Speaker A: So this actually speaks to like an unbundling, right? Of Ethereum where today sort of the consensus and execution client is one and the same and it will be different.
00:45:24.664 - 00:45:37.346, Speaker C: After the merge, right? Yeah. You can think of what is Geth. Geth is like this kind of thin, relatively simple consensus algorithm called proof of work and then primarily dealing and handling with the execution layer.
00:45:37.346 - 00:45:53.222, Speaker C: So dealing with state and optimization and transaction. And if you looked at their code diff over the past few years, proof of work probably literally has been untouched except for people trying to introduce progpal. And what has been highly optimized and refined is kind of the handling execution layer.
00:45:53.222 - 00:46:14.190, Speaker C: And then what are these beacon node clients? What are the people that have been working on the beacon chain for many, many years now? This is a highly optimized and sophisticated proof of stake algorithm. So a client becomes the kind of the summation of the two. So we remove proof of work from Geth and others and kind of allow that execution layer to be driven by this other piece of software.
00:46:14.190 - 00:46:35.610, Speaker C: And it is an unbundling and it wasn't necessarily intentional at the start. It was unclear when the beacon chain was beginning to be constructed, exactly what Ethereum was going to look like and how these things were kind of going to come together. I would call it a happy accident because these are open source pieces of software because of the way kind of people self organized to build this stuff.
00:46:35.610 - 00:46:53.834, Speaker C: It's allowed for more specialization and it's allowed for scaling out the teams and individuals involved in this system. So you have people that are really good at proof of stake and then you have other teams that are really good at the EVM. In doing that, we actually have many more people at the table.
00:46:53.834 - 00:47:01.874, Speaker C: I think we allowed ourselves to build more sophisticated software and to kind of isolate changes and think about disparate parts of the system.
00:47:01.992 - 00:47:08.530, Speaker A: Because someone who is really good at building an execution client, they don't no longer have to be really good at building a consensus client.
00:47:08.610 - 00:47:17.910, Speaker C: Yeah, or they don't have to scale out their team to be able to build out the expertise, which is really nice in terms of components here.
00:47:17.980 - 00:47:30.186, Speaker A: So all of these new eve two clients that we are talking about, are all of them execution plus consensus or are they like just execution or just consensus or how does it work?
00:47:30.288 - 00:47:35.662, Speaker B: So I think when you mean the E two clients like basically what's used on the beacon chain today is that yes.
00:47:35.716 - 00:47:38.222, Speaker A: So those are these are all consensus, right?
00:47:38.276 - 00:47:38.542, Speaker C: Yes.
00:47:38.596 - 00:47:45.626, Speaker B: So, like, basically Prism lighthouse. Nimbus lodestar teku. They're consensus Layer clients at the Merge.
00:47:45.626 - 00:47:56.686, Speaker B: It's kind of what they stay you need to choose one of those five. And then on the execution layer we have like Geth, Beisu, Nethermine and Aragon and these are all execution layer clients. And same thing at the merge.
00:47:56.686 - 00:48:19.082, Speaker B: They remain kind of the same and so you kind of need to pick and choose one on each side and we can get into this after. But this is what makes testing the merge kind of this really big endeavor. It's like we need to make sure that these 20 pairwise combinations all work across all edge cases and that's basically what we're doing right now.
00:48:19.136 - 00:48:28.910, Speaker C: And you need to define a simple but robust enough interface between the two such that they can kind of communicate and handle Idiosyncrasies and things like that.
00:48:28.980 - 00:48:32.206, Speaker A: Why do we want oh, sorry, I.
00:48:32.228 - 00:48:54.738, Speaker C: Was going to say even consensus, the team consensus sys, they have Besu, which is an execution layer client and they have Teku, which is a consensus layer client. But even them, they have it decoupled and they have it actually as two different teams. We might see some people that experiment with more tight coupling but at the merge there's no team that is tightly coupled.
00:48:54.914 - 00:49:03.818, Speaker A: What is the reason that we want or that we encourage so many clients? I understand that this is like outside of ethereum. This is fairly controversial, right?
00:49:03.904 - 00:49:05.930, Speaker C: It's to make a headache for ourselves.
00:49:08.830 - 00:49:55.958, Speaker B: My thoughts have changed about this recently. But the real reason and the most important one I think is because you want the protocol specification to be sound and you don't want a bug in one of the implementations to alter your protocol, right? And for example, imagine a world where say we just have Geth and Prism like the two most dominant clients and either of one of them has a bug which prints 1000 ether or even like one ether, right, or removes one ether, something like that. Then we need to go through this entire process of what do you do and you need to wait, what cars the protocol more? Is it like allowing this extra ether or not? And whatever the decision is there the outcome kind of affects the credible neutrality of the chain.
00:49:55.958 - 00:50:08.734, Speaker B: So it's just like a really kind of terrible place to be in. And if you have only one client, that's basically your only other option. When you have multiple clients you can basically say these are the protocol rules and everybody should follow them.
00:50:08.734 - 00:50:27.138, Speaker B: And then if they don't, you can basically kind of fault them in protocol. And for example, something like this where say Get printed one east that was not part of the protocol rules. You would have other clients like Nethermine, Aragon and Bezu, they would assuming they don't have the same bug but the probability of that is pretty low.
00:50:27.138 - 00:50:35.622, Speaker B: They would then catch that. They would say hey, this is an invalid block, we don't accept this. And like in previous lake they would be able to slash the validator who hit that.
00:50:35.676 - 00:50:56.174, Speaker A: So what if I on question on that? Let's say I run ASU which is consensus execution client, right? And they have a bug that prints one ether to many or whatever and I run that client. So I understand what happens from everyone else's perspective, who doesn't run that client? What happens from my perspective? Do I see the right view of the state?
00:50:56.292 - 00:51:15.154, Speaker C: Right? So you could be a user or you could be a validator, right? I'm a user since a participant or just a user. So a user, you're going to look like you're kind of off in your own little world in some sort of like cryptoeconomically low weight minority chain, assuming that it was a relatively low weight client and distribution to network. So say it was like 15%.
00:51:15.154 - 00:51:39.638, Speaker C: So you would look and feel online but you probably in that event you wouldn't finalize. So maybe you wouldn't be making economic decisions and things would look a little bit weird and you'd probably maybe your client gives you some warnings, maybe you see a lot of missed block proposals and you go and figure out what's going on. You probably run a patch for your software, if not, maybe run it, pick a different piece of software to run that is able to follow the rules.
00:51:39.638 - 00:51:54.482, Speaker C: So you can still see a live chain but ideally if one client branches off, that client in and of itself would not be able to finalize. So I wouldn't necessarily make economic decisions based off of that.
00:51:54.616 - 00:52:00.450, Speaker A: Yeah, I mean from a user, wouldn't it be better if the change has stopped?
00:52:01.270 - 00:52:03.454, Speaker B: But the thing is you can't how do you halt?
00:52:03.502 - 00:52:09.630, Speaker A: Yeah, exactly. So I'm saying isn't this more dangerous?
00:52:09.710 - 00:52:12.774, Speaker C: So you do halt with respect to finality depending on the weights here.
00:52:12.812 - 00:52:26.074, Speaker A: Yeah, which is nice, true. But from the user perspective, I don't necessarily notice that I have forked offer that feels more dangerous than me staying in consensus with everyone else on a chain that maybe has like a slight problem.
00:52:26.272 - 00:52:52.638, Speaker B: I think even if you're right, like if it's networks, from the perspective of that user, the perspective we think is not just like of an individual user, it's like of the entire protocol. So the sum of the users. So it's like sure, if you're the person running baseu and you left your validator, you're about to go for a three month no WiFi trip and besu hits a bug the day after, it is kind of bad for you and that's unfortunate.
00:52:52.638 - 00:53:25.610, Speaker B: But from the Ethereum protocols perspective, it also means the chain did not halt and kind of kept chugging along and all the other users, realistically clients are able to reach the vast majority of end users with upgrades and emergency releases. I mean, every time we have hard forks on Ethereum, we literally have to reach to every user and get them to upgrade their nodes. And we do especially, even more so like consensus participants, like miners and validators, they literally have funds at stake.
00:53:25.610 - 00:53:36.326, Speaker B: And so typically if there is an emergency release for a client, the vast majority of people will adopt it. But of course there are cases where that might be unfortunate for users.
00:53:36.378 - 00:54:00.982, Speaker C: Another important thing here is kind of the social layer. Why can't there be many clients who's saying so? Who is deciding that there's only one client? What does that even mean? What is a protocol? A protocol is kind of just an abstract definition of rules. And there being many implementations of that protocol is kind of what we see as very natural and helps, like a more diverse conversation around the protocol helps.
00:54:00.982 - 00:54:26.158, Speaker C: There not being one team enshrined that gets to dictate the protocol rather than if there's only Ethereum core, then whatever is written into Ethereum core is now canon. Whereas if there's more abstract notion of what the protocol is, and there's many implementations of the protocol, you have, one, a diversity of voice in trying to decide what actually goes into this thing. And two, you have a diversity of option in the event that people disagree.
00:54:26.158 - 00:54:52.534, Speaker C: And so there's not only just this kind of resilience, it is it is more of a network resilience tactic rather than kind of an end user, singular user resilience tactic. In any of these bad network events where people disagree, there's going to be a user that maybe is hurt or maybe is at least not live or is kind of confused or things like that. But the network in most of these scenarios, we think is much more resilient.
00:54:52.534 - 00:55:01.498, Speaker C: But it's also kind of in this social layer, this governance layer, this abstraction of what is the protocol, what it can be, who is at the table, all that kind of stuff.
00:55:01.584 - 00:55:01.978, Speaker A: Yeah.
00:55:02.064 - 00:55:40.662, Speaker B: And one thing I'll add there, which is the thing that kind of hit me recently, is I do not think we could have as many smart people working on Ethereum if we just had one client. My experience over the past years, working with many of them is really smart people are very opinionated about things and giving them the protocol as a minimum spec to implement and then having many degrees of freedom with how they can write their own implementation, I think is the only strategy that brings the really smartest people to work on clients. And I was like, when I started working on a client, I assumed that the yellow paper would represent 90% of our code base.
00:55:40.662 - 00:56:04.446, Speaker B: It was like, you look at the yellow paper, maybe you add a few more things, but that's pretty much it. And the absolute opposite, it's like writing EIPS is usually like ten to 20% of the work and 80% of the work is optimizing things, designing sync algorithms, designing database formats, layouts. And none of that is part of the yellow paper or the consensus specs, right? Like, we don't care how you store data.
00:56:04.446 - 00:56:22.386, Speaker B: And I think, yeah, if we had only a single implementation, you would not see any innovations happen there. And even in practice, we've seen a ton of. Things like Aragon spent years kind of refining a whole new database layout and achieved basically an order of magnitude reduction on an archive node.
00:56:22.386 - 00:56:30.346, Speaker B: Yeah. So this would not have happened if they would have had to be, say, part of the guest team and under the same management and working in the.
00:56:30.368 - 00:56:47.658, Speaker C: Same yeah, there's well over, like, 100 people working on this thing. And with a diversity of perspective and design decision, whereas we could easily have more, like 20 people working on this thing and not have a lot of resilience and design and exploration and new things and innovation.
00:56:47.834 - 00:56:58.782, Speaker A: Yeah. I think I'm also slowly coming around to this idea that client diversity has some benefit. I think I disagree with you, Danny, that this is like, quote unquote, like a natural outcome.
00:56:58.782 - 00:57:04.594, Speaker A: I think the natural outcome is one of heavy client centralization. But that doesn't mean that we should endorse it.
00:57:04.632 - 00:57:14.210, Speaker C: Right? Sure, maybe natural is not the right word, but in relation to it is unnatural for there not to be able to be an additional client.
00:57:14.370 - 00:57:24.182, Speaker A: Yeah. We should make it as easy as possible for there to be competition at each layer of the blockchain stack right. Including making a client.
00:57:24.246 - 00:57:24.474, Speaker C: Right.
00:57:24.512 - 00:57:39.054, Speaker A: And I think what we have in Ethereum now with being such a clean separation between consensus and execution goes a super long way towards that I can focus on being a good execution dev, don't have to be good consensus dev and so on.
00:57:39.092 - 00:57:55.806, Speaker B: And I would push that even farther with what we've seen with L two S. Right. And Danny, you hinted about the sharding roadmap way at the beginning of this, but it's worth noting there was like a massive change in what sharding means on Ethereum in the past, in this 2018 to 2022 period.
00:57:55.806 - 00:58:22.426, Speaker B: And the original sharding roadmap for Ethereum was this idea of sharded execution where Ethereum would eventually have like I think first it was 1000 and then it became 64 different basically parallel EVMs running side by side. And this is the way we would provide kind of scalability to users. It's saying, like, we kind of add all these kind of copies of the protocol within itself and execute them all in parallel.
00:58:22.426 - 00:58:35.714, Speaker B: And the stepping stone to get there was saying, well, we're going to have these shards not actually run operations, but they'll just store data because it's easier to kind of come to consensus across huge amounts of data.
00:58:35.832 - 00:58:41.380, Speaker C: It's not only easier, but data availability is a hard problem in and of itself. So solve that first.
00:58:41.830 - 00:58:58.594, Speaker B: Easier is underselling that's right. It's extremely hard to come to consensus on this large amount of data and it was like an order of magnitude or two harder to come to consensus on all these computations. We had this roadmap where it's like, okay, well, first you just shard the data and then you eventually shard the computations.
00:58:58.594 - 00:59:36.910, Speaker B: And then in parallel to that kind of this ecosystem of layer, two solutions started to emerge and the biggest kind of thing there is today you have both ZK roll ups and optimistic roll ups and they will rely basically just on the execution chain that we have today and they already do. So they don't need kind of these multiple parallel EVMs running, they basically become them. And this way we can only ship the data sharding where we don't have to figure out how do we kind of scale sharded computations as a base layer this kind of came from like it's nice to technically simplify the roadmap.
00:59:36.910 - 00:59:53.562, Speaker B: But I also think in practice having different teams with different perspectives work on this problem has led to better outcomes. I don't see a world where, say, it was a top down EF design process. We would not have optimistic roll ups and ZK roll ups live in production today.
00:59:53.562 - 01:00:02.154, Speaker B: We would have probably had to choose one and kind of deal with that. So I think, yeah, letting people kind of build infrastructure is really valuable.
01:00:02.202 - 01:00:33.094, Speaker C: Yeah, it's like enshrining execution and state for these more scalable zones or shards would mean there would be one design period and hopefully we coalesce on a good one. Whereas leaving that as a free degree for other teams to kind of come in allows for better competition of ideas, allows for better exploration and more dynamic nature of this scalable zone over 510 or 100 year time horizon, you could say.
01:00:33.292 - 01:00:39.030, Speaker A: I have put it this way before. Ethereum has deregulated the market for execution.
01:00:39.390 - 01:01:02.446, Speaker C: Yeah, I know a guy that's writing. He's doing an exploration to try to think about the l one as kind of like the federal zone and has some federal rules but then has these essentially subsidiary states that get to make their own decisions and get to have a competition of ideas within kind of this federal zone, which, I don't know, we'll see. Hopefully be an interesting paper, but there's some parallels there.
01:01:02.446 - 01:01:14.146, Speaker C: But yeah, it's deregulated in the US. You have the Fed, but then it's like if you had states could be much more dynamic and interplay and compete for space and compete for ideas and stuff.
01:01:14.248 - 01:01:34.214, Speaker A: Yeah, I think whenever you have this level of competition you just see way better results. And I think it makes total sense. If you find yourself in the position of the quote unquote gardener, like how the EF likes to put itself as well, then the best thing that you can do is nurture this competition and make the barriers to entry.
01:01:34.214 - 01:01:44.414, Speaker A: As low as possible to the market at each layer of the stack, be it the client development or building roll ups on ethereum, building bridges to other ecosystems and so on.
01:01:44.452 - 01:01:45.040, Speaker C: Right.
01:01:45.810 - 01:01:51.930, Speaker A: So how do you prepare or test for such an important upgrade?
01:01:52.090 - 01:02:09.718, Speaker C: A lot of work and I mean, that's the bottleneck here, right? And I've been saying for six months, maybe twelve months security and testing are going to be long tail here because we have to do it right. We have to do it fast, as fast as possible. But even more so, we have to do it right.
01:02:09.718 - 01:02:29.574, Speaker C: Ethereum is a massive ecosystem securing tons of value and different applications and users. So how do we approach it? You start at the bottom and you work your way up. So on both the execution layer and the consensus layer, we have what we call kind of consensus tests built kind of based upon the specification.
01:02:29.574 - 01:02:49.582, Speaker C: On the consensus layer, we actually build them off of the specification because it's executable. And these give kind of the baseline rules. Given certain inputs, do we have the same outputs? Given a pre state and a block, do we agree on the post state? And so we have a diversity of kind of these types of tests that allow for just kind of baseline conformance and agreement.
01:02:49.582 - 01:03:10.562, Speaker C: But there's so much more, right? It's like, imagine like Tim said earlier, it's like if you just had you're able to test the yellow paper essentially with these, you kind of be testing that core logic. But it turns out clients are like 90% other in terms of database and network and all sorts of interactions and edge cases. So then we go into more of a kind of an integration testing.
01:03:10.562 - 01:03:36.546, Speaker C: Beyond that we have some frameworks that we use something called Hive where we can write a little bit more sophisticated tests. Like I have this node, I have that node, this node communicates that, this node communicates that and see if we kind of agree on some of these more complex interactions. So we're able to write structured deterministic tests, but eventually you run out of it's hard to write even more complex scenarios in there.
01:03:36.546 - 01:03:56.514, Speaker C: So then we have testnets where we spin up lots of different nodes under different test environments and see that they agree. And then we actually have this very important thing that we've stumbled upon, not stumbled upon, but really begin to use in earnest in testing. The merge is that we create shadow forks.
01:03:56.514 - 01:04:14.186, Speaker C: So we take a testnet. So maybe Gorely or we take main net itself and we spin up a few dozen nodes, maybe 30 nodes that these nodes are following main net. But they all know about a merge, they all know about a TTD and they're all kind of prepped and ready to go for a merge.
01:04:14.186 - 01:04:33.106, Speaker C: Whereas the main net itself really isn't all those nodes, they're not really paying attention to this. So they shadow fork the environment and create a merge testnet off of mainnet itself and kind of operate in their own little world. So they get a main net size state and they get to pipe all those transactions from mainnet into itself.
01:04:33.106 - 01:05:04.574, Speaker C: So this type of testing has been extremely, extremely valuable to kind of get mainnet complications and diversity of interaction and also kind of mainnet style and size states to shake out edge cases. So it's a matter of unit tests, integration tests, utilizing more interesting integration test frameworks, doing these test nets, doing these shadow fork testnets, and doing Fuzzing. We're using some more exotic tools to kind of test things.
01:05:04.574 - 01:05:21.634, Speaker C: We're using this deterministic hypervisor that fuzzes on network interactions and stuff. It's really like if there is a tool at our disposal and the resources to utilize that tool, we're doing it at this point. And some of these kind of feed back into itself.
01:05:21.634 - 01:05:56.906, Speaker C: So say you find something on a shadow fork, you find a bug, then you start asking yourself, well, is there somewhere earlier in the stack, somewhere in more controlled place in this testing stack, where I can actually highlight, I can induce that bug so that I don't have a regression here and the bug to pop up again? So maybe it finds a consensus bug. So I'm like, okay, I need to write a new consensus test. Or maybe it finds some weird sync edge case and then you're asking yourself, can I use Hive to kind of induce maybe three nodes to get into that edge case? And it's just kind of this big feedback loop and you're trying to highlight all the edge cases and get rid of all the bugs.
01:05:57.018 - 01:06:18.370, Speaker B: Yeah. And maybe this is like the state we're at today, right? Maybe it's worth taking a minute to explain how we got here, because it's literally been a year today that we've started working today like this month that we started working on the merge under its current design. So obviously, like Danny said, people have been working on proof of stake since way before that.
01:06:18.370 - 01:06:34.954, Speaker B: But I think about a year ago is when all the pieces were kind of there where we had the beacon chain. It's been up and running, battle tested. We had the general idea of this design reusing the clients on both sides and kind of keeping that and then transitioning outer proof of work that way.
01:06:34.954 - 01:07:01.362, Speaker B: About a year ago, I think Protolambda was the guy who kicked this off, but there was this Rayonism hackathon where we just got all the client teams together and said, can we hack together like a post merge Ethereum prototype in a month? And we did. And this was basically like a network where you had a proof of stake chain and then this execution layer underneath. And it was like validly submitting block from the proof of stake chain to the EVM, verifying the transactions and then sending them back.
01:07:01.416 - 01:07:12.374, Speaker C: And I don't think it did the transition. It didn't do the merge, which is a hard part in of itself. You have post merge stability and that's kind of what we showed, but also the complication of doing itself.
01:07:12.492 - 01:07:35.290, Speaker B: So it did work, obviously, but there was tons of bugs, and we spent all of last summer fixing those bugs and then last fall. We all got together in Greece for a week with the client teams and we basically spent a week trying to build a testnet which would run through the transition. So saying like we know, well we think this architecture postmerge is sound, let's see if we can run through the transition.
01:07:35.290 - 01:07:53.682, Speaker B: And we got everybody together for a week, locked them in a room and then they did manage to get this testnet done. Obviously again found a ton of edge cases and bugs, spent most of fall fixing those. And right before the holidays we had a specification for the whole thing we were generally confident in.
01:07:53.682 - 01:08:11.306, Speaker B: We thought it would work in the happy cases and kind of generally be sound. So we set up and we released the first public testnet called Kinsugi to the community. So we wanted to get feedback from application developers and just kind of a more broader set of eyes on it.
01:08:11.306 - 01:08:33.114, Speaker B: And we ran that for like a month or two and the specification mostly worked, but we found a couple of edge cases which we then fixed. And so now we released this new testnet called Kiln, I believe this was in March, on which we basically expected the specification to be final except for some minor changes. And it's been so it's been running since March.
01:08:33.114 - 01:08:55.474, Speaker B: And we've had infrastructure providers test on It applications. Obviously, all of the client teams and the EF testing teams have been using it. And then now the next step we've had after that is basically the shadow forks which Danny mentioned, which are taking the existing main net and only a small number of nodes and making sure can those nodes actually run through the transition smoothly in MainNet's conditions.
01:08:55.474 - 01:09:09.338, Speaker B: And we're basically kind of all green there. We find some minor issues but pretty much stable. And then the next step after that would be forking these existing public testnets and then mainnet getting close.
01:09:09.338 - 01:09:24.000, Speaker B: It's been a lot of incremental kind of expansions in complexity, but I think it's helped make sure that every part of the process we have a solid foundation to build from.
01:09:24.530 - 01:09:56.220, Speaker C: And I guess something that I left out is that what I was talking about is testing the protocol and testing the software. But users have to test, right? And so that's where Kiln and that's where these forking of the public testnets is very important. So that end users, those that run Validators, those that run infrastructure, those that run DApps and applications can kind of vet these new setups and see what does need to change, see what doesn't change, make sure their APIs all work, that kind of stuff.
01:09:56.830 - 01:10:08.378, Speaker A: Fascinating. Unfortunately we are out of time for this one. In the next episode we will talk in depth about the two very interesting topics that are left with regards to the merge.
01:10:08.378 - 01:10:24.982, Speaker A: First, the rise of liquid staking, primarily Lido. And then how does the merge affect the mev landscape on ethereum with Mapus, PBS, Flashbots and more. So make sure to also tune back into the second episode and we'll see.
01:10:24.982 - 01:10:25.798, Speaker A: See you in a week.
01:10:25.884 - 01:10:26.820, Speaker C: Yeah, thanks for having us.
