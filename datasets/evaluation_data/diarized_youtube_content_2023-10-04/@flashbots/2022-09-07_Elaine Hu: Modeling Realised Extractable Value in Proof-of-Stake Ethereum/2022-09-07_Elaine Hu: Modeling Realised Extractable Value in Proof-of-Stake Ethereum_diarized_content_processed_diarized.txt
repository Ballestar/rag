00:00:00.410 - 00:00:30.514, Speaker A: Good afternoon, everyone. Has anyone heard of genie index? Not so many. Not as many as PBS. Right. So this is a way of measuring wealth inequality. It's often used by the economists to compare the wealth inequalities between different countries. So I've collected some data here, but before that, I want to show you that a genie equals one means perfect inequality.
00:00:30.514 - 00:01:33.266, Speaker A: A genie equals zero means perfect equality. So looking at some of the data collected from the World Bank, this is all the countries in the world with their genie index. So I plot them on this chart. You can see they range from 23% to 63%. So the question is now, if we treat the mev from each block as an individual's income, where do you think that will lie? Is it going to be on the high end with South Africa on the low end, or somewhere in between the US. And the UK? Any guess? Okay, so it turns out that it's 77%, so it's even higher than the most unequal country. So now if we aggregate the mev to a minor address, so some are all the mevs belong to the same minus address, and recalculate this genie index, where do you think that will be? Higher.
00:01:33.266 - 00:02:11.920, Speaker A: Lower. Yeah. So it turns out it's 93%. So it's almost close to a perfect inequality. So we know that the mev world is very unequal, but that's not what I'm going to talk about today. I'm not trying to solve the problem of inequality either in the real world or in the mev world, but I think the problem of inequality in the mev world will be easier to solve because I've heard many people talking about ideas of how redistributing back the mev to the user could solve this problem. But that's not what I'm going to talk about today.
00:02:11.920 - 00:03:10.074, Speaker A: What I'm going to talk about is how to estimate a reasonable average mev. So why the quotation mark around average? Well, because surprisingly, if you take the average mev, it doesn't actually give you the average an ordinary or normal miner is expecting to get, because there are so many outliers, there are so many extremely large values that will skew your average result. So there's a story about the statistician who has his head in the oven and his feet in the freezer. On average, he feels fine. So as you see, there's a problem with averages, and we can't really just take the simple average here. So here is a chart showing the total validator return, given different numbers of validators on the network. And we're using two methods.
00:03:10.074 - 00:03:43.978, Speaker A: One is the mean, the other one is the median. So as you can see at the current number of validators we are seeing using the mean, we can reach 9.7%, but using the medium only 6.2%. So this is already a significant difference of 3.5%. So last year, Flashbot has published a paper, a piece of analysis estimating the expected validator return. And in that analysis we use the medium. So we get to the 6.2%
00:03:43.978 - 00:04:44.990, Speaker A: by using a constant medium mev and we simulate the block reward along that. So that's how we get to the 6.2%. But you can see medium is already more reasonable and a more robust way better than the averages, but that's not enough. The problem is, with a lot of the analysis out there, you see that the validator returns are always predicted as this perfectly smooth line, right? But in reality, this shouldn't be smooth. There should be some fluctuations, some variations. The expected return should be affected by some other factors, right? For example, the gas price, the base fee, the ethereum price, it should respond to the network conditions, it shouldn't be a fixed value. So what I'm proposing today with the new model is try to estimate the mev with a more dynamic and responsive way by including these possible features or exogenous variables.
00:04:44.990 - 00:05:56.706, Speaker A: So how do we do that? Well, before you start modeling anything, the first thing is to collect the historical data. And we know that mev as a concept is only theoretical, so it's the maximum extractable value. So in reality, some of the observable values will extractable values will not be observable because for example, if a searcher wants to pay the miner directly, he could be paying the miners through an L two solution or even through a bank transfer, right? So that kind of extractable value would not be on chain and we will not be able to observe that value. So we have to accept the fact that there is a gap between what's theoretical and what can be collected and we have to do our best to collect the data which is the realized extractable value. So in the following analysis, I'm referring to mev as the Rev, which is the actual value extracted from the blockchain. So how do we get the data for the Rev? First we take the minus balance difference. We look at what the minus balance is before a block is mined.
00:05:56.706 - 00:06:35.998, Speaker A: And after that we also take away the two E static block reward in today's proof of work. And we also take away the burnt fees because the data is collected from post London fork. Lastly, we take away the transactions originated from the miners address. So this could be mining pool payout or any kind of internal transfers that's not part of the mev. So now we have the rev data. On the right hand side we can try to model or fit some variables to estimate the Rev. In the middle we have the block reward.
00:06:35.998 - 00:07:14.158, Speaker A: The block reward. Previously we take out the two static reward. Now we have to replace that with the proof of stake block reward. So that's dependent on the number of validators, on the penalties and different factors. So this can be simulated. So with the Rev, which can be estimated, and the middle part, the block reward which will be simulated, we can get to a more dynamic, validator return. So before we go into the model, let's have a look at what the rev we collected from the post London folk data look like.
00:07:14.158 - 00:07:59.950, Speaker A: So from here we see that actually more than 90% of the rev of the blocks have an rev of less than or equal to 0.5 east. And if you look at the lower end, it's around 56% less than or equal to 0.1 east. So it seems like most of the rev are concentrated in the lower bucket. So now looking at rev across different time intervals, so we want to know whether there's any seasonality or there's any trend in the rev. So in here we do the box plot and we plot every minute within an hour, every hour within a day, every day within a month, and every month within the year.
00:07:59.950 - 00:08:28.390, Speaker A: So here we're looking at the five values from box plot. So briefly, very quickly, the top and the bottom line are the maximum and the minimum. The top of the box and the bottom of the box are the top 25 and bottom 25 percentile. So the line in the middle will be the medium. That's the one we're looking at. So on the left hand side chart, these two, you see the medium are both very stable. You don't observe any trend or seasonality.
00:08:28.390 - 00:09:15.558, Speaker A: For the bottom right hand side chart, which is the monthly frequency, we see the median value is higher in the months from August to November. But bear in mind, this is only one year's data, so it's not that representative. So now looking at the hourly, we also see a slight elevation in the rev medium value. So from this is UTC hour 1300 to 200, we see slight increase in the medium value. So this period of time also coincides with the US stock market opening time. Interestingly. So we know that there are some seasonalities in the hours, so we want to aggregate the block level data to an hourly data.
00:09:15.558 - 00:10:09.478, Speaker A: So why do we want to do that? Well, the benefit of doing that is that it naturally removes the outliers. So if we take the median value of all the blocks rev, we naturally remove that little spike at the end on the left hand side. So this is the probability density chart, and you can see there's a spike on the left hand side chart at the end, and once it's aggregated to the hourly, it's removed. The other benefit is if we want to do some kind of time series model, the time intervals needs to be equal, right? So in the current block time, it's not equal, it's not yet 12 seconds fixed. So by doing this hourly aggregation, we're able to create this equally timed step. So that's the other benefit. Of course, the last benefit is that you reduce the number of data points from like 2 million over 2 million to only 8000.
00:10:09.478 - 00:10:35.710, Speaker A: So that makes the training of the model much quicker, faster. So now, looking at the models, I've tried a few. The first one I tried is the decision tree. So here I'm trying to estimate rev as a class A bucket. So the decision tree will predict what's the probability of rev falling into a specific bucket, let's say 0.1 east to 0.2 east.
00:10:35.710 - 00:11:10.358, Speaker A: So as you see, the accuracy score is very low. So accuracy score is just the percentage of correctly predicting the class. So we are only able to predict 39% correctly. Not very good. So then I started to try random forest, which is a very similar method to decision tree except that it builds many multiple trees and aggregate them to an average. So this time I model the rev as a continuous variable and the measurement used here is r square. So as you can see, it slightly improves the performance.
00:11:10.358 - 00:12:11.230, Speaker A: So r square is basically the percentage of variation of the actual points that can be explained by the model. So we can roughly explain 44% of the actual rev here, still not so good right now. What about the time dependencies? What if there is some kind of correlation? What if the past rev can predict the future? So here with the Arema model, you're able to throw in the different lags of the rev. So we're using the past 1 hour rev or the hour before that to predict the current rev. And in this case, you can also throw in the other exogenous variables like gas price, base fee, gas used. So with a few iterations we can see the final model has these variables. So we found the past hour rev has a positive correlation with the current hour, the hour before the past hour has a negative correlation.
00:12:11.230 - 00:12:56.482, Speaker A: We also see that gas price, gas units used have a positive correlation and base fee has a negative correlation. So this is just the optimal model after a few iterations. So now let's look at the prediction. So on the left hand side we see a log transformed prediction. The green line is the predicted rev, the blue dots are the actuals and the gray shaded area is the confidence interval. So as you see that on a training set, most of the gray shaded area kind of covers the blue dots. So even in some cases, the extremely high actual revs are kind of covered.
00:12:56.482 - 00:13:45.366, Speaker A: Similarly, on the right hand side we have the testing set as the green line. So here interestingly, the testing set is during the recent period. So we know there's a downturn in the recent period and the model is also able to capture that downward trend. So that's a promising result. Now, what if we plot the prediction along with the block reward? So we assume that every validator gets a full block reward and add that upon the estimated rev we just did. So even though the left hand side, random forest was able to capture some of the extreme values on the top here. But the recent period, the last thousand data points, you see the prediction is very flat.
00:13:45.366 - 00:14:23.414, Speaker A: And also the confidence interval is also very narrow. So that's not so good. That's probably also why the performance is not so good. But on the contrary, the Areema model. Similarly, the left hand side, you see the predictions on the green line. This time, the last 500 points. On the right hand side, you can see the prediction has some kind of fluctuation, right? So the time dependency creates this momentum and mean reversion little cyclical prediction which covers the recent period pretty well.
00:14:23.414 - 00:15:19.718, Speaker A: So the confidence interval is also able to capture some of the extreme points. So finally, I want to show this graph because the model predicts at an hourly level. So given one number of validators, you will have multiple predictions of validator return. But in this chart, I take the medium of all the predictions and plotted this chart. And this way we can compare what we had at the beginning of the presentation, which is just a smooth line with what we have now with the new model, which has these fluctuating predictions with confidence interval. Even though the random forest is not a good model, it still manages to create some variations in the prediction. So the time dependency Arima model we see, it captures the volatility during the time when we had 200,000 validators.
00:15:19.718 - 00:16:15.060, Speaker A: So this is around the time, I think summer last year, where the mev is very volatile. And you see the confidence interval are also wider, whereas now the volatility has died down, the confidence interval is much narrower. Just to wrap up, we had the old model which we used the medium mev as a fixed value, simulated the block reward estimated a fixed return which is 6.2%. But given the same number of validators we have today, let's say 410,000, we're able to with the new model, we're able to predict a range of different validator returns. So ranging from 4.8% to 7.8%. And these differences are dependent on the different conditions of the network which are indicated by the different variables we use.
00:16:15.060 - 00:17:07.456, Speaker A: So in the new model, we are able to predict with all the exogenous variable and predict better what the validator return will be. So now we've moved away from the average mev to a dynamic mev we also moved away from the old model is a smooth, perfectly smooth curve to a more dynamic, volatile, responsive curve. So hopefully the validators can look at this and think more realistically about the return they're expecting. Okay, quickly on the improvements, there are a lot of things we can play around with this model. For example, the assumptions I made in all the charts are that participation rate is 100%, everyone gets the full base reward. These obviously can be changed or reduced. And you can also include other exogenous variables.
00:17:07.456 - 00:17:52.320, Speaker A: For example, the trading volume of ethereum or the volatility or even some other token price or volume. You can, of course, change the frequency of the prediction. So in the model I showed us hourly, you can change that to 12 seconds post merge 24, 36, whichever frequency that you think might improve the performance. You can also, of course, try the other models, like the neural network autoregressive model or also the Markov chain state transition model. So these are a few options to explore. That sums up everything. I've included some of the results in the code I did and also the previous papers we published.
00:17:52.320 - 00:17:57.150, Speaker A: Thank you. Thank.
