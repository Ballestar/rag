00:00:00.330 - 00:00:01.102, Speaker A: Thank you.
00:00:01.236 - 00:00:09.946, Speaker B: Let's get started. I brought you five idea. One of the ideas is applied that which took gorgeous.
00:00:09.946 - 00:00:17.502, Speaker B: So look forward to that. That's going to be number two. This is vaguely motivated by me reading stuff.
00:00:17.502 - 00:00:21.838, Speaker B: I normally work in macros design. Microeconomic theory. Yeah.
00:00:21.838 - 00:00:47.318, Speaker B: Can we switch this off? Distracting me. Just switch out the TV. I normally work in magnetism design or microeconomic theory and there I'm like much more of an economist than an algorithmic game theorist and I recently also work in reinforcement learning and they're actually coding realities.
00:00:47.318 - 00:01:16.790, Speaker B: So some transition into more practice. But you can query the economist Tat who's a classical social scientist and you can also query the reinforcement learning Hat who is going then to talk more about practical coding realities or how you build systems. The first idea is just an overview of how you improve systems in the corporation sense that we have already seen.
00:01:16.790 - 00:01:37.500, Speaker B: And one structure that I want to get into your head, which is the first idea is that really three different approaches that you can go for. One I would call the mediator base. You're having a couple of agents that interact in some way and you're introducing a mediator into the system.
00:01:37.500 - 00:01:58.690, Speaker B: There is a ton of work under the name of Stackleberg games that work in this environment. You're essentially introducing a new agent that proposes mediators. Let's just wait for no, no, I prefer having we're not in a rush.
00:01:58.690 - 00:02:34.540, Speaker B: So we have introduction of new agents. This comes out, for example, when I read that the protocol becomes aware in the ram that was in one of the texts or the protocol tries to do something. There's an agentic nature to the protocol and you're introducing a new age.
00:02:34.540 - 00:02:59.742, Speaker B: The second approach I would call the declared leader. This would be like August Sand, the sign. This is endowing particular agents with specific actions that cause you to commitments.
00:02:59.742 - 00:03:19.050, Speaker B: We have this in the real world a lot where we have hierarchical structures that allow some agents to make commitments for others. And the paper that I'm going to present uses this style and we can digest that. The third one is equilibrium.
00:03:19.050 - 00:03:44.142, Speaker B: Here this one agent thinking of the threat which is learning something of the of the form. I am going to not be too naughty here on this ship because otherwise the police come and puts me into jail. Probably I would need to really break a lot of norms so that that happens.
00:03:44.142 - 00:03:58.650, Speaker B: But this threat is there and I'm cooperating for that reason. This is what I would call subgame perfection. The reactive, these three ones.
00:03:58.650 - 00:04:32.134, Speaker B: And for me as an outsider to crypto kind of I read a bit before this talk. Now I would say that the first one which has a bunch of work now in academic venues but also with just stackelberg technology, doesn't sound very crypto in some way because actually it introduces a centralized agent that has, again, a lot of power. So here the question becomes, okay, there are two ways.
00:04:32.134 - 00:04:40.870, Speaker B: One, learning punishments. I actually disagree with the statement that punishments are bad. Like, we have a lot of very good emergent behavior because we actually don't punish.
00:04:40.870 - 00:04:56.880, Speaker B: If the punishments are in place, people choose to not do it. And there are other things that are essentially technology. And I will interchangeably also talk about agents versus the technology.
00:04:56.880 - 00:05:42.830, Speaker B: Sorry, this technology shouldn't go here as the second idea. To give you an example of concrete enforcement learning domains that we're working in, this is the main thing that I think for you is interesting here. It is Motivating social dilemmas, which I think you as an audience don't need.
00:05:42.830 - 00:06:10.230, Speaker B: But you will see what the game looks like as a simulator. I'm going to finally talk about two very concrete things that are apparent if you're training reinforcement learning systems that I haven't heard or read. The main question of cooperation, and also this paper is how do you get selfishly motivated agents to cooperate? We already heard of a lot of them.
00:06:10.230 - 00:06:21.066, Speaker B: There are many that are actually still wildfire away. The autonomous vehicle things were probably ten plus years away. To see that.
00:06:21.066 - 00:06:36.686, Speaker B: We have, however, things where control systems are already out there. Smart grids are control systems and having smart meters and Google Nest have things that have agentic nature already right now. So this is quite close.
00:06:36.686 - 00:06:58.870, Speaker B: This paper is very motivated by warehousing solutions where you have robot fleets that are in an Amazon warehouse doing packaging fully automatically. Here you have some very clear cooperative framing. Is Amazon controlling all of the robots? But the robots are independent for computational reasons.
00:06:58.870 - 00:07:17.450, Speaker B: There are also other esoteric ones in manufacturing that I'm not going to go into. The domain that we are working in is a grid world domain which features a river, a tree, and a robot. The robot gets reward by picking apples.
00:07:17.450 - 00:07:39.966, Speaker B: When the robot goes and grabs apples, over time, there's a build up where's this yeah, I can't see mine. It's weird. I cannot see the cursor.
00:07:39.966 - 00:08:00.582, Speaker B: But now I can admit there is dirt building up in the river. And the robot needs to go and take another action, which is cleaning pollution. Pollution cleaning does not yield reward, so they clean the reward.
00:08:00.582 - 00:08:13.554, Speaker B: There is again, an apple. So in a single agent setting, you have the cycle from polluted. The robot goes to clean apples, grow, the robot picks apples.
00:08:13.554 - 00:08:23.440, Speaker B: It builds up pollution again, and it is in the cycle. This is something that a reinforcement learning agent could learn reasonably well. You need a couple 10,000.
00:08:23.440 - 00:08:31.426, Speaker B: I'm just going through this bit in like two minutes. There's a good time. Whatever is it, we have more agents.
00:08:31.426 - 00:08:50.940, Speaker B: You might hope that they grow, they clean, they grow. At some point, they realize that they could divide labor and one cleans one eats apples. Pollution builds up they both harvest does this happen? Yeah, not really.
00:08:50.940 - 00:09:15.762, Speaker B: Because both agents so what? You see, both agents end up waiting for apples that don't grow because the river is there. Because the agent who would be in this model, the harvesting agent gets apples, the other one gets dirt. But the second agent doesn't really get anything.
00:09:15.762 - 00:09:38.280, Speaker B: We have a social dilemma in this one, which has also an intratemporal component to it. So I think now is a good time. Do you have a question? I was just wondering if the pollution is in some way related or dependent on the apple picking just by time, just over time.
00:09:38.280 - 00:10:32.810, Speaker B: The logic is if there's a certain dirt level, no apples grow but pollution increases over time at a constant rate. Now how do you train this thing in environment already said for single agent you can just view this game as a very big game. There's some state, there's some actions, there is this now algorithm that pretty widely uses called proximal policy optimization which optimizes whole policies so whole functions from state to actions and yeah, here I will report rewards analysis number of apples that are picked per an episode.
00:10:32.810 - 00:11:07.990, Speaker B: You could do a separate training and you will converge to some form of Nash equilibrium where agents might randomly go out and clean and go out and pick apples. But this is some approximation to complicated looking mixed equilibrium which is not very high performance. You can have one agent that controls all robots in this environment and you see some performance interestingly.
00:11:07.990 - 00:11:37.214, Speaker B: You can also look at benchmarks that I'm not going to talk about and you can allow for commitment. And you see here that this example the commitment decentralized solution that we are proposing this paper outperforms even joint, which looks for now surprising. But I will tell you why that so let's see what commitments can do here.
00:11:37.214 - 00:11:50.450, Speaker B: I think it's clear the whole meeting here is about commitments. I will commitment share my apples with you. If you clean the river? The other robot says yes.
00:11:50.450 - 00:12:20.298, Speaker B: What happens in this case? A certain contract is in place which alters in this case how many apples you effectively get. Some apples are automatically transferred. Robots clean, apples grow, they go into some division of labor, and they sit there's no path that brings them into a dilemma state.
00:12:20.298 - 00:12:37.010, Speaker B: So what happens here? Well, normally as normal it happens, but there is a transfer of apples, which is huge. Normal employment contract, you could think of that. They're learning from some set of possible commitments.
00:12:37.010 - 00:12:57.350, Speaker B: This doesn't require any altruism at every point. This is the right thing. As soon as the one robot committed to give the other one apples for cleaning, this is in the best interest for the cleaning agent clean and the other one yeah, sure.
00:12:57.350 - 00:13:07.660, Speaker B: If they don't have anything to do, they will eat apples. So at this point this is clear is also the choice to propose a commitment device. Incentive compatible? Yeah.
00:13:07.660 - 00:13:39.510, Speaker B: You're comparing an outcome which is paradise essentially there's a working agent who gets something and there's an agent that eats apples. This is a good outcome to the dilemma state which is bad for both. So to every state you're purely into self interest and I think there that commitment devices are actually the use of commitment devices is incentive compatible, is something that's well known.
00:13:39.510 - 00:13:52.250, Speaker B: How do you make this formal? This is a Markov game. We have a state space, we have some initial state. This is like the grid world.
00:13:52.250 - 00:14:14.080, Speaker B: These things look all a bit more high dimensional. You have vectors of actions which could here be move up, move down, move left, move right, pick apple, clean garbage from the river. You have a transition function that map state actions to new states.
00:14:14.080 - 00:14:35.468, Speaker B: Here the state contains apples, location of garbage and the locations of the robots. And you have a reward function which in this case gives you number of apples. So we want to bring something new in this.
00:14:35.468 - 00:14:53.584, Speaker B: So we bring commitment devices into this Markov game environment. Environment. And this we do via a set of potential commitment devices that's quite similar to the proposal, I forgot the name again of the specification with commitment devices.
00:14:53.584 - 00:15:13.752, Speaker B: It was in one of the things, one abbreviation and a function mapping the set of commitment devices to essentially semantics of the commitment device. The commitment device here is zero sum transfer depending on state. So here it is.
00:15:13.752 - 00:15:49.700, Speaker B: If you're in a state where the river is clean, there is a transfer from one of the agents to the other agent and this is a form of commitment that we're looking at in the Excel. We also need to blow up slightly this state space by tracking which contracts are currently in force. The rest of this is going to be why this second small idea I promise the other three will be shorter is why this works in theory and why this works in practice.
00:15:49.700 - 00:16:03.850, Speaker B: Yes. Can you go back 1 second point out explicitly why cannot you incorporate the commingment devices as part of the state space while doing the extra.
00:16:06.220 - 00:16:06.696, Speaker A: What is.
00:16:06.718 - 00:16:35.364, Speaker B: It in the commandment devices that you cannot code in the cornerstone? Oh, you can totally view this as this will compile down to another Markov game and when I tell you how to solve it, you'll see person theory. We have a theorem for an arbitrary full information markup game. Full information is here important.
00:16:35.364 - 00:17:11.410, Speaker B: I'm going to talk in number three about that then. For any sufficiently rich contracting space. All equilibrium of the augmented game are equal to a jointly optimal policy profile of the original game under the contract proposed in the equivalent if that doesn't make sense to you right now, that is fine, because it says only that allowing agents to contract with sufficiently rich commitment devices mitigates social blood under complete information.
00:17:11.410 - 00:17:23.768, Speaker B: Yes. What does cetation gain? Weight, high punishment. This is high possible transmits.
00:17:23.768 - 00:17:41.832, Speaker B: I need to have that, otherwise I don't really care. It's going to be degenerate. But there or this theorem I essentially want to force you if you're not doing the best thing, I'm just forcing you.
00:17:41.832 - 00:18:35.060, Speaker B: Does it also depend on the dimensionality of theta? If it's a parameterized kind of space and so on or it's not what rich means here? No, it's really about the possibility because it is about so here it is really the property that you need is that your contract space is rich enough to detect deviators in some sense there have a look at the paper. I think the theory here is less interesting than actually implementation and my goal for you is to get to the point of how would you actually train this? We can take this game and this is as you ask, this is just a normal game. You could totally encode that and you can train it with a generic algorithm.
00:18:35.060 - 00:19:09.284, Speaker B: You're just saying there's a first also stage, there's some acceptance stage then you go fair, performs fine. How can it outperform joint? Well, the thing is that if you're doing joint you really need to look what caused what because then essentially you are having two agents and you're doing something to two agents. So you have a two dimensional action space and you need to learn okay, so if I'm doing this one agent then this affects welfare in the following global way.
00:19:09.284 - 00:19:48.210, Speaker B: If you have a single agent environment that is working under some contract then they will very easily figure out what their part of the reward, namely their apples. How that depends on their action. This is called in reinforcement learning credit assignment which part of your action do what? It's a much easier problem to decompose and one of the reasons why you actually would look at independently trained agents because their problem is much more local, it respects locality that actions that are taken by a particular agent actually affect their reward quite directly but not so much the global way.
00:19:48.210 - 00:21:00.810, Speaker B: The problem is that this doesn't really scale well to more agents and one problem is that the contracting space to value for the agents so to utility for the agents mapping it's more and more discontinuous over time like in some sense you have optimal reactions in sense of best responses and this gets harder and harder to learn function and it might not be that there is enough exploration. So what you do to solve this is to say well, let's decompose the problem, let's first solve for different possible worlds under different enacted commitments and just learn policies and hence learn values and after that just optimize over that. You then have just a function that maps your contract parameters to how much utility is there and you can optimize as a proposing agent what you do I see that you're not? No.
00:21:01.660 - 00:21:02.410, Speaker A: Yeah.
00:21:06.860 - 00:21:15.800, Speaker B: The agent is learning what contract to propose in the second stage. Yes. In the first stage, they're only learning to play under an arbitrary contract.
00:21:15.800 - 00:21:21.696, Speaker B: I see. Yes. And decomposing these essentially gives you an expiration bonus on this if you want to view it.
00:21:21.696 - 00:21:42.480, Speaker B: But it also essentially brings down the reinforcement learning only to this parameterized problem. This one is then classical search, which is computationally much easier. The key idea hence is to learn how to play given a contract before learning to contract.
00:21:42.480 - 00:22:21.916, Speaker B: Yeah, this is, I think, partly answers my question, but I was going to say, does this basically end up then being kind of like a stackelberg game in which you first have to select the contract and then well, there's some element of kind of subdate perfection here, where what you're doing is like for any contract. How am I going to best act optimally? And then you have here the thing is there is interpolation in that in that you're really relying somehow on the geometry of your contract space to give you okay, I only tried a thousand different ones. Let me use my neural net magic to actually interpolate.
00:22:21.916 - 00:22:31.508, Speaker B: Okay, fine, play otherwise. But yes. So here in Stackleberg's sense, one thing going back to this, there's not even a new agent.
00:22:31.508 - 00:22:40.992, Speaker B: We're just declaring one of the agents to have that possibility. How you assign that is nothing new. You're not changing the agendic environment.
00:22:40.992 - 00:22:53.150, Speaker B: There's still an agent one. Yes. Quick follow up question, which maybe you're going to answer anyway, but on what kind of time frequency does those contracts change or get updated? I will come to this.
00:22:53.150 - 00:23:13.570, Speaker B: Let's look at more experiments. Prisons dilemma and public goods is two classical static gains. Harvest is another grid worldy environment which is mostly an intertemporal social dilemma where if you over Harvest now, things won't regrow again.
00:23:13.570 - 00:23:27.560, Speaker B: Cleanup is the domain that we have been talking about on these slides. Merge is a self driving car domain which is essentially dots on a line chasing each other. So relatively far from realism.
00:23:27.560 - 00:23:57.330, Speaker B: And we're comparing the four baselines that I told you about already and this two step procedure that I proposed in the earlier slide, which is multi objective contracting, augmentation learning Mocha. And what we see is that MOCA scales more nicely. This is kind of what we set out to do and the experiments confirmed it.
00:23:57.330 - 00:24:30.784, Speaker B: Future work, scaling to more agents here one big question is how can you transfer between agents the knowledge of what it means to contract? If you can do that, then you can just clone things and you can do it like the dota five where you have five times the same agent. So dota five was this OpenAI bot that played defensively engine. A second one is learned contract functions.
00:24:30.784 - 00:24:49.440, Speaker B: So far I said that there is some given logic to contracts, but you might actually want to have a lower parameterized set of contracts that are more reasonable. And for this you would need to learn the lot. And there is renegotiation, which is after which time.
00:24:49.440 - 00:25:04.070, Speaker B: So you allow after some time for contracts to last. There's always a trade off in that contracts run out. The long term incentives are broken because you know that the commitment is not going to last.
00:25:04.070 - 00:25:37.440, Speaker B: So in summary of this paper, social dilemmas arise in multi agent systems when managing shared resources. Contracting or commitment devices in this environment create incentives for post social behavior without assuming any altruism and letting agents trade voluntarily mitigate social in both theory and in practice. So we saw now an example of what's the did I started point no.
00:25:37.440 - 00:26:14.460, Speaker B: Anyways, so you now saw the talk at Ms that my collaborator gave and two comments on here on your stated goal of today with the complete domains and I will give them names. The first is centurion here. We are working in simulators and all of reinforcement learning right now happens in higher or lower fidelity simulators.
00:26:14.460 - 00:27:22.690, Speaker B: One of the questions that I will give to the crypto community that I see in many of you is how are you thinking about a simulator versus reality in a crypto environment? Because we have rollouts. So these are simulations of the environment on 100 CPUs in parallel of these worlds. Are you going to simulate a ledger in training? Are you going to simulate a blockchain or are you going to simulate smart contracts while you're training or is this purely a deployment thing? I think there are several questions around that and here how would even reinforcement learning training look with cryptographic techniques while also realizing that you need to get onto the order of hundreds of millions of environment steps that this is realistic open question.
00:27:22.690 - 00:28:04.480, Speaker B: The second one for an environment like here is a very particular type of wire heading. And I also would like to pose you as an open question if you're saying that commitment devices are going to intervene directly into rewards that agents get. And I think there are a lot of arguments for doing that as opposed to intervening into actions or delegation, which I see very often say later why you will run to a situation where you might as a robot be able to evade receiving the reward signal from the ledger.
00:28:04.480 - 00:28:37.370, Speaker B: If you can do that, that's a perfectly fine way to wirehead yourself because there needs to be some reward signal that you get instead if the centralized blockchain tells you, oh, this was actually really not nice of you that you didn't clean, but you, for some reason, can move to a part of the Earth where you are actually not able to access the ledger. And there is some decentralized computation which you will need in robot days. What do you do? They don't know what wire heading is.
00:28:37.370 - 00:29:04.336, Speaker B: Okay, so here you're messing around in your own hands. Here it means you can influence the reward signal that you yourself are getting. And here I'm thinking about the application of if the reward signal depends on some public ledger and you can evade receiving that thing, then that's a very clear reward hack that I see in such an application and I think that's something to really think about in a real deployment.
00:29:04.336 - 00:29:23.210, Speaker B: There was a question yeah, I guess back on. Okay, yeah. So are the contracts proposed or like enacted unilaterally? I'm going to come to that in the fifth part.
00:29:23.210 - 00:30:11.730, Speaker B: Okay. These are like two concrete things that I would love for someone in the crypto space to think more about the next party is that I would like to be a bit more clear about the thing that I already earlier said with delegation versus transfers about interventions into games. So if you're augmenting a game to get cooperative outcomes, you can do some of the layer essentially hey, I'm not going to do that because I'm actually delegating to another agent.
00:30:11.730 - 00:30:39.930, Speaker B: The second thing is, oh yeah, I'm going to just insert world state going to transfer. If you're delegating then what this means is you need to have a fully flashed policy that is run somewhere else otherwise it could influence it that actually acts for you. If you're thinking about a real robot picking up stuff, you're not going to be able to do that.
00:30:39.930 - 00:31:01.804, Speaker B: Any computation on a blockchain will be too slow to get that. You need some local computation, you can send some certificates, but I don't see that happening under reasonable latency. And here like a bit more of the blunt person here.
00:31:01.804 - 00:31:17.990, Speaker B: The whole idea of oh, we inspect each other's source code, I don't really believe that that will be at all working. One thing that can work is to say you have a reference world states. You can call this reference world state the S and P 500.
00:31:17.990 - 00:31:48.872, Speaker B: Nowadays it can be the Fit rating for Google or for Italy. And based on this you have certain automatic rules flashcrash a lot of things based on these automatic world states you're saying I'm going to allow reward transfers based on that? You don't need to know anything about the local computations of a rocked. You don't need to know their action spaces, you don't need to know something about their internal representations.
00:31:48.872 - 00:31:59.344, Speaker B: The only thing you need is to know that this thing is optimizing. It's maximizing in reward and you can give it reward. You need some way to make reward comparable across agents.
00:31:59.344 - 00:32:23.892, Speaker B: But there that's straightforward in many environments. If you're going for an Amazon warehouse you could say it should be the same reward for every agent to pick up one box easy your welfare criteria but you can do it. So here delegation solutions I don't see as not reinforcement learning head I don't see as scalable transfer solutions.
00:32:23.892 - 00:32:41.730, Speaker B: I think there is this problem with wireheading. I think it's much more trapped. This brings us to the question of contractual outcome number four.
00:32:41.730 - 00:33:05.804, Speaker B: It was actually super crucial for the paper that we don't have incomplete information we can contract on the true state of the world. This is going to not be true and in one of the papers from yesterday or it's abstract was exactly that question. You don't have that.
00:33:05.804 - 00:33:28.210, Speaker B: The chain knows everything. And in thinking about in thinking about how contactingput will work in the real world you really need some sort of the chain meets the real world. So you need some sensors that you can actually rely on.
00:33:28.210 - 00:33:41.510, Speaker B: This I would say, is not necessarily saying that there is centralization going on. You just need to have some trust. If you're leaving the chain, you will need to leave the chain at some point.
00:33:41.510 - 00:34:18.252, Speaker B: So one question here is what is contractable outcome that you would like to go about to incorporate into your system? These types of measurements, if you're going beyond finance application or transaction applications are actually not as straightforward. You're thinking about the upper right one which was the electrical grid. These are things you might want to contract on that are actually very hard to really control as stable and not hackable.
00:34:18.252 - 00:34:42.200, Speaker B: These would be the New York Times headlines. This would be certain things happening even in Mexico south of the border if you're talking about Texas, this could be rainfall numbers in northern America. A lot of things you need to inform these contactable.
00:34:42.200 - 00:35:15.430, Speaker B: If the contactable outcomes are there, you can write good contracts. If they're somewhat limited, you're running into problem called moral hazard that I'm not going to explain to not get over my allotment time number five is let me just rewrite this. I think I finished slightly early.
00:35:15.430 - 00:36:30.560, Speaker B: Some more conversation is about the concept of the proposal here. One of the main things why commitment devices are amazing for welfare is that the proposing entity can actually the proposing entity is aligned with social welfare because they can extract all surplus from doing so. And one thing I want to put into your head in this last thing is in many applications that's not a bad thing.
00:36:30.560 - 00:36:52.862, Speaker B: The beauty of working with robots, if you're in an Amazon warehouse and you're making one robot extremely poor you don't fucking care. It's not really problem because robots are on Star. There's a question of like there's ownership of robots but you don't necessarily really care in these environments.
00:36:52.862 - 00:37:33.326, Speaker B: Having one agent who is can give it a lot of names, a lot of human society names that sound bad but you don't need to. You can have one robot who extracts all the surplus from the system and the system behaves amazing. All the problems of commitment races or other miscoordination problems arise from this assumption that you need to allow every agent in your system to get the fruits from that.
00:37:33.326 - 00:37:46.578, Speaker B: And I want to give you to you if you're really going and engineering a multi agent system. Just don't allow that. Just declare one agent who is going to be really rich and run with it.
00:37:46.578 - 00:38:29.970, Speaker B: In many environments there's actually going to work. The only problem is the miscoordination from this desire of humans to create something fair which arbor in robot situations is not clear, not that the robot is entitled to getting a certain amount of reward and you also get into more into more miscoordination also leads to then arms races and ineffective investments in that. Just saying there are solutions all of that if you just drop your fairness consideration between robots, which I think is fine if you're thinking about.
00:38:29.970 - 00:39:01.112, Speaker B: So going through the five ones again, I first invented three examples of mediators declared leaders and sub imperfection as broad solutions to the coordination problem. Then give you one situation of the declared leader and talked about how you would do that in grid world. I also gave you two concrete problems that I think are important to work on.
00:39:01.112 - 00:39:37.732, Speaker B: I argue that transfer based commitment devices are the way forward as opposed to the aviation based ones. We will need some contractable outcomes where the chain meets the real world and thinking about which these should be is an important question and I argue that caring about distribution is often a problem that could be a problem that makes it much much harder to actually make progress which you might not care in AR. Yeah.
00:39:37.732 - 00:39:40.950, Speaker B: Looking forward to all of your thoughts. So.
00:39:46.940 - 00:39:48.216, Speaker C: Great talk by the way.
00:39:48.318 - 00:39:49.224, Speaker A: Thank you.
00:39:49.422 - 00:40:30.730, Speaker C: So I have one question on ragency and I guess what Mitches said. So something that some people are thinking about is proposal commitments in a blockchain context, right? And I think that highlighted some of the ways or basically highlighted kind of the concept of contracts that validators in a blockchain can enter into. And something interesting about that is that the contracts that are actually importable are the contracts that importable by the chain are the contracts that the chain has information about.
00:40:30.730 - 00:40:56.172, Speaker C: Something that we saw was like two ways in which the blockchain can learn, which is the first one is obviously through consensus. So learning whether for example, say a contract was the commitment was properly executed in the real world or something in the real world happened. And the other way of learning about the real world from inside the chain is through cryptography.
00:40:56.172 - 00:41:09.136, Speaker C: So for example, posting a cryptographic group something and then verifying the proof as part of the protocol, so to speak. The other thing I wanted to ask.
00:41:09.158 - 00:41:36.776, Speaker B: You about is I quickly comment on that because one thing that here also just as outsiders look or now economists have look on things that I read in crypto for this, there seems to be an interesting bias in crypto. Talking about talking about what the chain does, that's very financial economics. I would say it's all about information aggregation.
00:41:36.776 - 00:41:39.980, Speaker B: If. All information is aggregated on the chain. That's good.
00:41:39.980 - 00:41:57.404, Speaker B: And here it's not necessarily that you really need for a well performance system to know all these things. The thing that's really important is, okay, what in the real world happens? Are there flash crashes and so on. I totally understand it.
00:41:57.404 - 00:42:12.730, Speaker B: If there is this more narrow view of the chain as being a transaction medium, then it's all about that. But you can have contracts with severely limited information. There is going to be strategizing with that.
00:42:12.730 - 00:42:22.552, Speaker B: But that's not bad. And that's not bad. Many people are saying, oh, we need to make it strategy proof to essentially make it robust against strategizing.
00:42:22.552 - 00:42:28.060, Speaker B: That's not my view. It's gorgeous. A lot of good outcomes are coming out of auto strategy.
00:42:28.560 - 00:42:49.300, Speaker C: I totally agree by the way. In fact, people argue that those systems have a bit of an optimistic flavor where for example, there's asymmetry or information that will arise in the future. Then some commitment or some trust is put on today.
00:42:49.300 - 00:43:02.250, Speaker C: And when that information becomes available, then the agent that made this assertion, so to speak, about the real world can be punished as a way of enforcing the fact that.
00:43:04.380 - 00:43:24.404, Speaker B: And here also these are all relational contracts. One very standard way to do these are the punishments we call relational contracts. Which is just like I'm struggling to work with you like if you lie to me but the totally natural way of humans to enforce things or your ending friendships.
00:43:24.404 - 00:43:25.264, Speaker B: Okay, yeah.
00:43:25.302 - 00:44:08.540, Speaker C: And then what about contracts where the enforcement so you were saying they're contracts that due to incentive compatibility they have to be enforced by, say some external thing to the parties involved in the contract. But what about contracts that lie in between? Where they can be, for example, mightly influence the enforcement of the contract, can be mightily influenced or somewhat influenced by the parties that take part in the contract. So in other words, they lie somewhere in between the absolute enforcement depending on the external world, but they can be influenced by the agents.
00:44:08.540 - 00:44:15.250, Speaker C: And if you think there are some interesting ways in which that could be quantified say for example, these contracts are.
00:44:16.580 - 00:44:58.780, Speaker B: Robust, say here, read this paper, that should be hard. The Oliver Hard, the novel laureates in the Harvard Business Review should be 20. There is like there is this thing that there is this very funny thing that companies that are cooperating with each other are writing down values from they're written down things but they're clearly not enforceable.
00:44:58.780 - 00:45:45.900, Speaker B: This paper makes the claim that there is a coordination aspect to partly formalized but actually punished in the real world by actual humans things. And one thing I'm kind of kind of thinking here is that these okay, I'm also implying, I think here that you can influence how something that's semi automatically enforced then that won't be that tractable. The thing that I'm saying though is that having written things that govern or that outline how punishment is going to happen in a more sub imperfect world.
00:45:45.900 - 00:46:04.384, Speaker B: That we have these with humans and these are potentially totally feasible for a ledger that you're essentially saying here are recommendations to punish for people. We're not forcing you to do it. You do it in the real world.
00:46:04.384 - 00:46:13.940, Speaker B: But let's write this down and this hand promise. But there I think people probably have much better ideas. Just read this very general audience.
00:46:13.940 - 00:46:26.368, Speaker B: You are first, then you, then yeah. So just one comprehensive question. Yes.
00:46:26.368 - 00:47:00.590, Speaker B: In the paper that you talked about, your paper is it just there's one distinguished we give general enough. Like you could have an arbitrary agent and you can transition back into contracting nodes. As soon as you have several contracting agents, you lose the theoretical guarantee because I'm going to try to influence the world of days to make you propose in the future state a contract that's better.
00:47:00.590 - 00:47:23.280, Speaker B: And you totally see that. Like if you have for example, a reelection in associations that people will try to influence future elections already, arguably the United States is one such association. Then you question about the kind of partial observability.
00:47:23.280 - 00:47:42.120, Speaker B: So obviously that breaks a bunch of things. I wonder if there's a way do you have any thoughts on the feasibility of kind of like it failing gracefully. So the more uncertainty you have then the more kind of bets are up, so to speak.
00:47:42.120 - 00:47:43.896, Speaker B: Or is there just like pretty kind.
00:47:43.918 - 00:47:45.704, Speaker A: Of discontinuous thing being like cool?
00:47:45.742 - 00:48:08.908, Speaker B: If it's not fully observable then you can just get kind of like arbitrary. This might not be cool, this is like very econ maps but one lesser known, lesser known saying. So the underlying thing of incomplete information is the theorem of Myerson.
00:48:08.908 - 00:48:29.860, Speaker B: Instead of that's, I'm taking your question as is this continuous or is this continuous? If we're getting imperfect, I'm claiming that this is continuous. Continuous. Continuous in the distribution.
00:48:29.860 - 00:48:46.430, Speaker B: So continuous in the distribution. So here in Mars and Saturday you have two agents, the buyer and a seller. Depth is good.
00:48:46.430 - 00:48:56.850, Speaker B: Let's make it an iPhone. Both have some type. There's some uncertainty over that.
00:48:56.850 - 00:49:12.244, Speaker B: Any mechanism tries to elicit this information. The claim is that there is no efficient. That means whenever there would be gains from trade, the trade actually happens.
00:49:12.244 - 00:49:31.230, Speaker B: Mechanism for this. The reason is that the fact that you would be willing to trade at a certain price tells me something which I want to extract something from. There's like the whole thing about information rents here.
00:49:31.230 - 00:50:06.280, Speaker B: The thing here is that if you're moving to the vicinity in some metric on probability distributions of a concentrated thing so where you don't have limited information, that actually it probably would need to be something like Earth mover distance. It's actually going to be continuous because you can only lie so much. It's like very arbitrarily small gain that you're making, which is going to reject only arbitrarily few trades, which makes this continuous.
00:50:06.280 - 00:50:32.112, Speaker B: So here, I would take this one as it's not that problem. The main thing with the really bad outcomes with unraveling in the market for lemons after loss, the thing is this is very, very strongly non observable. You don't observe a ton of things about cars in that.
00:50:32.112 - 00:50:50.730, Speaker B: So here yeah, almost meaning there's not reading this mark lemon what here. Lemon is meaning cars, not meaning fruit. Awesome.
00:50:52.300 - 00:51:11.836, Speaker A: Thank you for said that. The highlight is implementation, but I won't pass founder of theory. When you describe the main pyramid, there is enough space of you said that you need some kind of punishment, right, to be able to import the outcome.
00:51:11.836 - 00:51:16.400, Speaker A: And I think analysis phase is the slashing that is implementing.
00:51:18.260 - 00:51:24.124, Speaker B: Say again what is implementing what is implementing what on the chain slicing.
00:51:24.172 - 00:51:28.868, Speaker A: So if some people do something wrong, they lose a lot of money. They lose all their money.
00:51:28.954 - 00:51:29.204, Speaker B: Okay.
00:51:29.242 - 00:52:00.172, Speaker A: And so that's called slicing deposit at your current place to be a barren thing, perform some duty. And there's a question can we show that some things are impossible if we don't have these very strict punishments and seem that it's also centered in your work that if we don't have this rich space, although we don't have these very punishments, we cannot show something do you think that are there interesting results without punishments or that we can have.
00:52:00.306 - 00:52:02.044, Speaker B: Everything is possible without punishment?
00:52:02.172 - 00:52:06.816, Speaker A: You think everything is also possible but why do you need there is space so if you don't have oh, the.
00:52:06.838 - 00:52:24.790, Speaker B: Proof is easier, the group is easier. So here the thing there is what you exactly need is dependent on your game. It's just like it's a very simple thing to just say I'm going to burn down the earth if you're disagreeing easy.
00:52:24.790 - 00:52:44.412, Speaker B: But really what you need is what is your best deviation? Let me just charge you to make to disincentivize you to take that best deviation, all right? This is exactly what you need. And often the best deviation is not that profitable. So you need something very small.
00:52:44.412 - 00:53:08.496, Speaker B: And often even the value of deviations will some smooth function if the function is a smooth function also underlying parameter. So you don't need crazy things and there's no impossibility in that one thing here that I didn't bring up very good.
00:53:08.518 - 00:53:10.060, Speaker A: We are definitely in the binary.
00:53:10.140 - 00:53:24.968, Speaker B: This is now Beng Chong's term. This is moral hazard and observability. It here it is.
00:53:24.968 - 00:53:31.100, Speaker B: There is some hidden action that is taken. There's some outcome that's observed. You don't know exactly.
00:53:31.100 - 00:54:03.780, Speaker B: How bad it is. You have some smooth actually incentive contract that is enforcing and it's mostly depending on risk preferences which I think then as a governance group for netherland blockchain, I think actually thinking about risk preferences of people on the chain would be relevant to see like how much do you need? In our work, everyone is risk neutral. So you need extremely strong punishments.
00:54:03.780 - 00:54:10.632, Speaker B: A question about this delegation risk transfer thing yes.
00:54:10.686 - 00:54:13.610, Speaker C: Which actually might be somewhat related to ask for stuff.
00:54:13.980 - 00:54:42.796, Speaker B: Yeah, I think you make some good points in favor of transfers. But I wonder about the limits. Essentially, if you can delegate to an arbitrary agent, let's say you could delegate rather than delegating giving a specific algorithm, you can just delegate a reward function and have them optimize this reward function, then you have a lot more freedom than if you can only commit to transfers.
00:54:42.796 - 00:54:58.440, Speaker B: Right. Because if you can only commit to transfers, like utility has to be concerned or maybe you are allowed to burn money, but that's obviously somewhat unfavorable. Whereas if you can delegate arbitrary utility, then you can yeah, I mean you just have a lot more freedom which will be useful.
00:54:58.440 - 00:55:24.800, Speaker B: Yeah, pushback by saying I think it's kind of a science fiction temp critique like there is a more direct world here. So who is delegating? Clearly we are already delegating to machine. We're delegating to machines and Amazon warehouses.
00:55:24.800 - 00:55:52.680, Speaker B: What you seem to be saying with delegation here is that we have robots delegating to other robots. One of the questions or one of the beauties of impossible learning policies is that they're digital goods, you can just copy them. So if you would like something that optimizes in an amazing way, just employ it directly.
00:55:52.680 - 00:56:19.330, Speaker B: No delegation is needed to do that. If you are saying that the delegation actually gives me more flexibility to implement stuff. Well, the theorem that I showed you is telling us that at least in the full information case, no, it doesn't give you more because in some way you can view transfers as like range relaxation of limits on what you can do.
00:56:19.330 - 00:56:41.572, Speaker B: Like I'm just disincentivizing you in some smooth way from doing something. But there's like Iclaim is actually not more powerful. Even if it were more powerful in a world where we have algorithms delegated to other algorithms, I think we first have the step that actually algorithms just use certain technologies independently.
00:56:41.572 - 00:56:53.164, Speaker B: One of the beauties I think of this paper is it essentially is a black box intervention. You're just switching this on, it's only going to improve the performance. You don't need to tune anything about that.
00:56:53.164 - 00:57:19.940, Speaker B: If they can contract, they will get better social outcome. With this distributional confidence they will get better social outcome. I think the main point here is and that's maybe something for Offline is that I am claiming that you are not going to be more performant or it's also a lab hair with that so that flexibility actually doesn't buy yeah, let's do that offline.
00:57:20.760 - 00:57:23.316, Speaker A: Yeah, just coming back to I thought.
00:57:23.338 - 00:58:10.704, Speaker B: It was really interesting that you had this kind of two step part of the mocap. We have basically more explanation expression. I was wondering if that two step kind of structure of kind of hierarchical sense is needed to achieve this better outcome or if you just kind of added them are you interested in that as a pure enforcement learning question? Are you interested in that in a system engineer? I'm interested in both, I think.
00:58:10.704 - 00:59:03.348, Speaker B: But I'm kind of interested in what what's the role of the hierarchical path and two step circle the model in achieving those builder results? Or is it just like kind of a way to augment the correlation parameter? The short answer is you could build systems where you have separate exploration rates. This is going worse because in early phases where everyone is kind of stupid, you even trying to learn to contract doesn't make sense. You need to have a head start on how you have it.
00:59:03.348 - 00:59:17.624, Speaker B: It's actually really performing better to say let's first offline learn to behave reasonably as agent and then do it. It will work. We didn't run this, but I expect that it will work reasonably well.
00:59:17.624 - 00:59:57.864, Speaker B: It will be joined, but it won't be list. I think you also mentioned something regarding patency concerns. Yeah, I might only have to put that in the abstract but when I was talking about this wire netting example, there's all this underlying question around if you are deploying this, there needs to be some ledger or some public computation engine that gives these rewards to you.
00:59:57.864 - 01:00:32.864, Speaker B: And the problem with that is that leads happen very, very frequently. If you are in a, let's say warehousing as an example, there is all super centralized. If you want to give a good reward signal for an agent which is well shaped, you need to at least when every package, when any box in the whole warehouse is picked up, you need to send something to all robots and they need to receive it.
01:00:32.864 - 01:01:13.650, Speaker B: So this puts you into an environment that at least the computation that you're having with sending is on the order of half a second 2nd, which you can totally do there. Depending on which environments and how distributed we are, this is going to be a concern and for me the more crucial one, which I then also wrote down is this wire heading that the agents have an incentive to make it harder to receive the rewards. But honestly, I don't even know how fast computation on the Ram is right now.
01:01:13.650 - 01:01:20.624, Speaker B: Yeah, 500 millisecond is realistic. Okay, so it's half a second. Good.
01:01:20.624 - 01:01:46.920, Speaker B: So then we are exactly, in this use case, realistic that this could happen on the ledge. But essentially the question is like Ed, in what timeframe would you like to give agents something about some information about their externality for them to take good actions? And if it is not too fast, it is. I think we have five minutes on Kim Casper.
01:01:46.920 - 01:02:07.072, Speaker B: Yeah, thanks. Yeah, wrote me an email if there's more things that you would like to tell us and thanks for having me.
01:02:07.126 - 01:02:08.780, Speaker A: Yeah, thanks for coming. Incredible.
