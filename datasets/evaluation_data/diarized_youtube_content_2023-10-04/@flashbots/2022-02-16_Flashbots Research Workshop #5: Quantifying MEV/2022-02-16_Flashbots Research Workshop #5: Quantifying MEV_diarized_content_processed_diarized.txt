00:00:02.650 - 00:00:08.270, Speaker A: I probably will rejoin. The networking is a little bit lag. Give me a minute.
00:00:12.680 - 00:00:14.390, Speaker B: It seems perfect to me.
00:00:32.060 - 00:00:40.640, Speaker C: Maybe just start with a brief introduction of your health and your team of researchers.
00:00:46.300 - 00:00:49.320, Speaker D: I believe he left and he's rejoining.
00:00:50.220 - 00:00:51.210, Speaker C: Got you.
00:00:56.720 - 00:01:00.110, Speaker D: But I'm sharing my screen with the paper. I don't know if that's useful, but.
00:01:04.420 - 00:01:18.580, Speaker E: I guess while we wait, if anyone hasn't read this paper, including myself, now take some time to read it in more depth.
00:01:23.140 - 00:01:28.848, Speaker D: Right? I shared the link out of the call. Oh, hi guys.
00:01:28.934 - 00:01:30.290, Speaker A: Can you hear me?
00:01:30.600 - 00:01:31.204, Speaker D: You're back.
00:01:31.242 - 00:01:32.470, Speaker F: Yeah, we're fine.
00:01:33.000 - 00:01:33.750, Speaker D: Yes.
00:01:36.760 - 00:01:49.432, Speaker A: I can hear you now. Yeah. So my name is I'm working on DeFi and blockchain stuff. So yeah, I'm happy to share that.
00:01:49.486 - 00:01:50.600, Speaker G: We recently.
00:01:52.620 - 00:03:03.650, Speaker A: Released a paper on quantifying the blockchain extractable value. So basically we measured in the past two years how much value can be extracted or has been extracted from the ethereum blockchain, from liquidation, arbitrage, and gender wish attack. We also provide a general record transaction replay algorithm that allows an adversary to replay the transaction from the victim and front run this victim transaction to extract the value. And also we captured transactions that are privately mined by the miners, which means that these transactions are mined into a block but never appears in the memory pool. And I think this is the three main parts that we are present in this paper.
00:03:12.290 - 00:03:20.036, Speaker F: So I have some questions if you're.
00:03:20.068 - 00:03:20.650, Speaker D: Okay.
00:03:24.460 - 00:03:55.600, Speaker F: So I read the paper quite interesting. So a few questions I had in your measurements of the past years. You basically just look at just blockchain data, just data that you could already read? Or was some of the measurements done on the Mempool itself in terms of what transactions that you are looking for or is just what was actually happening? On chain.
00:03:58.760 - 00:04:23.020, Speaker A: We only like in in the like in the value measurement, we only look at the transactions that are finally submitted into the into the main chain. We haven't captured the transient API in the mempool, but never mind into the eastern blockchain.
00:04:25.040 - 00:05:38.590, Speaker F: Yeah, makes sense. I wonder if there is more interesting information that can be extracted besides looking at the final transactions. I mean, I read you had some heuristics of what's destructive front running and others. If anyone has any ideas on that, then please mention them. But for example, transactions, probably there are transactions that, for example, were canceled and were trying to take the same thing but were not actually successful because they failed or because someone else has grabbed the Emev already before them or won the gas war, et cetera. So how do you categorize them? Or did you even look at these? Is it interesting to look at them?
00:05:39.920 - 00:06:41.070, Speaker A: Yeah, it's definitely interesting to look at these transactions that are not executed successfully or even not mined into the main chain. But it's more challenging than just to look at these transitions executed successfully because for those are not executed connections. It's hard to dive into what happened or what's the underlying logic. Essentially those reverted connections are not executed on the expected block state. Right. So it might be challenging to figure out what is going on there and whether it's value extracting transaction or not, but it's definitely interesting to look deeper into them.
00:06:42.500 - 00:08:10.990, Speaker F: Yeah, that's what was the first question we had. Second, regarding the private transactions that we had in the last section, as I understand that their measurements were made on a single node over several weeks and basically you compare what was seen by the node and what was not but was later on the blockchain itself. Besides this, have you take a look, a deeper look at what these transactions are? For example, Easter Mine, I know that's trying to hide that they are distributing mining rewards in the top of the block without gas payments. And I guess these would appear there as well. I mean, they're not broadcasting the transactions used to distribute the funds. I guess we can distinguish these kind of payment rewards and transactions that are actually private dark pool transactions or however you want to classify them. So I wonder if you looked into.
00:08:16.000 - 00:09:16.770, Speaker A: So? Yeah, thanks to these comments from Tina and other guys, there are some transactions that are actually payment transactions. So we already updated this table which hasn't been reflect on the archive, but yeah, we already updated the table. So we distinguish the transaction that only cost 21,000 gas, which means that this is the single payment transaction and also this that cost more than 21,000 gas, which means that these transactions are like invoking contracts. And we find that around 8% private mine transactions that invoke smart contract. So these are more interesting transactions that we should look into.
00:09:17.780 - 00:09:20.032, Speaker F: 8% of the ones you have here.
00:09:20.086 - 00:09:27.060, Speaker A: Or 8% 8% out of the list transactions are already listed.
00:09:30.920 - 00:09:32.550, Speaker F: Okay, yeah, makes sense.
00:09:34.600 - 00:09:35.350, Speaker G: Thanks.
00:09:37.240 - 00:10:29.050, Speaker E: I also have two questions, I mean, happy to also see any other results you find interesting. I don't want to derail with all of our questions, but I like it when people ask random questions too. So I guess one of them is can you talk more about the front running taxonomy and how you guys arrived at that? And another one is have you thought about maybe checking if there's any overlap between the transactions that you've seen that are replayable in the past and kind of insertable with the minor address? So the kind of last attack in the paper, if there's any overlap between that and privately mined transactions. So that would indicate that those are open front runnable transactions that are being taken by the Miner, I guess.
00:10:30.620 - 00:11:30.696, Speaker A: Yeah, that's a good question. Regarding the taxonomy, we basically extend the taxonomy from the previous Sokfc paper and we change the name a bit, which we think is more so like in the previous Sok paper. Basically it covers the front running and back running. So we basically extend it. We distinguish two kinds of different front running techniques. One is destructive, which means that you don't want the victim transaction can be executed successfully. And one is the cooperative front running, which means that you want the transactions that you are front running can be executed successfully.
00:11:30.696 - 00:12:21.790, Speaker A: Then you can kind of for example, in the Sandwich attack, you can first front run Evicting transactions. Then you perfect from another back running transaction. And you will also add in clock in transaction, which means that you dump a lot of transactions into blocks to delay the execution of your target transaction. So basically, we kind of want to solely summarize all the transaction ordering technicals. And regarding your second questions, if I understand correctly, you're asking whether these replayable transactions are from miners, right?
00:12:24.080 - 00:12:24.492, Speaker D: Yeah.
00:12:24.546 - 00:12:35.170, Speaker E: Like if any of those the replay transactions that you identified as being vulnerable were also not broadcast on the Mempool, if they were in that time period that you were.
00:12:38.420 - 00:13:22.700, Speaker A: Basically, we measured the replay transactions in the past two years, but we haven't recorded Mempool in this period. So unfortunately we haven't captured this. But during the week that we measured the privately mined connections, we evaluated our replayable algorithm on these privately mined transactions and we find only I remember it's 13 transactions that can be replayable. I mean, 13 privately mined transactions can be replayable.
00:13:24.980 - 00:13:31.152, Speaker E: That's super interesting. I'm sure those transactions will be interesting to dive deep into.
00:13:31.286 - 00:13:50.340, Speaker A: Yeah, we check a bit. All of them are like one inch transactions, so they are just one inch. Because the users of one inch can submit private transactions, all one inch transactions.
00:13:51.020 - 00:14:23.320, Speaker E: That makes sense. Yeah, I guess. Were there any results that surprised you or that are especially impactful?
00:14:23.980 - 00:14:54.580, Speaker A: Yeah. One thing interesting is like we case studied the top high profit replayable transactions which is present in table nine. We find that the BZX attackers and the other DeFi attacks, that these attacking transactions that can be replayed. So which means that if I was running this replay Acrosion at the time of attack, I actually can steal the profit from these DeFi attackers.
00:15:00.410 - 00:15:02.120, Speaker D: Why do you think it wasn't done?
00:15:04.570 - 00:15:12.166, Speaker A: I'm not really sure. Maybe I don't know. Maybe the attacks are not aware of such vulnerabilities.
00:15:12.358 - 00:15:22.780, Speaker F: Probably at least with what was it? What are they called?
00:15:25.390 - 00:15:26.170, Speaker D: BZ?
00:15:26.770 - 00:15:44.520, Speaker F: No, not VZX. The other exchange. Bancorp. The bancorp. White hack. So there definitely were some transactions that were actually being frontlined. So maybe there is some double counting here as well.
00:15:44.520 - 00:16:01.420, Speaker F: It is possible that some of these are actually bots, but it's hard to tell. But it is possible that some of these are actually bots who are front running. Someone else who is doing the same.
00:16:04.670 - 00:17:36.726, Speaker D: Feel like there might be an interesting opportunity to have a transaction replayer savior in a way. Or for all these hacks, they get front run for the one that can be replayable and then the money is given back like a white hat generalized front runner. That makes sense. One question that I wanted to ask Taiwa sorry, if I'm not pronouncing your name right when you count liquidations, I understand that currently for fixed spread liquidation, you basically compute what that fixed spread is depending on the amount of collateral and stop there in terms of estimating what the profit is. But shouldn't it make more sense to include the because when someone executes a liquidation and has a bunch of collateral on their hand, they usually want to get rid of it as fast as possible. And so wouldn't it make more sense to calculate the profit of the liquidation from after they've gotten rid of the collateral that they just got from the liquidation or if they don't get rid of the collateral? Scott was mentioning this to me yesterday to do something where you look at the deepest liquidity pool at the time and you calculate some kind of implied price slippage. Because in general, a lot of liquidators, while they do for example, make 5% from the fixed spread, they might pay 8% in slippage.
00:17:36.726 - 00:18:10.260, Speaker D: Where the actual profit from the liquidation? Or this whole endeavor is negative for them instead of positive. So where do you draw the line to estimate that? I guess a follow up question to that is how do you see the number that you currently have? I think it's around 20 million for the last two years. Let me find yes, these are my questions. Sorry.
00:18:11.430 - 00:18:56.100, Speaker A: Yeah, so currently we just simply calculated the difference between the I mean, we calculated the price from the price oracle of the liquidation platform, and we just count the difference between the debt you repay, the liquid data repay, and the collateral he buys, and we calculate the difference as the profit. So it's quite simple, but I think your metric is quite interesting. And yeah, we would properly consider it because it makes sense to consider.
00:18:58.790 - 00:18:59.106, Speaker G: The.
00:18:59.128 - 00:19:04.050, Speaker A: Other stuff, but not just calculate the difference as the profit.
00:19:06.810 - 00:19:07.462, Speaker G: Cool.
00:19:07.596 - 00:19:27.580, Speaker D: And in that same kind of vein, what do you expect are improvements to the paper that you're kind of looking forward to do as an improvements in some of the way you collect metrics or maybe in the heuristics you currently look at?
00:19:30.050 - 00:19:32.910, Speaker A: Sorry, I can hear you, Khaled.
00:19:33.330 - 00:19:58.840, Speaker D: Sorry. Are there any obvious improvements you're going to work on for new iterations of this paper? By improvements, I mean in the way you quantify some of these metrics or some of these numbers or maybe adding more nuance to some of the data if you think some of it would benefit from it. So is there anything there that you're thinking of?
00:19:59.210 - 00:20:43.800, Speaker A: Yeah, I mean, the thing you just mentioned is quite helpful to improve the paper, and also probably we receive some feedback regarding the private mined transactions. Like we probably may set up more nodes to listen from the peer to peer network to make the result more accurate. And we probably would back deeper into these private mining transactions to figure out what is going on there, whether the miners are actually involved in the MDV or PEV, something like that.
00:20:45.290 - 00:21:30.500, Speaker D: Cool. One tiny comment, and it's not strongly held opinion, but in your front running taxonomy, I would potentially consider changing the cooperative term because it sounds like relatively positive or something like this, especially when it's opposed to destructive. In fact, the cooperative can be increasing someone I mean, the cooperative type of front running can potentially cause more user harm than the quote unquote destructive front running. Right. In the sense where one is trying to snatch an ARB and the other one is increasing a user's slippage, for example. It's a tiny detail, to be honest, but that's kind of what came to mind when I was reading it.
00:21:32.870 - 00:21:44.360, Speaker G: Hi, Alex. Sorry for joining a bit late. Thanks for those very invaluable comments. We had a hard time to find the right words. It's so tricky for these terms.
00:21:44.730 - 00:22:03.886, Speaker D: No, but it makes sense in the way where the front runner needs the transaction that it's trying to front run to also execute. Right. So there's like an element of dependency or conditional, maybe? Conditional front running could be parasitic front.
00:22:03.908 - 00:22:09.920, Speaker E: Running, I don't know. You want to get more moral about it?
00:22:11.170 - 00:22:24.514, Speaker D: Constructive, parasitic. So anyway, Arthur, we were talking about your paper, and Kai was giving us a rundown of some of your results and answering questions.
00:22:24.712 - 00:23:04.340, Speaker G: Very nice. Yeah, we have good comments from Tina already on Twitter. Regarding the private transactions, one thing I'm not sure you discussed already, but on Ether Scan, you can actually see whether it's a private transaction or not according to Ether Scan, because Ether scan shows you time mined for a transaction. Time it took to mine a transaction. If it doesn't show a time, then I suppose according to Ether Scan, it's a private transaction because at least for those that we tested manually, we found that to be the case. So somebody could go and scrape the Ether scan data because they probably have bigger data sets, right, or ask them to provide.
00:23:07.030 - 00:23:41.680, Speaker F: I had another question regarding you had some metrics on gas usage of the front running tanks. So how did you measure or how did you quantify the gas used for back running? Most of these transactions won't be I mean, it would take some parsing to figure out that they were participating in back running. So how did you quantify that?
00:23:43.890 - 00:23:47.280, Speaker G: I would relate to Kahua. I'm actually not sure right now.
00:23:50.290 - 00:23:54.580, Speaker A: I'm not very sure. So which table or figure are you referring to?
00:23:55.510 - 00:23:59.730, Speaker F: You have a figure of gas prices?
00:24:07.610 - 00:24:12.520, Speaker A: Yeah, that's the one for the liquidations, right?
00:24:13.290 - 00:24:53.460, Speaker F: In this case, yes, for the liquidations. I guess for fundraising, it makes sense to take the price of the transaction that won the nev race. But in the case of back running, there is one transaction who is winning and maybe hundreds of others that don't do anything, which basically clog up the network a lot more. So I wonder if you took these in consideration, and if you did, how did you find that they are indeed part of the back running?
00:24:55.050 - 00:25:30.886, Speaker A: So, the way we test whether liquidation transaction is front running or back running, we just execute it at the top of the block. So if it's still executable, which means that it's based on the state of the previous block. Right, but if it's not, then basically it rely on the transactions that are executed before it was executed.
00:25:30.938 - 00:25:31.442, Speaker D: Right.
00:25:31.576 - 00:25:42.050, Speaker A: So then based on this methodology, we tested whether liquidation transactions is back running or fraternity.
00:25:45.450 - 00:26:40.230, Speaker E: I guess if you're just looking at guay and not total gas usage, it's probably a reasonable proxy to use the successful back running for all back running because you don't know which one's going to succeed. So you'll get some random sample of viable back running transactions as the one that actually succeeds. I think it would also be cool to see the total gas used for back running, but I think that's maybe also one thing that we want for Mev Explorer, because it's a very important kind of community relevant metric of how bad the blockchain is getting congested. And I also think the direction you mentioned about studying the private minor transactions more is obviously super interesting, and I'm sure there will be a lot more of those soon.
00:26:45.240 - 00:27:14.080, Speaker B: So something else that's interesting is that I believe Compound uses an Oracle mechanism where you bring a signed transaction to the chain. And so I think that that leads to a lot less back running possibilities since your transaction effectively is the Oracle transaction. So you can kind of see it in that chart that back running is more significant in Aave and dYdX, where that is not the case, where there's some sort of like a chain link or other Oracle update.
00:27:14.740 - 00:27:27.280, Speaker A: Yeah, we mentioned in the paper that Compound is different, that the average gas price of Compound background transactions are actually higher than the front running transactions.
00:27:27.720 - 00:27:29.060, Speaker D: That makes a lot of sense.
00:27:29.210 - 00:27:31.270, Speaker G: Yeah, that's cool.
00:27:32.840 - 00:27:51.530, Speaker F: It's a relatively new change, right. They changed it around March, I think, or something like that, but you can share the data. It would be interesting to see how it changed before and after they apply this change.
00:27:52.560 - 00:28:43.470, Speaker G: Nice, I didn't know this recent change. Thanks. So regarding the private transactions, I think Tina mentioned there's this Thai Chi network, so we couldn't really figure out what the contract does that we referenced for Spark Pool, but it seemed like they're doing arbitrage this two contract and it doesn't seem to be related to the Tai Chi network. So I think you tried that out quickly yesterday or today. So we're not sure whether it's related to the I mean, I also have to say we only mean the data here is only for a week of network data. It's not that much. Right.
00:28:43.470 - 00:28:47.950, Speaker G: So there might be many more insights if you look on the longer term.
00:28:54.470 - 00:29:37.090, Speaker E: Yeah, I think there's Tai Chi and also Archer is on main net presumably mining things. So there should be some maybe replayable transactions there eventually in addition to the one inch ones. I would expect at least if there's not, maybe they have protection against that or something. Yeah, that might be. The other thing that could be interesting, looking at replay protection, because a lot of these bots have been replaying each other's transactions for a while, so they have super sophisticated mechanisms to make the simpler replay algorithms fail. So I wonder if you guys ran into those or if that's like future work, because I know there's definitely some out there.
00:29:37.240 - 00:29:52.600, Speaker G: Yeah, the abstract no, the appendix. I think you wrote some Honey Pots example in the appendix for simple replay algorithms. You can maybe present that quickly or discuss.
00:29:55.530 - 00:30:43.910, Speaker A: Yeah, the basic idea is, like, when you replay a transaction, you just execute the transaction on the state of a previous block. So actually an adversary can actually issue two transactions that the first one changed the state of the contract or whatever. And then when the replay attacker try to replay the second profitable transaction, it actually will be executed upon after the first we call it first transaction, then it will actually lose money, which is the different result from the local emulation when the transaction is executed on the previous block.
00:30:56.880 - 00:31:14.796, Speaker E: Yeah, that makes sense. I think it's been super interesting seeing the games people have played with that kind of cat and mouse game. I know that those bots also griefed each other for a while to try to make each other waste gas on those kinds of Honey Pots and things.
00:31:14.838 - 00:31:19.350, Speaker G: Like that by playing with nonsense and gas prices. Right.
00:31:22.840 - 00:31:24.070, Speaker E: Exactly. Yeah.
00:31:36.990 - 00:32:08.760, Speaker D: All right, if you don't have any more questions, I expect for those of us that haven't necessarily dove very deep in the paper yet, that more questions will arise as we examine it very closely. But unless there are any other questions or Arthur and Kyra, unless there's anything else you kind of wanted to talk about or bring up on this call in particular to discuss with the people here, we could potentially move on to the next part of the.
00:32:11.610 - 00:32:27.040, Speaker G: Mean. I would I'm just really happy that you guys find that interesting and gave these great comments. I think it's a great community effort in the end, and we're looking forward to working on the FRPS as well.
00:32:28.370 - 00:32:50.790, Speaker E: Yeah, super interesting work. Thanks for presenting it. And definitely I think the more measurement we have on this, the better, because oftentimes I think otherwise this would outpace our ability to understand it very quickly. So it's a very interesting, fast paced, I think, topic of analysis.
00:32:53.450 - 00:33:13.774, Speaker D: Totally. I also look forward to non intuitive results that are backed by data. I think I'm excited about any of those that are found. Sorry. Let me know if you hear noise. I don't know if that's a little.
00:33:13.812 - 00:33:15.120, Speaker E: Bit, but it's not bad.
00:33:17.730 - 00:33:26.370, Speaker D: Yeah, I can't do much, I'm sorry. All right, Tina, should we move on to the next part of the agenda?
00:33:27.750 - 00:33:50.940, Speaker C: Sounds good. Yeah, I think it's highly related. That's why we grouped these topics together this time, so that we could dive deeper into the empirical aspects of quantifying the MDB realized. So go ahead, Alex. All right.
00:33:51.870 - 00:35:11.842, Speaker D: Yes, this is related, I guess. As some of you may know already, part of the initiative at Flashbolt is to illuminate the dark force, similar to Arthur's and Kyo's and Lee's paper, and to provide this information to the community at large. And so one effort to do this is to give the community a general dashboard linked to our data collection and inspection efforts. So I want to kind of show this dashboard really quickly, or at least the latest version of this dashboard, which is still being refined in its design and also in the the kind of data that's collected in it should be able to see. You can. All right, so we called it Mev Explorer. To preface all of this, the data is data that we've collected ourselves with Ethereum full archival node, and it is data from February 11 to today.
00:35:11.842 - 00:36:18.082, Speaker D: There are gaps in the data because there's still blocks that we need to run our protocol inspectors on, but it's good enough right now that it's worth presenting. So kind of diving deep into the numbers on the left here you have oh, and another thing to mention. So this data here quantifies the value that's extracted from the ordering of transactions. So in essence, most of this value today on Ethereum is through snatching an arbitrage transaction, snatching a liquidation where there's a sense of ordering where you want to be first to snatch that opportunity. It discounts hacks, and it discounts the quote, unquote, basic mev, which would be the value miners get from transaction fees and block rewards. Right. Which is also mev in the sense where it is value that the miner is getting for transaction ordering.
00:36:18.082 - 00:36:49.282, Speaker D: It's just quote, unquote, naive value, which we think are less representative. I mean, we could add these numbers here, but currently they're not added. And yeah, one last disclaimer. This is not trying to quantify the theoretical maximum value that could have been extracted. This is going back historically and quantifying the value that has been extracted. So the realized mev revenue. Yeah.
00:36:49.282 - 00:37:34.260, Speaker D: So first on the left here is a cumulative realized mev revenue. And you can see kind of how it has grown very quickly somehow with how DFI has been growing. The actual full number up there, or at least the number from around today, is $142,000,000. On the right here is the same but not cumulative. So you can kind of see the distribution. The y axis here is a power notch. It's the best y axis, to be honest, but when you keep it linear, it's not necessarily can't really see anything.
00:37:34.260 - 00:38:36.578, Speaker D: The whole point of having these two and having this one on the right in particular is because we kind of want to show that mev opportunities are not distributed evenly within blocks and the distribution is actually highly irregular. And that's, I think important to point out for several reasons, both for traders trying to extract this value and to understand the consistency of their profits. Similarly, from a consensus security perspective, the less this value is distributed evenly, the more this kind of instability around minor incentives. Right? Which is important to point out. When I dive deeper in Arthur's paper, I definitely want to compare some of our figures to yours. Definitely. This number here, 13 million is total fees traders have paid for unsuccessful mev transactions.
00:38:36.578 - 00:39:35.526, Speaker D: So reverts and the equivalent wasted block. So if you take the reverted transaction and their gas spend and then you take the aggregate gas spend and you divide that by the gas limit of Ethereum blocks, which is 12.5 million for that period of time, you get about 32,000 Ethereum blocks that could have been filled with these reverted transactions. So quote unquote wasted blocks from the perspective of the transactions weren't useful and they need to be recorded on the Ethereum blockchain. And then we kind of get into naive splits of some of the values above. So the revenue, quote unquote revenue split between miners and traders. So separating from that 142,000,000 figure, how much of that was transaction fees? So about 13.2
00:39:35.526 - 00:40:19.640, Speaker D: million and how much of that was the rest? Similarly, separating by strategies. So this is currently, again, it's mostly charged about only 5% are. One thing I should have mentioned is the coverage of what we're doing here. Let me actually pull up we cover a lot of protocols, but not all of them. So for example, dYdX liquidations are not counted in right now. So I'll give you more of that in a second. And the protocols interacted with.
00:40:19.640 - 00:41:17.754, Speaker D: So for the ARBs that go across several protocols, we kind of assign the value of the ARP equally between each protocol and we weigh the protocol interaction that way to get to these percentages. Right. So really quickly, I should have mentioned this is what's currently covered minus dYdX and ESD DSD is still to be covered. So I have a compound curve balancer zero x. And what we do, essentially, is with our Ethereum full archival node, we kind of go back in Ethereum transactions and we inspect its trace. And first we inspect it ourselves, and then we build an understanding of specific types of trading strategies. And we build that mechanically with an inspector that we then run against the rest of our data.
00:41:17.754 - 00:42:14.454, Speaker D: And that way we have a database of quote unquote mev transactions with their related revenue. Because part of the inspection has to do with understanding the value flow, which a lot of the time can be relatively complex. But can be simplified a little bit similar to if you've seen similar similar to their little table of evening out where the value is flowing. Basically, going back to Mev Explorer, we then have a little bit more granularity by showing the latest mev transactions that have gone through our system. That's it. For Explorer. We do have several more kind of like transaction views that would be interesting to people that want to drill a little deeper.
00:42:14.454 - 00:42:56.290, Speaker D: But in general, the idea for Mev Explorer is to cater to a very general audience that is just starting to understand what mev is and starting to nuance. But we do want to be precise, of course. We want to be rigorous right there's to strike here. So before I kind of move on to something related to that, I'd love if you guys have any questions on the metrics itself or anything else there.
00:43:03.890 - 00:43:23.160, Speaker E: I have one, which is so I guess these numbers are just starting, it looks like, in 2020. I guess there's obviously some sort of recency bias. Is that just because the old mev types aren't supported? Or is that just because there's a much more mev that started at that point?
00:43:25.930 - 00:44:16.354, Speaker D: I think there's much more mev started at that point, yeah. And it's also because the way our infrastructure currently works, running these inspectors is quite time consuming and having all that block data actually accounted for can be a long process. And so for the benefit of wanting to push something out that is meaningful, but at the same time, that won't take several extra months to do. We're kind of going with 2020 onwards. I also think most of the value has been extracted. I mean, if you discount hacks, most of the value has happened as DeFi has grown tremendously and as volumes has grown and as more liquidity pools have been created. And a lot of these ARBs are across sushi swap and uniswap, across Balancer and uniswap.
00:44:16.354 - 00:44:40.000, Speaker D: And all of these have seen the light of day relatively recently. Right. Balancer was in December, sushi swap was in December 2019. Sorry, sushi swap was in 2020, of course. So, yeah, this is the current idea. Do you think there's a significant amount of data that was before 2020 or like a value? Sorry.
00:44:43.490 - 00:45:08.310, Speaker E: I think that's a reasonable approach. Yeah, it's hard to say, but yeah, no, I think that's reasonable. I guess another question is have you thought about how to let other people contribute to this easily? And is there a way, I guess especially anyone on the call who's interested can either refine these or introduce new metrics?
00:45:09.850 - 00:45:50.390, Speaker D: Absolutely. That's another great question. So as we release this, there'll be this kind of document that I kind of. Went on afterwards where all the metrics and the queries would be explained. We'll have a public GitHub and we'll also have documentation for people to contribute. We definitely want this to be a community effort going forward. So while we're kind of seeing this as like a bootstrapping way of putting it out there and getting people's attention, I think we want, for example, Arthur's team and their numbers to be potentially included in or any kind of granularity there that we can add would be amazing.
00:45:50.390 - 00:46:59.420, Speaker D: For example, I also think a lot of protocols themselves will want they have intimate knowledge of their smart contracts and how value flows within it and they have an incentive to also help us have the right numbers on there. So I definitely want to involve the protocols. I'm thinking uniswap balancer. I also don't think it's a massive engineering project on their side. It's like really a side project and it's part of the ethos of this ecosystem of contributing to public goods. So there are definitely plans to involve the community and ideally, I think maybe too idealistic, but ideally we definitely want this to be just general community maintained effort where like Flashbots of course commits to maintaining it and commits human capital to maintaining it. But that generally it's kind of cruising thanks to a community effort and as a new protocol arises and creates new interesting opportunities that they're added in, that is definitely the vision for this.
00:47:04.000 - 00:47:58.960, Speaker G: Awesome. I was curious about what are the specific metrics or heuristics you use for each of the sources? Because for instance, from the work we've done, I can see that I have less confidence in the heuristics for sandwich attacks and arbitrage than for example, for the data from the liquidation events. Well, depends the back running, like identifying what is back running and whatnot is also kind of a heuristic. But I think it's always good if we can quantifying the error is probably difficult, but if we can give an indication of the level of confidence we have on the results totally. Have you written down the heuristics you use or the metrics you use like a bit more formally maybe sentences?
00:47:59.380 - 00:49:00.550, Speaker D: Yeah, I wouldn't say formally, especially given the academics on this call. I wouldn't say formally, but we have written the caveats to the metrics, to our approach. I think one thing that's important to say is this doesn't account for sandwiching attacks. So this is single transaction mev anything that's beyond single transaction, it requires more work on basically the heuristics that you guys applied right? Which is like looking at several transactions and deciding at a general level what is a sandwich attack or not, depending on the behavior of those transactions together. So currently it's limited to single transaction behavior. So anything that's over multiple transactions is not accounted for in these metrics. And so that kind of applies to all forms of back running.
00:49:00.550 - 00:49:23.960, Speaker D: I think it's important to quantify back running. But for back running, for liquidation, for example, you can still quantify the liquidation profit. I guess the back running element is more to discount part of the profit based on the gas that it took and the effort that it took to spam the network with backgrounding transactions.
00:49:25.920 - 00:49:35.036, Speaker G: That's pretty cool. Yeah. I wonder why you get so much more revenue than we do. Do you consider more exchanges or what.
00:49:35.058 - 00:50:31.308, Speaker D: Do you think is no, not really. I think our approach differs in the way where we kind of look at the traces of each transactions and there might be something there where the inspectors were running. There is extra there's there's, you know, if you if the I mean, depending on the rigor with which you code the inspector, I mean, you can have heuristic still valid, and that can capture a lot of value related to transaction ordering in a way that might be harder to do if you don't have access to the trace in your guys case. So I think that might be an answer there. Another thing that I definitely want to reiterate is these are estimates that we're kind of refining. There's definitely stuff there that might not be as accurate.
00:50:31.484 - 00:50:51.910, Speaker G: Yeah, I'm just looking at it would be awesome to find what we missed. Right. Because I think it's great that we have two different separate approaches of doing things and ideally we should try to merge towards the same numbers at some point because.
00:50:54.120 - 00:51:38.420, Speaker E: As like an obvious next step to make sure that all of the numbers that you guys released are kind of also included here. And maybe part of that is generalizing to multiple transactions. Maybe that's like a clear action item on the explorer side. And I think also the reverse of cross validating, I think is why it's important to have these independent efforts. Because the data is just like there's so large of a volume of data having worked with it for years now and the heuristics tend to be pretty complicated. And you're looking at so much data at once that I think it's just good to have multiple people trying it independently and then converge, I think is a good approach.
00:51:41.180 - 00:52:01.310, Speaker G: So that's interesting. Can you share maybe later right offline, like maybe any transaction hashes where you think that having access to the trace actually allows you to find a revenue that you wouldn't capture through, for example, simple risks that we use. That would be quite.
00:52:03.840 - 00:52:09.250, Speaker D: I mean, I wonder if Scott has any first ideas on that.
00:52:10.340 - 00:52:55.228, Speaker B: No, but I do think that as everybody's saying here, the comparison is by far the most important part. If you're in the dashboard there, you could just show every transaction hash is easily identifiable as the way that it was categorized and how much it made and what protocols we attributed that to. And I think that checking against another source that has a different methodology would be really important. These numbers are definitely not there's no perfect here because it's, I think, impossible to perfectly analyze every single ethereum transaction without fault. So I think that we just need to continue to drive towards reducing the miscategorizations. And I think, yeah, the only way.
00:52:55.234 - 00:52:56.208, Speaker D: We could do that is through a.
00:52:56.214 - 00:52:58.096, Speaker B: Couple of different approaches and ways of.
00:52:58.118 - 00:52:59.330, Speaker D: Thinking about these things.
00:52:59.860 - 00:53:01.330, Speaker G: We can try to be less.
00:53:05.700 - 00:53:06.016, Speaker A: A.
00:53:06.038 - 00:53:30.984, Speaker B: Really it's a really hard problem, but I think just even getting the numbers that are in the right direction or at least like relative months or what happens during price spikes or gas spikes, I think is just really eye opening to see how much the mev is spread out and is it seasonal? Even if the numbers aren't absolutely correct, their relative directions, I think, are some of the most fascinating bits of data.
00:53:31.022 - 00:53:32.090, Speaker D: We get out of this.
00:53:34.140 - 00:54:22.810, Speaker E: Yeah, I think one of the interesting things is there's a PH, I guess not, a research scientist in our lab who's developing a paper that requires an archive node right now. And our archive node broke like a few months ago and I've been too lazy to fix it, so I couldn't really provide him one. So he's had to go hunting because he really needs this per opcode data. And what he ended up having to do is get access to a service that's run by Morgan. So I was like, it's pretty hilarious that in the open financial system, the only way to get the data to answer the questions that you need on how this protocol is executing is to beg Morgan for an API key. So, yeah, hopefully we don't end up in that future. But it's definitely some parts of the system are trending there already, I think.
00:54:23.840 - 00:54:29.640, Speaker F: Isn't Alchemy providing free archive nodes?
00:54:29.800 - 00:54:56.436, Speaker E: He said something about Alchemy, I don't remember. So I think a lot of the ones that provide free archive nodes don't let you do per transaction tracing. Or he had some other or, like, don't let you do full opcode level tracing. That's what inferior says. Even though they have archive nodes, they claim not to store that data. They just prune it for some reason, which in my opinion is probably because they're going to have a paid service to charge money for it. That's the only reason I can see.
00:54:56.436 - 00:55:09.604, Speaker E: But anyway, I don't know. I will go back through our logs and I'm pretty sure he said he tried Alchemy, but if he didn't, I'll suggest it. He said he got it from archivenode IO eventually.
00:55:09.652 - 00:55:21.710, Speaker G: I know that the second time I had an archive node crashed or broke, and it takes like months, I mean, maybe weeks to sync it.
00:55:24.400 - 00:55:48.310, Speaker E: So he claims that he's been talking to people on Twitter and it might be possible that Turbogeth has fixed all these issues, but we're testing that now, kind of on a separate note. But yeah, we'll see. But either way, I think no matter what, this data is going to outpace our ability to be perfect about it. So the more Heuristics and the more accurate Heuristics we can get to the community, I think the better.
00:55:52.550 - 00:55:53.010, Speaker D: Cool.
00:55:53.080 - 00:56:39.250, Speaker G: So one thing we tried actually for I think it was the Arbitrage, not the Sandwich Attack Heuristic. Yeah. So Sandwich Attack Section 4.1. Heuristic five. So basically it took a magic number that the second sandwich transaction, which is the back running one, that it must be within 90% to 110% of the amount bought in the first one. These are just random numbers, right? I mean, we can take out any number out of our hands. So we also played around with this a bit, but we don't really know what is a good number and what is correct and what is wrong or what is less wrong, in a sense.
00:56:39.250 - 00:56:44.120, Speaker G: So, yeah, I would always call these results estimates because of that especially.
00:56:49.900 - 00:57:47.320, Speaker D: Cool. Awesome. All right, well, kind of moving on. Related from this and general quantification and the Flashbots research process is some of these metrics. And some of the metrics in Arthur's paper are related to our first research proposal, fr One metrics are floating. And given that this call is kind of collaborative and we're brainstorming together, there were definitely a few metrics there that I would love to get your thoughts on. And metrics that are mentioned in FRP One, this being FRP One, let me send the link.
00:57:47.320 - 00:59:07.060, Speaker D: Needs to be cleaned up a little bit, but purpose of this call? Yeah, so I guess let's go through some of these metrics as they are, and I would love for general comments if you have an idea. Some of them they're already done, some of them would be great to do. And we have people on this call here that are participating in our research efforts and I think might use some of these metrics. It would be great to get more information from them as well. All right, so gas metrics, I guess one of the metric we want to have is measure the gas saved with perfectly efficient hours in the sense where the gas that would be spent by the winning transaction when it lands on chain, you wouldn't have the same kind of bidding involved in gas price auction. And so the gas needed for these trades will generally be less, like the gas spend will be less. It'll be enough for the compute needed for the logic of the transaction itself, but it won't be iteratively bidding the gas price up.
00:59:07.060 - 00:59:59.800, Speaker D: So it'd be interesting to understand that. I think we can calculate that pretty easily. So I don't have any kind of strong question there. The other gas metric is the gas currently wasted by traders on chain. So in my eyes, that's the gas that traders use on reverted mev transactions, which we've quantified in here till Fee servants, and this is going from the gas pen. I wonder, actually, aside from reverted mev transactions, are there other wasted by users on chain?
01:00:05.590 - 01:00:07.700, Speaker F: Can you repeat the question, please?
01:00:10.630 - 01:00:13.400, Speaker D: Sorry. Is that any better now?
01:00:15.050 - 01:00:15.800, Speaker F: Yeah.
01:00:16.330 - 01:01:01.010, Speaker D: All right. So as part of our effort to understand the negative externalities of current mev extraction techniques and just the activity of mev extraction, we want to measure the gas that's quote unquote wasted on chain by inefficient mev extraction. Right. And so currently, the way I understand this metric is the gas for reverted mev transactions that was spent, but it was quote unquote wasted because these transactions didn't have to live on chain. And so I wonder if that's all the wasted gas or if there's other types of transactions that could be added to this metric.
01:01:05.270 - 01:01:06.478, Speaker E: Yeah, go ahead.
01:01:06.664 - 01:01:39.840, Speaker F: Sorry. There are the cancellation transactions, but I guess they are tougher to detect. They are not necessarily reverted, they're just paying to self or something like that, but they wouldn't be sent unless they were trying to cancel out some mev transaction that they figured won't be profitable or was already the mev was already extracted or it's too risky for them to try, so they canceled it.
01:01:42.470 - 01:02:17.290, Speaker B: There's also the fact that a flashbox transaction will save gas in the actual execution of the winning transaction. Since it no longer needs to check chain state, since it executes at the top of the block, it doesn't need to if you're arbing, let's say, uniswap against uniswap, if you submitted it as a standard broadcast transaction, you would need to go check the reserves and go do a bunch of math on it. But because you have a perfect knowledge of what that chain state is going to look like when you run, you don't need to perform those read calls to protect against landing improperly.
01:02:20.530 - 01:02:29.470, Speaker E: Yeah, and I think that's also the case if the miner takes the ARB or in any situation where they're partnering in like a trusted context with the bot.
01:02:29.890 - 01:02:31.060, Speaker D: Exactly. Yeah.
01:02:31.430 - 01:02:49.990, Speaker B: You don't need to say like, hey, is this position still available for liquidation? You just liquidate it because the fact that you're running means that it's there versus if you're bidding 100,000 GWe, you probably want to check first before you start doing a bunch of work that's going to eventually revert.
01:02:54.760 - 01:03:15.948, Speaker D: Totally. I think that's a great point. It's more for that first bullet point. Right. But generally it'd be in terms of general market efficiency. There's a significant gain in efficiency by having ARBs running through such a system. That'd be super interesting to point out.
01:03:15.948 - 01:03:34.210, Speaker D: And if there's a lot of savings from the user's perspective in terms of gas they spend and the related network savings, I think that's also super interesting. Alex, still, you unmuted yourself.
01:03:39.160 - 01:03:39.990, Speaker G: All right.
01:03:43.400 - 01:04:56.460, Speaker D: Moving on. The last kind of gas metric that I think would be really interesting to look at again, to further our goal of quantifying negative externalities of current mev extraction techniques and understanding if something like flashbox. I mean some of the numbers behind Flashbots. I think it would be trying to quantify the upwards pressure on gas prices from bot operator activity. I think that's maybe a little harder than the two ones above. But generally understanding how have gas prices been historically influenced by trading activity that could have been completely running in a separate channel and not in the kind of quote unquote public ethereum mempool and potentially graph the gas price over time versus what the gas price could have been with a system like Flashbots in place. But generally understand, I guess that's maybe even quantifying user harm to some extent when in periods of high volatility of ethereum price you do have extremely high gas prices that make the network unusable for a lot of users.
01:04:56.460 - 01:05:28.950, Speaker D: So I'm not entirely sure how to quantify that because gas prices you would have to look at the mempool as well to some extent and understanding the impact of different transactions on gas prices but also kind of make the assumption of if these transactions didn't exist, what would gas price look at? So maybe just taking the rest of the mempool and averaging it that way, but it does sound like mempool data would be relevant here.
01:05:32.470 - 01:05:47.910, Speaker E: I think another interesting metric might be like the throughput loss of each protocol. So like what percent of each protocol's gas is ARBs mev related activity and what percent is users that are using it over the longer term.
01:06:01.310 - 01:06:02.726, Speaker D: Sorry, what do you mean by throughput.
01:06:02.758 - 01:07:19.710, Speaker E: Loss where I guess throughput throughput penalty, right? Like, I don't know, let's say uniswap uses gas usage on just bots that are extracting mev. Then I guess that kind of implies that if uniswap as a whole system were to pay the same gas prices for operation on Ethereum and it had a different structure that had less of a percentage of that going to bots, then would the users get more throughput that way in this kind of competition for resources scenario? I see there's also some deeper research questions here about so if you look at market fairness in traditional continuous markets, people look at the negative externalities imposed on liquidity providers by snipers and HFT traders. But in that setting there's not really this gas market or there's not as limited of a resource people are competing for. So I wonder how these systems are different from a fairness point of view.
01:07:26.020 - 01:08:39.904, Speaker D: Sorry, I'm writing it down. It that's a great question, but like a deeper one in the sense of metrics and quantification. Right, cool. Maybe moving on to the network metrics, unless people have comments on all those, I definitely would like to be able to measure the PTP network overhead from back running and the overhead for a full node, the overhead for a minor and similarly wanting to measure the PCB network overhead from front running as well. I think it's part of quantifying the negative externalities on ethereum of this type of activity. One concern that I have of it is the gossip topology of the ethereum network is not uniform. And it might be hard, I mean, for each transaction, depending on how the person signing the transaction, how well peered they are, the network overhead might be different in some way.
01:08:39.904 - 01:09:01.950, Speaker D: So it feels like either you have to apply a lot of simplification to it or maybe there's a way to be rigorous about it given this extra layer of complexity. I don't have a clear vision on that, so I would love to get comments on that, actually.
01:09:09.100 - 01:09:46.420, Speaker E: Yeah, I mean, I think one possible way is to just run a node and kind of classify things into Mev Explorer in real time and measure what percentage of messages on the node relate to ARB versus non ARB transactions. We would also probably have to classify things that don't go into the chain in that case, because probably a lot of the overhead on nodes is like transactions that never get mined. So that could be another naive, like how much does each node waste on transactions that never get mined and how many of those transactions are also from addresses that do ARBs on chain.
01:09:49.640 - 01:10:27.830, Speaker D: Yeah, to get to your point, there's also just in the front running case, I think for me, the network overhead you want to measure is how much of an overhead are these gas replacement transactions, given that sometimes you have a lot for a specific opportunity. Yeah, interesting. How would you measure the waste from the node? Would you just measure its capacity in some way and see how much of that is dedicated to I'm not sure, actually.
01:10:31.320 - 01:10:31.876, Speaker E: Fair way.
01:10:31.898 - 01:10:32.084, Speaker D: Yeah.
01:10:32.122 - 01:10:37.248, Speaker E: What percent of the capacity that's being used is dedicated to this?
01:10:37.434 - 01:11:01.550, Speaker F: Maybe I think that the challenge is detecting them in real time or maybe recording them somehow for later inspection, but maybe just using gas as a proxy, like how many taking their gas cost as if they were computed and.
01:11:04.160 - 01:11:05.390, Speaker G: Something like that.
01:11:09.620 - 01:11:49.390, Speaker E: Yeah, I haven't looked at the maybe we should maybe someone else on this code knows or sorry, someone else on this call knows. I haven't looked at the data for spam back running personally on the node level. So I wonder if there's an obvious heuristic there. But the PGA one is pretty easy, which is that if you see a replacement transaction and the gas price is much higher than the average gas price in the block, that's almost definitely a PGA. We can probably come up with similar broad strokes heuristics that are pretty good for back running and spam also.
01:11:54.600 - 01:12:46.810, Speaker D: That sounds good. That sounds good. That sounds exciting to measure. I haven't seen a lot of measures of that kind of network overhead, maybe because it's really hard to have good measures for that type of stuff, but generally quantifying the impact on the PDP network itself sounds really interesting. As we expect DeFi to grow connectivity to proliferate even further. If you don't have solutions that come online like Flashbots or Archadao, we might get to scenarios in the future where it's very clogged in some way or the impact is actually quite important. So at least quantifying it and getting more understanding around that feels important.
01:12:46.810 - 01:12:57.496, Speaker D: And I guess that kind of leads to the next question. Right? Sorry, Phil, I was just going to.
01:12:57.518 - 01:13:27.750, Speaker E: Say I think it's super important for users, and I just think the only reason it hasn't happened yet is because no one's done it and we should definitely do it. Maybe this could even be part of Paper two, the ethical side of things, quantifying user harms of just like systematically enumerating all the resources that an Arbitrage bot uses that it's not paying for directly, and then coming up with metrics for each of these. So pretty much what we're doing here.
01:13:30.600 - 01:14:38.090, Speaker G: It'S not that hard either. For the private transactions, we recorded the network layer and okay, there are ways of optimizing also the data structure in how you store it. But this was quite a substantial amount of data we collected. And we collected not only the first peer that send us the data, but we recorded the first six IP addresses that send us the data. Because this could also be interesting for later usage, for example, for privacy, Gmization identification and so mean, okay, it depends also a bit on how many connections you build up right. How many network connections you build up on the PTP network. So I could imagine that those that do adversarial front running, back running, and so on, they will have more connections and therefore they will also negatively affect the network, meaning they will make other people work more because they kind of request more.
01:14:38.090 - 01:14:53.810, Speaker G: So there's also the impact of kind of we call them spy nodes or like spider nodes because they have a web of connections. Right, but this might also be something to consider here.
01:14:56.260 - 01:15:27.240, Speaker E: Yeah, we ran like six or seven nodes like that for the Flashbots original kind of paper, and I think we ended up spending tens of thousands a month in AWS bandwidth. Which means that since we were probably mostly peered with AWS nodes, that on the other side of it, like 255 people per node spent like 10,000 over 255 of bandwidth sending stuff to us on average. So yeah, I think you can probably even quantify the cost, at least for cloud nodes.
01:15:28.720 - 01:15:36.350, Speaker G: The problem is because everybody's on AWS, you have to be on AWS, otherwise you're too slow. Right, that's the irony of it.
01:15:38.320 - 01:15:39.470, Speaker E: Yeah, exactly.
01:15:43.220 - 01:16:33.040, Speaker D: Since we have ten minutes, I'm going to jump to some of the other metrics, but thank you, Arthur, that's super helpful. I wrote down what you mentioned. Yeah. So in terms of bot metrics, definitely want to measure bot activity in a more granular way, like has been done in Arthur's paper. So want to break down the realized MVZ by protocol and by strategy. That's kind of something we were doing with Explore, right? Very naively, but still doing so. And the two other bot metrics that I think are interesting is one is now looking at it not from a general ethereum user perspective, but from Bot operator perspective.
01:16:33.040 - 01:17:30.210, Speaker D: It'd be interesting to look at the efficiency of Bots. So how often do they fail or burn gas versus succeeding? I guess it's another perspective on looking at wasted gas and that type of stuff and understanding if there's a way to make the market structure more efficient. Right. So it goes back to that first metric of measured gas save with perfectly efficient art. Having looking at that would be super interesting. And I can think of we have known bot addresses and we can that have failed and I think we have some heuristics there about how we can use those to arrive at some measure of efficiency. So naively, I think this is potentially something we could do there.
01:17:30.210 - 01:18:35.168, Speaker D: And then related to Bot metrics is also just understanding the breadth of our analysis. That's particularly hard in the sense where it's not static. Right. As more protocols come on and sometimes as they tweak a parameter, that opens up a strategy and there's a new bot spun up to do that. But generally it's related to what Arthur said before about having some extent an error metric as well to understand what kind of boundaries we're looking at when we're measuring these things and the percentage of coverage. So we kind of did something that Phil mentioned there currently, where we cross reference some of the data we're running and the addresses we have database with the top hundred addresses that are using gas tokens. And we try to see if with the assumption that most of these addresses must be bought operators.
01:18:35.168 - 01:19:09.040, Speaker D: And then we try to see if those have been quantified. If they're not in our database, we kind of look at them manually and see if there's anything wrong with them or not if there's anything wrong with them, but if they should be added to our database in some way. So if we should code up an inspector to include that type of strategy. That's the way we're kind of currently naively approaching this. And I would love to think of more advanced ways of looking at basically coverage metrics in general types of coverage.
01:19:11.940 - 01:19:12.256, Speaker A: Yeah.
01:19:12.278 - 01:19:49.230, Speaker D: And lastly, seven minutes left. Kind of minor metrics I think is particularly interesting. I think these two first, the minor latency is a little bit hard to both. No flashpots. I mean, there's no minor latency. I guess in that case, given the given the system and with Saturation, I think miners can get overwhelmed by transactions and in that case, I'm not sure how the infrastructure handles that and it might be a case by case basis. So this is maybe less relevant as of today.
01:19:49.230 - 01:21:21.740, Speaker D: This is also maybe less of a metric question. I think one thing that I'm particularly interested in and is more particular to Flashbots is trying to understand in a world where Flashbots is adopted, what do minor revenues look like? And while it is thinking about Flashbots and maybe less of a wider metric, I think it's important in the sense where you're trying to understand minor revenue is a proxy to understand also network security to some extent, right? Like miners need to be incentivized to secure the ethereum network and contribute to consensus. And so there are several competing forces within that. So if in a world of Flashbusters widely adopted gas prices are less high and so miners make less money from that, at the same time some block space is freed up because you don't have these reverted transactions as much anymore. So there's new activity from that. At the same time, the ethereum network is more usable because you don't have these periods of high gas price that make it unusable for smaller users. And at the same time you have some type of increased mev activity to a system like Flashbots because of the no loss situation you're in and also because of the Seal bid auction where people will be incentivized to bid as much as they can while still being profitable.
01:21:21.740 - 01:22:53.610, Speaker D: So there's these kind of different things to think about there where it'd be interesting to have a rigorous analysis of this and arrive at maybe some it sounds like it might be a scenario analysis to some extent given different levels of adoption and also given different level of what the ethereum network is used for and these type of things. But I thought that was a cool question to also look at for FRP one if you guys have comments on that one and if not that's kind of going over some of the metrics and things that I look at and have been listed on Flashbot's first research proposal. And as you can see in the side question here, arthur data collection efforts, well, maybe not combined as we were talking about before, but definitely integrate the data in some way or generally contribute to each other's efforts, I think that would be super cool. So both for this research paper, but then more broadly, I definitely want to make sure the flashbox organization, anyone from the organization on the research front, et cetera, can contribute from the data we're collecting. So making sure there's a way for people to interact with it, contribute to it, and use it for their own research.
01:22:59.490 - 01:23:43.840, Speaker E: Sounds good. Thanks, Alex. I think the one thing I would mention from my end that brainstorming might be interesting is also per protocol metrics of externalities and congestions. It might be a way to force the DAP developers to adopt good design. So showing like when you move your Oracle to this new design, it decreases the amount of failed transactions. And your griefing factor on the network or whatever goes down in terms of mev. So just like creating a metric for how bad of a citizen each protocol is in terms of the mev it exposes and how problematic it is in the different types of exploitations I think would be cool.
01:23:46.610 - 01:23:48.720, Speaker D: I love that. Thank you.
01:23:49.810 - 01:23:52.660, Speaker C: That'll be great for paper too, as well, right?
01:23:54.470 - 01:23:55.220, Speaker D: Yeah.
01:23:58.070 - 01:25:01.270, Speaker C: Well, I think we're near the end of the call, but definitely anyone who wants to stay, feel free to stay and chat more. I think one thing is I want to ask two questions. I want to ask the general, well, folks here who are interested in research. First, is there any Rust developers here we would love to add to the mev inspect program as much as possible, as soon as possible. And so if anyone here you are a Rust fan or you well, it's never too late to get your hands on it, right? As some folks who wrote this stack in Rust, I believe it's a superior language in all way, shape or form. There was a tweet about it earlier today. Yeah.
01:25:01.270 - 01:26:11.194, Speaker C: So I think any meaningful contribution or attempts will be super interesting. And also I think it would be a really cool crash course kind of on for folks who's relatively new to DeFi Arbitrage in this world to be able to get your heads on. Yeah. And if you know any great Rust developer in DeFi, feel free to bring them to our discord. The second question I have here is, I think for our next rows as well as research workshop, now that we alternate on Thursdays, we have a couple major topics we want to kind of go deep dive into. Similar to today will be actually one is the auction mechanism where I think we have so far flashbots alpha being live. The somewhat sealed bid nature of the auction already starts to invite questions from the mev searchers these days that there are a lot of challenges on the merits of Seal bid auctions.
01:26:11.194 - 01:27:06.330, Speaker C: Phil, you may have missed some of the fun, interesting feedback we've gotten from the mev searchers that are live at the moment and bot runners. A lot of questions on the merits of a Seal bid auction and their tendency to want to get an edge and to get as much information as possible. So that's actually one of the things I pinged Seriah about that was super interesting from an empirical standpoint. Some of the more nuanced questions, alex was communicating a lot more, interviewing and surveying some of the bot. So yeah, exactly. They want more information. So there is always a tendency to want to reject a system where they can't have an edge and go to places where they can have an edge.
01:27:06.330 - 01:29:03.410, Speaker C: And also that in a world where adoption is a gradual facing and there are multiple options there. So they essentially place this all over the place on PGA, in other network and trying to collude with miners and mining pools that is not yet using Flashbots and also placing bids on Flashbots while also trying to see if they could collude with mining pools in getting more information about what are the price like, the pricing of the bits are on flashbots because at the current moment, there is essentially a more loose trust or something there. And miners and mining pools at the moment can still see these information. So it's a very various kind of interesting information or behaviors that we're observing already within the couple of weeks of Flashbass alpha being live. So those are things that are interesting to kind of think a bit more and that's also related to kind of the last FRP one sets of metrics in terms of what would be the world like without Flashbots and with Flashbots and the likes of the system. So I think in general like auction mechanism literature survey, auction mechanism design and examine some of the behaviors we are observing and feedback from the players in the ecosystem I think constitute an entire session that we would love to dive deeper into. And another session that we would like to host is on privacy.
01:29:03.410 - 01:30:45.590, Speaker C: Essentially, I think there are various approaches, but on ethereum potentially the options are limited in terms of achieving full privacy, namely the original Flashbot spec was specifically suggesting SGX research and Phil can share a bit more details into it. So I think we would also love to host another session that's in depth about essentially transaction privacy as well as the various available solutions out there out of the box and kind of theoretically what would be the ideal approach given the different architecture. Those are the sessions that I could think of, and I would love to kind of hear more from you guys in terms of which directions or in the next weeks beyond. You would love to kind of attend and be part of and also any other kind of chunk of questions that you would love to kind of dive deeper with a more in depth session like this. Yeah, so those are essentially open questions for us all here and I wonder whether essentially Seraya, you have anything you would like to share and post as questions for our future sessions for the folks here, since you are leading the charge on the survey of the auction mechanism.
01:30:46.410 - 01:32:13.310, Speaker H: Yeah, so it seems to me that at least in serving auction literature, that any idea of sort of collusion or cooperative strategy is kind of ignored altogether in almost every sort of auction marketplace. And the reason is that they really only care about strategy proofness and then they rely on, I think both the legal aspect of it as well as sort of the competitive nature of it always incentivizing someone to break from any cooperative strategy. And so I think the ad auction literature kind of completely falls short of any consideration of conclusion or even small conclusive sets, I think they just completely ignore it altogether. And so we may have to sort of engineer that theory on our own, which I think, though, is probably going to be along the lines of preexisting sort of theoretical auctions like this Hector Mosh auction I just brought. Up and sort of just playing around with revealing enough information to make, like you're saying mev searchers happy, yet still getting at least close to truthful bidding.
01:32:14.790 - 01:32:55.482, Speaker D: I mean, that's super interesting. It might help the transition for Searchers because definitely a lot of them, especially the very successful one, a lot of their edge is in being really good at bidding and that gas auction that is on ethereum. And so asking them, hey, just forget about all this edge that you have and come to our system where you're running in a seal bid auction and you don't really know how much you should bet. Yeah, you're right. All right, thank you. Great. Thank you all for joining and for listening.
01:32:55.482 - 01:33:05.650, Speaker D: Thank you, Arthur Kaiwa, for presenting your paper. I'm definitely going to be reading it even more carefully. I'm still going through it. I'll be sending you more comments separately.
01:33:05.990 - 01:33:06.834, Speaker G: Awesome.
01:33:07.032 - 01:33:15.170, Speaker E: Yeah. Thank you so much, guys, for presenting. And thanks, everyone for coming. And thanks, Alex, also for all the great discussion on Metrics and Explorer.
01:33:17.050 - 01:33:18.406, Speaker D: All right, thank you all.
01:33:18.508 - 01:33:49.070, Speaker C: And also ping me on discord or leave a note on GitHub. I think a lot of you guys have written great posts on this subject matter and have been, especially Alex, the other Alex, that we would love to see how we could dig further, as well as some of the other researchers here. Just get up or discord, whatever, just open an issue or leave a note.
01:33:50.050 - 01:33:50.702, Speaker G: Cool.
01:33:50.836 - 01:33:52.958, Speaker E: Thanks, guys. See you next time.
01:33:53.124 - 01:33:54.634, Speaker C: All right, bye bye.
01:33:54.682 - 01:33:55.182, Speaker D: Thank you.
01:33:55.236 - 01:33:57.438, Speaker G: Have a good day. You too.
01:33:57.524 - 01:34:00.570, Speaker D: Bye, guys. Bye.
