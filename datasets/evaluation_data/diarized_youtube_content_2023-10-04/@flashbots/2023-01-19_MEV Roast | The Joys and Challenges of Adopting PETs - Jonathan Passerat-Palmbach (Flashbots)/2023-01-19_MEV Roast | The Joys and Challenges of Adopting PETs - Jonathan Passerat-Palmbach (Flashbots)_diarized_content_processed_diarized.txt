00:00:03.530 - 00:00:09.466, Speaker A: Hello everyone. It's going to be a bit high level. I'm not going to go into too much details.
00:00:09.466 - 00:00:37.910, Speaker A: It's mostly about giving you a flavor of what's possible as of today. I think it's going to echo what Robert just showed you in terms of runtime and challenges to get things up and running and what we could expect or what we could do to improve the current situation. So, first of all, pets, privacy, nancing technologies.
00:00:37.910 - 00:01:14.834, Speaker A: This representation is like something that we've agreed upon in the pest community, basically splitting them into three different categories depending on what is their main feature. Do they protect the privacy of the input, the outputs, or are they more of a governance technology? And you see that some of them are kind of like overlapping. But what I think is important to understand from this representation is that we tend to mix sometimes the notion of privacy, confidentiality and all these kind of things.
00:01:14.834 - 00:01:42.780, Speaker A: So in the world of pets, we try to stick to using privacy for what we mean as like output privacy. So technologies that guarantee that the results of an application remain private or at least do not disclose any information about the input. So that's kind of like the guarantees that differential privacy would give you, for example, which we're not going to address in this talk.
00:01:42.780 - 00:02:04.100, Speaker A: And similarly, you've got overlapping text I'm not going to address NORX because I think it's probably like the most familiar piece of technology you have in this community. So we're going to stick to the technologies that provide confidentiality. Mostly going to cover Tes, MPC and she for that matter.
00:02:04.100 - 00:02:26.620, Speaker A: So in terms of the applications of privacy at Flashbots, I think a good reference to the use cases that are listed on the left. So mostly we're going to be working at applying privacy to these use cases. It's very high level.
00:02:26.620 - 00:02:37.914, Speaker A: I think Robert's presentation was like one component within the other. Flow auction, for example. Quintess's talk could give you more details.
00:02:37.914 - 00:02:57.694, Speaker A: And I think in general, it's worth exploring the writings website as well as the forum to get a better understanding of each of these problems. But that's basically the context to which we want to apply these privacy texts. And basically it's not about providing privacy.
00:02:57.694 - 00:03:48.658, Speaker A: I think probably a lot of us are very keen on privacy as a right and things, but we need to see beyond that. We need to understand that privacy is actually a mean to unlock new use cases, but it's also a way to increase the decentralization of the services that Flashbots have been providing for a while and that the community has trusted Flashbots to provide. So by injecting privacy in these services, we're hoping to increase their resilience, increase their decentralization, but also to foster collaboration between actors that would normally not have the incentive to collaborate into performing a joint task together that might even be considered to be in competition when trying to perform a task.
00:03:48.658 - 00:04:17.150, Speaker A: Like I'm thinking of block building, for example, where if we want to hope to have distributed block building, we need block builders to share information that it would not have any interest to share normally in terms of the requirements that we're expecting from these technologies, we want a lot of things actually. We want speed in order to not be too constrained by the flawed transactions. We want to be able to scale to large number of users.
00:04:17.150 - 00:04:39.222, Speaker A: Of course we want the maximal security guarantees for the minimal assumptions and especially we want well defined assumptions. That's going to be important in some of the text we're covering. We'll see it's not always extremely clear what is the threat model, maybe something we've not addressed too much today.
00:04:39.222 - 00:04:59.034, Speaker A: When you start applying these kind of like technologies, MPC and FH in particular, you might lose some precision in the calculations you're making. My take is that this might have an impact and we need to be extremely aware of that too. The one thing we don't need, and that's good because tends to be changing as well.
00:04:59.034 - 00:05:31.714, Speaker A: Sometimes to provide is forward secrecy. So we know in the case of trying to implement an encrypted mempool for example, or private smart contracts like Andrew presented, everything that we are manipulating as like, sensitive data is short lived in the sense that it doesn't remain sensitive forever. Once the computation is done and the information has been processed and the transaction lands on chain, it doesn't really matter if this computation can be disclosed afterwards.
00:05:31.714 - 00:05:54.510, Speaker A: So at least I don't think we need forward secrecy. In terms of the roadmap itself, I think you've understood by now that in the short term we're pretty much focusing on Intel SGX. Again, I want to make it very clear that we're not like SGX fanboys.
00:05:54.510 - 00:06:27.766, Speaker A: We're just like taking the pragmatic stance of what is the most easy thing to deploy and the most ready thing to deploy today that can bring us closer to the end game. But definitely our goal is to phase out SGX and even maybe like hardware dependencies in general and you've just seen like a first item in doing so with Robert's presentation on the background by any means. That doesn't mean we committed to this path either.
00:06:27.766 - 00:06:44.590, Speaker A: We're exploring multiple paths in parallel. You're more than welcome to join us in this journey. Without further ado, just diving into each of the texts, we've heard a lot about Te, so I think a great reference to that is of course, Andrew's presentations.
00:06:44.590 - 00:07:06.574, Speaker A: SGX as a te is by far like the most popular of the Tea as of today, which is good in a way because it's received a lot of scrutiny. You've got lots of papers about it. So just to summarize what it is, technically it's just a set of CP instructions that allow you to create enclaves.
00:07:06.574 - 00:07:41.310, Speaker A: What we call like initially what was trusted portion of an application and that I've grown into sometimes being like whole applications containers and maybe not even like whole virtual machines. And the idea is that you'll get total confidentiality of your data during the computation. So anything that happens within this enclave cannot be seen by any external party of the system, not even like the root administrator, the club operator, if you're in a cloud setting.
00:07:41.310 - 00:08:14.010, Speaker A: So it's very much like this protected environment. And we know that SGX received a lot of bad press, it's been broken over and over again by various attacks, lots of academic papers in this direction. So I guess the interesting question there is why? So we've started to touch upon it, as in it's received a lot of scrutiny because it's been one of the first Teas that was available and one of the easiest to get your hands on.
00:08:14.010 - 00:08:46.530, Speaker A: It was just an SDK just extending like C, so it was relatively easy for people to start developing with it. My take on that is that the problems that we see repeatedly with SGX are mostly due to the way it's been designed. So it's been designed by intel on top of Intel CPUs, which means that it's sharing various components like architectural and microarchitectural components like cache lines for example, that allows of the side channels effect that we've seen against SGX.
00:08:46.530 - 00:09:32.142, Speaker A: But it's also sharing other components of the system, like the memory address bus, which make it pretty hard to defend thoroughly against some of the attacks. So the problem with that is that it was maybe not like what intel wanted to see from these technologies that initially was not maybe meant to have this kind of application. So we've seen a move by intel recently since the Ice Lake generation of Intel CPUs that answered one request from the community to have larger enclaves.
00:09:32.142 - 00:10:04.094, Speaker A: You might have caught from Andrew's presentation that the first generation of SGX enclaves could get as much as like 128 megs of memory. And intel decided to lift these constraints and they're like multiple hundreds of gigabytes of memory. The trade off for that is that we've lost the integrity in memory, so we don't really have the same security guarantees from this previous generation of SGX to the new one.
00:10:04.094 - 00:10:52.800, Speaker A: And the way intel defended against that is that they repurposed SGX as a cloud only technology or a cloud first technology. So it's not meant to be massively deployed on desktops or laptops anymore, which of course hurts decentralization and maybe pushes us even further to try and find alternatives and what could they be? Of course we're going to see the pure software crypto, but maybe in the meantime, before they get ready for the prime spot in production, we can also explore other Teas that might not have the same design flows as JX has. So I'm thinking for example Keystone, which is very interesting project put together by Don Sung and the team.
00:10:52.800 - 00:11:23.074, Speaker A: Maybe we might want to design a custom enclave if that would be even more challenging and resource demanding. I guess one notion that would be interesting as well in the context of decentralization is to try and see whether a heated regenerate network of tes would make sense, bearing in mind that the main challenge is that they do not have a common threat model. So it doesn't mean the same thing to have an enclave.
00:11:23.074 - 00:11:40.134, Speaker A: We've kind of call it an enclave running in SGX or Sed, for example, let alone TDX and all the other alternatives. So that could be an interesting challenge to see if we could define this common threat model. Right? Switching to software crypto.
00:11:40.134 - 00:11:51.998, Speaker A: You've just seen a very interesting application of secure NPC with Robert, so I think you've understood the global ID. So you have input data. Here the five.
00:11:51.998 - 00:12:14.306, Speaker A: And we're basically splitting into different shares. And due to the homomorphic properties of these shares, we're able to apply operations before recombining these shares to obtain the result you would expect. That kind of works well until basically you need more complex operations.
00:12:14.306 - 00:12:38.430, Speaker A: So when you start from a general purpose secret sharing, either like additive or shy me secret sharing protocol, you can do these kind of things. Now if you want to get to better performances. Usually what happens is that you want to design custom protocols, right, as soon as you want to start to do custom functions.
00:12:38.430 - 00:12:59.254, Speaker A: And the interesting thing when you start designing custom protocols is that if you look in the literature and in this presentation, most of the examples are drawn from the privacy preserving machine learning literature. There's two reasons for that. That's my background, so I've got a few papers and readings on that.
00:12:59.254 - 00:13:34.590, Speaker A: And the second is that it tends to be like very large problems that requires a lot of computing time and memory. So they tend to set like a higher bound, an upper bound of what you should expect from the behavior of these custom protocols as well. And in particular, what I like to note in these two tables that you've got on the right is that you can see that MPC has to be considered into different settings.
00:13:34.590 - 00:14:08.090, Speaker A: We tend to just like in experiments, just report the things that we've done that we've run on our local infrastructure. But if we think into the context of Flashbots or any other decentralized application, we should be more interested in how NPC applications behave in one setting. So across the internet with longer latencies, the reason why is that the main bottleneck of NPC protocols is communication.
00:14:08.090 - 00:14:51.660, Speaker A: So basically the game in NPC to improve the performances is to try and address this communication bound problem. So either reduce the amount of communication that are required or redesign the protocols to make sure that they're generating less data or have less rounds of communications, for example. Another hope with this regard is that in this PhD studies, you have a diagram on the right.
00:14:51.660 - 00:15:37.350, Speaker A: You can see that with the right custom protocol, with a problem size growing, the author has been able to basically switch the natural problem of MPC from being communication bound to being computation bound. So it means that with the right designs, we can potentially go beyond this communication bounds problem with MPC and then focus on accelerating the computation and kind of get the best of both worlds. That's kind of like what I was hinting at in the chat before when Robert was making this suggestion.
00:15:37.350 - 00:16:09.442, Speaker A: There's quite a few protocols in NPC that also try to use homophic encryption natively within the protocol to reduce the amount of communication rounds that are required. So we'll see that it's pretty important to not see all of these technologies as standalone, but as like pieces of a bigger jigsaw, maybe switching to homophobic encryption. Okay, so same thing.
00:16:09.442 - 00:16:34.310, Speaker A: I'm not going to go into nitty gritty details about how Fhe works. We could do that another day maybe, because we're definitely going to go over time, way over time if we do that. What you get from Fhe, if you've not code that yet, is that you're able to perform alphabetic operations on ciphertext.
00:16:34.310 - 00:17:16.098, Speaker A: And when you decrypt the ciphertext, if you've done things right, you basically obtain the same results you would have by performing the same operation or sequence of operations on plain text. So you once again have these problems expressed as circuits, as we have in Snarks and as we've seen in the MTC context as well. What's interesting with homophobic encryption, what is it desirable, especially in our kind of fields, is that the security assumptions of most schemes are relying on lattice based cryptography, which is believed to be post quantum secure.
00:17:16.098 - 00:17:41.280, Speaker A: So there's a lot of conversations about the state of cryptography and quantum computers coming and breaking everything. At least we have stronger guarantees. In the world of homophobic encryption, we have different types of schemes that tend to manipulate different data underlying data.
00:17:41.280 - 00:18:00.258, Speaker A: So schemes like focusing on manipulating binary data like just booleans others. Typically, like in the, in the machine learning space, there's a strong appeal for schemes manipulating floating points, but also integers. They can be combined.
00:18:00.258 - 00:18:21.742, Speaker A: So again, you can come up with more interesting protocols by doing these kind of things. There's two things that we need to pay attention to when dealing with holistic encryption. I think Justin hinted at the fact that there is this notion of depth in the circus we're building.
00:18:21.742 - 00:18:54.550, Speaker A: So what's to be considered really is like what we call the multiplicative depth. So what is the maximum number of multiplications we need to go through in order to reach the output? Because in homophobic encryption, in order to protect the ciphertext from being revealed, basically we're adding noise to this ciphertext. And whenever you perform operations, the noise from the two ciphertext that you're adding or multiplying combined.
00:18:54.550 - 00:19:13.150, Speaker A: When you perform additions, the combination of these noise is like relatively easy to keep under control. But when you perform multiplications, the noise grows exponentially. So that's why you want to make sure that the multiplicative depth of your circuit doesn't go too far.
00:19:13.150 - 00:19:29.886, Speaker A: Otherwise you have to go through an operation called Bootstrapping to reset the noise of your circuit. But that introduces extra computational overhead. And the second point that we need to pay attention to that is often not evokes.
00:19:29.886 - 00:19:54.300, Speaker A: We just tend to focus on the Runtime ciphertext tend to be big with homophobic encryption. In some schemes, the ciphertext to plaintext ratio is like several orders of magnitude larger, which is called like, ciphertext expansion. So it's definitely an area that we want to pay attention to with Fhe in terms of what's possible of today.
00:19:54.300 - 00:20:16.062, Speaker A: I've got two, again examples drawn from the privacy preserving ML literature. So on the left you've got a pure software approach that's drawn from a company called Zama. They are building an Fhe library called Concrete.
00:20:16.062 - 00:21:09.234, Speaker A: And they've introduced a very nice trick when they're doing this Bootstrapping operation that I was mentioning before, they make the most of it by, in a way, introducing lookup tables in there that allows them to encode very complex operations. And as you can see, by doing so, they came up with a scheme where even for relatively large number of operations in a neural network evaluation, they only have a 100 x overhead, which is pretty impressive compared to what we were used to before for homophobic encryption. And on the right we've got the hardware version of that, which is a program by DARPA called Deeprive, whose goal was to reduce the overhead of homophobic encryption to just ten x.
00:21:09.234 - 00:21:27.750, Speaker A: So just a single load of magnitude. As far as I know, it's going pretty well. I was pretty pleased to see that as of yesterday or two days ago, a company called Duality was awarded a contract to enter phase two of the Deepry program.
00:21:27.750 - 00:22:22.246, Speaker A: So that means that at the same time we have innovation on the schemes and also innovation on the hardware. You know, makes me pretty confident and make me want to take the same bet, as Justin was saying before, that it's not crazy to think that app specific Fhe is either within rich or it will be soon, in a matter of like a few years, I think. And again, bearing in mind that these workloads are probably, like, way larger than what we'll see in the type of applications that we've discussed today, except for maybe distributed block building, which might be in the same order of magnitude in terms of how much data it needs to manipulate and how many operations it needs to go through in order to get to results.
00:22:22.246 - 00:23:07.238, Speaker A: Okay, so what's next in she? So, as we've seen working on scheme level optimizations, either improvements like the lookup tables by Zama or even brand new schemes that would ideally work on reducing the size of Stifetex. I think hardware acceleration is coming. You've got this deepry program by DARPA and something pretty interesting that I've seen recently, there's a demo of a fully homophobic chip like imagine like a CPU that is fully homomorphic, which could be interesting because we've had this discussion at Defcon around FH EVMs, so maybe like some inspiration to draw from there.
00:23:07.238 - 00:23:25.514, Speaker A: That reaches like 250 MHz, which is not too bad for those of us that was used to pension two S. And these older CPUs kind of goes to these kind of performances. And I'm sure that I think Sam is in the attendance.
00:23:25.514 - 00:24:10.786, Speaker A: And you could probably add to this conversation about what can be done in terms of hardware acceleration. Another thing that's pretty important tooling and especially compilers that will make all these drinks available to most developers and something that is very often discussed as well in the ZK community is how do you perform security audits when these tags tend to be more and more complex. I think there's a discussion, ongoing discussion on how do you audit like Zkvms and are we confident that this could work? Okay, so maybe a final remark is that you see this trend of compilers.
00:24:10.786 - 00:24:37.138, Speaker A: I think it's getting pretty obvious that it's important to improve the UX for developers but also to make sure we make the most of the hardware. And what I like is that we see a similar trend in different privacy texts. You start to see compilers, people working on compilers and DSLs for ZK but also for homophobic encryption.
00:24:37.138 - 00:25:22.226, Speaker A: And in particular you've got like three, I think, very strong candidates in Microsoft Eva concrete from Zama that we discussed and a transpiler jointly worked on by Google and duality transforming C into he circuits. Okay, just a final reminder that we shouldn't see these technologies as standalone components. If you remember these toys from Power Rangers, the small ones, they were pretty useless on their own, right? But if you put them together, they make the big robot on the right, whatever was his name, and it's very much the same thing here.
00:25:22.226 - 00:25:36.258, Speaker A: If we consider pets on their own, we're not going to go too far. They're just like primitives, they're just like building bricks. What's interesting is like the protocols that we can build to address their shortcomings.
00:25:36.258 - 00:26:09.540, Speaker A: So for example, I did mention that, but homophobic encryption doesn't guarantee that the computation was done the way you intended. So it's pretty interesting to see how you can combine it with Snarks. But not in a naive way where you would plug Snarks maybe on top of Fhe or the other way around, but maybe verifying that the data was encrypted decrypted correctly and combining that with other tools to make sure that you've got some kind of privacy in depth approach to this kind of stuff.
00:26:09.540 - 00:26:32.074, Speaker A: Right. So as a summary, please bear in mind that these notions are different privacy, confidentiality, verifiability, and that it's mostly about collaboration, it's about using privacy to foster collaboration. That's pretty much the takeaway of this talk.
00:26:32.074 - 00:26:51.600, Speaker A: I think we need to combine these technologies. So yeah, that's the other takeaway, I think, like collaboration and putting them together to get to very interesting protocols and that can achieve our design goals. Thank you very much.
00:26:51.600 - 00:26:54.340, Speaker A: I hope I didn't go too much of a time.
00:26:56.070 - 00:26:57.794, Speaker B: Thank you. Jonathan. Yeah.
00:26:57.794 - 00:27:24.570, Speaker B: At this point, only the true enthusiasts remain. We've had 3 hours of content already. One thing that kind of I didn't completely appreciate is you were mentioning SGX kind of had 128 megabytes of memory and then that grew to hundreds of gigabytes of Ram and we lost something integrity.
00:27:24.570 - 00:27:41.120, Speaker B: And that as a consequence meant that we could only really use SGX in the context of semi trusted cloud providers. What do you mean exactly when you say so?
00:27:41.970 - 00:28:19.258, Speaker A: The first version of SGX was kind of like this very nice primitive because it was giving you confidentiality of the inputs because of the enclave guarantees. But what you were getting was that by combining the remote Attestation mechanism that Andrew talked about, when you do remote Attestation, what you do is that you guarantee that the enclave, the program is like in the state you're expecting it to be when it starts. So you know that you've loaded the right program that you were expecting to run.
00:28:19.258 - 00:29:06.460, Speaker A: Now that doesn't mean that the execution of the program is not going to be tampered with by the underlying system. Because remember, in SGX we're not trusting the OS, we're not trusting the machine itself, we're just trusting the CPU and the intel would have trust. So what used to be the reason why they had this limitation of 128 megs is that they were using, I think, a merkel tree under the hood to verify that whenever the memory pages were encrypted or decrypted in and out of the CPU registers, they were not modified from one use of the memory patch to the other.
00:29:06.460 - 00:29:34.500, Speaker A: So that's what I mean by memory integrity with this mechanism. What happens is that you start from this known initial state that is guaranteed by the remote attestation. And because you know that the execution cannot be tampered with, the memory cannot be modified by a malicious actor on the system, you know that you're going to go through all the way and obtain the result that you should obtain, regardless of where the application is running.
00:29:34.500 - 00:30:03.126, Speaker A: So that kind of gives you similar feeling to what ZK proof gives you, to what snark gives you this verifiability of the computation. But now that they've removed this memory integrity feature, because the key to unlock larger enclaves was to remove the metal trees, which was taking too much space, and maybe also introducing something other head. You can't really trust what's going on on the machine.
00:30:03.126 - 00:30:33.220, Speaker A: You don't know that or at least I've not seen anything yet that says and maybe Andrew or Tom can chip in there if you've got more information than I do, but at least as far as I know, you can't really guarantee that the pages are not modified when they're not being processed by the CPU. So you've lost this memory integrity and by such you've lost the verifiable computation feature that the first version of SGX used to have.
