00:00:02.010 - 00:00:17.342, Speaker A: Today I will talk about decentralized builders. So who here is familiar with proposal builder separation? Okay, well, I'll have a brief introduction to PBS. Anyway, there it is.
00:00:17.342 - 00:01:05.294, Speaker A: So the general idea of proposal builder separation, this is this concept that we're looking at getting into, the Ethereum protocol, a extra protocol, sort of somewhat trust based version of this exists and is going to be activated post merge in the form of mev boost. But the basic idea here is that we split up the role of creating blocks into two parts, right? One part basically involves what we call builders, these highly specialized actors that construct candidate blocks. So they do all of this work of gathering transactions, figuring out what kind of block optimizes the mev at a particular time and construct these blocks and send off bids to get those blocks included.
00:01:05.294 - 00:01:56.266, Speaker A: And then proposers just listen for bids and they just accept whatever the highest bid is, right? And so the goal is basically that proposers can continue to be these very simple actors that just run a very dumb algorithm. So it's this completely automated role and builders absorb the responsibility of being this very specialized actor that runs custom algorithms and constantly keeps very custom algorithms updated, right? So builders absorb the economies of scale and proposers stay decentralized. So why decentralized builders, right? So the philosophy behind proposer builder separation is to basically kind of take all of this sort of like junk, this centralization pressure from economies of scale, and kind of sweep it all onto one side of the room.
00:01:56.266 - 00:02:18.134, Speaker A: So the other side of the room can be completely clean, right? But maybe it's still a little bit bad for that side of the room to be dirty, right. Even though proposal builder separation can greatly improve validator decentralization, it does have this extra consequence that it leaves builders very centralized. And there are downsides to that.
00:02:18.134 - 00:02:56.182, Speaker A: And the question is to what extent these downsides can be mitigated, right? So the downsides, they could be pretty serious, right? Like, we could be entering a world where potentially one builder produces more than 50% or even more than 90% of all blocks. If there's one builder that has a reliably better algorithm for mev optimization than other builders, then it's possible that that one builder will just reliably, almost always win. And it could even be a self reinforcing effect where if the more that one builder wins, the more other builders don't have revenue, and so they can't keep updating their algorithms and so they're forced to shut down.
00:02:56.182 - 00:03:46.210, Speaker A: And you could risk creating a situation that's pretty centralized where basically you have one builder and the only time that other builders come in is when there's exceptional transactions that those other builders capture that one that the original builder does not. Or if the first builder feels the need to censor some subset of transactions, then the second builder eventually comes along and makes a block just like full of basically just those Censored transactions and they win, but you risk having a market that's pretty lopsided. Now, we can mitigate a lot of the consequences of this with transaction inclusion lists, right? So the proposer of a block would be able to specify a list of transactions, and that list of transactions would be required to be included by the builder.
00:03:46.210 - 00:04:08.826, Speaker A: This basically prevents the builder from censoring. And so instead of it being a block auction, it becomes a partial block auction where the proposer provides a bunch of transactions that they believe should unconditionally be included. And then the builder comes along and they choose the order and they have the right to add a few transactions of their own.
00:04:08.826 - 00:04:48.482, Speaker A: And this is good, but it's still not very nice if one builder produces more than 90% of all the blocks. So the question that I want to ask here is, could we make the winning builder itself be a decentralized protocol? So what makes sense to decentralize in a builder? Right, so one thing that makes sense to decentralize in a builder is algorithms for choosing transactions, right? A builder is ultimately an algorithm that collects inputs in the form of transactions and puts those transactions in some order and possibly adds transactions itself. This algorithm could possibly be decentralized.
00:04:48.482 - 00:04:58.534, Speaker A: The inputs to the algorithm could be decentralized. So that's the first thing that we could decentralize. The second thing that you could decentralize is the resources for block construction.
00:04:58.534 - 00:05:17.902, Speaker A: So this matters particularly in the context of full Dank sharding. In the context of full Dank sharding, you have a really big block and you want to potentially not require one node to be responsible for the task of creating and distributing the entire block. You might want to just split up the resource requirements of that task.
00:05:17.902 - 00:05:37.030, Speaker A: And finally, third category, this is a more exotic thing, which was actually originally intended to be the topic of my entire talk, but builder decentralization started to be more important. And so I'll sort of squeeze that talk into this one. But the concept of extra builder services and particularly pre confirmations.
00:05:37.030 - 00:06:13.810, Speaker A: So another question, what do we mean by could? Right, so one type of could is technical feasibility, and I think we'll argue that technical feasibility here is going to be achieved. The other kind of could is, would a decentralized builder actually be a market winning actor? Right? There are ways in which decentralized things are inherently less efficient than centralized things. Do they have advantages that are large enough to be able to reliably or at least even potentially beat out the advantages that centralized builders would have? So part one, algorithms for choosing transactions.
00:06:13.810 - 00:06:48.714, Speaker A: So this is the builder searcher architecture, right? So this is roughly the way that Flashbots mev geth works today, except I don't think flashbots mev geth ever really got to kind of adding any layers of encryption. It was all trust based and this sort of architecture is the sort of thing that you need to have in order to have a decentralized builder, at least in terms of decentralizing block construction. So the idea here is that we split the builder role into two types of actors.
00:06:48.714 - 00:06:59.986, Speaker A: There are searchers and there is the aggregator. The job of searchers is to create bundles. So a bundle is a collection of one or more transactions that are intended to be included in a particular order.
00:06:59.986 - 00:07:16.600, Speaker A: Bundles may also have a flag that says you have to add this bundle at the beginning. Or bundles could be just like addible whenever. They could even have some prefix requirement that says, here are things I want to be true about the state before you include them, or they might not.
00:07:16.600 - 00:07:37.610, Speaker A: So you have these kind of partial blocks that we call bundles. And the idea is that you have searchers that might be specialized toward finding specific types of mev. So you might have one searcher, for example, that is really good at doing arbitrage between uniswap and SushiSwap.
00:07:37.610 - 00:08:06.626, Speaker A: And all that they do is they just become really good at that and they publish bundles whose sole purpose is arbitraging, uniswap and SushiSwap. So you have these bundles and from searchers they come together into an aggregator. And the job of the aggregator is to construct the entire block and then the aggregator and the proposer use some aggregator proposer interaction protocol, could be a VV boost, could be some ethereum unit protocol, BBS in order to actually get that block out there and published.
00:08:06.626 - 00:08:40.798, Speaker A: So, challenges one is how to protect searchers from mev stealing. So this is a kind of equivalent problem to the problem that has to be solved within PBS itself, right? In PBS there's this problem that if the builder provides a block to the proposer, how do we prevent the proposer from examining the block and instead of paying the builder, just creating a block that just replaces those transactions with transactions that use the same strategy but that pay the proposer directly. So that's the same problem, except here we have to protect searchers from the aggregator.
00:08:40.798 - 00:09:28.318, Speaker A: Another question is how do we allow the aggregator mechanism to even combine searcher inputs, right? If he wants to protect searchers from EV stealing, then the bundles cannot be in the clear. If the bundles are not in the clear, then how do you aggregate them? How do we ensure that the aggregator mechanism can actually publish the block? Right? So if the bundles are not in the clear, well, the contents of the block are going to have to be in the clear eventually. Like what's the process from sort of just ciphertext being out there, going to plain text also being out there? And whatever that process is, how do we make sure it works without requiring searchers to be honest? And how do we protect searchers not just against aggregators but even against aggregator proposer collusion.
00:09:28.318 - 00:09:47.910, Speaker A: So those are some of the challenges. So one idea is trusted hardware, right? So the idea here is that searchers send bundles where those bundles are encrypted to some key. That's where the corresponding private key exists in a trusted hardware module.
00:09:47.910 - 00:10:57.898, Speaker A: The aggregator runs some merging algorithm inside of the trusted hardware module and the trusted hardware module unlocks decryption only when it sees the signature from the proposer, plus some proof of the availability of the proposed signature, right? So the reason why we need this proof of availability is to basically protect against proposer aggregator collusion, right? So just in case the proposer is the aggregator, the reason why we want to see the proposal signature is because when you have a proposal signature, you have this thing that stops the proposer from later making another block that proposes something else. Because if the proposer proposes two different things, they can get slashed, right? But this argument does not apply if the first proposal signature can just be hidden forever. And so you need some reason by which the TPM can be convinced that the first proposal signature actually did get published, right? Because only if the proposal signature actually did get published did the proposer actually commit to publishing and supporting only that particular block.
00:10:57.898 - 00:11:31.206, Speaker A: So one option is attesters, right? So if the ethereum protocol is designed in a particular way, like let's say you have one round of attestation happen after the proposer, then the TPM could check for the proposer and enough attested signatures as well. Another option is you could have some M of n assumption within the aggregator, right? So if the aggregator itself is like some distributed M of n system, then you could have a threshold thing and you can trust that the threshold thing is honest. Also you could have some kind of low security real time data availability oracle.
00:11:31.206 - 00:12:23.580, Speaker A: So if you want, you can use chain link or whatever and basically just attest to the fact that this signature is out there on the internet and it is going to be rebroadcasted, right? So basically the kind of idea recapping step one, searchers send encrypted bundles and these bundles are encrypted to this key. And the only place where the corresponding private key exists is inside of a trusted hardware module that's inside the aggregator. So only the trusted hardware module can decrypt and nothing can come out except basically the header of the block that gets created, header of the block goes to the proposer, proposer signs it, and then when the TPM is convinced that the proposer signs it and the signature is published, then the aggregator can go and release it.
00:12:23.580 - 00:12:33.334, Speaker A: Or the TPM inside the aggregator can release the rest of the block. So this is option one. Option two is if we want to be a bit more clever.
00:12:33.334 - 00:12:53.330, Speaker A: So this requires an M of n aggregator, right? And it assumes the M of n aggregator is honest, but we can get rid of the TPM searchers send bundles that are encrypted to the sum threshold key. And these bundles also contain an access list. So a list of what accounts and storage slots they access.
00:12:53.330 - 00:13:32.190, Speaker A: And they also come with a Ziki snark of correctness. The aggregator chooses the global kind of total bid maximizing disjoint set of bundles, right? So we basically are only aggregating disjoint bids. There are ways to potentially improve on this a bit further, but just to keep it simple, we'll only be aggregating disjoint bids, right? And then finally it's the aggregator's job to compute the state route, right? Computing the state route is a challenge, right, because to compute the state route, you do have to actually see the transactions in the clear so that you can process them, or at the very least, you have to see the state updates in the clear.
00:13:32.190 - 00:13:55.234, Speaker A: But the problem is that even the state updates themselves might be enough information to do mev stealing. So one option is to have one aggregator node decrypt and compute, but then they can collude with the proposer. Another option is to compute only after the proposer makes some commitments that whatever this block is and whatever state route it has, the proposer will only support that block.
00:13:55.234 - 00:14:23.722, Speaker A: Right. So this requires kind of the eigen layer technique, right, where the proposer makes some kind well, not on chain off chain message committing that the only block that they will produce during this turn is a block that contains this set of bundles, whatever they are. And only after the proposer makes that commitment do the bundles get decrypted and the state root gets calculated.
00:14:23.722 - 00:14:40.470, Speaker A: Right. And if the proposer violates this commitment, then the proposer gets slashed, right. This is done with the eigen layer technique, where basically the proposer sets their withdrawal address to a smart contract that also enforces these extra slashing conditions before giving them their money back.
00:14:40.470 - 00:15:07.898, Speaker A: So a couple of different ideas. This is something that can be improved on, right? But if you want to try to decentralize block creation, you want to go in roughly this sort of direction block construction post dank sharding. So this gets into this more complicated territory, right? So post full dank sharding, a full block is 16 megabytes, and it's possible that it will increase even more than to 16 megabytes.
00:15:07.898 - 00:15:27.782, Speaker A: Publishing the block requires publishing across a huge number of subnets, right? The point of dank sharding is to avoid requiring any individual node to download the entire block. And so instead there's like some kind of architecture, could be subnets, could be some new form of DHT. This is also an active research problem.
00:15:27.782 - 00:15:52.390, Speaker A: Joachim recently made a great post kind of detailing some options. I think it was on the Paradigm blog. So basically the problem is that you don't just need one node to do a bunch of complicated work to create a full block that contains all of these different data sets and that has all these different polynomial commitments and relations between polynomial commitments and erasure coding.
00:15:52.390 - 00:16:10.546, Speaker A: But that node would also need to connect to all of these subnets and have super high bandwidth. We want to avoid requiring a single node to kind of have this really complicated function. So distributed erasure coding, right? So this is something that can be done and it's actually not very hard.
00:16:10.546 - 00:17:01.966, Speaker A: So step one, whoever includes each data transaction is responsible for encoding that data transaction and propagating the Blobs, or let's just say propagating the chunks, the chunks of the Blobs to the subnets and the data availability network, right? So if he wants to push through a data transaction, you are responsible for encoding it, and you're responsible for sort of making it available in the way that the Data Availability Protocol requires the aggregator. When they choose which data transactions to include, they can use some real time Data Availability Oracle that does the sampling for them, right? Well, actually this is a bit trickier. I think you need the oracle to actually try to download the whole thing because data availability sampling is not secure when there's only one person doing sampling.
00:17:01.966 - 00:17:30.778, Speaker A: And so you can't just have the aggregator sample. You actually need to have some oracle that's some distributed process that's trusted, that actually tries to download the whole thing and then the network can fill in the columns, right? So here basically we have this sort of two dimensional aggregation erasure coding process where the data gets sort of extended horizontally. So each Blob is 512 chunks, but it gets erasure encoded to 1024.
00:17:30.778 - 00:18:19.830, Speaker A: But then you also have this vertical process here where you have like, let's say if you have 32 Blobs, then they get extended to 64 Blobs, right? And so you have polynomials going horizontally and polynomials going vertically and filling in the columns can be done by the network. Now, the reason why you can fill in the columns, it depends on KZG commitment math, right? KZG commitments have this lovely property that they are linear, right? So if you have a KZG commitments to A and a KZG commitments to B, then A plus B commits to the commitment of A plus the commitment of B is A commitments to A plus B. And furthermore, you have linearity of proofs, right? So this is something that IPAs do not have, right? So merkel trees and even starked merkel trees, they do not even have commitment linearity.
00:18:19.830 - 00:18:52.274, Speaker A: IPAs have commitment linearity. They do not have proof of linearity, right? So basically, if QA is a proof that A equals some value at some coordinate and QB is a proof that B equals some value at the same coordinate, then you can make a linear combination of QA and QB. And that itself is a proof that the same linear combination of A and B has the right value at the same coordinate, right? And this linearity property is actually what is needed to make it possible to kind of fill in the rows.
00:18:52.274 - 00:19:16.910, Speaker A: Right? Basically, if you have proofs for like, let's say, the third column going all the way from zero to 31, you can use that to generate proofs for the third column all the way from 32 to 63. And this is like a really mathematically amazing thing, but it only works with KZG. If we don't have KZG, then the best technique that we know about is it's the builder's job to create row commitments.
00:19:16.910 - 00:19:27.778, Speaker A: So commitments to all the basically commitments to all the rows. Well, actually, the builder doesn't provide these. The first half of the row commitments is just the Blobs, so that's fine.
00:19:27.778 - 00:19:53.766, Speaker A: The builder would have to provide the rest of them and then the builder would also have to provide some column commitments. By the way, thanks to Dan Bonet for helping us to think through some of this and then the ideas that these commitments have to match. Right? So it's like basically the Ith row commitment at the Jth coordinate equals the Jth column commitment at the Ith coordinate, right? So one of them is sort of saying, we're going to go through rows first and then columns.
00:19:53.766 - 00:20:12.586, Speaker A: The other says, we're going to go through columns first and then rows. It's actually kind of similar to what the original 2D Data Availability Paper did, except instead of using fraud proofs to check equivalents, you can do a ZKP to check equivalents. So this can be done distributed, but it's harder to do distributed.
00:20:12.586 - 00:20:35.638, Speaker A: It requires at least two round protocol, whereas the KZG thing, it's just one round, right? It's like the builder first just checks all of the blobs and then the builder just publishes and then there's just like a totally separate process the builder isn't even involved in that fills in the rows here. Like, no, the builder has to be involved. So possible, but harder extra builder services.
00:20:35.638 - 00:21:02.302, Speaker A: So in the last four minutes and 20 seconds, this is my entire original speech for this. So builders can provide pre confirmations. This is this fascinating concept where you can basically try to give so ethereum in general, right, I think it has this weakness that it has these fairly long block times and the fancy VC chains, they often argue like, oh, we have these amazing 500 millisecond block times.
00:21:02.302 - 00:21:20.626, Speaker A: And I think my opinion is that a major credibly, neutral public blockchain. Trying to provide 500 millisecond block times directly is a big mistake because, well, you need to have large safety buffers for decentralization. And there are even more subtle reasons why you need to have large safety buffers.
00:21:20.626 - 00:22:14.726, Speaker A: Like, for example, if censorship is happening, then you need large safety buffers to be able to have good network wide consensus about who is responsible for censorship and whether or not a transaction is being censored or whether you just have a transaction sender that keeps publishing the transaction like half a second late. So long block times and also longer block times enable much larger validator sets. But the question is, can we have the best of both worlds? Can we have Ethereum with a finality time of like, let's say, 30 seconds and at the same time have some high degree of confirmations that come more quickly? So let's have the builder do this, right? Let's say the builder publicly agrees that if a user sends a transaction with a priority fee of at least five, the builder will immediately send an enforceable signed message agreeing to include it in that slot if the builder does not do this.
00:22:14.726 - 00:22:45.906, Speaker A: So if the builder does not make a block in that slot, that gets included, the builder gets slashed. If the priority fee is greater than eight, then the user even gets a post state route, right? So basically an even higher priority fee forces the transaction to get included in order and so the user knows what the consequence of it that transaction is immediately. This can be done with the fact that this server exists and service exists and the fact that you get this fast confirmation at all.
00:22:45.906 - 00:22:56.690, Speaker A: If you provide a fee that can be done either with builder reputation or you could have a third party data availability. Oracle. So can a distributed builder do this? Right? This is the question.
00:22:56.690 - 00:23:53.382, Speaker A: So you can run? Yes, I mean, you can just have a system that runs tendermint internally and internally has a 500 millisecond block time or 1 second block time or whatever that just does this exact thing. And the final signing can be done threshold style, right? And you would have to penalize participants in the distributed builder either for breaking the tenement, right? So for doing double finality inside the tenement or for contributing to a final signature of the block that is incompatible with what the tendermint agreed to. Now, if you want maximum security, this does require some kind of account abstraction for final builder signing because Threshold BLS is unattributable, right? So if builders just have to BLS sign the final block, then you don't know who did it and so you have this kind of problem of, well, who do we slash if there is a misalignment? But if you have some kind of more abstracted form of final signing, then you can do it.
00:23:53.382 - 00:24:43.442, Speaker A: The more participants, the more security, right? So this is where we get to kind of the holy grail, right? Which is if you have a distributed builder that has many thousands of participants and you even maybe do like fancy eigen layer stuff and you get like 10% of the Ethereum validator pool to participate in the builder, then you can basically have a pre confirmation service that has huge security deposits behind it. So within 1 second you can get a guarantee that either your transaction gets included or this pre confirmation service has to burn 1000 ether or whatever a distributed builder what other advantages does it have? It could be more easily trusted by searchers. A distributed builder is more well protected against cheating searchers and even if people collude to cheat the searchers that collusion is easily detectable.
00:24:43.442 - 00:25:25.938, Speaker A: Distributed builders because they're protocols and not operators they are more censorship resistant. So there are these different advantages that exist but at the same time distributed builders have these weaknesses which are basically one is just inherent latency of distributed system and the other is that any decentralized thing is slower to adapt. So the question is like well can the competitive advantages of distributed builders overcome the disadvantages and can we find other competitive advantages? Like what other things can distributed builders do and how do we make these protocols be as efficient as possible and as trustless as possible and try to really minimize disadvantages?
00:25:26.034 - 00:25:48.762, Speaker B: Are this open?
00:25:48.816 - 00:25:49.758, Speaker A: Okay yes.
00:25:49.924 - 00:26:00.100, Speaker B: So I have a question on the post state route you said you can provide the post state route. Could it be possible that you might have conflicting state route because of distributed builders or is that not a problem?
00:26:00.550 - 00:26:35.082, Speaker A: Oh is the question is there a possibility that you can't compute the state route because you have this distributed builder mechanism? Yeah, that's a good question. It is a challenge and it kind of intersects with the other challenges. Right? What you could do is you could have a mechanism where the proposer for example keeps on making updated proofs, right? So it's like you have round one within a salot, which basically where the builder agrees on some transactions and the builder says, hey, proposer, can you pre agree to sign a block with this prefix? And the proposer assigns an eigenolier message that says that, yes, I pre agree.
00:26:35.082 - 00:26:43.770, Speaker A: And then after that, the builder reveals and then you have round two. And then the builder kind of accepts a second batch of transactions they send to the proposer. The proposer pre agrees.
00:26:43.770 - 00:27:05.942, Speaker A: So you could do kind of fancy multi round stuff like that and I guess you will probably have to. And in the trusted hardware route it just gets easier because it is pretty safe to give state routes. But well, I guess giving state routes is not very pointful unless you actually give people receipts as well.
00:27:05.942 - 00:27:28.730, Speaker A: So that could probably be abused for strategy stealing. Well, I guess though even in that case it's in the trusted hardware so it would not actually output the full header for anything other than the correct block. So I guess yes, in both modes there is a pretty natural solution.
00:27:28.730 - 00:27:29.720, Speaker A: Thanks.
