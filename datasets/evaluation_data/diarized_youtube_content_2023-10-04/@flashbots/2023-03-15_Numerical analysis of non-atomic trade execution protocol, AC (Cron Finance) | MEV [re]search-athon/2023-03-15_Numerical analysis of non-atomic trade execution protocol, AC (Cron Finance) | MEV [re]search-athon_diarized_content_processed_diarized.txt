00:00:03.160 - 00:00:11.144, Speaker A: Okay. Numerical analysis of Non atomic Trade execution protocol. Excellent.
00:00:11.144 - 00:00:47.604, Speaker A: Here we go. So, importance of doing a numerical analysis for a PWAM, otherwise known as a time weighted Average market maker, is actually considerably more important than your constant market maker that you're all familiar with, like Uniswap. And the reason for this is that the TWAM trades are much, much larger, or expected to be larger, where typically in the past, we've seen orders as large as maybe one to 5 million on what I call a CPAM constant product automated market maker in TWAM because the orders are spread out over time and we'll go into this and the orders are larger.
00:00:47.604 - 00:01:14.320, Speaker A: If you don't get your numerical analysis right, you're going to be losing a lot more capital for people, and you don't want to do that. So before we dive into this numerical analysis, I'm just going to give a brief overview of CPAM and Gpam just so that everyone's kind of on the same page. A concept product automated market maker that I refer to as CPAM has this interface that you see and the main way people interact with it would be this mint burn and swap functions.
00:01:14.320 - 00:01:33.156, Speaker A: Inside of the pool itself, you have two assets, token X and token y here, and then you have pool tokens for people who are providing the liquidity that people are trading with. When people perform a swap, the swap is bound by the constant product formula. So token X times token y, the reserves of each is a constant.
00:01:33.156 - 00:01:56.088, Speaker A: And that maps to this bonding curve that you see here on the right, which determines price action. Another part of it to know is there's also an instantaneous price that's determined by the reserves divided by the reserves of the other token. However, in practice, if your trade is much larger than the actual reserves in there, you're going to move the price and get a very different effect price.
00:01:56.088 - 00:02:05.532, Speaker A: I'm not going to go into more detail on this. The Uniswap papers are really an excellent source for this. Now let's just consider the price action of a simple trade.
00:02:05.532 - 00:02:18.884, Speaker A: So we have a pool and the reserves are 100 of each token. And at this point we expect the price of token X, or sorry, the price of token y to be one token X per token. So now we're going to swap 500.
00:02:18.884 - 00:02:49.916, Speaker A: And notice that I said 500 token because that's much bigger than the 100 token that are in there in reserves for the opposing token. So what happens when we do this is the 500 token get added to the pool, and I'm ignoring fees and a bunch of other complexities, and we move along this bonding curve to this disadvantageous point here, where now? The Token X reserves are 600. And we've significantly reduced the amount of token y because we've put 500 Token X in and we've removed 84 to give to the users as proceeds.
00:02:49.916 - 00:02:59.216, Speaker A: So now there's only 16 token y remaining. The thing that catches the person trading may be expected to pay one token per token. They ended up paying six tokens.
00:02:59.216 - 00:03:10.224, Speaker A: So that's six times higher. And you'll hear this referred to as slippage. So now I'm going to show you the time weighted average market maker and how it differs.
00:03:10.224 - 00:03:35.564, Speaker A: And what you see here is you have the same three functions that we had before mint, burn and swap. But we also have a partner swap and the partner swap kind of ties into the last presentation because it's basically like a pay for airflow mechanism. And we're partnering with Rook and Mike actually to allow people to give auctions to bidders and we reduce the fee that they pay to incentivize swaps.
00:03:35.564 - 00:04:08.036, Speaker A: And the reason we do this is those swaps formed arbitrage that actually moved the trade back on the bonding curve to a position that's advantageous where people were expecting to pay the price that they had. So rather than suffer massive slippage by incentivizing order flow from these arbitragers, we give long term traders a better fill. And the other benefit is when he was talking about tips, we take a portion of those and we feed those back into the pool liquidity so that the liquidity providers are now getting a benefit.
00:04:08.036 - 00:04:24.344, Speaker A: And what this net effect does is takes the mev and it gives it to the pool instead of to the miners. And so that's how it ties into the previous discussion. So the other major thing that you'll notice is on this interface is the long term swap cancel and withdraw.
00:04:24.344 - 00:04:36.204, Speaker A: And the pool is also significantly different. In addition to the X and Y reserves and the pool tokens, you have these order and proceeds pool. So when you put an order in, it doesn't execute immediately.
00:04:36.204 - 00:04:54.004, Speaker A: It's not atomic, it executes over time and your orders capital is stored in this order pool and then it slowly moves across into the proceeds pool where you're able to withdraw. So just some major features to highlight. It's not atomic, so it's occurring over time, not just in one transaction or one block.
00:04:54.004 - 00:05:08.108, Speaker A: You can withdraw your order one or more times. And this is actually really interesting because in future we plan to kind of adapt that to streaming applications where you can now have a time waited order that goes across multiple pools. You can cancel the order.
00:05:08.108 - 00:05:26.736, Speaker A: In working with folks, what we discovered is you want to be able to delegate the management of an order because a Dow has difficulty getting attention of folks to do things. So you might want to say we want to put in a limit order, let's have this person be able to withdraw, cancel it. But the capital is always going back to the originating address.
00:05:26.736 - 00:05:39.888, Speaker A: So this way you can kind of delegate the work of managing the order to somebody else or a service. There's some significant features that paradigm built into this when they wrote the paper to reduce gas use. That's concurrent opposing swaps.
00:05:39.888 - 00:05:47.992, Speaker A: So a conventional pool can't do this today, where you swap in two directions from X to Y and Y to X. In a single transaction. TWAM is able to do that.
00:05:47.992 - 00:06:01.356, Speaker A: In long term swaps, your orders are pooled. So if you and I are making the same trade in the same direction, our orders are pooled together and we only pay gas to enter the system. We don't pay additional gas to keep having that trade happen.
00:06:01.356 - 00:06:13.088, Speaker A: And the person who pays that gas is actually the person who acts on the pool next. And that's the lazy evaluation part. So if nobody participates in the pool or sends a transaction to the pool, nothing happens.
00:06:13.088 - 00:06:26.640, Speaker A: The state is basically where the last transaction left it, even though as time marches on the reserves and the trade is actually changing. It's virtual, so we have a bunch of lookup functions for that. And then the last thing I'll say is that there's an index expiry and update.
00:06:26.640 - 00:06:41.172, Speaker A: And what I mean by that is trades can only end on certain blocks. If they ended on every block, the amount of gas to do this would be too high. So let's take a look at the same price action on a TWAM pool, but now in a vacuum.
00:06:41.172 - 00:06:54.312, Speaker A: And the reason I'm doing this in a vacuum is I want you all to see that the math and the bonding curve is effectively the same when nobody else is present. So we have the exact same trade. We're doing 500 token, but now we're doing it over five intervals.
00:06:54.312 - 00:07:08.832, Speaker A: So what that means is we have five discrete points where we're injecting 100 tokens into the pool. And you can see on the bonding curve here that we have five movements across the curve. The math is actually exactly the same in a vacuum because nobody's interacting with the pool.
00:07:08.832 - 00:07:22.104, Speaker A: So you get the same terrible proceeds of 84 instead of the 500 you expected from the instantaneous price. Now, this vacuum scenario is not real. And the reason it's not real is because this is distributed over time.
00:07:22.104 - 00:07:41.816, Speaker A: You're going to have arbitragers like the folks from the last presentation come in and sell bids. And what will happen in this hypothetical situation that I've shown here? We move two intervals worth of price and one of these arbitragers somehow wins their auction and says, hey, this is great, I'm going to move the price back. And that's the green line where we've now moved back on the bonding curve.
00:07:41.816 - 00:07:54.000, Speaker A: And for this five interval example, if you crunch the map, what you end up discovering is that the proceeds are now 184. So that's much, much greater than the original 84. And your effective price is about 2.7
00:07:54.000 - 00:08:12.856, Speaker A: tokens, which is actually less than half that you paid in the previous interchange. So there's a lot more detail I can go into in Twan, but I'm not going to here because I don't have a lot of time. But I think one thing that's important to understand is just how the process goes.
00:08:12.856 - 00:08:45.856, Speaker A: So any of these transactions that get sent to the pool, mint burn swap, partner swap, long term swap, cancel, et cetera, result in the execution of virtual orders. And in order to do that, we compute reserves at the last time something was happening, we execute all of the virtual orders that are present and that gives you the new reserves to have the new transaction enact upon. And in this execute virtual order is that lazy L valuation, the pooling of orders and the index expiry that I was talking about.
00:08:45.856 - 00:08:55.824, Speaker A: Another important thing to note that I'm not going to go into too much detail. The longer the amount of time since that last order, the more gas you're paying. And the reason is you have to run the loop iteration.
00:08:55.824 - 00:09:09.130, Speaker A: And the loop iteration has a fixed gas cost. So it's like N times that gas cost. Okay? So before we dive full on into the numerical analysis, I need to show some optimizations that took place in this.
00:09:09.130 - 00:09:17.416, Speaker A: And this one has to do with virtual order calculations. So you're all familiar with no active orders. That's basically no math.
00:09:17.416 - 00:09:26.056, Speaker A: You don't have to do anything a unidirectional active order. So that's where you're going from X to Y or Y to X, but not both. That's exactly the same as the CPAM math.
00:09:26.056 - 00:09:36.204, Speaker A: And we kind of just show that as one trade. And then the more significant case that the paradigm paper addresses is the TWAMP calculation. And that's where you have X to Y and Y to X concurrently.
00:09:36.204 - 00:09:48.484, Speaker A: The arithmetic for that is significantly different. And I'm not going to dive into this heavily, but this is the math from the paradigm paper and you can see that it's fairly complex. There's exponentials and square root functions all over the place.
00:09:48.484 - 00:10:04.700, Speaker A: It's about 15 operations and the team from Fracs came up with a fantastic approximation here and that reduces it to very simple. I think it's like three or four operations. And in Desmos there was a great comparison done and you can see that it really tracks very closely to the original function.
00:10:04.700 - 00:10:30.532, Speaker A: We've kind of gone beyond this with our editors auditors, but I can't address that today in terms of time. But suffice it to say, you can see it tracks very closely and the trade off is definitely worth the gas savings and the complexity in the compute analysis. Another thing to show with that is you can see that the math, when we do an estimate of the gas used, we cut that by literally ten x.
00:10:30.532 - 00:10:45.016, Speaker A: So we go from about 17,000 gas to 1800 gas from that exact same arithmetic. The next optimization that I think is significant for us to point out is scaled proceed storage. So if you were to profile this and take a look at your gas use.
00:10:45.016 - 00:10:57.852, Speaker A: That algorithm that I just showed was about a third of the gas. This is two thirds of the gas. So what's happening here is every time you execute virtual orders at one of those index points that I was talking about, you're storing two slots of data.
00:10:57.852 - 00:11:20.120, Speaker A: Two slots of data is going to run you 40 grand in gas in the worst case. So what we did is we took that and we said, what if we stored that in one slot? And the reason I'm pointing this out is this is what made our numerical analysis much more tricky. When you have 256 bits to play with, you can get the scaling factors wildly wrong and you've got the resolution and headroom and dynamic range that you can get away with murder.
00:11:20.120 - 00:11:40.516, Speaker A: When you cut it down to 128 bits, you can have a disaster ensue and you're about to see the disaster that ensued for me. But just to show you how much this is worth, this is a different gas use comparison. And what's happening here is we run our benchmark test and we compare the numbers before and after, and you can see that we're getting about 20% improvement in gas production.
00:11:40.516 - 00:11:48.860, Speaker A: So that's why that's incorporated. So now we can kind of dive into numerical analysis. So our numerical analysis actually started into code.
00:11:48.860 - 00:11:53.944, Speaker A: Neither of us are solidity devs. Neither of us come from the space. Both of us come from the silicon domain.
00:11:53.944 - 00:12:09.372, Speaker A: So what we started doing was we actually had a variable naming convention. And the reason we did that is solidity variables are best when they're native 256 bit, but a lot of the values that we contain in them never exceeded another number. I'll give you an example.
00:12:09.372 - 00:12:20.048, Speaker A: Units cop 112 bits. So we would append a suffix, u, 112 to values and so forth. And then we added tags to explain sections where we had unchecked math.
00:12:20.048 - 00:12:31.224, Speaker A: Now our protocol is implemented in balancer, which is solidity .7. And what that means is all the map is unchecked unless you check it yourself. So this was really helpful for us to actually see.
00:12:31.224 - 00:12:49.810, Speaker A: And you can see I don't think you can see this text, it's probably too small from here. But by looking at the suffixes, you could actually tell if you were going to overflow based on the expectation and you knew where to put safe operations and unsafe operations. And by having these explanations in the code, it made the auditors a lot happier because they could see that thought kind of went into this.
00:12:49.810 - 00:13:05.764, Speaker A: We also use tags to explain where we expected over and underflow. So in Oracles, you would expect overflow because it's actually the difference between the values, it's not the actual net value. And I'm just going to kind of skip over that for now.
00:13:05.764 - 00:13:18.648, Speaker A: But one thing I'll add is where we had fractional bits. We actually added that to the suffix as well. So another thing that we did is because both of us come from the silicon domain, we actually started doing system diagramming as well.
00:13:18.648 - 00:13:37.952, Speaker A: And you can see the code that I had showed you previously here is actually diagrammed here in a schematic and the schematic is annotated with the container type. So the 256 bits as well as the expected amount that it's representing. And by looking at that in all these ways, you can actually start to get a picture of where things might go wrong and where they're right.
00:13:37.952 - 00:13:54.656, Speaker A: So here you can see we have unchecked math and the value coming in is colored in blue. And the reason is we're like we think it's 112 bits but we don't know for sure. And then we perform a check subtraction and that exclamation mark there tells you this thing throws if things go wrong in the pool.
00:13:54.656 - 00:14:24.524, Speaker A: And this was actually quite useful in the broader context because that block that you just saw now is put into a larger subsystem and we were able to examine it in the context of other scenarios and understand what the side effects were in stock. And getting this alternate perspective kind of gave us an ability to carefully review all of our existing code and it revealed a lot of gas optimization opportunities. And as I mentioned before, we had color coded areas that were kind of like to dos in the code as well.
00:14:24.524 - 00:14:37.350, Speaker A: So where signals were red, we knew that we had to really find out if that worked. So we would actually write test cases for that to make sure that that was checked elsewhere. And because we're using balancer, some of their code did a lift for us and did that check as well.
00:14:37.350 - 00:15:00.260, Speaker A: So from there we kind of went into focus area abstractions because the Schematic level is just too low level to look at this. So this diagram here kind of shows the main area that we were concerned with and we were able to kind of break it into two parts. The update algorithm, which is like the TPAM and TWAM math and then how we stored the scaled proceeds.
00:15:00.260 - 00:15:17.148, Speaker A: And this whole thing shows basically two orders coming in and the sales rate gets calculated per block and then merged. This is the pooling and then you can see the reserves come into here, the iterations of the loop and then the storing of the scaled proceeds. And we had static scale factors of 64 bits.
00:15:17.148 - 00:15:26.416, Speaker A: Initially. That was my first yes, because I'm not really from this space, I hadn't really thought about parameterizing this yet and I was really concerned about this. This was like kind of keeping me up at night.
00:15:26.416 - 00:15:49.224, Speaker A: It's kind of a weird thing to keep you up at night, but I think it kept me up at night for about a month or so. And I was wondering, would the gain or the amount of tokens coming through this overload this and cause these scaling factors to overflow. And I was walking around and I was thinking about this a lot and I started to realize things about the CPAM model that I hadn't really thought of.
00:15:49.224 - 00:16:04.908, Speaker A: And it's law of conservation of energy is really applicable here. And what that means in this case is tokens can't created or destroyed. If I'm creating tokens or I'm destroying tokens, then I'm replacing the functionality, the mint and the burn in the ERC 20 contract with this.
00:16:04.908 - 00:16:23.504, Speaker A: So what that led me to do was avoid any operation that rounded up because what happens if you're rounding up is you're creating tokens. And we have another method that I'm not talking about in here, but it gives us a degree of freedom where the reserves are not actually stored. All of the accounting around the reserves are stored and those are calculated from the balancer notion of the vault.
00:16:23.504 - 00:16:43.496, Speaker A: And what that does is it allows the rounding errors to disappear and we no longer trade or destroy tokens with our intermediate map. And then the last realization about this is that the algorithm is basically bounded input, bounded output. So if I throw 112 bit value in, I can't get a 256 bit value out unless my math is wildly wrong.
00:16:43.496 - 00:17:04.268, Speaker A: And the most I should be able to get out is the amount of reserves that's in the pool. And that's actually a limit that you can't reach because it never quite hits that point. So the real problem that we were trying to figure out is how many trades can a person perform in this system before a user loses proceeds and a user loses proceeds.
00:17:04.268 - 00:17:17.760, Speaker A: If these scaled proceeds overflow. And the reason is it's the difference between those proceeds that determine how much the user gets their share. So if it overflows twice in that period, the user misses an entire segment of proceeds.
00:17:17.760 - 00:17:29.016, Speaker A: So you can't have this thing overflow. And our maximum trade length is five years. And the reason it's five years is we wanted to kind of support the DCA example where you could dollar cost average in and have longer term positions in play.
00:17:29.016 - 00:17:41.100, Speaker A: So more than one overflow, huge problem. So what I did was I thought, okay, I'll do a concrete example. I'll use Wrap ETH to wrap bitcoin and that has 18 to eight decimals.
00:17:41.100 - 00:18:10.052, Speaker A: And let's see how my 64 bit scaling example works. So this is obviously way too complex to get into here, but basically we did the calculation for that trade and we had a pool with about 60,000 Rap ETH 4000 wrap bitcoin and we traded 40 wrap bitcoin over two weeks, that's 1344. And you can see the worst case scaled proceeds is like two times ten to the 32, which is huge.
00:18:10.052 - 00:18:28.156, Speaker A: But it turns out when you crunch the math that actually gives you 29 years before overflow that's actually pretty good, but 42 wrap bitcoin not a particularly big trade. So if you have a bunch of trades like that, maybe you have six of those, all of a sudden you're down to five years. So I still didn't know if it was really good.
00:18:28.156 - 00:18:36.112, Speaker A: And that's where our auditors came in, because I told the auditors, hey, I'm freaking out about this. I'd really like you to look at this. They had more experience in the domain than me.
00:18:36.112 - 00:18:43.132, Speaker A: So they picked the worst case. They picked DUSD and Die. And that's 18 to two decimals.
00:18:43.132 - 00:18:57.348, Speaker A: And the decimal part isn't really important. The important part is the numbers are so much larger in this case. So when we work through that same trade map, what you find is we're now at about one times ten to the 37, much, much larger number.
00:18:57.348 - 00:19:05.130, Speaker A: And now we're overflowing every 4 hours, way too many overflows. That's 84 overflows in two weeks. So that's not going to work.
00:19:05.130 - 00:19:21.436, Speaker A: And what we realized is we had a couple of solutions we do to this. We hadn't scaled the internal math. And the problem is, if we would have done it the way the balancer does, it would have added a lot of gas, a lot of developer time, and a lot more to the testing and audit and contract size.
00:19:21.436 - 00:19:37.330, Speaker A: So we came up with an innovative approach that allowed us to actually just tweak our scaling factors. Asymmetrically for the two tokens in the pool at construction, and let's just see what the effect of that is. So for that same example, you would now use, instead of a 64 bit scaling, you would use the number 100.
00:19:37.330 - 00:19:55.000, Speaker A: So with 100, we find that we now get only seven times ten to the 90, much, much less than ten to the 37. And what that means is we're actually now overflowing every, like, I think it's 8 trillion years. So that's going to support a much larger number.
00:19:55.000 - 00:20:07.832, Speaker A: It's a much safer way to do this. And basically now we're at the end of this numerical analysis segment and just some basic conclusions for you guys. Automated tools would have made our process a lot easier.
00:20:07.832 - 00:20:15.000, Speaker A: We took more approaches than this. We even did finite element analysis on it. So we're actually really looking forward to trying hardsol.
00:20:15.000 - 00:20:34.224, Speaker A: The next gen solidity security tool that Brock was talking about yesterday, our development process was really, really helpful. We had a safety test that had about 300 different tests in a lightweight TypeScript model, as well as hand capulated analytical inspection points. So whenever we changed things, we could run it against that and see if we were breaking assumptions in how this thing would operate.
00:20:34.224 - 00:20:47.764, Speaker A: And we also had a benchmark test that allowed us to see if we were affecting gas. And that was actually super useful. So in summary, combined numerical analysis methods and extensive preparation gave us a really excellent leveraging of our auditor skills.
00:20:47.764 - 00:21:01.870, Speaker A: They didn't have to look at the basics because we didn't have a lot of smaller basic errors. We had more complex errors where we could really leverage their work. I just want to say thank you to Grants from Balancer and Rook and if you want to follow us, there's information there.
00:21:01.870 - 00:21:04.190, Speaker A: Thanks. Any questions?
00:21:05.920 - 00:21:17.650, Speaker B: I'll be honest, I did not follow all of it. I would be curious, it would be helpful to me if you could just go back and summarize the relationship between the auction and the T one.
00:21:19.220 - 00:21:34.440, Speaker A: What assumptions? This part is a larger note in the system we know today in the general chain, arbitrage happens. We know it happens for small amounts. I've seen values as low as $10 for arbitraging.
00:21:34.440 - 00:21:48.504, Speaker A: The idea here is our pool is entirely considerable. The fees and the reason we have that for each individual pool is this is a competitive marketplace with a race to the bottom situation going on. So different pools are going to have different fees.
00:21:48.504 - 00:21:55.996, Speaker A: Now we have a general purpose swap that any person can use, like uniswap today. And then we have a discounted swap. We took an initial guess.
00:21:55.996 - 00:22:15.140, Speaker A: We took uniswap's value for the fee. We divided that in half and said, okay, let's throw this to folks who are doing arbitrage. The folks who are doing arbitrage have different mechanisms for selling that arbitrage, right? But what we've given them in the pool is an exclusive right to whitelist a bunch of addresses that are able to arbitrage.
00:22:15.140 - 00:22:40.444, Speaker A: And with them we have to work out, and this is a more complex system that is yet to be determined. To be honest, we need to work out a way of figuring out a percentage of the auction fee or the tip to rebate to the pool liquidity providers. So we have a mechanism to donate liquidity to the pool with no LP token expectation and that allows these people to pay the pool at various points.
00:22:40.444 - 00:22:52.988, Speaker A: But as for the fees and the assumptions, it was too difficult to model because there really isn't data for this. So we opted into that to make it configurable and to just kind of play it by ear with a first pool.
00:22:53.084 - 00:23:01.568, Speaker B: So to be clear, the auction is like the right to arbitrage after a play has happened or is it like first right access to it's?
00:23:01.584 - 00:23:11.476, Speaker A: First right access. So let me give you an example. If there's an arbitrage opportunity for $20 but you're an arbitrage partner, you get a better margin on that because the fee is lower.
00:23:11.476 - 00:23:37.036, Speaker A: So the idea is to incentivize the arbitrage before the general purpose population would actually see it and take advantage. So if it's not worth it to you at $10 but this person's making 20 because they get a better margin, then the idea is that they'll bid for it and take it in advance. And the reason we're doing that is because it gives a better fill for our long term trade customers, because it returns the price on the bonding curve, and it gives a better deal to our LP providers.
00:23:37.036 - 00:23:48.230, Speaker A: Because instead of having that competition happen on the chain with the miners, it's happening in another environment that our partners are managing. Like Brook, for example.
00:23:48.920 - 00:23:51.972, Speaker B: Okay, thank you.
00:23:52.106 - 00:23:54.804, Speaker C: Is that mechanism specific to TWAMP, or.
00:23:54.842 - 00:24:04.010, Speaker A: Could you just have that with the bank? You could have that with the regular ANM. I believe Uniswap actually discussed it. PB actually knows more about that than I do.
00:24:04.010 - 00:24:24.460, Speaker A: When we went into audit, we actually had a more complex mechanism because we were worried about people gaming the system and gaining knowledge of when those funds would be given. So you could do a liquidity attack, put money in and then pull out an undue amount of the reward and exit. And you could be especially advantageous if you knew about that early.
00:24:24.460 - 00:24:36.320, Speaker A: But the problem with that system was it just added too much gas, too much complexity, and we didn't have data. And without having data, it wasn't worth it to expose that surface area to risk.
00:24:36.980 - 00:25:09.740, Speaker B: So just coming back to the arbitrage question, this idea of giving a better fee tier to arbitrage, so if I have two arbitrages coming to arbitrage, same opportunity, and it's like worth $10 for them, and you give them both some fixed discount, from their perspective, does that cost any different? Because the competition is like everyone competition is benefiting the same way. I guess the one effect that I'm also curious about, aren't you reducing competition in these kind of auctions? I don't know how many parties.
00:25:10.880 - 00:25:23.888, Speaker A: So that aspect is our partners, not so much us. So we have the ability to add infinite partners, but what they do behind the scenes is more opaque. And what we've seen is it's kind of been a to be determined thing.
00:25:23.888 - 00:25:29.280, Speaker A: They've started with one guess, and things have kind of evolved. Their interfaces have evolved. So I can't really speak to that.
00:25:29.280 - 00:25:44.804, Speaker A: You would think that open market competition would be the best way to go. And I hit PB with that question one night, and you're like, oh, my gosh, what are we doing here? Maybe this is a ridiculous idea. And then we worked it through and realized the thing that we were Circumventing was giving money to the miners.
00:25:44.804 - 00:26:00.956, Speaker A: If you have that competition in a place where people are dedicated arbing and they're competing with one another, instead of the capital going to the miners, the capital goes to them in terms of margin and goes into our pool through the liquidity providers. So that's the hope and that's the thinking, as opposed to going open market.
00:26:01.138 - 00:26:03.100, Speaker B: And how does it relate to fee tier?
00:26:03.920 - 00:26:14.268, Speaker A: So the fee tier for those folks doing that arbitrage is half right now, but it's visible. You can change it based on what you see in the data. And by having a lower fee.
00:26:14.268 - 00:26:27.076, Speaker A: Tier they arbitrage more frequently, they get a better deal than the general purpose person and that better deal results in them giving a portion back to the pool. That makes sense. Good question.
00:26:27.258 - 00:26:52.860, Speaker C: So is there kind of fundamental idea behind this to solve the LP profitability issue? So I guess the question is why pursue the IMM model? Is that leaving that in the long term? Why not do the CLO, like assume that that LP profitability is solved by other types of decentralized exchange?
00:26:54.160 - 00:27:07.836, Speaker A: I'm not the best person to talk on this because I'm not a crypto Twitter person. I'm more of this. But this is what I'll say that I understand from what I've seen the complexity and I think concentrated liquidity brilliant.
00:27:07.836 - 00:27:18.788, Speaker A: I love it. But the complexity of managing those positions has been really cumbersome for unsophisticated investors. I don't know that the tools when it came out were able to help them bridge that gap.
00:27:18.788 - 00:27:46.908, Speaker A: So that created this vacuum where the sophisticated investors with resources came in and did it for us. This was kind of a simple foray that met our technical skills and also kind of gave the advantage potentially back to the more passive investor. And instead of giving money to the mining, giving money to the LP and still having mining in place seemed to be a great way to start.
00:27:46.908 - 00:27:58.544, Speaker A: We don't know for sure though. I thought about modeling this and the problem was I know how to do the big data with ethereum but there's no data that was realistic that I could pull in to look at this. Everything was like pseudo data.
00:27:58.544 - 00:28:05.430, Speaker A: I could make up a value but it's like what's the value of that? It's my guess. I'm not throwing millions into these things so I don't have that intuition myself.
00:28:06.920 - 00:28:43.116, Speaker C: One thing I wanted to throw on there is that in terms of who might use this over concentrated liquidity an example is Dows that have a financial incentive to market make, say some token that they produce and they're willing to accept the divergence loss of an AMM for the simplicity of not having to have some centralized team constantly market making. And they're okay with the trade off because having that liquidity there is net value for them and then they get the ability to buy and sell tokens easily as well, like in a safer manner. Must worry about slippage.
00:28:43.308 - 00:28:44.050, Speaker A: Yeah.
00:28:44.580 - 00:28:51.380, Speaker B: Do you think that with this mechanism you can actually get better execution than.
00:28:51.530 - 00:28:54.150, Speaker C: Say an OTC desk would give?
00:28:54.680 - 00:29:09.624, Speaker A: That is an interesting question and I can give you some thought around that. With an OTC test there's an immediate disadvantage in terms of trust. You're delegating trust to humans so you don't know what they're going to do to you or not.
00:29:09.624 - 00:29:28.344, Speaker A: The other thing with an OTC test they have avenues that we don't but we're transparent and on chain. And if they were to try to do this on layer one without some special contract, they would have to pay gas fees for each of those discrete trades. The paradigm breakthrough was brilliant and it rolled that into a socialized hang of the gas.
00:29:28.344 - 00:29:52.608, Speaker A: To do that and an aggregation. So we suspect an environment where people are not willing to delegate trust, want greater transparency in what their trade is doing, that this will probably yield a better execution. But the corollary to that is that in an environment where there's black pools that we don't have access to, it might be possible to get better execution.
00:29:52.608 - 00:29:55.490, Speaker A: But at that cost of that risk and.
