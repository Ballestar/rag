00:00:00.490 - 00:00:01.230, Speaker A: Yeah, nice.
00:00:01.300 - 00:00:02.320, Speaker B: Okay, cool.
00:00:02.930 - 00:00:07.690, Speaker A: So I think I've kind of changed.
00:00:07.770 - 00:00:10.366, Speaker B: Tag slightly from what was on the.
00:00:10.388 - 00:00:29.302, Speaker A: Kind of abstract thing and what I said I was going to present, partly because I ran out of time to prepare a bunch of new slides and partly also from just kind of overhearing conversations yesterday where I think I had multiple times. Like, I don't know what even is this corp of I think it kind of makes sense. And maybe there's some relevant bits that.
00:00:29.356 - 00:00:30.994, Speaker B: Shin and others have already been gesturing.
00:00:31.042 - 00:00:36.598, Speaker A: At, but maybe it's actually not really clear, I think, exactly what I mean by that and exactly what sorts of.
00:00:36.604 - 00:00:40.786, Speaker B: Problems I personally in that research community are interested in solving.
00:00:40.978 - 00:00:48.618, Speaker A: So I thought I'd kind of give just like a little kind of whistle stop kind of tour and quick kind of overview and intro and just to.
00:00:48.624 - 00:00:49.750, Speaker B: Kind of set the scene.
00:00:49.910 - 00:01:00.138, Speaker A: And then I'm going to hand over to other people so Andreas and Casper and Anthony and various people who are going to talk about some actual kind of slightly more kind of concrete bits.
00:01:00.154 - 00:01:01.246, Speaker B: Of work in this space.
00:01:01.348 - 00:01:04.174, Speaker A: Relevant to this kind of question of.
00:01:04.212 - 00:01:10.990, Speaker B: How we might be able to use credible commitments and commitment devices in order to reach more cooperative solutions in general. Some games.
00:01:11.150 - 00:01:15.986, Speaker A: Okay, so I'm going to move through the first part relatively quickly because this.
00:01:16.008 - 00:01:22.042, Speaker B: Is just like a relatively kind of intro presentation aimed at people who are maybe potentially slightly less technical.
00:01:22.126 - 00:01:23.078, Speaker A: And so I'm going to kind of.
00:01:23.084 - 00:01:26.662, Speaker B: Gesture at things at quite a high level and really just focus on this scene setting stuff.
00:01:26.716 - 00:01:27.638, Speaker A: And then I'm going to raise a.
00:01:27.644 - 00:01:29.462, Speaker B: Few questions right towards the end about.
00:01:29.516 - 00:01:36.586, Speaker A: This idea of real world. Kind of test them for deploying prop to AI algorithms, in which I'm hoping that you'll be able to kind of.
00:01:36.608 - 00:01:39.926, Speaker B: Like, mull over a little bit as we're seeing some of the other presentations.
00:01:40.038 - 00:01:42.174, Speaker A: And try to potentially kind of link.
00:01:42.212 - 00:01:43.546, Speaker B: This and think about how this links.
00:01:43.578 - 00:01:47.646, Speaker A: To work in the kind of real world settings that you, as researchers and.
00:01:47.668 - 00:01:49.760, Speaker B: Practitioners, are involved in.
00:01:50.210 - 00:01:57.930, Speaker A: Okay, so this is the outline, quick bit of motivation talk about what cooperative AI is.
00:01:58.020 - 00:01:59.090, Speaker B: Talk a little bit about the real.
00:01:59.160 - 00:02:00.658, Speaker A: World, although I try not think about.
00:02:00.664 - 00:02:01.700, Speaker B: That too much sometimes.
00:02:02.150 - 00:02:05.566, Speaker A: And then I mentioned I put discussion here. I don't know whether we'll have discussion.
00:02:05.598 - 00:02:08.326, Speaker B: After this or whether it's better to.
00:02:08.348 - 00:02:11.446, Speaker A: Move on to other things, but we'll see how that goes.
00:02:11.468 - 00:02:13.880, Speaker B: Maybe some questions will come up during the talk as well.
00:02:15.050 - 00:02:20.262, Speaker A: Okay, so this field in general is motivated by kind of two things.
00:02:20.316 - 00:02:23.034, Speaker B: And the first thing is cooperation is really important.
00:02:23.152 - 00:02:24.954, Speaker A: And the second thing is AI is.
00:02:24.992 - 00:02:27.030, Speaker B: Going to be increasingly important for cooperation.
00:02:27.190 - 00:02:28.426, Speaker A: And therefore we ought to think about.
00:02:28.448 - 00:02:29.958, Speaker B: AI in the context of cooperation.
00:02:30.054 - 00:02:31.642, Speaker A: So the importance of cooperation is probably.
00:02:31.696 - 00:02:33.450, Speaker B: Something that I don't necessarily need to.
00:02:33.520 - 00:02:38.606, Speaker A: Tell you all about, but just that many of the kind of biggest and.
00:02:38.628 - 00:02:44.974, Speaker B: Most important problems that we're facing are problems of cooperation. So this might range from kind of climate change and war pandemics and all sorts of things like this.
00:02:45.012 - 00:02:46.846, Speaker A: All of the kind of global kind.
00:02:46.868 - 00:02:49.300, Speaker B: Of coordination problems and challenges that we face.
00:02:49.750 - 00:02:52.926, Speaker A: And yet also many of our greater successes have been due to our ability to cooperate.
00:02:52.958 - 00:02:55.518, Speaker B: So we have trades, treaties, collaborations, et cetera.
00:02:55.694 - 00:02:59.614, Speaker A: And I think one of the takeaways here is that as societies and economies.
00:02:59.662 - 00:03:04.178, Speaker B: Military, et cetera, become kind of more powerful, more interconnected, then the need for cooperation grows.
00:03:04.274 - 00:03:12.778, Speaker A: So when we're all kind of little tribes living on little isolated islands at small scales, you didn't really need to the kind of cooperation tools that we could get away with using were kind.
00:03:12.784 - 00:03:17.530, Speaker B: Of relatively primitive and this was kind of all fine, and now that's kind of changing.
00:03:20.110 - 00:03:25.486, Speaker A: And so this is the kind of second part of the story, which is that I think, and I think it's reasonable to think that advanced AI is.
00:03:25.508 - 00:03:33.166, Speaker B: Going to both increase the power and interconnectedness of those actors and therefore the need for, but also hopefully the feasibility of cooperation. So on the one hand, you might.
00:03:33.188 - 00:03:36.782, Speaker A: Think that AI systems might exacerbate our worst cooperation problems.
00:03:36.836 - 00:03:41.618, Speaker B: So probably everyone has some idea of what this represents. But this is a little graph of.
00:03:41.624 - 00:03:44.674, Speaker A: The flash crash, which I don't need to talk about in this room because.
00:03:44.712 - 00:03:46.980, Speaker B: I'm sure you're all more than familiar of what that was.
00:03:48.390 - 00:03:54.854, Speaker A: There we go. And then on the other side of the thing, you might realize instead, you might ask instead if we can actually.
00:03:54.892 - 00:03:57.302, Speaker B: Kind of reverse the question and maybe.
00:03:57.356 - 00:04:00.906, Speaker A: Instead of AI being making things much worse, we can in fact kind of.
00:04:00.928 - 00:04:12.618, Speaker B: Leverage AI to make things a little bit better. And so this is what this incredibly trite and cliche graphic at the bottom. Oh, is this advancing? Yes, it is.
00:04:12.618 - 00:04:13.098, Speaker B: Okay?
00:04:13.184 - 00:04:17.054, Speaker A: So one thing I want to pick up on, which just really quickly before I move on, which is also something.
00:04:17.092 - 00:04:18.526, Speaker B: That surfaced in Jin's talk the other.
00:04:18.548 - 00:04:25.966, Speaker A: Day when he was talking about the space of commitment devices. That you might be able to design and how that might kind of be useful at some levels but actually then.
00:04:25.988 - 00:04:34.270, Speaker B: Lead to kind of worse outcomes from a kind of welfare perspective depending on kind of how you design that space. And this is basically the idea of this sliding.
00:04:34.430 - 00:04:36.946, Speaker A: So in some sense, cooperative capabilities can.
00:04:36.968 - 00:04:40.014, Speaker B: Be dual use, so forming credible commitments.
00:04:40.142 - 00:04:43.702, Speaker A: Can be used to kind of make offers and so on and do all the things that we want to.
00:04:43.756 - 00:04:47.480, Speaker B: They can also be used to make credible threats. Right, this seems bad.
00:04:48.250 - 00:04:52.154, Speaker A: Reaching mutually beneficial bargaining solutions would also.
00:04:52.272 - 00:04:54.214, Speaker B: Lead to collusion when we don't want collusion.
00:04:54.262 - 00:04:59.626, Speaker A: So I think it's interesting that I'll touch on this in a bit. But one of the areas that we.
00:04:59.648 - 00:05:03.722, Speaker B: Already see sophisticated multi agent systems already.
00:05:03.776 - 00:05:06.106, Speaker A: Existing in the real world are markets.
00:05:06.138 - 00:05:08.080, Speaker B: And automated trading agents and so on.
00:05:08.450 - 00:05:10.766, Speaker A: And yeah, actually we usually think that.
00:05:10.788 - 00:05:12.430, Speaker B: Competition there is a good thing.
00:05:12.580 - 00:05:15.854, Speaker A: So we actually would be maybe less happy if a bunch of our automated.
00:05:15.902 - 00:05:22.178, Speaker B: Trading agents suddenly decided to start colluding with each other. This might be bad from the market's perspective, so to speak, because of the.
00:05:22.184 - 00:05:28.486, Speaker A: Negative externalities induced and then final example is the idea that if you can.
00:05:28.508 - 00:05:30.902, Speaker B: Form alliances and factions, that actually could.
00:05:30.956 - 00:05:32.594, Speaker A: Lead to greater conflicts.
00:05:32.642 - 00:05:32.806, Speaker C: Right?
00:05:32.828 - 00:05:34.006, Speaker A: So here is a picture kind of.
00:05:34.028 - 00:05:37.334, Speaker B: Illustrating kind of the Cold War blocks in some ways.
00:05:37.532 - 00:05:43.766, Speaker A: And actually it was partly the fact that these two superpowers emerged and could kind of gather so many extra capabilities.
00:05:43.798 - 00:05:48.746, Speaker B: And resources that precipitated some of the kind of worst crises of that event.
00:05:48.928 - 00:05:50.406, Speaker A: And if you couldn't in fact join.
00:05:50.438 - 00:05:51.834, Speaker B: With your neighbors to try and take.
00:05:51.872 - 00:05:54.718, Speaker A: Down other people, then maybe this would.
00:05:54.724 - 00:06:08.370, Speaker B: Be something that would be more avoidable. Okay? So ideally what we want is this notion of what I'll call differential progress on cooperation. So I don't know if this might already be something that's familiar to some of you, but here's a graph.
00:06:08.370 - 00:06:12.130, Speaker B: It's a toy graph, it's not a real graph.
00:06:13.430 - 00:06:24.214, Speaker A: And the basic idea behind differential progress is that you might want to give AI systems capabilities which can improve their.
00:06:24.332 - 00:06:37.882, Speaker B: Kind of ability to cooperate more than it can improve their ability to do other things. And so capability here could be some kind of cocktail capabilities and capability here could be something else.
00:06:37.936 - 00:06:44.106, Speaker A: I don't know if it messy here, but the idea is that kind of.
00:06:44.128 - 00:06:50.606, Speaker B: Yeah, what here sort of for most things, I don't know. Obviously we have a bunch of stuff which is actually like superhuman at this.
00:06:50.628 - 00:06:53.838, Speaker A: Point and maybe like image recognition is.
00:06:53.844 - 00:06:55.726, Speaker B: Like here or something on this axis.
00:06:55.838 - 00:07:04.066, Speaker A: And I don't know, writing novels is like here and there's like a bunch of planning physics experiments is maybe even.
00:07:04.088 - 00:07:05.154, Speaker B: Less good or something.
00:07:05.272 - 00:07:11.926, Speaker A: So we're somewhere in this extremely high dimensional space, right? And then there's this idea that well, what we really want to do when.
00:07:11.948 - 00:07:13.734, Speaker B: We'Re kind of navigating this space is.
00:07:13.772 - 00:07:20.870, Speaker A: To kind of steer development of AI systems in a way that kind of can give them better cooperative capabilities more.
00:07:20.940 - 00:07:23.034, Speaker B: Than it kind of allows them to do other things.
00:07:23.152 - 00:07:24.426, Speaker A: And the first thing you might think.
00:07:24.448 - 00:07:35.882, Speaker B: Is like, okay, is this even possible? Is this even a coherent concept? Isn't just everything like Arbitrarily dual use, which I think is an interesting question, maybe like one extremely minimal existence proof.
00:07:36.026 - 00:07:42.986, Speaker A: Is that you can think of just in kind of purely cooperative settings where you still face kind of equilibrium selection.
00:07:43.018 - 00:07:49.506, Speaker B: Problems so where quarter coordination is kind of nontrivial. Then you can introduce things like a cheap talk channel, right? So in a cheap talk channel we.
00:07:49.528 - 00:07:52.338, Speaker A: Can use that to kind of coordinate our actions together.
00:07:52.424 - 00:07:53.746, Speaker B: And yet worse comes to worse, you.
00:07:53.768 - 00:07:58.238, Speaker A: Can just ignore what I'm going to say, right? And so it's not going to make.
00:07:58.264 - 00:07:59.910, Speaker B: Anyone worse off necessarily.
00:08:00.730 - 00:08:04.866, Speaker A: And so it might be interesting to think about whether there are certain kinds.
00:08:04.898 - 00:08:09.798, Speaker B: Of capabilities or certain sorts of things that we can imbue our artificial agents with that have those sorts of properties.
00:08:09.894 - 00:08:13.946, Speaker A: Or that, even if they don't kind of have no kind of downsides to.
00:08:13.968 - 00:08:21.686, Speaker B: Them, might kind of steer us roughly in the right direction. So, quick thing about what is cooperative.
00:08:21.718 - 00:08:24.222, Speaker A: AI is and what I actually mean by that. So I'm first going to talk about.
00:08:24.276 - 00:08:32.446, Speaker B: Where cooperative AI is. So general project of making AI systems go well and then you can kind.
00:08:32.468 - 00:08:34.398, Speaker A: Of carve this up very crudely at.
00:08:34.404 - 00:08:38.290, Speaker B: Least though not perfectly into technical work and non technical work.
00:08:38.440 - 00:08:41.394, Speaker A: And then you can maybe carve things up in technical work into stuff like.
00:08:41.432 - 00:08:45.854, Speaker B: Alignment and interpretability and robustness and corporate.
00:08:45.902 - 00:08:51.782, Speaker A: AI and other things. And in nontechnical work you maybe have.
00:08:51.836 - 00:08:53.906, Speaker B: Policy, you have norms, you have institutions.
00:08:53.938 - 00:08:55.766, Speaker A: You have governance mechanisms, you have all.
00:08:55.788 - 00:09:04.410, Speaker B: Sorts of kind of socioeconomic kind of standards and kind of practices, et cetera. Okay, wow, this is a little delay.
00:09:06.350 - 00:09:28.462, Speaker A: Here'S where property bear is. So it's kind of interesting in that it's like technical. So we work specifically on kind of building better AI systems, but we solve problems that we try to solve problems that looked a lot like non technical and governance problems, which is maybe something that you are all also familiar with, right? Because this is kind of similarly sort of the kind of crypto value proposition.
00:09:28.526 - 00:09:29.860, Speaker B: In some ways as well.
00:09:31.670 - 00:09:40.658, Speaker A: And then kind of like what is the so it's broadly a kind of subfield of AI that aims to improve the ability of AI systems to engender.
00:09:40.674 - 00:09:46.950, Speaker B: Cooperation between humans, machines and organizations by which I just broadly mean groups of humans and machines.
00:09:47.770 - 00:09:52.406, Speaker A: And the basic aim is to help produce risks. I thought I took this out, but.
00:09:52.428 - 00:09:55.466, Speaker B: This is an X risk, which means existential risks, which is something I care about.
00:09:55.488 - 00:10:05.838, Speaker A: But come and talk to me about that afterwards, if you'd like, from cooperation and failures resulting from AI and it forms, therefore it can natural kind of complement some of the technical work in alignment as well as some of the.
00:10:05.844 - 00:10:07.630, Speaker B: Non technical work on AI governance.
00:10:08.690 - 00:10:30.598, Speaker A: And so one other way to kind of cash this out is to talk about the task of improving the cooperative intelligence of AI systems, where one kind of like hand wavy working definition is this, which is based on the rather well known within the field of AI at least thing called the Leg Huter definition of universal intelligence, which is a.
00:10:30.604 - 00:10:31.958, Speaker B: Kind of informal notion of what it.
00:10:31.964 - 00:10:33.846, Speaker A: Means for any given agent to be.
00:10:33.868 - 00:10:35.590, Speaker B: Intelligent, but it nonetheless has some very.
00:10:35.660 - 00:10:38.518, Speaker A: Interesting kind of theoretical basis to it. And so the idea is that the.
00:10:38.524 - 00:10:45.340, Speaker B: Cooperative intelligence of an agent is its ability to achieve high joint welfare in a variety of environments with a variety of other agents.
00:10:46.270 - 00:10:48.906, Speaker A: And one thing I want to point out is that this definition focuses on.
00:10:48.928 - 00:10:51.018, Speaker B: Capabilities rather than dispositions, right?
00:10:51.104 - 00:10:53.054, Speaker A: So this is like I, in fact.
00:10:53.092 - 00:10:59.214, Speaker B: May be capable of kind of helping everyone to reach a very, very cooperative solution. I may just not want to and.
00:10:59.252 - 00:11:02.350, Speaker A: Likewise I might really desperately want everyone.
00:11:02.420 - 00:11:05.614, Speaker B: To get on and for the world to reach this huge kind of fantastic.
00:11:05.662 - 00:11:08.946, Speaker A: Cooperative kind of outcome and yet I.
00:11:08.968 - 00:11:10.130, Speaker B: Might be unable to.
00:11:10.200 - 00:11:11.934, Speaker A: And so there's a kind of distinction.
00:11:11.982 - 00:11:28.114, Speaker B: Here between capabilities and dispositions which is related to this issue of alignment which is like closely related to cooperation but very much not the same thing. And that's something I'll touch on in just a second in fact right now. So what is alignment?
00:11:28.162 - 00:11:51.854, Speaker A: You've probably heard about this, it's in the news a lot right now but better or worse. So alignment is basically the problem of you have a human or you have some principle that you care about and you have an agent and here's some kind of and basically what you want color. Color denotes kind of preferences or objectives or something and you want the other agents, the robots to care or act.
00:11:51.892 - 00:11:55.966, Speaker B: According to the human values. Great. That's what alignment is in a cartoon.
00:11:55.966 - 00:11:59.986, Speaker B: Here's what cooperation is in a cartoon, at least as construed by me, which.
00:12:00.008 - 00:12:01.570, Speaker A: Is you start with a bunch of.
00:12:01.640 - 00:12:09.830, Speaker B: Agents and they all have objectives already, right? They've all got nice little colors indicating they all have nice little objectives.
00:12:10.970 - 00:12:14.134, Speaker A: What you instead want to do is you want to take them with their.
00:12:14.172 - 00:12:15.846, Speaker B: Kind of preferences fixed as input to.
00:12:15.868 - 00:12:20.262, Speaker A: The problem and you want to make everyone get on and to reach good.
00:12:20.316 - 00:12:23.094, Speaker B: Outcomes where good might be something like.
00:12:23.132 - 00:12:24.218, Speaker A: I don't know, getting closer to the.
00:12:24.224 - 00:12:27.194, Speaker B: Pareto frontier as like a first approximation of this, right?
00:12:27.312 - 00:12:29.258, Speaker A: But then there are all sorts of other ways you might want to try.
00:12:29.264 - 00:12:36.174, Speaker B: And cash that out. And then one kind of, again, extremely cartoonish way that you can think about.
00:12:36.212 - 00:12:46.046, Speaker A: This is that sometimes people are like oh, kind of let's just focus on alignment, et cetera. Let's not worry about these corporation problems. But basically alignments doesn't get you all.
00:12:46.068 - 00:12:48.578, Speaker B: Of the way, right? It gets you part of the way.
00:12:48.664 - 00:13:01.926, Speaker A: But then here we have just like a bunch of axes who all maybe have I have my AI system, you have your AI system. Great, they're aligned and now we're going to be potentially in some kind of general sum scenario and this might end.
00:13:01.948 - 00:13:03.282, Speaker B: Badly for us if we're not careful.
00:13:03.346 - 00:13:04.806, Speaker A: So we still also want to be.
00:13:04.828 - 00:13:08.758, Speaker B: Working on this part of the problem too. So you can kind of compose these two things.
00:13:08.844 - 00:13:13.354, Speaker A: This more or less captures the kind of broad problem from a technical sense.
00:13:13.392 - 00:13:15.820, Speaker B: Of making AI systems go well.
00:13:16.350 - 00:13:18.138, Speaker A: And one kind of alternative view on.
00:13:18.144 - 00:13:24.254, Speaker B: That is that alignment can be seen as addressing this kind of vertical problem and cooperation can be seen as addressing a horizontal problem.
00:13:24.292 - 00:13:28.782, Speaker A: So what do I mean by this here? I'm indicating that by this kind of.
00:13:28.836 - 00:13:36.350, Speaker B: Vertical axis, I'm indicating some kind of normative precedence, right? So we just in fact care about this agent and their preferences more than this agent.
00:13:36.420 - 00:13:37.426, Speaker A: This is why they're kind of like.
00:13:37.448 - 00:13:39.026, Speaker B: Delegating to that agent, right, where this.
00:13:39.048 - 00:13:44.882, Speaker A: Is like a little robot and in kind of cooperation problems, we usually don't think that any one particular agent is.
00:13:44.936 - 00:13:48.580, Speaker B: Privileged beyond any others, right?
00:13:49.110 - 00:13:50.806, Speaker A: And so it's much more kind of.
00:13:50.828 - 00:13:54.200, Speaker B: At the same level of which we're trying to get them to coordinate together.
00:13:55.290 - 00:13:59.046, Speaker A: And then obviously the world is really much more complicated than this. And in fact, the world is much.
00:13:59.068 - 00:14:02.780, Speaker B: More complicated than this picture too here. I'm sure you've all noticed that.
00:14:04.910 - 00:14:07.066, Speaker A: And this brings me on to the.
00:14:07.088 - 00:14:14.640, Speaker B: Real world, which is a segue that I didn't have in my head that actually worked really nicely. Yeah, okay, cool.
00:14:16.770 - 00:14:18.238, Speaker A: Just quickly then, just talk about kind.
00:14:18.244 - 00:14:19.818, Speaker B: Of problems and progress in cooperative AI.
00:14:19.834 - 00:14:20.734, Speaker A: In terms of what people are working.
00:14:20.772 - 00:14:21.534, Speaker B: On at the moment.
00:14:21.652 - 00:14:23.118, Speaker A: So you might first ask, why are.
00:14:23.124 - 00:14:28.242, Speaker B: Cooperation problems in the context of AI different? Why is this not just like regular AI stuff?
00:14:28.376 - 00:14:30.434, Speaker A: Why is it not just regular study.
00:14:30.472 - 00:14:34.500, Speaker B: Of cooperation stuff which has been studied in a bunch of different subjects for a very long time?
00:14:35.350 - 00:14:37.102, Speaker A: So the first point is that cooperation.
00:14:37.166 - 00:14:44.198, Speaker B: Can be harder because AI. Agents don't possess some of the features that allow biological creatures to cooperate. And equally, cooperation can be easier in.
00:14:44.204 - 00:14:45.106, Speaker A: Some cases because AI.
00:14:45.138 - 00:14:50.310, Speaker B: Agents have, or at least can be constructed to have non biological features that might also allow for better cooperation.
00:14:50.970 - 00:14:52.838, Speaker A: And so these features introduce a range.
00:14:52.854 - 00:14:54.314, Speaker B: Of kind of interesting open problems.
00:14:54.432 - 00:14:58.666, Speaker A: Some of them are more or less open. So the first is kind of things.
00:14:58.688 - 00:15:06.542, Speaker B: About understanding so the world, the behavior preferences of other agents and dealing with recursive beliefs. So I believe that you believe that. I believe that you believe stuff that comes up in game through all the time.
00:15:06.542 - 00:15:08.350, Speaker B: We can also think relevant to this.
00:15:08.420 - 00:15:10.686, Speaker A: Topic, setting the ability to kind of.
00:15:10.708 - 00:15:15.614, Speaker B: Form commitments by devices such as delegation contracts, including hardware and so on.
00:15:15.812 - 00:15:18.002, Speaker A: We can also think about communication, so.
00:15:18.136 - 00:15:23.390, Speaker B: Dealing with issues around kind of common ground, important problems of bandwidth, latency teaching, learning, et cetera.
00:15:23.550 - 00:15:24.786, Speaker A: And then finally, at a kind of.
00:15:24.808 - 00:15:27.390, Speaker B: Meta level, we can also think about institutions more broadly.
00:15:27.470 - 00:15:29.718, Speaker A: And this might be informal institutions such.
00:15:29.724 - 00:15:31.526, Speaker B: As norms or reputation systems, but they.
00:15:31.548 - 00:15:38.806, Speaker A: Might also be kind of much more formal as well. And so then the thing that I want to kind of leave you with and I'm going to kind of use.
00:15:38.828 - 00:15:40.026, Speaker B: This as an excuse to pretty much.
00:15:40.048 - 00:15:49.066, Speaker A: Wrap up and hand over is that I think one thing that the AI field likes is to drive progress is.
00:15:49.088 - 00:15:52.538, Speaker B: Like test domains and challenges and things like this, right?
00:15:52.624 - 00:15:54.254, Speaker A: So I don't know if you heard.
00:15:54.292 - 00:15:57.422, Speaker B: About the ImageNet competition back in around.
00:15:57.476 - 00:16:00.206, Speaker A: 2012 which really ended up driving much.
00:16:00.228 - 00:16:03.626, Speaker B: Of the progress on deep learning and kind of highlighted to everyone how powerful.
00:16:03.658 - 00:16:05.486, Speaker A: These methods really were and kind of.
00:16:05.508 - 00:16:06.898, Speaker B: Kick started a lot of work in.
00:16:06.904 - 00:16:08.626, Speaker A: That area or a lot more work in that area.
00:16:08.648 - 00:16:09.538, Speaker B: Obviously it was going on for a.
00:16:09.544 - 00:16:17.926, Speaker A: Long time and yeah, in general this is like a feature of kind of lots of AI research is that people.
00:16:17.948 - 00:16:20.166, Speaker B: Get hooked on these specific problems and.
00:16:20.188 - 00:16:21.638, Speaker A: Challenges and what sorts of tools and.
00:16:21.644 - 00:16:25.400, Speaker B: Capabilities they can use to solve them.
00:16:25.770 - 00:16:27.206, Speaker A: You might actually argue that this is.
00:16:27.228 - 00:16:30.006, Speaker B: Now shifting away from this paradigm with.
00:16:30.028 - 00:16:31.242, Speaker A: The use of kind of these called.
00:16:31.296 - 00:16:33.162, Speaker B: So called foundation models and so on.
00:16:33.216 - 00:16:37.578, Speaker A: Where we actually just train some massive system based on a lot of kind.
00:16:37.584 - 00:16:46.890, Speaker B: Of relatively unstructured data and information and then fine tune it just to help us solve downstream tasks. But nonetheless, I still think this is an important paradigm.
00:16:46.970 - 00:16:50.046, Speaker A: So with my fields building hat on.
00:16:50.068 - 00:16:51.854, Speaker B: Which is like pilot work I do.
00:16:52.052 - 00:17:01.186, Speaker A: Then I want to claim that we actually need some kind of real world experimental test beds in which to empirically test corporate agents and this needs to.
00:17:01.208 - 00:17:03.182, Speaker B: Have ideally kind of certain features.
00:17:03.326 - 00:17:08.546, Speaker A: So the first is that they want to be kind of large and complex and actually relatively similar to real world.
00:17:08.568 - 00:17:14.918, Speaker B: Problems that we are likely to face but still kind of scoped enough to be studyable. Second, you need something that can be.
00:17:14.924 - 00:17:16.838, Speaker A: Done kind of safely and in at.
00:17:16.844 - 00:17:19.960, Speaker B: Least relatively low to moderate stakes way.
00:17:20.970 - 00:17:22.922, Speaker A: Yeah, you don't want to accidentally mess.
00:17:22.976 - 00:17:28.022, Speaker B: Anything up too badly. If you're deploying systems in the real world you also need to be informative.
00:17:28.086 - 00:17:31.158, Speaker A: So you need to be able to kind of monitor these experiments and systems.
00:17:31.174 - 00:17:33.210, Speaker B: And actually gain relative information about them.
00:17:33.280 - 00:17:35.034, Speaker A: And ideally, although this is always challenging.
00:17:35.082 - 00:17:37.962, Speaker B: In the real world you want them to be to some extent at least kind of reproducible.
00:17:38.026 - 00:17:45.278, Speaker A: This is all stuff about just like good science, right? Basically and ideally as well, you also as far as possible want them to.
00:17:45.284 - 00:17:50.274, Speaker B: Be kind of important and relevant to real world problems that people actually care about. So there might be something that's happening.
00:17:50.312 - 00:17:52.098, Speaker A: In the real world such as I.
00:17:52.104 - 00:17:54.359, Speaker B: Don'T know, I'm not going to be.
00:17:54.359 - 00:17:55.540, Speaker A: Able to think of a good example.
00:17:55.990 - 00:18:02.980, Speaker B: Like the way in which pigeons congregate in certain parts of London or something.
00:18:04.650 - 00:18:06.422, Speaker A: No one really cares about that or.
00:18:06.476 - 00:18:07.142, Speaker B: It'S not that important.
00:18:07.196 - 00:18:08.134, Speaker A: It's an interesting real world.
00:18:08.172 - 00:18:11.058, Speaker B: It's like a real world problem maybe that pigeons end up in certain parts.
00:18:11.074 - 00:18:11.978, Speaker A: Of London and you don't want them.
00:18:11.984 - 00:18:13.574, Speaker B: To be and maybe you can shepherd.
00:18:13.622 - 00:18:19.178, Speaker A: Them into various others. I don't know where I'm going with this example, but ideally if you want.
00:18:19.184 - 00:18:26.670, Speaker B: To hook people in like image recognition, image recognition comes up all the time. It's like a useful thing. This is partly why people are like oh wow, image recognition.
00:18:27.490 - 00:18:28.638, Speaker A: And so ideally you want to try.
00:18:28.644 - 00:18:38.660, Speaker B: And hook people in with something important but also relevant. But in reality there are actually pretty few general sam sophisticated multi agent systems out there right now.
00:18:39.110 - 00:18:43.458, Speaker A: This probably looks like it will change in future. So I think this produces then a.
00:18:43.464 - 00:18:45.378, Speaker B: Really open question of how can we.
00:18:45.384 - 00:18:49.538, Speaker A: Start working on some of these things at scale in the real world, trying out some of these kind of slightly.
00:18:49.554 - 00:18:51.286, Speaker B: More weird and wonderful ideas from the.
00:18:51.308 - 00:19:06.074, Speaker A: Kind of computational sciences and so on. So basically it leads people to work on things like this which is I don't know if you've seen this, this is the AI economist and if you actually look at the paper this is just like a graphic that they made.
00:19:06.112 - 00:19:08.874, Speaker B: On top for the paper. The real world doesn't even look this.
00:19:08.912 - 00:19:20.830, Speaker A: It doesn't even have 3D graphics or anything. There's a reason this has like it's really bright here. It's actually just an insanely basic grid world but nonetheless they put this on the website because it looks cooler anyway.
00:19:20.830 - 00:19:24.766, Speaker A: But essentially what you end up testing in these things is you have very.
00:19:24.788 - 00:19:26.926, Speaker B: Primitive kind of grid world esque models.
00:19:27.038 - 00:19:28.494, Speaker A: And you do some multi agent RLE.
00:19:28.542 - 00:19:38.894, Speaker B: Type stuff and so on. And this is good and important for testing intuitions. But as we all know, the real world is a bunch of kind of complexities which can't necessarily easily be captured by these things.
00:19:38.952 - 00:19:40.214, Speaker A: So I don't necessarily want to be.
00:19:40.252 - 00:19:42.742, Speaker B: Disparaging towards goodwills because they are nice.
00:19:42.796 - 00:19:45.398, Speaker A: And they are very useful and this.
00:19:45.404 - 00:19:48.054, Speaker B: Is more like this is my impression of like this is what the real.
00:19:48.092 - 00:19:51.386, Speaker A: World is like, of course, but this.
00:19:51.408 - 00:19:53.500, Speaker B: Is maybe in contrast to this.
00:19:54.750 - 00:19:58.090, Speaker A: Okay, and the final thing I'm really going to quickly going to do, okay.
00:19:58.240 - 00:20:05.054, Speaker B: This is like think about this. Your job for the rest of the.
00:20:05.092 - 00:20:13.646, Speaker A: Afternoon is to pitch interesting things about where some of these things that I've very loosely touched upon and things that.
00:20:13.668 - 00:20:15.166, Speaker B: You'Ll see much more detail on in.
00:20:15.188 - 00:20:19.858, Speaker A: The following talks and just try to think about some of these ideas because.
00:20:20.024 - 00:20:21.826, Speaker B: I do think there are interesting synergies here.
00:20:21.848 - 00:20:30.326, Speaker A: I think that maybe there's stuff that we can contribute and maybe there's also stuff that you can help us by. Kind of like you can help us get a nice shiny stick and point.
00:20:30.348 - 00:20:34.600, Speaker B: To your thing and be like, look a real world problem and we're going to solve it.
00:20:35.850 - 00:20:39.206, Speaker A: And then yeah, as a quick kind of plug to some of this. So I work at a place called.
00:20:39.228 - 00:20:40.650, Speaker B: The Cooperative AI Foundation.
00:20:41.230 - 00:20:42.406, Speaker A: These are the trustees.
00:20:42.438 - 00:20:43.446, Speaker B: So this is Alan the foe.
00:20:43.478 - 00:20:44.614, Speaker A: He's at deepmines.
00:20:44.662 - 00:20:46.102, Speaker B: Eric orbitz Microsoft.
00:20:46.166 - 00:20:48.486, Speaker A: Jillian hadfield she's at OpenAM Toronto.
00:20:48.518 - 00:20:52.874, Speaker B: This is Dario, the CEO of Anthropic. And this is Rue, who's at Polaris.
00:20:52.922 - 00:20:57.594, Speaker A: Ventures, who's our funder. And we're a kind of charitable entity.
00:20:57.642 - 00:20:58.990, Speaker B: We're a research foundation.
00:20:59.330 - 00:21:06.002, Speaker A: We're a field building organization. This is kind of our is, if you're interested.
00:21:06.056 - 00:21:07.950, Speaker B: This is my regular Spiel slide.
00:21:08.030 - 00:21:09.474, Speaker A: So we have a bunch of things.
00:21:09.592 - 00:21:11.700, Speaker B: That you can find out about us online.
00:21:12.790 - 00:21:15.922, Speaker A: And I think you've already mostly done.
00:21:15.976 - 00:21:23.430, Speaker B: All of the further reading because Jin was very insistent that everyone should read these things despite the incredibly short notice.
00:21:23.850 - 00:21:26.774, Speaker A: So you've already read these things, I guess. But if you want to read more.
00:21:26.812 - 00:21:27.926, Speaker B: Things, come and talk to me or.
00:21:27.948 - 00:21:29.158, Speaker A: Read the papers that are about to.
00:21:29.164 - 00:21:31.434, Speaker B: Be presented on which maybe you will have also read by this point.
00:21:31.472 - 00:21:33.340, Speaker A: But anyway, that's it.
00:21:34.110 - 00:21:34.474, Speaker D: Yeah.
00:21:34.512 - 00:21:35.818, Speaker B: I don't know what time it is now.
00:21:35.904 - 00:21:37.290, Speaker A: When there's time for discussions.
00:21:37.790 - 00:21:42.000, Speaker B: 230. Fantastic, thanks. We've actually got loads of time.
00:21:46.210 - 00:21:48.126, Speaker A: I don't really know. I could open it up, I guess.
00:21:48.148 - 00:21:53.946, Speaker B: First of all, just in case there are any kind of questions or anything that anyone wants to kind of raise.
00:21:53.978 - 00:21:55.166, Speaker A: Or flag or chat about, or if.
00:21:55.188 - 00:21:58.318, Speaker B: Anyone has any solution, like interesting examples.
00:21:58.334 - 00:22:02.274, Speaker A: Of these kind of real world problems and so on, that can be an interesting thing to do.
00:22:02.312 - 00:22:03.694, Speaker B: Kind of like now ish.
00:22:03.822 - 00:22:20.454, Speaker A: And then I kind of like, slightly threw Andreas under the bus because I came to his talk yesterday morning when I arrived in London, or his poster, and it was really cool and it's relevant to commitments, so I've now invited.
00:22:20.502 - 00:22:22.634, Speaker B: Him to give a talk in this.
00:22:22.672 - 00:22:27.322, Speaker A: Slot instead, or as well as me. So he's very kindly agreed to do that.
00:22:27.456 - 00:22:36.000, Speaker B: But I still think we have capacity. So I'm going to stop talking now and if anyone wants to questions and we can do that.
00:22:42.080 - 00:22:54.256, Speaker E: When you talk about real world examples, you look at online auctions often, so Ebay is one example where people have whatever, but maybe more specific examples like.
00:22:54.278 - 00:22:59.190, Speaker A: Ad auctions as well. Yeah, lots of people do work on this.
00:22:59.640 - 00:23:01.524, Speaker B: I do not personally work on this.
00:23:01.642 - 00:23:04.324, Speaker A: I guess this is one of the.
00:23:04.442 - 00:23:07.616, Speaker B: Main applications of serious algorithmic gain theory.
00:23:07.648 - 00:23:10.890, Speaker A: And so on, which is like a very good example.
00:23:11.900 - 00:23:19.610, Speaker B: Yeah, no, I've not really thought about that personally, but yeah, this is great. More like this, please. Yeah.
00:23:25.280 - 00:23:49.120, Speaker C: So I found very useful the description for distinguishing versus operational. It does seem to me that what you call AI in the two cases might be different or different levels of analytical problem. You need the AIS to develop their own goals.
00:23:49.120 - 00:24:04.632, Speaker C: An AI complex enough to have their own goals so that I can disappear. Comparative AI main impression is that with much less powerful AIS, you already have an interesting thing to talk about. Right.
00:24:04.766 - 00:24:06.552, Speaker B: In the sense that if you're like.
00:24:06.606 - 00:24:24.616, Speaker C: Any of AI that's fairly recommend you want to play some game with humans and with the use of programs to make the outcome better, you already have wrong there and the AIS there don't need to have the capacity to rid of their own goals.
00:24:24.808 - 00:24:26.384, Speaker B: That makes sense.
00:24:26.582 - 00:24:33.856, Speaker A: Yeah, I think it does. And I do agree that alignment, when you have extremely basic agents, is not.
00:24:33.878 - 00:24:35.920, Speaker B: That much of an interesting or not.
00:24:35.990 - 00:24:40.692, Speaker A: As much of a problem. The basic thing, you can just train.
00:24:40.746 - 00:24:42.676, Speaker B: Some incredibly basic agent to perform some.
00:24:42.698 - 00:24:45.844, Speaker A: Incredibly basic task and I don't know.
00:24:45.962 - 00:24:47.412, Speaker B: Probably it just does the first time around.
00:24:47.466 - 00:24:49.028, Speaker A: If not, if it's incredibly simple, you.
00:24:49.034 - 00:24:50.976, Speaker B: Could literally just look at the algorithm.
00:24:51.008 - 00:24:51.876, Speaker A: And just change it.
00:24:51.898 - 00:24:58.088, Speaker B: This is just like the basic idea of debugging code. Or even if it's doing something slightly not what you want, it's just kind.
00:24:58.094 - 00:24:59.752, Speaker A: Of like operating on some small system.
00:24:59.806 - 00:25:00.888, Speaker B: And you can just fiddle around with.
00:25:00.894 - 00:25:01.784, Speaker A: It and hack around with it until.
00:25:01.822 - 00:25:05.068, Speaker B: It basically works and you can be kind of satisfied that it's going to do what you want it to do.
00:25:05.154 - 00:25:07.624, Speaker A: And then if you have extremely powerful.
00:25:07.672 - 00:25:09.356, Speaker B: And capable agents that are doing much.
00:25:09.378 - 00:25:12.156, Speaker A: More complicated tasks, potentially even tasks that.
00:25:12.178 - 00:25:33.316, Speaker B: Humans can't really understand. Right, so this is the ideal aim with AI research, is that machines will be able to do things that humans can't even do. And then as soon as you have that sort of thing, then it becomes much more tricky with the alignment thing and maybe cooperative problems do emerge, maybe earlier in some sense, because it's more.
00:25:33.338 - 00:25:37.476, Speaker A: Feasible that things could be kind of.
00:25:37.498 - 00:25:39.430, Speaker B: Misaligned with each other, so to speak.
00:25:40.040 - 00:25:43.416, Speaker A: At the same time. However, when games are small and when.
00:25:43.438 - 00:25:46.056, Speaker B: Agents are simple, we can just look.
00:25:46.078 - 00:25:49.496, Speaker A: At them using standard kind of game theoretic methods and kind of analyze them.
00:25:49.518 - 00:25:51.416, Speaker B: And solve problems and proof theorems about.
00:25:51.438 - 00:25:54.324, Speaker A: Them in a kind of quite nice way, in a way which we can't.
00:25:54.372 - 00:25:56.952, Speaker B: Usually do with big, sophisticated AI systems.
00:25:57.016 - 00:25:58.988, Speaker A: So I think that both problems definitely.
00:25:59.074 - 00:26:07.024, Speaker B: Scale and get much more difficult with kind of the complexity of the agents. Yeah.
00:26:07.062 - 00:26:39.080, Speaker F: I was interested in what you said about the biological features that should be either avoided or maybe copied to facilitate cooperative AI. And I didn't agree with the premise, but I was kind of wondering whether you had more specific ideas of things that might be nice to replicate in terms of biological features and things that shouldn't be avoided. What's your take on it?
00:26:39.150 - 00:26:42.216, Speaker A: Yeah, maybe like one idea for a.
00:26:42.238 - 00:26:48.476, Speaker B: Biological feature which would be nice to replicate machines, although it was potentially risky, is theory of mind.
00:26:48.578 - 00:26:50.830, Speaker A: So humans have theory of mind, right.
00:26:52.080 - 00:26:55.276, Speaker B: By which I mean kind of I kind of like look at you and what.
00:26:55.298 - 00:26:59.664, Speaker A: You're doing. And I can make some pretty good inferences about what you kind of want.
00:26:59.702 - 00:27:03.504, Speaker B: Or are thinking about based on your actions because I happen to at least.
00:27:03.542 - 00:27:06.512, Speaker A: Believe that your mind is sufficiently similar.
00:27:06.566 - 00:27:07.548, Speaker B: To mine where I can do this.
00:27:07.574 - 00:27:10.160, Speaker A: And machines don't have really this notion.
00:27:10.320 - 00:27:21.952, Speaker B: So kind of inference of intentions and so on. And intent is something that humans actually find pretty mostly quite easy. At least we sometimes think of it as hard, but mostly in complex settings.
00:27:21.952 - 00:27:27.050, Speaker B: Most of the time it's actually pretty simple. And machines don't really have this yet, so this could be a good thing.
00:27:27.900 - 00:27:32.988, Speaker A: And then yeah, with the kind of non biological things I'm thinking more about.
00:27:33.074 - 00:27:34.190, Speaker B: Things like.
00:27:36.320 - 00:27:38.872, Speaker A: The speed at which machines.
00:27:38.936 - 00:27:43.340, Speaker B: Can operate, the fact that they can operate in highly distributed settings, the fact.
00:27:43.410 - 00:27:46.844, Speaker A: That they are written in code and.
00:27:46.882 - 00:27:49.232, Speaker B: We can actually, in theory at least.
00:27:49.286 - 00:27:50.896, Speaker A: We can look at code and figure.
00:27:50.918 - 00:27:58.124, Speaker B: Out what it's doing and why. Now, this is pretty hard when you have big inscrutable neural networks, but this is why a bunch of people work on a turbulency.
00:27:58.252 - 00:28:01.750, Speaker A: And so, yeah, there are kind of things like that.
00:28:02.680 - 00:28:18.824, Speaker F: Just a small pull up. Have you thought a bit more? But maybe the memory part of what I can because it's been studied in AI. Right, quite like how models can forget what you deal with that.
00:28:18.824 - 00:28:31.372, Speaker F: And I was wondering whether you can take inspiration on some biological pictures of actual human regarding memory and kind of I was just wondering if this was something I know.
00:28:31.426 - 00:28:37.292, Speaker A: Yeah, so there's definitely work about this in the idea of computational boundedness obviously.
00:28:37.346 - 00:28:41.152, Speaker B: Or boundedly rational agents does come up all the time in game theory and so on.
00:28:41.206 - 00:28:52.596, Speaker A: And it is studied too in kind of like AI stuff. But my impression is actually not as much as you would expect given that AI people CS theory people love to.
00:28:52.618 - 00:28:58.340, Speaker B: Talk about computational bounds on things and complexity stuff.
00:28:58.490 - 00:29:00.224, Speaker A: And yet when it comes to applying.
00:29:00.272 - 00:29:28.080, Speaker B: Those sorts of ideas to kind of like games and kind of multi agent kind of interactions, my impression is that this has been done less nathaniel actually might be like a person who's able to comment on this more easily than I because this is slightly more of his wheelhouse. But yeah, I don't any is that a thing? Am I, like, mischaracterizing people actually work on this loads. I would agree to boundably rational stuff is less popular.
00:29:28.080 - 00:29:35.410, Speaker B: I shouldn't say unpopular, but it's oh, yeah, perfect.
00:29:37.320 - 00:30:21.184, Speaker E: The problem is that mathematically so there is work in the last few years on mechanism design specific form of rationality, which is absence of contingent reasoning that works for humans but doesn't work for machines. There is work done by Alpen and tries to model computational complexity in decision making, but by themselves it's not being picked up after. My work is trying to actually the techniques I've developed for it's called obvious strategy progress to see whether I can apply them to this Harper and Pass computational complexity stuff.
00:30:21.184 - 00:30:30.792, Speaker E: But it is complicated and I agree it's very much understudied, but it's because of the and I can hear these risk quests rewards.
00:30:30.876 - 00:30:31.076, Speaker B: Right.
00:30:31.098 - 00:30:40.230, Speaker E: So if you work on soft and well published, that's not something many people went to. That's unfortunate ideology and not that many people went to it.
00:30:40.600 - 00:30:48.296, Speaker A: That is the one paper that I always think of halpen and Pass is kind of thing. It's called like game theory with computational costs or something.
00:30:48.478 - 00:31:08.140, Speaker E: Yeah, ice cream is cleaner, I think, as a mathematical model. But again, it works for human software. You mentioned a concept of this search contingent.
00:31:08.140 - 00:31:17.996, Speaker E: Basically people are not able to limit the nuts in their head when they make a decision. Just agree with you. So this is experimental finding.
00:31:17.996 - 00:31:46.808, Speaker E: If you run sell on price options with their people and you do it in silly passion and we are sending price passion, it's exactly the same option. So people should not play differently, but in practice they misunderstand the formula much more than so this OSP concept exactly captures this difference between interpretations. And there's also what I mentioned yesterday in the Q A with David.
00:31:46.808 - 00:32:13.670, Speaker E: There's also work done on experimental strategy, which again tries to get to the bottom of why people understand certain options or mechanisms. But I think in the arm, in this context, if the programming of the system misunderstands incentives, we still have the same problem because they cannot call it up properly. I had a different question, but I don't know.
00:32:14.280 - 00:32:21.050, Speaker D: Yeah, I got a very quick question. So I was thinking about ways of enforcing these collisions. Right.
00:32:21.050 - 00:32:29.400, Speaker D: A very interesting question is how can you ensure that if there's a greater collision it's not split up?
00:32:29.550 - 00:32:33.004, Speaker A: And something that I was wondering and.
00:32:33.122 - 00:32:46.370, Speaker D: Has been obviously explored in the real world is credible threats. And an interesting thought could be whether enforcement could be from one agent onto the other. So for example.
00:32:49.380 - 00:32:51.168, Speaker A: If say some agent.
00:32:51.334 - 00:33:22.360, Speaker D: Breaks from liquidation, then the other artificial agents could leverage I'm giving a very stable example, right. Could try to find some kind of vulnerability or a way to attack and so forth. And the threat of this, given that these agents, say are very capable or whatever, has the effect of enforcing to some degree coalition.
00:33:22.360 - 00:33:33.950, Speaker D: So my question is, do you know any resources or work exploring threats among agents as means of enforcing these things?
00:33:37.920 - 00:33:55.524, Speaker B: I guess there's loads of stuff on this. This is basically, for example, what the grim trigger strategy does, which is the kind of source of the folk theorems in game theorem, which is like some of the most kind of fundamental results, which is basically like playing iterated version of business like Lana or something. And you're like cool.
00:33:55.562 - 00:33:56.196, Speaker A: The way I'm going to get you.
00:33:56.218 - 00:34:06.410, Speaker B: To cooperate is if you ever defect, I'm just going to punish you for eternity or something, at least in infinitely repeated game.
00:34:09.260 - 00:34:12.696, Speaker A: And in terms of coalitions, I think you can do this as well.
00:34:12.718 - 00:34:17.160, Speaker B: I actually haven't worked that much on this kind of cooperative game theory type stuff where you think about coalitions.
00:34:17.240 - 00:34:20.972, Speaker A: But yeah, often there's posited to be.
00:34:21.026 - 00:34:30.144, Speaker B: Some mechanism via which you can in fact kind of punish deviators, essentially, and this is what allows you to form the coalitions to begin with.
00:34:30.342 - 00:34:35.824, Speaker A: So I kind of mentioned threats as like an obviously bad thing, but it's actually far from true that they're an obviously bad thing.
00:34:35.862 - 00:34:40.796, Speaker B: Right. We have socially kind of desirable kind of threats made all the time, right.
00:34:40.838 - 00:34:42.388, Speaker A: Like the fact that the state has.
00:34:42.394 - 00:34:47.408, Speaker B: A monopoly on violence and if you kind of go and do something horrible, you're going to get put in prison.
00:34:47.504 - 00:34:49.264, Speaker A: Is know that's a threat.
00:34:49.312 - 00:34:51.280, Speaker B: If you do this, you're going to get to prison.
00:34:51.360 - 00:34:56.872, Speaker A: But it actually just turns out that we want those sorts of things mostly. And so, yeah, I think it gets.
00:34:56.926 - 00:35:05.230, Speaker B: Back to kind of Jin's point before about, okay, what commitments do you want agents to be able to make? What commitments do you not want agents, certain agents to be able to make and so on.
00:35:08.720 - 00:35:12.830, Speaker D: Between artificial agents, so to speak. Right.
00:35:14.880 - 00:35:15.630, Speaker B: Yeah.
00:35:18.160 - 00:35:25.948, Speaker E: I had a comment and maybe more than a question. So your framework, you went human regions, alliance.
00:35:26.044 - 00:35:26.448, Speaker B: Yeah.
00:35:26.534 - 00:35:29.060, Speaker E: Agents, wages, cooperation, right?
00:35:29.210 - 00:35:31.808, Speaker A: Yeah. Although you can also think of cooperation.
00:35:31.904 - 00:35:39.316, Speaker B: As happening at a level between kind of humans and AIS. Or like humans and humans or any.
00:35:39.338 - 00:35:40.896, Speaker A: Sort of agent where you don't immediately.
00:35:40.928 - 00:35:44.090, Speaker B: Think there's some kind of normative precedence, I guess.
00:35:44.460 - 00:35:46.904, Speaker A: So it's that what does the work.
00:35:46.942 - 00:35:53.550, Speaker B: And usually we think of humans as having normative precedence over, say, machines. And who knows if that will be the case.
00:35:56.400 - 00:36:06.024, Speaker E: I have this example in mind of pricing algorithms. The alignment is perfect. So as a supplier, I want to maximize my reward.
00:36:06.024 - 00:36:20.980, Speaker E: So the agent does exactly that. And when they go into the real world, there's emergent cartel. Even though they were not programmed to do the cartel, then they do the cartel, which means the problem is in the societal alignment.
00:36:20.980 - 00:36:24.580, Speaker E: That's what I'm trying to say. This is much harder than you need.
00:36:24.730 - 00:36:26.150, Speaker A: Yeah, definitely.
00:36:26.680 - 00:36:28.388, Speaker B: But you also might think, and I.
00:36:28.394 - 00:36:35.476, Speaker A: Think Casper might talk about this a bit later, potentially, is that if we can in fact delegate to AI systems.
00:36:35.508 - 00:36:41.016, Speaker B: To make some of these decisions on our behalf, they could actually end up being better at cooperating than we are.
00:36:41.198 - 00:36:42.792, Speaker A: And so they might take our interest.
00:36:42.846 - 00:36:47.260, Speaker B: Into account, but they might help us reach kind of more kind of cooperative outcomes.
00:36:48.160 - 00:36:54.060, Speaker A: But yeah, fundamentally there is definitely like a bunch of these problems are going to actually surface at the level of.
00:36:54.130 - 00:37:00.450, Speaker B: Societal kind of cooperation problems instead of what's happening at the agent agent level. So to speak.
00:37:02.900 - 00:37:11.460, Speaker E: We brought up about social progression, new York's model norms or how AIS may learn some social norms that we have as humans.
00:37:12.200 - 00:37:12.900, Speaker A: Yeah, definitely.
00:37:12.970 - 00:37:14.596, Speaker B: There's a bunch of work in that area.
00:37:14.698 - 00:37:16.612, Speaker A: It's not something I personally have worked.
00:37:16.666 - 00:37:26.810, Speaker B: On myself, but there are lots of work on kind of like norms and AI systems and so on. Norms and multi agent systems and various different ways of formalizing them as well.
00:37:29.740 - 00:37:30.200, Speaker A: Exactly.
00:37:30.270 - 00:37:30.456, Speaker B: Yeah.
00:37:30.478 - 00:37:34.856, Speaker A: So some people do it as kind of like these logical constraints. Some people do it as kind of.
00:37:34.878 - 00:37:36.460, Speaker B: Soft kind of losses.
00:37:37.040 - 00:37:37.500, Speaker D: Yeah.
00:37:37.570 - 00:37:39.276, Speaker A: Some people do them as kind of.
00:37:39.298 - 00:37:41.244, Speaker B: Like system imposed, some kind of try.
00:37:41.282 - 00:37:44.224, Speaker A: To model the emergence of norms and.
00:37:44.262 - 00:37:50.896, Speaker B: Yeah, there's very many different ways of approaching it. I don't necessarily think it's a solved problem, but it's definitely something that people.
00:37:51.078 - 00:37:53.840, Speaker E: Brought more of your studies.
00:37:55.940 - 00:37:56.690, Speaker B: Yeah.
00:37:58.100 - 00:38:03.360, Speaker A: In general I'm kind of fits this audience. I guess I'm kind of like anti.
00:38:03.440 - 00:38:15.768, Speaker B: Approaches where you have to rely on some massive system designer who can just impose a norm on everyone else. Ideally you want something that's kind of like self sustaining and somewhat decentralized because.
00:38:15.854 - 00:38:19.210, Speaker A: That is actually what's going to function at scale in the real world.
00:38:21.020 - 00:38:21.432, Speaker D: Yeah.
00:38:21.486 - 00:38:25.756, Speaker A: And I'm also usually a bunch of this norms work was done in the.
00:38:25.778 - 00:38:41.650, Speaker B: Context of various kinds of logics in multi agent systems and so on, which was very fashionable in the so on and has not really stood the test of time. If you ask me personally, I can say that because my supervisor isn't here.
00:38:42.740 - 00:38:46.548, Speaker A: Who'S basically like the person yeah, I.
00:38:46.554 - 00:38:55.364, Speaker B: Don'T know if anyone knows him, but yeah. Okay, thank you. Cool.
00:38:55.482 - 00:38:57.750, Speaker D: There's also work on learning to.
00:38:59.720 - 00:39:00.096, Speaker B: Training.
00:39:00.138 - 00:39:02.344, Speaker D: Agents to be able to learn what.
00:39:02.382 - 00:39:13.592, Speaker B: Normally exists and cooperate with that. Yeah, including SCR or the san her along, which is where the fan I used to work I also temporarily worked, in fact.
00:39:13.646 - 00:39:15.004, Speaker A: Yeah, I don't know. Is Anthony here?
00:39:15.042 - 00:39:17.452, Speaker B: Anthony's worked on this. Is Anthony going to present later?
00:39:17.506 - 00:39:18.200, Speaker A: I've forgotten.
00:39:18.280 - 00:39:20.524, Speaker B: Yeah. Great. You can ask him about this.
00:39:20.524 - 00:39:21.710, Speaker B: He's worked on this.
00:39:22.960 - 00:39:23.324, Speaker D: Yeah.
00:39:23.362 - 00:39:29.244, Speaker A: And then one kind of obviously interesting question is like okay, so say we get to the pareto frontier.
00:39:29.372 - 00:39:30.608, Speaker B: Great, that's good.
00:39:30.694 - 00:39:31.968, Speaker A: Now we just have to pick a.
00:39:31.974 - 00:39:35.536, Speaker B: Place on the pareto frontier. All of a sudden that gets pretty.
00:39:35.558 - 00:39:39.664, Speaker A: Challenging because you can think about kind of like what trade offs to make.
00:39:39.702 - 00:39:42.676, Speaker B: Between different agents and so on and what is right there.
00:39:42.858 - 00:39:44.308, Speaker A: And it may in fact be the.
00:39:44.314 - 00:39:47.156, Speaker B: Case that agents just have kind of different intrinsic norms about this.
00:39:47.178 - 00:39:55.688, Speaker A: So maybe I adopt some kind of total utilitarian style. So maybe you're kind of like more of an egalitarian and you want to kind of maximize the welfare of the.
00:39:55.694 - 00:39:58.696, Speaker B: Kind of worst off person and things like this and these things are then.
00:39:58.718 - 00:40:00.136, Speaker A: Just like in some ways just can.
00:40:00.158 - 00:40:01.656, Speaker B: Be kind of incompatible often.
00:40:01.758 - 00:40:03.160, Speaker A: And so then there's a question about.
00:40:03.230 - 00:40:06.876, Speaker B: What do you do when you get to games with incompatible norms? And this is some of the stuff.
00:40:06.898 - 00:40:08.108, Speaker A: That Anthony was working on.
00:40:08.194 - 00:40:15.556, Speaker B: So you can ask a question, what.
00:40:15.578 - 00:40:18.216, Speaker D: Does field beauty mean?
00:40:18.398 - 00:40:26.024, Speaker B: Yeah, I guess it depends what kind of field you're trying to build.
00:40:26.142 - 00:40:27.768, Speaker A: This is like an academic field.
00:40:27.854 - 00:40:29.980, Speaker B: So field building in an academic field.
00:40:30.050 - 00:40:33.644, Speaker A: Mostly looks like trying to build up.
00:40:33.682 - 00:40:41.790, Speaker B: A community of researchers and practitioners who are dedicating their working efforts to try and solve these problems.
00:40:42.880 - 00:40:44.972, Speaker A: And in practice, what that looks like.
00:40:45.026 - 00:40:52.524, Speaker B: Is doing things like helping co run events like this or workshops or putting on.
00:40:52.562 - 00:40:54.368, Speaker A: We're doing a Kind contest at Europe's.
00:40:54.384 - 00:40:58.390, Speaker B: Which is the big AI conference that's happening in December. There'll be a big AI contest there.
00:40:59.080 - 00:41:05.510, Speaker A: We publish materials online. We're hosting a summer school. We're going to do kind of grant making.
00:41:06.200 - 00:41:15.650, Speaker B: Yeah, sort of like mostly what you'd expect, but yeah. You have any ideas for how to do field building better also come talk to me and tell me how to do it.
