00:00:13.930 - 00:00:28.870, Speaker A: Hello everyone, thanks a lot for being here. I'm Lloyd, I work at Atlantis Labs. And today in this talk, I will present to you why and how we migrated away from Zegraph.
00:00:28.870 - 00:00:43.740, Speaker A: So, Zegraph is a very well known product in the CRM ecosystem. I will assume that most of you are familiar with it. If you don't, I'm sorry, the next 15 minutes will be a bit long.
00:00:43.740 - 00:00:52.014, Speaker A: The graph is an amazing product. We used it for quite some time at Atlantis. We launched our production with it.
00:00:52.014 - 00:01:07.970, Speaker A: It worked very well. It was just that at some time we realized it was not properly aligned with our intended objectives. So in this talk, I will first begin by presenting to you the reasons behind this migration.
00:01:07.970 - 00:01:20.998, Speaker A: Then we'll see how it was done or what has replaced it. And finally, I will share with you the lessons that we learned during this journey. A little bit of context first.
00:01:20.998 - 00:01:50.750, Speaker A: So what is Atlantis? So, it's a capital efficient credit protocol connecting DeFi with real world business use cases. On one side, it will allow borrowers to set up their own liquidity pool on chain and to borrow from it. And on the other side, it will allow lenders to select their borrower, deposit their funds in the associated liquidity pool at the rate of their choice, the rate being the rate at which their funds will be later borrowed.
00:01:50.750 - 00:02:12.918, Speaker A: What do I do in all that? So, I'm a software engineer for around five years. I entered the space in a company named Consensus and I joined Attendees Labs since April 2022. My job is quite simple, I basically make sure that everything works smoothly, from the smart contract end to the front end end.
00:02:12.918 - 00:02:40.782, Speaker A: So I will spend a bit of time on the front end helping on structural decisions, I will help a bit on the smart contracts in order to take decisions and implement the smart contract. And most importantly, I will spend most of my time on the back end, which is used in order to connect the dots between smart contracts and the front end. I'm the one developing it and maintaining it, and it's the service replacing the graph in our architecture today.
00:02:40.782 - 00:03:12.120, Speaker A: So that's why I'm here talking about it to you today. So first, why did we migrate at the time? Our needs were first to serve the data that the front end needed. So we have a DeFi app, so we have a lot of complex data to show, we have amounts everywhere, rates everywhere, and the goal was simply to be able to serve this data and simplify as much as we could the work that was needed and done front end side.
00:03:12.120 - 00:03:41.202, Speaker A: Then we wanted to have some insight about what was going on in our protocol. So the ability to be able to send notification based on particular events or perform some data analytics in order to see what's going on. Atlantislab is not the kind of app where we spend 12 hours a day clicking buttons and sending transactions so we don't need a crazy charge in terms of user and we wanted to keep things simple, as simple as we actually needed it.
00:03:41.202 - 00:04:06.742, Speaker A: And finally what we wanted the most was flexibility. So we are a young startup and things are moving fast and wanted the flexibility that we needed reflected at the code base level. Then on the second side, what were the issues that were faced with the graph at the time? So as I said, the front end was getting more and more complex so our subgraph subgraph code was getting more and more complex.
00:04:06.742 - 00:04:20.750, Speaker A: We had a lot of entities to manage, a lot of handlers to implement and to properly handle each entity's change. So the subgraph code was growing by quite a lot. The debugging was quite hard.
00:04:20.750 - 00:04:51.994, Speaker A: We definitely did not spend enough time on proper logging. But even with that where we had an issue in a deployed subgraph, actually replicating the issue locally and being able to fix it as fast as we can was quite painful. A third point was we were using the hosted service so we were naturally limited by the weight limiting which is perfectly normal and also we were suffering from the downtime that may happen at the hosted service at the time.
00:04:51.994 - 00:05:05.194, Speaker A: And finally but most importantly, the hosted service was actually getting duplicated. So in any case we needed to make a move. So we came up with two possible decisions.
00:05:05.194 - 00:05:33.638, Speaker A: The first one was to actually use the graph node of the graph and run it on our site. So we knew that we were starting from a strong base with actually a nice code that has already a lot of features. The downside that we saw was actually that the code base was in Rust so it's very good for the performance but actually most of our team is heavily oriented towards JavaScript and secondly it doesn't serve well the needs front end side that we had.
00:05:33.638 - 00:06:01.534, Speaker A: But in any case we needed to build another microservice on the side in order to answer the other needs which means managing an infrastructure and we didn't want to spend time on that at the time. The second solution is more simple actually it's to redo everything. So we redevelop everything, we maintain everything and the things that we saw in that was to gain the full control that I think we wanted and most importantly the flexibility that we wanted.
00:06:01.534 - 00:06:20.150, Speaker A: So that's why we decided actually to go for the second options and not going to lie. A bit of personal challenge I would say. So how was it done? So in terms of technologies, we are talking about something not very complex, we are talking about a simple node JS server.
00:06:20.150 - 00:06:34.186, Speaker A: We are using Nest JS with the framework on top of Express. And most of the technologies that you will find here are probably the normal technology that you will find in today in a node. JS backend today.
00:06:34.186 - 00:06:59.074, Speaker A: One important choice is that we kept the GraphQL API in order to limit the migration work that was needed front end side in terms of timeline. So we started the development back in April 2022. We fully replaced the graph in production in September 2022 with this new back end and then everything went fine.
00:06:59.074 - 00:07:17.874, Speaker A: And back in February 2023 we started again the development in order to bring the improvements over the API for the V, two of the protocol and some technical improvements. A quick word about the architecture. So on your left you will find the networks on which we index.
00:07:17.874 - 00:07:31.066, Speaker A: So in our case, polygonmatic and polygon mumbai. And then you will find all these squares which are actually modules in a backend microservice. The first one that you will find is the logs indexer, which is the most crucial one.
00:07:31.066 - 00:07:51.470, Speaker A: I would say his job is fairly simple is to read from configuration object the networks and the addresses that need to be indexed. And then his job will be to be in sync with the remote blockchain state and pull the logs and save it inside the local database. Once this is done, we will reach the second module which is the local indexer.
00:07:51.470 - 00:08:11.740, Speaker A: It is another indexer and its job is a bit different. It will read pull the logs from actually the local database that we just saw, decode the logs thanks to the APIs in the configuration, obtain the events and actually send or dispatch the events to the modules that need it. What are these modules? We have two.
00:08:11.740 - 00:08:29.834, Speaker A: The first one is notifications module which is in charge of sending notification based on particular event. And the second one is the Views. So the Views will be a set of database tables that are updated according to a set of rules in some handlers.
00:08:29.834 - 00:08:57.186, Speaker A: So basically what you will find today in a subgraph, an important fact is that there is the last module which is the GraphQL gateway. So it is not automatically derived from the Views table as it is done in the graph, but is defined separately. It will be a set of resolvers that define their own GraphQL schema and resolve it by performing arbitrary queries in the Views table.
00:08:57.186 - 00:09:16.154, Speaker A: And also this is the place if you want to aggregate some off chain data or other non indexed blockchain data, it will be the place to do so. A quick focus about the two indexes. So the two indexers are actually two different implementations of the same underlying state machine.
00:09:16.154 - 00:09:24.000, Speaker A: And this state machine is a bit the logical heart of the backend. So it's a fairly simple. It has only three states.
00:09:24.000 - 00:09:38.850, Speaker A: The first one is the sync state. So in this state the state machine will assume to start from a null block and try to reach the latest block. Once the latest block has been reached, the state machine will transit to a new state, which is the polling state.
00:09:38.850 - 00:10:09.086, Speaker A: And in this state, the state machine will just try to pull the network, see if there is a new block, deal with it and go back to the pulling state. If at some point a chain reorganization happens during this process, either in the sync or the polling state, the state machine will go to the third state, which is the rewinding state. And in this state you will have a reconciliation between the remote blockchain network and the local state generally by finding the latest command block ancestor between the two.
00:10:09.086 - 00:10:31.960, Speaker A: And once it has been done, you go back to the sync state and you go again. So finally, what were the lessons that we learned during this journey? So the first one is that the indexing is by far the most critical part of the app. It's not necessarily the biggest, but it's just the part that you want to be very careful with.
00:10:31.960 - 00:11:04.962, Speaker A: So if you have issues in this part of the app, generally the issues will propagate in your logs table and if you have issues in your logs table, at least in our microservice, it will generally stop the process of any new events. For example in this case and until you have solved the issues manually, you will be stuck with a fixed time views basically. So we are talking about some done times until you have not solved the issue manually, which may be very painful with a lot of logs to deal with.
00:11:04.962 - 00:11:44.234, Speaker A: In our case, the success was to add proper monitoring, so we made sure to include a lot of routines running over and over, checking the sanity of the database and checking sanity of the backend microservice. And if there are some issues or warning or whatever, some errors slack and mail messages are instantaneously sent in order to alert us as fast as possible and on our site be able to fix the issue as fast as possible. The second point is that in this kind of service the time is a very important factor.
00:11:44.234 - 00:12:04.542, Speaker A: So when you will start, you will have only a few blocks and a few logs to index. Everything is very fast, very smooth and life is beautiful. And as time increases you will have more blocks, more logs and everything will begin to be very slow and at some point you'll be extremely slow and you need to take that into account in your development lifecycle.
00:12:04.542 - 00:12:30.698, Speaker A: So you need to take that into account first in terms of features, because at some point we'll need to be able to take into account improvements, performance improvements of your code base or just features that improve the performance in order to deal with this kind of issue. Not issue, but improvements. And the second one is to take that into account inside in your development setup, if you want to be able to reproduce an issue that is going on on staging or production.
00:12:30.698 - 00:13:02.566, Speaker A: You need to be able to have a way to reproduce locally this state without the need to reindexing everything. So as a kind of conclusion, rebuilding everything was definitely not a light decision and it is a long term investment. I think what we did well was to not rush things and we plan each step quite carefully and as a result now we have a back end which have very limited number of issues actually.
00:13:02.566 - 00:13:22.750, Speaker A: So we are not here with maintenance burden on our hands and of course it fits very well our needs and we perfectly know how to scale it if we need to scale it. So definitely happy with it. Thanks a lot for your tension and benefit.
00:13:39.190 - 00:13:49.220, Speaker B: Two questions. First is as OT services were going to be shut down, did you consider to host your graph node yourself?
00:13:51.210 - 00:14:05.434, Speaker A: Yes. So it was one of possible takeaway, so we could have done it. I think what was going to be an issue in our case was first the fact that it was a Roscode base.
00:14:05.434 - 00:14:39.170, Speaker A: So in our case we are mostly JavaScript engineers and for us it was kind of a problem to rely on a very heavy Roscode base. And the second point was at the time we did not enough resources and time to properly handle this infrastructure because we needed in any case to have another microservice and we're a bit shy to proceed with an infrastructure. So that's why it was some downside on this kind of decision.
00:14:40.230 - 00:15:01.466, Speaker B: Thank you. And second question I was curious about as the guard does not offer the possibility to send notification natively, did you consider using the WebSocket API of degraph and build a notification system that's triggered by state change?
00:15:01.648 - 00:15:14.000, Speaker A: Yes, thanks a lot. Very good question. I think it's definitely something that we could have used and it is definitely in favor of the first solution and for sure it's very interesting to consider.
00:15:14.000 - 00:15:52.698, Speaker A: So for sure now, if our needs were simply on the front end and this notification part, I think for sure it was something that we will strongly consider. It's just that on our side we had a bit of unknown on both the data analytics part and the flexibility that we wanted because we were moving quite fast and for example, as of now we need some way to perform some KYC. So that's why we ended up with going with a full flexibility option because in case we needed to add some options.
00:15:52.698 - 00:16:09.460, Speaker A: But definitely if your needs are well defined and your job is to do front end plus notifications, then for sure it will be actually today very interesting to rely on an existing code base and existing quality product.
00:16:09.910 - 00:16:10.626, Speaker B: Thank you.
00:16:10.728 - 00:16:17.460, Speaker A: Thanks a lot. Thanks.
00:16:18.230 - 00:16:41.706, Speaker C: Having gone down both roads now, like building your own and using the graph, it sounds like a classical buy versus build decision. If you had to generalize the key decision drivers for newer project to go either pick the graph or roll your own. When would you pick either out? And I know it depends, but if you had to generalize as best as.
00:16:41.728 - 00:17:01.886, Speaker A: You can thanks a lot. So I would say in 90% of the cases I will go with the graph definitely like this because it gave you so much in such a little time frame. So in our case at the beginning, we just developed the smart contract, the front end and we knew we could rely on the graph.
00:17:01.886 - 00:17:16.854, Speaker A: We launched production with it. It was amazing. So definitely at the beginning, I think it will be hard for me to say if you are not 100% sure of what you're doing, go with the graph because it will save you so much time in any case.
00:17:16.854 - 00:17:54.930, Speaker A: And then you will have to say okay, what do I really need actually? Do I need all these other features or do I have time? If I need other features, do I have the skills in order to manage my infra? And then you can take good decisions. But for me at the beginning, if you want to go fast, if your goal is to go fast and have something of quality in production real quick, it will be hard to not advise the graph on my side. Then once you have your needs really well targeted, you know what you want, you know what skills you have in your team, then you can take more informed decision.
00:17:54.930 - 00:18:11.334, Speaker A: I know a lot of people that say yeah, I have the infrastkills, so I don't mind having my own graph node at home and doing my other microservice said don't throw, it will be perfectly fine, I do worst every day, et cetera, et cetera. So very good. It was not the case for us.
00:18:11.334 - 00:18:42.830, Speaker A: But there's definitely, I think what your needs first are that you need to define and what the team is actually what the skills that you may find in your team. But in general, at the beginning, you don't really know what you want and you don't really have well, it depends on the project, but you don't necessarily have time, you don't really know what are the skills of your team maybe, I don't know. So I will say it's a very quick win and to go with the graph.
00:18:43.250 - 00:18:43.854, Speaker C: Thanks.
00:18:43.972 - 00:18:44.880, Speaker A: Thank you.
00:18:48.770 - 00:18:49.294, Speaker B: Thank you.
00:18:49.332 - 00:18:49.740, Speaker A: Thanks a lot.
