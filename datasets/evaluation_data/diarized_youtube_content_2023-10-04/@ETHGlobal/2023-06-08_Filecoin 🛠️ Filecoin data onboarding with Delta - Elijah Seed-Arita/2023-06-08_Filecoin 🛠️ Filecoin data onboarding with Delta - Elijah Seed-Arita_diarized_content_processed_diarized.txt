00:00:32.600 - 00:00:44.030, Speaker A: Hello, everyone, and welcome to the Hackfs workshop. Filecoin data onboarding with Delta. Joining us today is Elijah Cedarita, who will be taking us through this session. And with that, I'll pass it over to Elijah to get the session started.
00:00:45.200 - 00:01:32.556, Speaker B: Okay, thank you. Hello everyone. My name is Elijah. I am on the outer core engineering team at Protocol Labs, and we've been working on a set of tools called Filecoin Data Tools, and it's a collection of tools to help with onboarding data onto filecoin and we'll get more into it very soon. So I want to start off the talk, just talking about the current process today of onboarding data to filecoin yourself. So if you haven't done it before, it can be a bit of a tedious process. If you have done it before, you probably already know that there are a lot of steps involved.
00:01:32.556 - 00:02:16.056, Speaker B: And of course, that first step involves finding your filecoin client. That could be Lotus Venus, it could be something else like Fill client. There are a lot of going to be. You got to download your client, read the instructions for it, set it up, fund it with your wallet, configure it, connect it to the filecoin network, et cetera. And of course, you will probably only be doing this once. To be fair, however, it's just another stepping stone in the whole just process of getting your data onto filecoin. Part two is data preparation.
00:02:16.056 - 00:03:07.180, Speaker B: So you're going to be processing your data into the correct format, and that's just being car files. You got to split your data up into 32 gigabyte segments so they can fit into pieces on your filecoin storage provider. Following that, once you've prepared all your data, you're going to have to find your storage provider. So you're going to search for providers, providers. You're going to get information on chain listed, they're going to advertise their price, their piece size, et cetera. You might also want to take into consideration some sort of off chain indexes, such as availability or reputation indexes and things like that. So once you find your storage provider, you can move on to part four, which is deal submission.
00:03:07.180 - 00:03:42.664, Speaker B: And this is going to involve generating your deal parameters. So you're going to have to specify your size of your deal, your duration, the max price that you're willing to pay. You got to prepare the PCID, the payload, CID, et cetera. So once you've got all these deal parameters put together, you've got to submit the deal to the storage provider. The storage provider can then accept or reject your deal proposal. Then you got to check if it's been accepted. If it has been accepted, you can go ahead.
00:03:42.664 - 00:04:28.280, Speaker B: If it hasn't been accepted, you might want to retry with a different storage provider, different deal parameters, et cetera. And then once your deal has been accepted, you can prepare to transfer it to the storage provider. That might be over. Lib P two p, it might be over boost. If you're doing an offline deal, then you might want to set this up yourself with a storage provider. And of course after you make your deal, that is not all you need to keep track of the deal going forward. If it expires, if it fails, you're going to want to renew your deal with a different storage provider or with the same storage provider and just keep track of the deal and its health over time, et cetera.
00:04:28.280 - 00:04:59.756, Speaker B: So people do this obviously. This is how it's done, this is how it works and you can do it. It's just a lot, it's a bit tedious. This mainly comes from the issue of a fragmented tooling ecosystem. A lot of tools are split up between different teams. You're going to be often cobbling together different pieces of technology. You're going to have one tool for preparing your data, another tool for uploading.
00:04:59.756 - 00:05:38.460, Speaker B: It another tool you might have be putting together your own scripts and often there's just a lot of knowledge involved. And especially this becomes really prevalent as you're starting to get into uploading increasingly large amounts of data. And this is specifically a place where we are looking to address the problem. Overall it works, but it could be simpler. So that is where we're looking to come in with filecoin Data Tools. Filecoin data tools. It's outer core engineering's solution to this data storage complexity.
00:05:38.460 - 00:06:18.620, Speaker B: I wouldn't say problem, but something that can be improved. It's a unified and coordinated set of Falcon tooling. So it's multiple tools and they've all been developed in a standardized way by our team, outer core engineering. So it's got an opinionated architecture. And since we have been building up multiple layers of this technology, that means that we've been able to work on optimizations that go really deep through the stack, through multiple layers of the stack. So on top of that, we're a dedicated engineering organization. So this is not a contract, this is not like a consulting engagement.
00:06:18.620 - 00:07:00.200, Speaker B: So it means that we're going to be continuing to update this, continuing to support this. It's not just a one off project that we're going to have to move on from later. So we're going to keep supporting this and we're going to keep pushing out updates and improving things as we go. So we do have also a real SP on the team that's going to be JSON and Slack or Jason Shehulka. I hope I pronounced that name correctly. I've never actually asked him about that. But he's representing our storage provider gives us, we've been able to have a very grounded view on the requirements on the storage provider side of making deals.
00:07:00.200 - 00:08:12.050, Speaker B: And we've also got solutions architects, namely Bill Schreckenstein, and they're able to help with white glove data onboarding and we can help you out with getting your data on and just facilitating that process. So again, Filecoin Data Tools is a collection of tools and that includes estuary. It's Delta Ptolemy. Delta is the main focus of this presentation, but there's a lot and we've got great docs at Docs Filecoindata Tools and I'll visit that link very so the, the actual component that's going to be making deals, which is the focus of this presentation, is Delta. So Delta is filecoin data, tools, deal engine, microservice. And so it's API oriented, it's controlled through API. It's a daemon and it wraps up all the complicated filecoin deal making process into a single service.
00:08:12.050 - 00:08:46.280, Speaker B: One of our big focuses for this development of this application has been supporting large data onboarding requirements. And right here you can see we've got our GitHub link to the open source project. You can download it, build it, run it yourself, GitHub.com slash application research slash Delta. I will mention that. So we've got a hosted option and we've got the open source version, same version. You can run it yourself if you want or you can use our hosted version.
00:08:46.280 - 00:09:22.550, Speaker B: So Delta's features that involves car generation, comp generation, piece commitment generation, deal making. We do both online and offline deals. So if you're going to do an online deal, we'll transfer the data to the storage provider for you. If it's an offline deal, then we'll just set up the whole deal. We'll set up the deal basically. And then you can handle transferring the data yourself. We also do deal repair, which is where if your data goes offline or if it's coming close to expiring, we can repair it.
00:09:22.550 - 00:10:11.216, Speaker B: We've got deal status checking, very straightforward statistics, tracking of your deals, wallet management so you can choose which wallet you're going to be using for making your deal, and storage provider selection, of course. So the architecture of Delta pretty straightforward. We've got a Rest API in the front. The Rest API is just going to listen for user requests to make deals. And of course, next to that we've got an IPFS node. So if you want to transfer your data to Delta over IPFS instead of over HTP, that is an option. Behind that we've got the dispatcher job framework and this takes care of all the different types of jobs that Delta can do as I listed in the previous slide.
00:10:11.216 - 00:10:55.756, Speaker B: So that's going to be peace, commitment, data transfer, deal proposal generation, et cetera. And then behind that we've got a database, which I think is pretty straightforward. So when you upload a file to Delta, first thing that it's going to be done is it's going to compute a compete for your file. Then you have the option to provide a user provided wallet. Otherwise if you don't provide that, then the default wallet will be used for Delta. You've got a suitable SP will be identified after that and then a deal proposal will be created and submitted to that storage provider. Then the data will be transferred to the storage provider.
00:10:55.756 - 00:11:40.080, Speaker B: If the deal is end to end or online. If it's an offline deal, then we'll of course skip that step and you can handle yourself. So to note, there is a 1GB minimum size on files uploaded with Delta. However, that's not a hard limit because we have other tools in our stack are built to address this problem. So I'll get to that very soon. So again, I mentioned we're able to do some unique optimizations based on having a more unified and coherent stack. And just two of those that I'm going to go over right now, the first one is going to be compi generation.
00:11:40.080 - 00:12:29.040, Speaker B: So back in Sri V one, the compi generation step was taking three to five minutes, which is not great to be fair. However, we have been able to bring that down to about 2 seconds on our dedicated hardware, which is a pretty substantial improvement. Of course, more optimization lies in the future. We're looking towards hopefully getting to subsecond compi generation. And of course we're also a team within Protocol Labs as previously stated. So we do have direct access to the cryptographers that built the filecoin protocol. They can exchange information, requests, et cetera.
00:12:29.040 - 00:13:17.504, Speaker B: So here we've just got a graph. We've got delta over here on the right and gray. And this is just comp generation times for different sizes of data, up to 16GB over here on the right. So we've been able to make a great improvement. And this is mainly to help facilitate in uploading large amounts of data. The other optimization that I'll go over and of course there's more, but this is going to be our new filecoin data infrastructure or FDI. FDI is our hosted infrastructure platform that's been developed by our infrastructure expertise on the team.
00:13:17.504 - 00:14:10.480, Speaker B: So we've got infrastructure and Kubernetes expertise and we're looking to build effective horizontal auto scaling to match our load. The componentized architecture of course, also of delta allows the independent scaling of services so we can match our load very precisely. We have also learned from our previous project, our previous offering, SJV One, to be ready for the scale that's going to be involved. So that's why we've been putting a lot of focus on building up this dedicated infrastructure. So with that out of the way, I'm going to go and give a quick demo of using Delta. Delta is very easy to get set up for yourself. I'm going to start with a demo of setting up a Delta demon instance.
00:14:10.480 - 00:14:55.890, Speaker B: So again, we've got the hosted instance and then we have the Demon, the local instance that you can clone for yourself if you want to use it. So let me just open up my terminal. So of course this starts with Cloning delta. That's going to be application research delta as previously stated. I'm going to go into it, make it, I've already built it because I don't want to wait for building the application on stream right now. But the configuration is really simple. You're just going to be copying the example environment file into the actual environment file env.
00:14:55.890 - 00:15:33.020, Speaker B: And then after that you're going to have to get an Auth token. You can do that really easily by just running CRL to the Auth Sgua tech register newtoken endpoint. And this will just give you SUA Auth token. So you can copy that, go into your N file. And I have my old token here, but we can just put in this new one and there we go. And then you can run the demon. It's going to start up here.
00:15:33.020 - 00:16:09.240, Speaker B: And there we go, running on port one 4114. You can start making API requests to it and it's good to go. So for this presentation, I'm actually going to be using the we have a special hack FS hosted delta running right now and it's running on filecoin calibnet. So the deals aren't going to be actually on the main net. Let's get to that. So making a deal with Delta very straightforward as well. It's a single API request.
00:16:09.240 - 00:17:00.970, Speaker B: So what you're going to be doing is you're going to be making a request to the API v one deal EndToEnd, endpoint, and you just got to pass in your authentication token and your data as well as your metadata. So there's two options here. You can send your data over Http, or you can also choose to send your data over Lib PTP if you want. And that'll involve giving yourself or passing in a CID and a host to the metadata. But we're just going to be using HTP right now because that is simple. So let's just make this curl request. So actually Zsh has already remembered all this.
00:17:00.970 - 00:18:01.928, Speaker B: So we're going to be making this request to Delta API v one dealend. We're going to put our authorization header. This is going to be our new token that we've just copied. And after that we're going to be passing in the form data form oh data equals. And I prepared a file here called test file. It's a 1GB file because again, Delta itself requires a 1GB minimum file size. So this is just a file of random bytes that I prepared with DD and the random you random source.
00:18:01.928 - 00:18:50.670, Speaker B: Anyway, after that we can just pass in the metadata. You don't have to fill out the metadata, but you do have to pass this parameter. We can run this now, so it's going to take a bit to upload because this is a gigabyte of data. But again, so if you want to use Lib PTP to transfer your data instead of using Http and the form data like this here, you just pass in the CID and the host in the metadata. There's other options available for your metadata as well. You can choose which provider specifically you want to upload with, you can choose a lot more of the deal parameters here and that sort of thing, et cetera. So let's just wait for this to finish.
00:18:50.670 - 00:19:21.648, Speaker B: After that we're going to be able to check status here. Status checking is really easy. It's on the open endpoint or the open API instead of the V one API and you do not need an authorization token to access this. All right, so here we go. We've got our response here and inside of our response we're going to get CID. This is our content CID I think request meta. Yeah, CID.
00:19:21.648 - 00:20:12.004, Speaker B: This is our content CID. And then here we've got our sorry, this is the content CID. And then the content ID is local to the delta instance. It's 878. We can check the status of this content. So curl the get request to hackfs C-O-E-U-S-S techopenstatscontent eight seven looks like there's been four contents updated since I last ran this command and here we go. Yes.
00:20:12.004 - 00:21:14.088, Speaker B: So moving on, if we want to do 1GB oh yeah, and here's that API address for the hack FS delta instance. So if you want to do 1GB files, this is great. However, what if you want to do smaller files? What if you've got like, I don't know, a bunch of little tiny files? And so that's where Edgeur comes in. Edgeur is built on top of data delta, sorry and it's a content aggregator. So what it does is it'll take multiple different files that have been uploaded and basically pack them together and then make them all upload them all to storage providers in batch deals. So those files are going to be aggregated into this bucket and then they're going to be submitted to the storage provider. So again, this allows us to upload much smaller deals or much smaller pieces of data.
00:21:14.088 - 00:22:06.356, Speaker B: And it also will immediately make the data available over Lib P. Two P if you want to access it immediately, since the batch deal is not going to be made immediately, since it's going to wait for multiple pieces of content to be added. So making this deal, making a deal with Eduard, let's do that. So I've got my deals with Eduard example here. So we've got a hackfs instance for Edge as well. It's going to be hackfsco tech Edge instead of delta. So making a deal with Edge first let's switch to Edge and make a request.
00:22:06.356 - 00:22:37.524, Speaker B: Let's just LS here. I'm just going to make a deal for some little tiny file here. Let's just make it for the docker compose file of the Edge repository. So I'm just going to curl post https hackaps C-O-U-S ray tech Edge looks like we've got an autocomplete here already as well. Contentad again, add our authorization header. I lost my authorization header that I just got. I'm just going to reuse this one.
00:22:37.524 - 00:23:14.124, Speaker B: Here our data. That's going to be we're going to do the docker compose YML and then the last piece of the request, the form metadata, that's going to be nothing because we're not selecting anything specific. So let's run this. That happened really fast. We're now uploading a gigabyte this time, so that was much easier. Again, we're going to get the ID content. ID is going to be this here and we can use that again to look up the status.
00:23:14.124 - 00:24:22.660, Speaker B: Now we can curl to Macabre Tech and then this time we're going to query to Edgeopenstatuscontentententententent and then we're going to paste in what was that ID here? 16817 and we've got our status here. So our status success kind of cut off right there. But yeah, so this is going to be pinned now and you'll be able to access it over IPFS from the Edge IPFS gateway. So this is great. This lets us upload very small pieces of data. Now on the other side of things, what about large data sets? So large data sets is something that we've really been focusing on because we want to be able to onboard very large pieces of data and we're talking like very large data sets on the terabyte scale. And so the way that we are doing this is with Delta DM.
00:24:22.660 - 00:25:11.204, Speaker B: Delta DM stands for Delta Data Set Manager and it's basically a tool again built on top of Delta for managing massive data sets with potentially long running upload duration. So it's not something that you're going to be wanting to do in one Http request. So this is able to replicate data to multiple providers. You can manage your upload configurations, you can filter which providers you want to be getting, which data sets, which wallets you want to be using for which data sets. You've got a whole record of your actual deals that have been created. And the best part about it I think is that we have an accessible UI which is very exciting. So the repository for this is GitHub.com
00:25:11.204 - 00:25:48.284, Speaker B: slash application research. Delta DM again can build this yourself, check it out yourself, do whatever you want with it as it's open source. So the data set upload process with Delta DM starts with creating a data set. Then you're going to attach contents. Contents are going to be you're going to have a file generated in an external program. This is going to be Ptolemy or some other external program. Ptolemy is still in the works but this is going to be integrated into Delta DM.
00:25:48.284 - 00:26:43.232, Speaker B: So this is going to be a completely end to end process in this one UI, which is going to be very nice. So you'll attach your contents, that lets you upload the actual data that you want to send to storage providers, possibly multiple. Then you're going to register your wallet and associate it with that data set because you're usually going to be having, if you have a fill plus grant, that grant is going to be for that specific data set. So you're going to want to associate that wallet with the data set that it's designed for. After that you can add whatever providers you want to be able to make deals with and then you can set up your replication profiles which will start automatically replicating these data sets to the providers that you've specified. So let's just do a quick demo of this UI. I think I already have it running locally.
00:26:43.232 - 00:27:11.420, Speaker B: Here we go. So this is our UI. Very nice, very beautiful. So again, the first thing that you are going to be wanting to do is make a new data set. Can call it whatever you want. So hack of S for example, you can set your application, count your deal duration and then set that up. So now that's going to show up right here.
00:27:11.420 - 00:27:51.668, Speaker B: Then you can go in and attach content. I don't have any content to attach right now, but this is where you would attach a JSON file describing the content that's been generated in Ptolemy. And again, Ptolemy is going to be integrated into this UI. So that'll be very easy. After that you can go in and add a wallet. I've already got a wallet right here but normally you would add a wallet via the CLI so that you're not potentially transmitting your wallet secrets through http unsecured so that'll be done through the CLI. But after that, once you've got a wallet, you can go and manage and you can add which data sets you'd like to make deals with using this wallet.
00:27:51.668 - 00:28:37.908, Speaker B: So if I wanted to add my hack FS data set we can have that here, save it. And now that's going to allow hack FS to be done with this address. After that, we can go into our providers. We can add whatever provider we want. If I want to add F zero, blah blah blah blah blah, call it hack FS, go there, manage it, change whether we want it to allow self service, which allows storage writers to come in and request data for themselves. And after that point, we can go into replication profiles. And this here you can choose, like stated before, which data sets you want to be replicated to which storage providers.
00:28:37.908 - 00:29:33.236, Speaker B: So if I want to replicate, for example, to the hack FS search provider with the hack FS data set, we specify that and after that the daemon will start automatically making deals to this hack FS provider. That's our solution for making very large deals with filecoin data tools and with that I think that is actually all I have to present. So yeah, more info go to filecoindata tools for great docs docs are really well prepared, I'll actually pull that up right now. Docs Filecoindata Tools that's going to be this website here. Feel free to go check it out. Yeah, I think that's it. So thank you very much for coming, thank you for listening.
00:29:33.236 - 00:29:42.904, Speaker B: If there's any questions, we can answer that, but if not, then that'll be it. Yeah, thank you. Yeah. Cheers.
00:29:42.952 - 00:29:43.576, Speaker A: Thank you, Elijah.
00:29:43.608 - 00:29:43.756, Speaker B: Yeah.
00:29:43.778 - 00:30:21.636, Speaker A: If anybody has any questions, please feel free to type them in the chat or take yourself off mute. And if not, please remember that you can always go to the Data Tools Partner discord channel, where you can reach out to Elijah and Abais from the team and just the partner channels in general. If you have any questions specifically around the technology. Thanks, Elijah, for the great presentation. And thank you all for joining today. We'll be back tomorrow with a few more sessions, so cheers. Thank you all.
00:30:21.636 - 00:30:23.060, Speaker A: Have a great rest of your day.
00:30:23.210 - 00:30:24.210, Speaker B: Thank you. Have a good one.
