00:00:07.290 - 00:00:15.034, Speaker A: Hello everyone, and welcome to the Hack FS workshop. Storage and retrieval on filecoin part one. Joining us today is Matt Hamilton who will be taking us through this session.
00:00:15.034 - 00:00:17.840, Speaker A: And with that I will pass it over to Matt to get the session started.
00:00:18.530 - 00:00:28.870, Speaker B: Hello, thanks a lot. So yeah, my name is Matt Hamilton. I'm a developer advocate with Protocol Labs specific typically working on FVM, the Filecoin virtual machine.
00:00:28.870 - 00:01:23.394, Speaker B: And the point of this series, and this is part one of a series of probably two, maybe three sessions going forward, is to look in more detail about storage and retrieval on filecoin, specifically with a view of storage and retrieval initiated from a smart contract using Fem. So how can you write a smart contract in Solidity and get it to instigate some storage on filecoin? So hopefully this talk will give you a little bit more information on the options you've got available for how you can actually make that happen. So as a quick recap here, you may have seen this before on previous talks, especially in Sarah TM's talk that was on Friday regarding FalcoIn and Fem.
00:01:23.394 - 00:01:31.094, Speaker B: So the FalcoIn master plan is build the world's largest decentralized storage network. We're pretty much on our way there. We've done that.
00:01:31.094 - 00:01:44.170, Speaker B: We're continuing to build out. The network is something original, about twelve or 13 exabytes of data in size. So it's about the equivalent of about 1% of the total data center storage capacity is available on filecoin.
00:01:44.170 - 00:01:58.146, Speaker B: Step two is onboard and safeguard humanities data. So that is bringing on a lot of data from various places. Things like scientific data sets, archival stuff, NFTs, all this sort of stuff that we want to store on there.
00:01:58.146 - 00:02:25.002, Speaker B: And then step three is bringing compute to the data and enable web scale apps. Now that's kind of the area that we're going to be focusing on today, and that is regarding using Fem to do programmable storage. And what I'm going to do is in this talk I'm going to show you two different approaches to getting data stored on filecoin from Fem and those can serve as a basis for building other things on top.
00:02:25.002 - 00:02:47.300, Speaker B: And those other things we'll have a talk a part two of this talk. My colleague Longfei is going to do a talk on using these tools for things like self replicating storage and self healing storage. So we're going to just focus on the basics in this talk here and sort of where we kind of go from there.
00:02:47.300 - 00:03:02.486, Speaker B: So right at the bottom we've got kind of like IPFS, which is a peer to peer network. It's slightly separate from filecoin but is often used as a sort of staging layer for filecoin and for pinning data. So that's why that's mentioned there.
00:03:02.486 - 00:03:31.230, Speaker B: Layer zero filecoin, the storage layer. Layer one that we're at now FVM compute over state and then layer two will be building storage related DApps on top of that as well so storing data on filecoin. My colleague Sarah created this great little flowchart and this is available in the cheat sheet that we've created, the hackathon cheat sheet which will be linked to at the very end.
00:03:31.230 - 00:03:52.274, Speaker B: Or if you search for fem cheat sheet, hackathon cheat sheet, you'll find it on GitHub, no doubt, but there is a link at the end. So looking at this, we're starting with the point of view of I want some storage, so I want to store some data. Now there's two ways I can go about it programmable storage or programmatic storage or non programmatic storage.
00:03:52.274 - 00:04:16.602, Speaker B: So non programmatic storage, we can go down the sort of the blue side of this. I'm not going to cover that today that is doing storage without using FVM. And this has been something that has been available for a while now on filecoin and using on ramps like Web Three, Storage Lighthouse, et cetera, directly through say, an Http API and storing data onto filecoin.
00:04:16.602 - 00:04:33.346, Speaker B: That way I'm going to look at the other side which is using programmatic storage. So the idea here is I want to instigate some storage happening from a smart contract. Now there's two different approaches depending upon when you're looking at smaller amount of data or a larger amount of data.
00:04:33.346 - 00:05:00.486, Speaker B: And that boundary is roughly around about the four gigabyte size at the moment. So the reason for that being is that storage providers on the network, of which there's about three and a half thousand storage provider systems on the network, storage providers are generally looking to store large amounts of data. The unit they work in, called sectors, typically are 32 to 60 or 64 gigabyte sectors.
00:05:00.486 - 00:05:19.614, Speaker B: So they are generally looking for data that will fill one or more sectors. Think of it a bit like they are like the wholesalers of the storage. If you have a small amount of data, say you've got your 50 kilobyte monkey JPEG, then the storage providers are not going to be interested in storing that directly.
00:05:19.614 - 00:05:56.494, Speaker B: It's too small data for them to store efficiently within a storage deal and within a sector they would have 50 data and then pad out an entire sector out to 32 gig, which is a waste of storage for them and not efficient for them either in terms of compute or in terms of economics for the system. So if you're using a small amount of data, then you'd want to go via an aggregation service. So I'm going to talk about filecoin data tools here, which is one of the aggregation services in the system, and I'm going to talk about direct deal making.
00:05:56.494 - 00:06:19.602, Speaker B: So we're going to look at storing small data and we're going to look at storing large data and how you can go through these two areas from this flowchart. So to kind of summarize the two different approaches here, if you're going less than four gigada, you go via an aggregator. If you're going larger than four gigada, you're going for via direct deal with an aggregator.
00:06:19.602 - 00:06:47.386, Speaker B: Typically the retrievability is immediate because you can retrieve directly from the aggregator initially and then via filecoin once the deal is made. If you're doing a direct deal, typically you're not able to access the data until a deal is made or until the aggregator at least has picked up that contract and published the storage deal. In terms of flexibility with an aggregator, your data is combined with a load of other data and stored on the network.
00:06:47.386 - 00:06:59.374, Speaker B: So stored as one storage deal. So you would be using the storage deal parameters that are set by the aggregator. So in terms of the lifetime of the storage will be set via the aggregator.
00:06:59.374 - 00:07:09.160, Speaker B: If you're doing a direct deal, you can set those parameters yourself. So you can say when you want it, start end, how much you're willing to pay, for example. You can set those yourself.
00:07:09.160 - 00:07:31.002, Speaker B: In terms of complexity, going via an aggregator is simpler because the aggregator handles a lot of these and you'll see that a little bit when we go on further and we look at the commands used to actually instigate this. With the direct deal, the complexity, it's a bit more advanced, but you have more flexibility there. So it's that typical engineering trade up.
00:07:31.002 - 00:07:38.590, Speaker B: Between simplicity and flexibility there. So technology for the aggregator. What I'm going to show you today is filecoin data tools.
00:07:38.590 - 00:08:00.194, Speaker B: These are the tools there was a service called Estuary and filecoin data tools are the next evolution of the tooling behind Estuary. So filecoin data tools allows you to actually spin up effectively your own version of SGU yourself. So SG version two will be based upon this tooling set known as filecoin data tools.
00:08:00.194 - 00:08:20.750, Speaker B: But filecoin data tools you can think of as a superset of what s three did. But the part that we're looking at the moment is kind of similar to what some people may have covered with s three before. And if you're doing larger than four gigabyte deals, what I'm going to demonstrate today is a thing called the client deal contract.
00:08:20.750 - 00:08:33.390, Speaker B: So to set this in scene, let's have a quick look at what the actual deal flow is. And looking at this, you might think it's quite complicated. That is the case.
00:08:33.390 - 00:08:54.630, Speaker B: And that is why I'm kind of showing you two approaches to dealing with this. Now, the reason it seems quite complicated is that, remember, filecoin is designed to store vast amounts of data for a long period of time. So a lot of the process actually happens off chain because you are dealing with gigabytes terabytes petabytes of data.
00:08:54.630 - 00:09:16.170, Speaker B: And that is not something that is efficient to process on chain. So typically a lot will happen off chain and then the final proofs of the data storage are actually put on chain. So with the deal flow, typically a client puts some funds in escrow with the storage market Actor, which is a smart contract, built in smart contract on the blockchain.
00:09:16.170 - 00:09:30.306, Speaker B: The client then uploads a car file to a web server somewhere. A car file is an archive file. Think of it a bit like a zip file, but it is the file format that is used within filecoin for aggregating and storing data.
00:09:30.306 - 00:09:52.358, Speaker B: It actually stores data as a Dag, a graph, which means that you can access individual parts of it as well via hashes. The client sends a storage deal proposal to the storage provider. The storage provider is running some software called Boost with the URL of the car file.
00:09:52.358 - 00:10:06.586, Speaker B: Then step four boost checks that the client has enough funds. If so, Boost will accept the storage deal. Step six, boost will download the car file from the web server, publish the storage deal on chain, and then the client can check that it's on chain.
00:10:06.586 - 00:10:30.150, Speaker B: Now, that entire process may take hours, it may actually take days for that to actually happen because storage providers don't necessarily publish storage deals immediately when they come in. Depending upon the size, they may wait a while until they have enough storage deals to fill a sector. So it might be that you're waiting a day or two for that whole flow to take place.
00:10:30.150 - 00:10:47.014, Speaker B: So just sort of setting expectations. This is not a flow that happens in seconds, this is a flow that happens in hours, typically. So with aggregation, this is a diagram done by the FalcoIn Data Tools team, shows you a bit about aggregation.
00:10:47.014 - 00:11:09.250, Speaker B: So historically aggregation has happened off chain. So services like SG, Web Three, Storage Lighthouse have typically been off chain, but a number of parts of that now as a result of fem, are able to move on chain. So Filecoin Data Tools here I'm going to actually show you how you can access that from onchain.
00:11:09.250 - 00:11:27.250, Speaker B: So typically what happens is a file is uploaded to a service called Edge that is part of Filecoin Data Tools. Edge then will take these multiple files, put them in a bucket where it is aggregated together. The files are then stitched together to form a collection of data segments.
00:11:27.250 - 00:12:03.026, Speaker B: It generates a proof, and I'll talk about that towards the end, what that proof is used for, and then creates a car file which is then passed to a tool called Delta, which is another part of Filecoin Data Tools that actually will make the storage deal. So Edge is think of Edge as the Http microservice that you can interact with via Http and it does certain things like aggregation and allows retrieval as well of the data before passing it to Delta for storage. So I've got two demos I'm going to do now.
00:12:03.026 - 00:12:22.166, Speaker B: One a direct deal going through and one using an aggregator. So these are two different approaches, like I said, for a large amount of data in the case of direct deal or a smaller amount of data for an aggregator. Now I'm actually going to demo these on a local net that I'm running here on my laptop.
00:12:22.166 - 00:12:46.202, Speaker B: So with that local net I can actually make much, much smaller deals. So I can make deals in the range of sort of kilobytes of size because I'm running this locally, because the sector size that I'm running locally is 2 KB. Like I said, if you're running on the calibration testnet or main net, then the sector size is 32 or 64GB in size, so much larger.
00:12:46.202 - 00:13:08.934, Speaker B: But for the purposes of this demo, I'm using this local net. It means it can go through much quicker and I can kind of show you these things and show you both sides, both the client side and the storage deal side. So if you want to get started with localnet, there's a GitHub repository you can go to and I will put that in the chat here.
00:13:08.934 - 00:13:38.340, Speaker B: So that's a repository you can go to that will allow you, if you've got docker installed, then you can clone this repository and go into the directory type docker, compose up and it will set up a completely brand new blank filecoin network for you running locally on your computer. And you can then connect to that with tools like MetaMask and interact with that directly. Right, so there's instructions there on how to get that all set up.
00:13:38.340 - 00:13:58.978, Speaker B: So let's see, where do we need to go from here. So I have actually done this, I've got this running locally and we have here, this instance here, this is just the log files kind of going past for Lotus. Lotus is the name of the software node, the reference software node for filecoin.
00:13:58.978 - 00:14:14.922, Speaker B: And that is running here locally on my laptop. So this is just watching the log files go by. Now, the other part that I'm going to use for this is a starter kit.
00:14:14.922 - 00:14:25.300, Speaker B: So we have here the Fevham. I haven't actually got it loaded here, fevam hard Hat starter kit. And that is available there.
00:14:25.300 - 00:14:42.550, Speaker B: So this hard hat starter kit has in it a contract here. It has the deal client contract that we're going to be running here. So I have actually deployed this deal client to my local network.
00:14:42.550 - 00:15:00.042, Speaker B: So I did that, I ran Yarn hard hat deploy network localnet and it has deployed it to my local network. And I have this contract running locally. So now that it is running locally, I can interact with it.
00:15:00.042 - 00:15:10.718, Speaker B: I have boost running. So when I ran that docker image, one of the things it set up was a thing called Boost. And boost I can access locally and I can see the storage deals that are on here.
00:15:10.718 - 00:15:25.422, Speaker B: Now, for the sake of brevity, I just ran a storage deal about 30 minutes ago that is run on the network here. So I can show you here. With that hard hat starter kit, I can run a command.
00:15:25.422 - 00:15:49.450, Speaker B: Now you'll see a little bit here when I illustrate and say it's a little bit more complicated, you have to pass a lot of the parameters in yourself. So I'm running this make deal proposal method here, or task rather in hard hat. It is connecting to the contract that I have deployed, that is at this address here and I've filled in a bunch of information.
00:15:49.450 - 00:16:07.410, Speaker B: So PCID, P size, all this kind of stuff, the start epoch, end epoch, costs everything here. Now, where is it going to get this data from? I have uploaded it to a place called the Lighthouse Data Depot. So the Lighthouse Data Depot is just a tool.
00:16:07.410 - 00:16:18.178, Speaker B: You can do all of this locally. It's just a convenience tool that allows me to upload a file there. So I created a file called hello hack FS, just a small little text file.
00:16:18.178 - 00:16:45.118, Speaker B: I uploaded it to Filecoin Data Depot and it has calculated, it's created a car file and calculated a lot of these PCIDs and everything that I need here. So I was able to then plug those values into this command and run this command. Now running this command contacts that smart contract, that smart contract then emits an event and Boost, the software sees that event here.
00:16:45.118 - 00:17:19.334, Speaker B: So we can see here, these are the log files for Boost. And we can see at the bottom here that Boost has accepted my contract deal proposal. And then looking in Boost, we can see here that I have a deal here, it's accepted it and we can scroll down and see that it has actually fetched the data from where it needed to come from and it has actually started the process of sealing this sector, right? And it is now in the state proving.
00:17:19.334 - 00:17:33.002, Speaker B: So what that means is it has taken the data, it has created a sector. I've run published storage deal here. As a storage provider, I can hit publish storage deal.
00:17:33.002 - 00:17:47.666, Speaker B: There's none left to publish at the moment. But when I did that, it then started the process and we're now in this state proving. So this means that the data is on the blockchain and every 24 hours the blockchain asks the miner to prove that it still has my data.
00:17:47.666 - 00:18:10.570, Speaker B: Now, I'm going through this quite quickly. If you look on YouTube, you will see some videos where I've gone through this in much more detail and run it locally and gone through all the steps. I'm kind of skipping over the steps here for brevity but that has now stored that data on there and I can actually access that data now using a tool called Lassie.
00:18:10.570 - 00:18:20.374, Speaker B: And I have, I think I've got it here. Yes, Lassie. So Lassie is a tool to fetch data from IPFS or filecoin.
00:18:20.374 - 00:18:33.986, Speaker B: I can say Lassie fetch because I'm running it on localhost, I've had to tell it where the provider is. Normally you don't need to do this because it uses a tool called IPNI, the Interplanetary Network Indexer. But I've had to tell it I'm actually running on local host here.
00:18:33.986 - 00:18:54.246, Speaker B: So look on localhost and I've provided there the CID. So that's the CID of the data. If we look here in the deal ID, we can see this is this root deal CID that ends COHI and I have that here.
00:18:54.246 - 00:19:04.166, Speaker B: So if I run that command, lassie will fetch that data. It's downloaded it. I now have a car file, this car file here named after the CID.
00:19:04.166 - 00:19:17.870, Speaker B: I can say IPFS car unpack provide that car file. I'll now have a directory named after that. I can go into that directory and you'll see there's our hello hack FS.
00:19:17.870 - 00:19:23.634, Speaker B: I can do that. Let me just clear the screen so you can see it clearer. Hello? Hack FS 2023.
00:19:23.634 - 00:19:54.758, Speaker B: So that's the file I uploaded, right? So that's going through the deal client process that I talked about. So this is a direct deal and like I said, I was using local net and the hard hat starter kit. Now, the alternate approach and the approach that you're probably going to use with this hackathon is using the aggregator, or using an aggregator in this case, I'm demoing filecoin data tools and their aggregator and their tools edge and Delta.
00:19:54.758 - 00:20:22.018, Speaker B: Now, to do this, I've created a very simple tool here called FTD data monitor. And that is intended to be just an example that will allow you to see what's to actually monitor the chain. So I had some details of the direct deal.
00:20:22.018 - 00:20:37.494, Speaker B: That's the process I went through there with the direct deal. So just to recap, we send the URL and deal parameters to the smart contract. That was that big scary command, the contract emits deal proposal, event storage providers running boost, see that event and pick up the deal.
00:20:37.494 - 00:20:47.382, Speaker B: We saw that in the log files. Storage provider can then fetch the data and publishes it to the chain. So we saw all of that and we fetched it with Lassie, the aggregator.
00:20:47.382 - 00:20:58.640, Speaker B: We're going to do a slightly similar pattern here. We're going to have an aggregator smart contract. Now, again, this is just an example that I have created just to show you this.
00:20:58.640 - 00:21:27.526, Speaker B: So there's a repository here, FTT deal monitor. So I'll put that in the chat and there's a very simple contract in here that allows you to store a Uri and it will then allow you to kind of see that data there. So what it'll do is you call this store Uri with a Uri and it emits a event.
00:21:27.526 - 00:21:45.630, Speaker B: And what is actually happening is we have this tool, FTD Data monitor that is monitoring the event. So this is a JavaScript little demon. Again, this is just an example to show you that is listening to the event and will then send the data to Edge.
00:21:45.630 - 00:22:07.838, Speaker B: So we send a URL to this example aggregator contract. The smart contract emits an edge ur contract event. The local example, FTD deal monitor demon watching sees the event, makes a request to Edge HTP and Edge will fetch the data and put it in an aggregation bucket.
00:22:07.838 - 00:22:26.362, Speaker B: Once the bucket's full, it will then create this aggregation proof and will actually pass it to delta to make the storage deal with the storage provider. So, let's see this here. So that was our direct deal.
00:22:26.362 - 00:22:45.774, Speaker B: One example, I have this deal monitor here and I can run this. So I'm running this locally. So I've just checked out this FTT deal monitor, run yarn install to get all dependencies and I can run just node FTT deal monitor ethers.
00:22:45.774 - 00:23:11.286, Speaker B: There's a end file in which you need to specify the RPC endpoint and your contract address in there. So again, I'm using a local network for this, but you'll probably want to do this on calibration net, which is our main public testnet. I'm doing this locally just for the speed and efficiency of it during a demo I can now call a much less scary command.
00:23:11.286 - 00:23:26.190, Speaker B: That basically this hard hat store data and I pass it the contract and the Uri. So again, this Uri is the same Uri we used before from the data depot on Lighthouse Storage. And again, this is the local network.
00:23:26.190 - 00:23:49.398, Speaker B: So when I run that, that is going to make a deal proposal on the local network and we'll see when that emits an event, we'll see our deal monitor will pick that up. It will see that event. It will then take that URL that's been passed to it, send it to Edge, and Edge will then fetch the data and store it.
00:23:49.398 - 00:23:57.494, Speaker B: Right. And we'll see when that comes through. Hopefully when you're running on local net, it's a 15 2nd block time.
00:23:57.494 - 00:24:04.070, Speaker B: When you're running on calibration net and main net, it's 30 seconds. So here we go. It's sending the payload.
00:24:04.070 - 00:24:15.770, Speaker B: There we go. So let's just recap what it did here. It called this Edge instance we've got set up specifically for hack FS.
00:24:15.770 - 00:24:22.298, Speaker B: This uri here. And I'll put this in the chat and we'll put this in. It may already be in the hackathon cheat sheet.
00:24:22.298 - 00:24:33.422, Speaker B: If it's not, we'll add it in there and put it in discord as well. So it is calling this API request to fetch the URL. It pass in the URL, it goes and gets fetched.
00:24:33.422 - 00:24:43.720, Speaker B: And you can see here the responses come back. It's got an ID. This ID is local to Edge and it has the CID of the data.
00:24:43.720 - 00:25:05.438, Speaker B: But you can see here inclusion proof is null. So what is this inclusion proof? Once the aggregator has aggregated a whole series of data and created this aggregate car file that it sends to a storage provider, one of the things you're going to want to do is be able to prove that your individual file is within that aggregate. And that is what this inclusion proof does.
00:25:05.438 - 00:25:44.154, Speaker B: So once it's aggregated that will then include some proof that you can send to an oracle on chain and you can actually prove that your 50 kilobyte monkey JPEG is actually verifiably stored within that 32 gigabyte say, chunk of data, right? There's no deal ID because it hasn't made a deal yet, it's only just got it. But what we do have is a status here and a download. So I can actually take those, I can get that download URL and I can run that as well.
00:25:44.154 - 00:26:08.866, Speaker B: So I could say curl, pass that in and pass it to whatever out car and then I could do IPFS car unpack out car. Too many terminals, data makes no sense. Interesting.
00:26:08.866 - 00:26:15.880, Speaker B: Okay, for some reason that out car. Oh, that's because I've got the wrong one. I've got the status there, that's not what I want, that's not the URL I want.
00:26:15.880 - 00:26:30.544, Speaker B: I need the actual download URL which is 1 second here. I've just copied the wrong one, this one here. So that's just the wrong way around where it says status and download.
00:26:30.544 - 00:27:11.360, Speaker B: Those two labels are back to front there so I need to get those changed but we can fetch the data still. Let's try this again here. Curl, fetch that data out car and then Ipfscar unpack out car and if we look here again we've got a directory here and you can see it's actually got the same CID, right? So if I go into that directory there and look in there hello hack FS, there's our file again.
00:27:11.360 - 00:27:27.828, Speaker B: Right so this is going via Estuary. So Estuary, I can fetch the data from estuary there and it should have pinned it to IPFS. We can check the status again if I go back to our output here, where is it here.
00:27:27.828 - 00:27:46.540, Speaker B: So I can check the status. Like I said those two labels are back to front but I can check the status here and I can say Curl, get that status and if I pipe it through JQ just to format it we'll see here the status. Right? So this is telling us the status.
00:27:46.540 - 00:28:09.600, Speaker B: There's no sub piece information here, no inclusion proof because it hasn't done the aggregation yet and there's no deal information there. But if it has gone through all of that then you'll actually see all of that aggregation data there. One of the things our script did after it has done all of that is it's actually updated our smart contract.
00:28:09.600 - 00:28:34.456, Speaker B: So it has actually contacted our smart contract and passed in the information that's come from Delta. So if we look at our smart contract here it's actually called this update job ID method and passed in the job ID the CID and set the status to pinned so that we know it's there. So I could now look up with this contract and I could actually find out and see what's going on here.
00:28:34.456 - 00:29:07.190, Speaker B: So I could use this as a building block again, this is just a very minimal example. I could use this as a building block to then go on and build something like a replication service or a auto healing service or a data dow or whatever it might be. And once we have a deal ID, I can actually call that as well and I can update the deal ID information here and my smart contract once it has a deal ID and it has the proof information it would be able to actually verify on chain that my CID has actually been stored within a deal.
00:29:07.190 - 00:29:26.110, Speaker B: Right? So once it's got that information it can then put that all in there. So there we go. So that's a view then of going through aggregation that's taken us to half an hour.
00:29:26.110 - 00:29:37.280, Speaker B: Here is a number of Fem resources. You can scan this QR code or go to Linktree. Can I actually get that? I can't get that, I'll put that in the chat.
00:29:37.280 - 00:30:12.580, Speaker B: But yeah if you go to that QR code or go to Filecoin VM you'll get links to the docs, the hackathon, cheat sheet, use case ideas et cetera. And thanks a lot and this has been a team effort. This is just some of the people on here, but I'd also like to call out specifically the people within the Filecoin Data Tools team as well, Alvin Lodge Cake, who have been working on getting all of the stuff that I showed you with Edge and Delta up and running so that we could demo this and have it ready for the hackathon.
00:30:12.580 - 00:30:33.184, Speaker B: So there we go. Right, any questions here? I'm just looking in the chat to see what we've got here. Can you send a URL to YouTube video where explained in depth? Yes, I will do that.
00:30:33.184 - 00:31:25.360, Speaker B: I think actually if you go to YouTube IPFS thing probably find it here I'll find you the link to it. It's on the on the IPFS YouTube channel so I'll find it there, I'll put it in later. Can you provide links to where you can go through the aggregator example more slowly as well? Yes so I'll put all of this tutorial in or this talk rather I'll put the slides in and also this has been recorded as well so you can go back and watch it.
00:31:25.360 - 00:31:47.672, Speaker B: A lot of this is very much in flux at the moment and has come about very quickly literally over this weekend some of this functionality. So I hope to do a more in depth, slower run through of it in which we go through a bit more detail. What I will say is probably the best thing to do is on Thursday we have a live Twitch stream and I'll cover it in that.
00:31:47.672 - 00:32:06.750, Speaker B: So if you go to Phil Builders I'll cover that. That will be at noon Eastern time on Twitch and I'll go through it and if you've got specific questions as well related to that you can ask. There as well.
00:32:06.750 - 00:32:36.580, Speaker B: So Madiv asks, I'm creating a datadale smart contract. I have a question is, when the proposal gets accepted in the contract, how can we store the metadata in Filecoin storage? Store the metadata, the JSON file. So is that related to you on about the metadata specifically for something like an NFT, you would store in the same way via an aggregator, so you use the same process there? Hopefully.
00:32:36.580 - 00:32:43.124, Speaker B: I'm not sure if that answers your questions. Oh yeah, Rory's put in a link to the IPFS videos there. Thanks a lot, Rory.
00:32:43.124 - 00:33:03.488, Speaker B: And yeah, in there, there's a talk there regarding that going in more detail. Brainfried asks, is the CID generated based on the data content? Yes. That is one of the specific things about IPFS and filecoin is the CID is effectively a hash of the data.
00:33:03.488 - 00:33:15.988, Speaker B: So if the data changes, the CID changes. And this is one of the really nice things about properties, about IPFS and filecoin is when you fetch the data, you can check it against the CID I. E.
00:33:15.988 - 00:33:41.528, Speaker B: The address you asked for so you can actually verify yourself whether the data that's been served to you is the data that you actually asked for. So the same data will create the same CID? Yes, with a slight asterisk. Remember I mentioned that a car file is actually a Dag, a directed acylic graph.
00:33:41.528 - 00:34:02.390, Speaker B: Well, there's more than one way to generate a graph with the data. So if you think you could go like depth first or breadth first, actually, the same data can produce different car files, so can produce different CIDs. But yes, once you've got a specific Car file generated, then the CID would be the same.
00:34:02.390 - 00:34:14.648, Speaker B: But yes, some implementations do slightly different approaches to creating the car files I. E. The chunk sizes they use in the car files might be slightly different, so you might get different ones.
00:34:14.648 - 00:34:37.580, Speaker B: So you could have potentially multiple CIDs reference the same data because the data has been packed in a Car file slightly differently between different implementations. But if you've got the ceid, you can verify that that Car file is what you're expecting it to. Yes.
00:34:37.580 - 00:34:39.728, Speaker B: Madheva is asking. Yes, it's related to NFT. Yeah.
00:34:39.728 - 00:35:00.176, Speaker B: So you could store the NFT like JSON data file in the same way via an aggregator there as well. There's NFT storage, which is specifically aimed at NFT. They don't yet, as far as I'm aware, allow you to or have a way to get data from on chain into NFT storage.
00:35:00.176 - 00:35:21.196, Speaker B: But you could do the same kind of structure, the same process that I did here with this deal monitor here. So this deal monitor is a fairly it's about 100 lines of JavaScript. And literally what it is doing is it is listening for events on there.
00:35:21.196 - 00:35:43.088, Speaker B: It's listening for this store Uri event to be emitted. Once it's emitted, it decodes. The event gets the Uri from the event and then it actually sends it to Edge.
00:35:43.088 - 00:36:04.376, Speaker B: So this is actually a slightly older version here. Actually, I just realized I haven't I haven't pushed the latest version up yet, so I need to do that just after this talk here. But, yeah, it constructs a payload here and sends that payload up to Edge, and it also stores it in a local database as well.
00:36:04.376 - 00:36:21.470, Speaker B: Just a local JSON database so you could go back and reference it later and then it updates the smart contract. So, yeah, it's extended slightly from the GitHub version, but still about 100 lines of code. Okay.
00:36:21.470 - 00:36:41.844, Speaker B: Yeah, my discord and my Twitter is Hammer toe hammertoe. So you can find me on Twitter or Discord. I'll be hanging around in the ETHGlobal discord in the filecoin sponsor channel there.
00:36:41.844 - 00:37:04.730, Speaker B: So if you need any help, you can come in there and I'll find you there. Can we admit the event in the custom contract and use that? Yeah, that's just an example, so you can create whichever event you want and do that. Okay.
00:37:04.730 - 00:37:10.130, Speaker B: Right, I think that's it. Thanks a lot, everybody.
00:37:10.820 - 00:37:19.820, Speaker A: Yeah, thank you, Matt, for the great presentation. Thank you, everybody, for attending. And again, if you have any further questions, please reach out on the discord partner channels.
00:37:19.820 - 00:37:22.960, Speaker A: So, cheers, thank you all. Have a great rest of the day and have a good week.
00:37:23.110 - 00:37:24.812, Speaker B: Great. Thanks a lot. Bye.
