00:00:07.210 - 00:00:20.240, Speaker A: Hello, everyone, and welcome to the Ezko Workshop zero Knowledge Machine learning with Easyko. Joining us today is Jason Morton and Dante Camuto, who will be taking us through the session. And with that, I'll pass it over to Jason and Dante to get the session started.
00:00:22.130 - 00:00:33.000, Speaker B: Hey, thank you very much. Nice to meet all. We'll jump right in with the screen.
00:00:33.000 - 00:00:53.840, Speaker B: Okay. Yeah. So we're jason, Dante, we've been working on building this software for machine learning on chain or for doing Zero Knowledge proofs for machine learning.
00:00:53.840 - 00:00:55.296, Speaker B: And today we're going to talk about.
00:00:55.318 - 00:00:57.484, Speaker C: How to use that for Autonomous Worlds.
00:00:57.532 - 00:01:38.064, Speaker B: So that people become increasingly interested in in recent days. Okay, so what's the picture? So one way to think about it is that we're providing kind of a physics engine for Autonomous Worlds so we can do kind of arbitrary matrix computations, arbitrary kind of machine learning computations, convolutions, all kinds of crazy stuff, things that you wouldn't be able to write in solidity because of their gas costs. You can sort of offload those heavy computations to this engine so the game client or a server can prove sort of a state transition of a game.
00:01:38.064 - 00:01:57.880, Speaker B: So you can imagine there's a diffusion happening in the game or there's an evolution in a world state that's more complex than what you want to implement with, like the ECS. And so you can use this as a way to kind of shortcut that. You can also use it to build NTCs that are powered by AI.
00:01:57.880 - 00:02:27.440, Speaker B: And a lot of the examples people have done as well are things like kind of a sickle god model where you're trying to please someone like a cooking mama, or you're trying to please someone by creating some kind of pattern in the game. And then what's happening behind the scenes is there's a classifier model that's being run on your input? Of course, sometimes those models don't work as well as you would like them to. But the fun part about a game is that even those failures can become sort of fun part of the game, like beating an NPC.
00:02:27.440 - 00:03:03.516, Speaker B: So let me back up for a second and say, what does it mean to run a model on chain, an AI model on chain? So you're all familiar with an ECDSA signature, which is a kind of zero Knowledge proof, takes public inputs and a private key, combines them in a well known signature function, produces a result, which is the signature, and then you pass that to the chain to verify it, or someone else can verify that signature. And what a Zero Knowledge proof is is basically just a programmable signature. So we can replace that signature function with any function we like.
00:03:03.516 - 00:03:34.260, Speaker B: The functions we like are kind of ML models in linear algebra. And what we're doing with Ezekiel is to make it easy to create a model in Python. Describe the pipeline that you want in terms of who is responsible for approving, who's responsible for verifying and what's the setup? We bake that into a proverifier pair and then someone produces a proof either on the client or the server and then the chain verifies the proof and it's as though the model has run on chain even though it ran off chain.
00:03:34.260 - 00:03:57.352, Speaker B: And the big picture, of course is that AI is something that doesn't run well on chain now. So AI and machine learning and linear algebra are great but require trust either in the general context, trust from someone like OpenAI, or trust from person who's running the game server or wrote the game client. We can work in Python or other numerical languages.
00:03:57.352 - 00:04:19.488, Speaker B: There's a big library of existing models and transformations we can do in a game for on chain contracts. They obviously have the property of being decentralized, being trustless, being composable and having this property of autonomy that many will talk about. But they're less scalable because they rely on a consensus mechanism.
00:04:19.488 - 00:04:52.032, Speaker B: The same computation has to be repeated many times and they're kind of too slow and too expensive to do complex AI machine learning or linear transformations. ZK brings to the trustlessness decentralization and proposability some scalability because now instead of having everyone, all the consensus nodes have to do the same computation. A single model can be, a single machine can do the single client or a single server and then everyone else can trust it.
00:04:52.032 - 00:05:29.476, Speaker B: But of course we pay a cost in terms of math and security properties and a weird programming model and possibly weird languages. But what we've done with Ezekiel is to make it easy to use python models or python description of the linear algebra or the machine learning model that you want to run so that you can access the library of existing models and or train your own models and then make it easy to deploy it so that we can use it in an on chain game. And I want to mention an example by Dinoloric Rich, Patricia and Paul.
00:05:29.476 - 00:06:10.250, Speaker B: Henry Dimilo is on the call as well, I think. And what they did was to produce just so you get a sense of the kind of thing that happens is they made a world, the players could take actions and then you evolve the state of the world in response to those actions like trees spread and grow or fire spreads or whatever with a Zkml model. And there's another LLM that's not right now in Zkml but will be eventually that tells story about the things that happened just to give you a sense of the kind of evolution that might be possible.
00:06:10.250 - 00:06:49.972, Speaker B: The other piece I want to bring up here is how this might interact with Mud. So there seems to be a lot of connections with Lattice with Mud and I think we're going to see a lot more integration happening as people experiment with it. Right now if you look at the bottom part of this picture is kind of that without a mud we're going to be responsible for writing the code to ingest state from the chain or to take an action the player action prove an update of the state how the player's actions or inactions result in a change in the state of the game.
00:06:49.972 - 00:07:26.880, Speaker B: And then that proof goes to a verifier that lives on chain and updates the state and now it's back into state on chain. So there's kind of a game loop that's happening between the client and the state and the state that's stored on the chain where the more complex updates involving diffusion and whatever else happened, client side or server side and not in the smart contract that would require a fair amount of development. Like an example I showed you before, but with Mud you can use the ECS framework to sync the state with the client state and the chain state.
00:07:26.880 - 00:07:46.650, Speaker B: And we're sort of now agnostic about where we're going to do the proofs, either on the client side or the server side, but a lot less work in terms of producing that. Of course, much folks talk about that. So the basic idea of how a zkml proof works is very simple.
00:07:46.650 - 00:08:08.270, Speaker B: So we're taking floating point numbers and representing them as six point numbers. So we're picking a denominator like 256 or quantizing it and then we're representing a small floating point number seven six as literally seven. We're just storing the numerator and then we keep track of the denominator 256 in the type system.
00:08:08.270 - 00:08:28.720, Speaker B: And to prove dot product you could write a custom gate, in other words, make the constraint into the constraint system of the generalledge proof that says that y, the output of the two vectors dotted together is equal to whatever they are. That's not really what we do. We do something more complicated.
00:08:28.720 - 00:08:51.070, Speaker B: There's a lot of different arguments that can be made to accomplish that but sort of not critical for you to worry about exactly how that argument works because we've abstracted that away. And then to prove that y is equal to ax plus b, then a matrix multiplication and a shift has been applied. You just basically repeat that argument or arguments like that.
00:08:51.070 - 00:09:19.472, Speaker B: And then to prove that nonlinearity like ReLU has been applied, what we do is we prefill a table with all the possible inputs and outputs that might occur given our assumptions about quantization of the maximum size numbers that might appear and we prove that the input output pair lies in that table. That's called a lookup argument. There are other alternative ways to do that and there's lots of new arguments to pipeline to make it more efficient.
00:09:19.472 - 00:09:36.552, Speaker B: But it's already pretty fast. So I'm going to pass off in a minute to Dante. But I want to say depending about the big picture about how to use Ezekiel, basically what you do is you find a model that works for your use case.
00:09:36.552 - 00:09:47.532, Speaker B: You train it yourself. You design it yourself or you download it. For example, you're using it to compute a state transition, an evolution of the world or the natural world.
00:09:47.532 - 00:10:09.396, Speaker B: You might be using it for this judge. Does the player recipe make me happy? Or did the picture that the player drew or the song that they sung satisfy the basically on chain AI? Or to run an NPC AI or some other idea that hasn't occurred to anyone yet that you'll come up with. Then you take this model and you compile it.
00:10:09.396 - 00:10:42.736, Speaker B: You bring Ezekiel to sort of a triple of circuit which is expressed in the setup, a prover artifact which is either a was intruder or a binary prover and a verifier artifact which in our case will be an on chain EVM verifier. You have to deploy that verifier and sort of route the update into it and integrate the prover either on the server or the client. And then you launch your game.
00:10:42.736 - 00:10:59.062, Speaker B: And these are just our Telegram group and our GitHub you check out. And I think with that I will pass Dante to show you how it works. All right, I see there's a few.
00:10:59.116 - 00:11:16.586, Speaker C: Questions in the chat. I don't know if we linked to those now or later on. There's one question which is is the output a regression result or is it limited to classification results? You can do both.
00:11:16.586 - 00:11:46.486, Speaker C: I'm going to sort of demonstrate the computational graphs that you can build, but you're not really limited to either regression or classification. There's another one which is, can we use VTL for provable training as well as inference? We currently only support inference training. Yeah, training is a lot more computational expensive and hopefully at some point we'll support it.
00:11:46.486 - 00:11:57.530, Speaker C: But currently we're just working on with intros. Someone asked you to go back to the slide with the GitHub there in Discord.
00:12:08.540 - 00:12:14.810, Speaker B: I just put it into the chat. If you just search for easykl, also find it.
00:12:15.820 - 00:12:29.180, Speaker C: All right, awesome. All right, well, I am going to showcase what all of this looks like in practice. Basically, there's actually quite a few.
00:12:29.180 - 00:12:41.520, Speaker C: We've developed a lot of tooling to make it easy for you to use us. The main library is written in Rust. I know it's a challenging language, so we've sort of wrapped it in Python.
00:12:41.520 - 00:13:12.750, Speaker C: You can compile it to logging if you're running a browser application, so you can run proofs and verification straight in browser. But today, kind of as a neat sandbox, I'm going to be running you through how you might use Ezekiel to generate proofs inside of a jupyter notebook or something. Just so you have a nice sandbox to yeah, it's just kind of a nice sandbox to try out different models and see how they perform.
00:13:12.750 - 00:13:32.130, Speaker C: Let me share my screen. All right. This is kind of a pretty typical setup for starting off an Ezekiel project.
00:13:32.130 - 00:14:04.332, Speaker C: So what's kind of cool is that you can actually just define your circuit, or you can think about your circuit just as a PyTorch sort of computational graph. So you're going to start typically by defining a wrapper around ML module, which is really just like how you might define a neural network in Python more broadly. So how are we going to do this? All right, let me create some sort of model.
00:14:04.332 - 00:14:18.050, Speaker C: What's interesting is that I have copilot on so sometimes suggests layers as I'm typing. There we go. Let's see what it comes up with.
00:14:18.050 - 00:14:35.044, Speaker C: Kind of interesting to see what I might be interested building. All right, we're going to define okay, let's start simple. Let's start simple.
00:14:35.044 - 00:14:49.870, Speaker C: And let's just have a single comp layer and value nonlinearity as part of our model. And our model is just going to be applying oh, look at that. Stuff is amazing.
00:14:49.870 - 00:15:08.450, Speaker C: So our model is just going to be applying a con player and then subsequently applying a relative nonlinearity onto some data. Let's say that data is like MNS shaped or something, which is like images with a single channel.
00:15:08.820 - 00:15:09.600, Speaker B: Whoa.
00:15:11.060 - 00:15:22.292, Speaker C: And yeah, 28 by 28 images. And we need to instantiate the model once. We're not going to train it because we're sort of limited by time.
00:15:22.292 - 00:15:49.820, Speaker C: Also don't want to download a data set, but all you need to do now is just export this, get out locally. So let's run that cell, make sure everything works. All right, so we've exported the model, and the format that we use is something called Onyx.
00:15:49.820 - 00:16:03.744, Speaker C: The details of this don't really matter, but we've just saved this model locally. And what we need to do, as Judy described, we use a slightly different representation for. So we're not really working in floating point space.
00:16:03.744 - 00:16:19.560, Speaker C: We're working with fields, arithmetic. So what we need to do now is quantize the model to basically fit everything into that field and make sure everything runs correctly. And this is done with the forward method on the ZKL library.
00:16:19.560 - 00:16:37.944, Speaker C: There you go. So we've quantized the model. And now for a ZK proof, we need an SRS string, which corresponds to the parameters that we're going to be using to generate the Verifier key, the proving key, and subsequently approves.
00:16:37.944 - 00:16:51.570, Speaker C: We also provide a method to do this to make it really easy to do this. This takes a little bit longer to run, but hopefully not too long. And yeah, there we go.
00:16:51.570 - 00:17:11.252, Speaker C: And then we have what's called the setup phase, which is where we generate the Verifier key and the proving key. So the proving key in particular is important for it's in the name. It's important for generating proofs, and the Verifier key is important for the Verifier to then subsequently verify the proofs.
00:17:11.252 - 00:17:29.324, Speaker C: So this is all done in the same step, and then it can be distributed to the two different parties that need to either prove or verify proofs generated from one specific circuit. We'll run that and nice. Right.
00:17:29.324 - 00:17:56.664, Speaker C: So that we're out and then generate a proof using a simple method by providing the paths of all the things that you need, and then you can verify it and voila, that's kind of it. We also provide a bunch of methods for you to I don't have my ledger with me right now, but if you plug your ledger in, you can basically submit proof on chain. You can deploy Verifier contracts if you need.
00:17:56.664 - 00:18:17.790, Speaker C: So we provide tooling to do all of that internally to the library. We've tried to make it as easy as possible to generate circuits, prove, verify, generate on chain Verifiers, and then submit proofs to those Verifiers. But let's go back up and start generating something slightly crazier here.
00:18:20.020 - 00:18:20.624, Speaker B: Okay.
00:18:20.742 - 00:18:38.870, Speaker C: What do we want to add? Let's say we want to double X and then subtract it. And then we want to take it to the power squared. Let's see if this works right.
00:18:38.870 - 00:18:51.300, Speaker C: Let's run through those cells again. Basically, the idea is that you're not really constrained by anything when you're generating these circuits. We support a whole bunch of methods.
00:18:51.300 - 00:19:15.944, Speaker C: So if you want to generate something really crazy, you can chuck it into a PyTorch model and then start running proofs on it. We need to generate the setup base again and our proof ribbon. And there you have it verified again.
00:19:15.944 - 00:19:23.800, Speaker C: All right, let's see if we can chuck a second comp in. Make this a little bit spicier.
00:19:27.280 - 00:19:29.948, Speaker B: Yeah. All right.
00:19:30.034 - 00:19:37.996, Speaker C: Let's add that into here. Wow. And then maybe yeah.
00:19:37.996 - 00:19:57.072, Speaker C: All right, that sounds good. But yeah, in terms of circuit size, if you're wanting to prove something in less than 10 seconds, we've got in like, six, five, or six layer comps. Yeah, we've got in five or six layer comps to prove in less than 10 seconds.
00:19:57.072 - 00:20:05.620, Speaker C: So you can go pretty crazy in terms of parameter size. Yeah. Let's see if this one verifies.
00:20:05.620 - 00:20:18.300, Speaker C: I kind of want to see if I can break it at this point, like, see how much we can throw at it. All right, sweet. All right, that's still verifying.
00:20:18.300 - 00:20:39.592, Speaker C: God, I don't know if we're going to be able to get this to stop working today. Yeah, I see there's a few questions in the chat. Have you tried using Mojo with Bkml? I'm actually not too familiar with Mojo, but yeah.
00:20:39.592 - 00:20:56.172, Speaker C: Are there any questions on the specific demo as to how to use this? I'm a bit confused what the demo was trying to prove. Well, basically yeah. Okay, so what is the demo trying to prove? Could you explain? All right, yeah.
00:20:56.172 - 00:21:26.308, Speaker C: So if you were to try and build ZK circuits on your own to generate proofs of that a zero knowledge machine learning model was run correctly. You would basically either have to go into Circom or Halo two and start coding up like a gate for matrix multiplication, for a value, for a convolutional layer, all those sorts of things. And what we've done is we've started to abstract away a lot of that complexity.
00:21:26.308 - 00:22:01.444, Speaker C: And what does that mean? So it means that instead of having to go into those languages like Circom and those sorts of things to generate proofs or a circuit that represents a specific machine learning model, you can just use the frameworks that you are probably familiar with if you're a machine learning practitioner. Meaning that you can just use things like PyTorch or those sorts of things to generate a model. We take that model and then compile it into a circuit for so you don't even have to think about it.
00:22:01.444 - 00:22:21.764, Speaker C: Oh, what is the ZK proof proving? All right, yeah, good question. So currently we're just proving that the model executed correctly. So given that, you know, a specific given that there's a public model and there might be some public data, you're proving that you executed it correctly and then you can submit that proof as call data to the chain.
00:22:21.764 - 00:22:46.720, Speaker C: But you can generate a whole host of other proofs. Like for example, you could keep the parameters of the model private and run it on a set of public inputs. You could keep a set of private inputs, but run a publicly known model on those and then generate a proof that you ran it correctly and then submit that proof on chain, basically.
00:22:46.720 - 00:23:08.774, Speaker C: So we allow all those sorts of different scenarios to be realized. What model am I using currently? Well, I just ran through it. I'm sort of generating these models on the fly, so you can really generate kind of anything, sort of anywhere.
00:23:08.774 - 00:23:32.382, Speaker C: Can the code be shared? Yeah, we actually have a jupyter notebook demo straight in our repo. So if you want to play around with this, this is all available inside the GitHub repo, but yeah, definitely already available. Someone's asking, someone saying they're not familiar with Python.
00:23:32.382 - 00:24:03.530, Speaker C: Is it possible to have a similar tutorial with JavaScript eventually? So currently we are on the JavaScript side. What we're really focusing on at the moment is getting Provers and Verifiers in WASM so that you can run these things with relatively good performance straight into the browser. So you can generate proofs for a specific model or verify proofs for a specific model straight into the browser.
00:24:03.530 - 00:24:51.542, Speaker C: If you have interest in getting this stuff into JavaScript, definitely open up an issue and we'll take a look. Is there some Mud integration allowing game clients accepting events from the indexer? Currently we are not fully integrated with Mud, but that's definitely something we're moving towards that we're aiming to integrate as much as possible with them to make using our tooling within autonomous worlds as quickly as possible. In ML, a lot of people talk about keeping input data private.
00:24:51.542 - 00:25:00.960, Speaker C: I'd imagine it could be interesting to also be output data private in that scenario. Also, could you go through how a proof like that would be set up. Yeah.
00:25:00.960 - 00:25:35.978, Speaker C: What's pretty interesting is that the proof command down here, you can actually say what parts of the model you want to keep private and public. You can actually just pass a flag that says, I want my parameters, for example, of my model to be private, I want the output of the model to be private, or I want the input of the model to be private. So we allow all of that to be configured from start to finish.
00:25:35.978 - 00:25:40.220, Speaker C: So if that's something you're interested in, we can already do that.
00:25:42.350 - 00:25:47.230, Speaker B: We should also probably show something from EVM Verify.
00:25:50.050 - 00:25:57.540, Speaker C: Yeah, for sure. I actually don't know if we have that in rack. I don't know if we have that into the Python bindings yet.
00:25:58.230 - 00:26:02.340, Speaker B: Yeah, I can do the Docs or something.
00:26:03.590 - 00:26:08.180, Speaker C: Yes. Let me show that.
00:26:22.770 - 00:26:54.006, Speaker B: I'm successfully sharing. Are you sharing? But if you go to the Doc site, there's this actually on Verifying on Chain, which shows you have to generate a proof with the transcript set to EVM. And then you can generate a Verifier which emits code deployment code specific to visit the command line, specific to your model and solidity code.
00:26:54.006 - 00:27:13.946, Speaker B: And then you can test that it Verifies by using a verified EVM that Just is running the code that you generated just before it. And then you can deploy that code to a chain. And then you would need to assign it.
00:27:13.946 - 00:27:27.294, Speaker B: And you can also send proofs, or you can send those to Proofs manually or deploy that code manually. But just wanted to highlight that it is possible to create Verifiers. So a very common use case would be something like you have a state update.
00:27:27.294 - 00:27:41.960, Speaker B: You want to check that the state update is valid before you apply it. And so you would create that Verifier and use a Verifier called from your main smart contract in order to check that the update was.
00:27:47.620 - 00:28:11.464, Speaker C: And yeah, I think that answers another question in there, which is, are we making function calls to the Verifier contract? Yeah. So there's a method called Verify EPM where you can just pass a Verifier contract address and then submit your proof there. But yeah, we also have a bunch of methods for deploying the Verifier contract, so you don't have to think about that either, which was what Jason was.
00:28:11.464 - 00:28:19.520, Speaker C: Just one more question.
00:28:20.470 - 00:28:28.530, Speaker B: Strawberry detector should work. Yeah, I would love to see that if that's something that you end up doing. Or hot dog detector.
00:28:28.530 - 00:28:41.474, Speaker B: Some people have done using this doodles thing, the Google Quick Draw program, for example, where you have to draw a picture of a snowflake or a horse.
00:28:41.522 - 00:28:54.560, Speaker C: Or whatever, that should also work. All right, any more questions?
00:29:03.210 - 00:29:08.170, Speaker A: Yeah, if anybody has any other questions, feel free to type in the chat or take yourself off mute.
00:29:10.430 - 00:29:35.240, Speaker C: Very curious to know whether you have a complex use case that you've imagined that you can share with us another one. So there's two ways to answer that, I guess. There's complexity in terms of the models that we're trying to get in.
00:29:35.240 - 00:29:57.470, Speaker C: Most definitely we're aiming to get transformer based models in at the moment in that sense, for sure. And I guess we're also trying to get the Verifier contracts into larger DApps and stuff. So start composing complexity within those sorts of applications.
00:29:57.470 - 00:30:01.470, Speaker C: Yeah, there's a lot of interesting, I guess, Autonomous Worlds.
00:30:02.210 - 00:30:41.260, Speaker B: Yeah, in Autonomous Worlds, people thinking about, for example, having the world physics or evolution function being something that could be player owned or could be like the player commits to an evolution function and then is responsible for maintaining their planet or NPCs with serious AI powered by Transformers or whatever else that can even move between games. Stuff like that. So that kind of thing should eventually unlock, certainly the state updates and really basic AIS, but more sophisticated AIS would be really fun.
00:30:41.260 - 00:30:42.526, Speaker B: Yeah.
00:30:42.548 - 00:31:25.246, Speaker C: And I guess for Autonomous Worlds, people specifically have been using it not as ML, but as a sort of physics engine, which is what Jason sort of showcased earlier, which is like all the state transitions between based yeah, all right. This can be huge for verifying physical objects, useful or not. Yes, definitely not just strawberries and hot dogs, but also strawberries and hot dogs.
00:31:25.298 - 00:31:25.900, Speaker B: Yeah.
00:31:26.830 - 00:31:34.102, Speaker C: If you want to try relatively light. That's true. I mean, I'd be curious to see the strawberry Verifier.
00:31:34.102 - 00:32:06.770, Speaker C: So if you want to whip that up, definitely message us and we'll help you get strawberry verification over the line. But yes, I guess one more thing I wanted to address was the JavaScript question, just to go back to that. There's a bunch of ML libraries in JavaScript which will export to Onyx, which is the neural network representation we use.
00:32:06.770 - 00:32:24.342, Speaker C: It's a sort of universal format. So I think TensorFlow JavaScript can save to Onyx, and then you can point our CLI so we also have CLI tool into that Onyx file to then replicate the flow that we just showcased. So you're not limited to Python.
00:32:24.342 - 00:32:32.780, Speaker C: Basically any framework that can generate Onyx yeah. As usable, basically.
00:32:45.630 - 00:32:46.186, Speaker B: Awesome.
00:32:46.288 - 00:32:52.410, Speaker A: Well, cheers. Thank you all very much for this presentation. And thank you all, everybody else, for attending.
00:32:52.410 - 00:33:07.482, Speaker A: The record will be shared to you all. And so if you have any questions, please feel free to reach out to Dante and Jason on the discord channels. Yeah, so thank you all for this.
00:33:07.482 - 00:33:11.700, Speaker A: We have one more session later here, and then the team formation will be the last session of the day.
00:33:14.710 - 00:33:15.134, Speaker B: Cheers.
00:33:15.182 - 00:33:17.630, Speaker A: Thank you all. Bye.
