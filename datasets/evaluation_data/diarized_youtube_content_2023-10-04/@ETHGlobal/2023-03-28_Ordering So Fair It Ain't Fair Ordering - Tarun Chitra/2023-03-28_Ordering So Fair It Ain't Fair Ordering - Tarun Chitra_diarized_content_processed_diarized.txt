00:00:06.250 - 00:00:46.010, Speaker A: So yeah, first off, I I want to want to talk a little bit about the flaws of fair ordering. And this is not meant as a attack on any of the people who've worked on it before. I think, like, it's a really interesting idea to take some of the ideas from social choice theory and apply them to try to make blockchains more fair. But I think there's no way of doing it in the way that people have tried. There might be some other stuff. Hopefully we'll end with some positive notes. But before that, let's bring out the hatchet.
00:00:46.010 - 00:01:57.390, Speaker A: So Tina, as previous speakers have also mentioned, forced us to say the word mev economics somewhere. In my case, I, unlike John and James, kind of sort of wrote a slide. And the idea is that and this is a quote from one of the papers on fair ordering, which is fair ordering guarantees specific ordering in a finalized ledger on how transactions arrive to the network to reduce minor acceptable value. Yet virtually all fair ordering mechanisms say nothing about the economics of what they do. They guarantee that some subsets of orderings will be respected, other ones will not ever happen, but they never tell you the cost of that or whether certain payoff functions change with that. So one question is, if we're putting all this extra onus and extra work on top of validators, what's the real economic value of it? Is there economic value? Is it harmful? And that's sort of the main thing we'll be covering. So what is fair ordering? So fair ordering is a mechanism that coerces honest validators to respect a particular set of orderings or transactions.
00:01:57.390 - 00:02:53.182, Speaker A: I e. First come, first serve, block order, fairness, block, batch order, fairness. All of the papers are not duplicitous. They do all point out this fact that it's actually impossible to attain perfectly. Kenneth Arrow won the Nobel Prize in economics for proving this, which is that in ranked choice voting so you can think of a ranked choice vote as a validator giving you an order permutation of sequence of transactions and then sort of aggregating the vote together to construct a final outcome that it's impossible to get something that doesn't centralize, that doesn't have what's sort of known as a dictatorship. So instead, fair ordering tries to make algorithms that are approximately the average or majority vote or some type of property for most ordering. So here I put two different definitions.
00:02:53.182 - 00:03:45.910, Speaker A: The other thing that's kind of funny about all these papers is they all have slightly different definitions of whether a batch is fair or not. And they all involve sort of extra data, like some notion of timestamps and some assumptions of honesty amongst the validators. Thank you to John for making this meme. But there's something weird about these definitions, right? These definitions are properties of partial orders. They don't say anything about the economic value per unit partial order. Yet on the other hand, if you think about mev, mev is actually an economic profit, a rent that is extracted. That rent has a size, it has a I think, you know, the three main papers on fair ordering, themis Acquittus and quick block ordering.
00:03:45.910 - 00:05:05.994, Speaker A: They had good intentions but they completely ignore the economics and they're like hey, you do this thing, it's this combinatorial thing. We restrict the sets of ordering and it gets rid of MUV and it's kind of a crazy statement to make because again, you don't quantify the actual amount of mev or the value and you're agnostic to all applications which doesn't really make much sense, right? There's a reason that certain applications generate very regular mev like AMMS, some generate very spiky mev like liquidations or NFT auctions. Those are very different things. You can't really tell me that this ordering is universal to all of them. And so that's the thing we're going to kind of explain today to try to dismantle this kind of shambolic industry of hoping that we can violate the natural laws of social choice theory. So is it really a good idea to cause these to have validators have to do all this extra work when it doesn't really have an economic payoff? And hopefully you'll be convinced at least somewhat the full paper will be out soon so you can see the proof, but that it's actually harmful in some cases. So I want to also give a bit of historical context from other fields.
00:05:05.994 - 00:06:00.960, Speaker A: So other fields that use social trust theory, especially things in decision theory, things in AI have studied order fairness in different contexts, especially with regards to model quality or like how much you mutate a model by giving some fairness guarantees and they get these impossibility results. The difference is model quality is a bit like mev's economic value. There's still these kind of continuous magnitude objective functions depend on the input data depend on the particular thing you're inferring. But they all get these impossibility theorems. So why should you not expect that for fair ordering? Okay, so I've had my fun but now we actually have to go into what it is about fair ordering. That's so weird. So to try to dismantle fair ordering, what we do is we consider a two player game.
00:06:00.960 - 00:06:49.466, Speaker A: One player is the fair ordering protocol that is proffering a set of orderings or transactions that are allowed to be executed. And the other you can think of as an adversarial DeFi developer. So someone who is making a protocol that's trying to cause the fair ordering to give a worse value for the users of the protocol. The rough game mechanics are nature draws a set of transactions so that's users generating sets of transactions, the developer plays first and constructs a protocol. So think like a DeFi protocol adapted to a set of transactions and the protocol is a payoff function. So we'll talk a little bit about what those are in a second. But you can think of the payoff function as the expected value for the user.
00:06:49.466 - 00:07:38.750, Speaker A: Then the second player, the fair ordering protocol, draws a set of permutations from the symmetric group. That's a SN, that's a set of permutations and it draws it from a distribution p. And so one reason that we have this distribution is of course, in most fair ordering protocols, there's some extra metadata like the timestamps of when people different validators receive certain transactions that add some randomness. And that randomness means that there's not a unique ordering. Again, errors and possibility theorem sort of guarantees that. So there's a sort of set of orderings and it'll turn out that the size of the set of orderings controls how well the fair ordering protocol can win in this two player game. And finally, a value v is realized based on these.
00:07:38.750 - 00:08:35.460, Speaker A: So without getting too in the weeds to the right, you'll see something that defines the value of a protocol PA, which we can talk about via its distribution of orderings it generates. And we consider a Min max loss. So if you've seen sort of classical game theory, classical von Neumann morgenstern stuff, this is just traditional Min math. And there's a loss function which takes in an ordering, it's a permutation pi in the symmetric group and a payoff function f. And this loss function is meant to measure some notion of fairness. And we'll talk about how there are different loss functions, will give you different notions of fairness, but you sort of want this game to qualitatively have the same winner. Like usually, the DeFi developer wins for many loss functions, even if I perturb it.
00:08:35.460 - 00:09:31.710, Speaker A: And so in this game, the value of the game, if the game is positive, the adversary, the DeFi protocol developer, wins. If the value is negative, the fair ordering protocol wins. And of course this will depend on loss function, but we'll talk a little bit about that. So why does this represent fair ordering protocols? Abstractly again, you can view a fair ordering protocol as taking each validator sets of transactions and some metadata associated to those transactions, like timestamps. And the fair ordering protocol outputs a single permutation with those transactions in the set of allowable transactions h. But the thing is, there's many of these h and the key thing we'll find out is that Ferrotering protocols don't make h that small and that's where the adversarial developer can take advantage of them. Again, the randomness comes from network, latency, user demand, et cetera.
00:09:31.710 - 00:10:59.226, Speaker A: And how do we bound v this value of this game? Well, there's combinatorial constraints that arise from the set h another way of looking at this is given enough noise or entropy in the sets of permutations you're allowed, you can almost surely construct a payoff where if I restrict to those sets, I get a worse value than if I am unrestricted. And basically what the proof does is we explicitly construct a DeFi protocol whose rules for when liquidations are allowed to happen exactly is optimized to be bad on these permutations generated by the fair ordering protocol. In fact, the liquidation rules draw a lot of inspiration from a protocol many of you might know, which is liquidy LUSD. They have a staking pool that auto liquidates LUSD issued assets. And the only difference is that instead of giving parado rewards, we give fixed rewards. But it's a little bit more complicated to describe and what this sort of says is fair ordering preferences, particular applications. So this application that looks like a DeFi protocol that has particular liquidation rules has a worse value for users under fair ordering, which means that other protocols that have the same value are sort of preferenced.
00:10:59.226 - 00:11:55.910, Speaker A: And so there's sort of this very interesting thing that a fair ordering protocol is sort of picking winners and losers implicitly. So now the next question is what is this notion of a payoff? So a payoff represents the economic value each user represent gains from a particular set of transactions. A user payoff is a function from the symmetric group to the reels. You can decompose any payoff into this kind of sum over indicator functions and some of the end results we'll talk about the end, which I'll just sort of give a preview of rely on sort of some of the decomposition properties there. So what are examples of payoffs? One example is just an AMM payoff. And in this paper earlier we analyzed sort of how mev payoffs for sandwich attacks look under permutations and then sort of get some bounds there. The other example is liquidations.
00:11:55.910 - 00:12:40.510, Speaker A: And you can think of a liquidation as really an indicator function that's parameterized on a price and a threshold. So if the value of the asset is below some threshold that's the indicator function, then you realize a profit and if it's not, it's zero. And you can sort of look at this as a barrier option as well. So the real question is how do you choose this loss function? Well, there's a couple of different ways to define sort of fairness. One version of fairness is the extremes. What's the difference between the best case ordering like the one that maximizes social welfare, and the worst case ordering, the one that minimizes social welfare that should say best, not Bayes. The other is sort of mean.
00:12:40.510 - 00:14:03.420, Speaker A: So what's the deviation of a given ordering away from sort of the average? How much do the orderings fluctuate around the average we want to choose l this loss function in such a way to be robust to perturbations? And an interesting thing is that for finite groups there's actually ways to generate kind of lower bounds on things like V, like this value of this game that we talked about. These are uncertainty principles. These finite group uncertainty principles are similar but quite different to the uncertainty principle. You might have learned in a physics class or a real analysis class. But the interesting thing is that you may now ask this question is there an uncertainty principle for mev here? And there's a very interesting the final thing we'll see is that there's sort of this trade off between how much you restrict the set of orderings and how manipulable a payoff function is. And that trade off has sort of this lower bound and that's sort of this hidden cost and sort of complexity cost in mev, which is the reason these fair ordering things sort of can fail. So now let's find out how much orderings cost.
00:14:03.420 - 00:15:37.954, Speaker A: So the claim is, suppose I have the distribution P that generates orderings and suppose that with very high probability the size of h is omega of N factorial. That means it's a percentage of the total number of permutations. And if l is deviation from worst case so the extrema then you can show the value of the game is bounded by a positive constant, which in word says if the number of fair orderings is sufficiently large, then this minimax value is positive. And this sort of DeFi protocol we constructed always sort of does worse when you use fair ordering. Another way of viewing this is fair ordering is actually discriminatory to particular protocols. So you might say, okay, well, is it really realistic that a fair ordering thing generates omega of N factorial permutations? Shouldn't it just be generating constant in the number of transactions? And where this logic sort of falls apart is if you actually look at random ordered elections or random elections. And this was studied in 19th century by some actually French sort of philosophers who also did social choice theory, and this guy Gilbout, who kind of proved this very bizarro formula that in a random ranked election with three candidates, there is a 91% chance that you don't have a Condorse paradox.
00:15:37.954 - 00:16:10.286, Speaker A: A Condorse paradox is candidate A beats candidate B on a pairwise election basis. Candidate B beats candidate C on a pairwise election basis. But candidate C beats candidate A on a pairwise election basis. And so when you have those types of loops, then you have no perfect ranked ordering. And that's sort of like the simplest version of this. And again, to the credit of themis and Aquitus and all these papers, they admit that this is a flaw of theirs. The problem is such a thing guarantees that with enough randomness your set of orderings is still omega N factorial.
00:16:10.286 - 00:17:02.974, Speaker A: It's still a percentage of the symmetric group. And so that sort of says the hypotheses here are pretty likely to happen unless your timestamp distribution is really, really degenerate. Okay, so I've given you all this bad news and a lot of it involves like game theory, math, whatever. Is there anything you should be happy about? And what I would say maybe the positives you should take out. There are probably ways to do application specific order preferences. So there are two results in this frame of mind. The first one, which released yesterday by CEO and Guillermo, is basically they looked at the kind of arbitram first come, first serve mechanism and they only looked at the top transaction.
00:17:02.974 - 00:17:44.814, Speaker A: So they don't care about the whole ordering. They care just about the top transaction. And they were able to write a linear program to be able to bound its gap. And they were able to get a lower bound on the ratio of the price from the optimal ordering versus the sort of like first come, first server ordering and show a lower bounce so that it's always worse under some conditions. And so that's a kind of very nice, simple version. It doesn't capture all mev, but it kind of gets you at least kind of the minimum viable example. And then these papers from us that basically construct an uncertainty principle.
00:17:44.814 - 00:18:48.182, Speaker A: And these uncertainty principles give you this lower bound. The uncertainty principles actually show you things akin to the following where you can bound sort of the worst case part of the minimax loss by lower bound by a polynomial in something known as the Fourier degree of the payoff function and also upper bound it in terms of Fourier degree. So what this says is you can actually control the fairness measured in sort of like the deviation from worst case. How well you can actually achieve some fairness is related to the sort of inherent computational complexity of the payoff function specified by your program. And I think that's actually a really interesting thing because it basically says, hey, there's a reason people optimize smart contract programs for being less complex. They actually have less of this worst case unfair bound the upper bound in this case. So with that, probably did go over time.
00:18:48.182 - 00:18:52.530, Speaker A: So I'll answer some questions in the chat. Bye.
