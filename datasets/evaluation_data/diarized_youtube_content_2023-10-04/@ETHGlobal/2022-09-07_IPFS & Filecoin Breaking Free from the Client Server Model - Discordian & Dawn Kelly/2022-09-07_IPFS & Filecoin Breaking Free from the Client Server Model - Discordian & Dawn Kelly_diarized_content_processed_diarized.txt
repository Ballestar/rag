00:00:06.410 - 00:00:28.566, Speaker A: All right. Good morning. Good afternoon. Good evening. Depending on where you're calling from, we have the IPFS Filecoin team here today to do a workshop on breaking free from the client server model. This is going to be a pre recorded workshop by discordion from IPFS Filecoin. However, we have Dawn One here from the team who can help with any questions.
00:00:28.566 - 00:00:44.330, Speaker A: So definitely still ask your questions in the Zoom chat if you have any. And Don will take the last couple minutes of the workshop to answer any of the questions that you may have. So with that being said, we'll start the recording and yeah, feel free to ask the questions in the Zoom chat.
00:00:44.830 - 00:01:18.530, Speaker B: So hello, everyone. Once again, I'm discordian a community engineer from Protocol labs. I'm here to give you a 201 talk on breaking free from the client server model. This is meant to be a follow up to the intro to IPFS and Filecoin workshop. Having knowledge on what a server is and some knowledge about IPFS and Filecoin will be a huge asset to following along. But I've tried to structure this presentation in a way that it should be informative and hopefully enjoyable to anyone interested in our ecosystem or the distributed web in general. If you have any questions during this workshop, please feel free to ask them.
00:01:18.530 - 00:02:17.430, Speaker B: Any question I don't get to during the workshop, I'll do my best to get to afterwards. All right, so let's get started first by calling back all of the noise in the background, and let's focus for a moment on what the client server model even is. Through this, we'll rebuild the graphic and hopefully with it your understanding of the concepts at play as a whole. So what is the client server model? Well, today only a few companies are responsible for serving up most of the web. We have Google, Amazon, Facebook, and Vastly, just to name a few. These companies combined service our ads, web pages, social media, videos, images, and allow us to host servers and services on their infrastructures. To interact with these services, we also have to use their APIs, sending some data to their server and eagerly waiting for a response so we can process it and do whatever magic we need to do to create our apps and websites.
00:02:17.430 - 00:03:07.960, Speaker B: There are several problems with this design, though. From costs to privacy, free services are often funded by ads, further destroying our privacy, as all our data is routed through a few central points paid for by our attention and private information. For the most part, all our data routes through their servers. If you want to view a video, send a message, collaborate with remote teammates, or do anything on the web at all, really, you're going through someone's central server. This principle is fundamental to the client server model. We use a client such as a web browser or chat app and communicate with a single entity, and that's the server. In the client server model, relying on just one point for our source of truth also opens up a whole host of problems like trust.
00:03:07.960 - 00:03:51.800, Speaker B: Currently, we have SSL certificates to verify an entity is who they say they are. However, that doesn't do us much good if that service is compromised. And even if we are sure of who the entity is, it still doesn't help us know how Advertisers or others are using our data, or even what data is collected. For the development side, the source of truth in this instance could be the results of an HTP API call. If for any reason, the location of that API cannot be accessed, your service could also be impacted. Today, a popular way to build is to lean on the APIs of another remote service, which makes the web very fragile. If the service you rely on goes down, so does your service.
00:03:51.800 - 00:04:38.580, Speaker B: In the second half of this workshop, I'll go over some other ways to communicate with outside entities in a distributed way. To retrieve information without leaning on just a single entity too much. In the client server model generally, if you want some data, you must know where it is. If you want some image, you generally need the full URL to that image to retrieve and share it. If that image is no longer available on that service, you either need to search for it or upload a copy on another server to make it available again, sharing the new location of that image. This is assuming you even have a copy yourself. This principle is called location based addressing and is how most of the World Wide Web functions today.
00:04:38.580 - 00:05:29.044, Speaker B: If you go to a website, it has a URL, and all data from that website is addressed by the location of the data. If the image analogy is too hard to follow, think of how you would share a video on your favorite video platform. You'd likely just share the URL to the video, also known as the location of the video. By this point, you might be wondering, without addressing by location, how would I even find the data? We'll get to that, I promise. But for now, I'm going to finish unraveling what the client server model is, how it functions, and some of the drawbacks to the approach as a whole for the wider web. Speaking of drawbacks, a major drawback of the client server model is that a single outage can take entire services down. Think of when AWS goes offline fastly facebook or YouTube.
00:05:29.044 - 00:06:12.596, Speaker B: These events are treated as news and often slows or even halts productivity. When one of these services goes offline, it's much more impactful than just a single image or video being lost. You've now lost access to your ability to even make new data available at all. All the data you have hosted in these central hubs is temporarily completely inaccessible, or worse, permanently. The location of the data simply cannot be accessed because it's offline. You cannot simply ask for the data from the wider web either, and expect to receive it even if another related entity has the very data you're looking for. You could, of course, host your own service, maybe distribute out the locations of your servers in a way where outages are rare.
00:06:12.596 - 00:07:03.130, Speaker B: Let's say you even come up with the next big idea for a website or platform where you're in control of all your own infrastructure, not relying on some other central entity for your needs. Well, that's my segue into my next point. Scaling can be very expensive. These expenses don't always, maybe not even usually happen linearly, meaning an explosion of traffic can easily translate to an explosion of costs. Whether you host your own infrastructure or you pay someone else to, you'll be paying for it if you want it to scale and run effectively. The more users you have, the more traffic that generates your CPU, Ram and bandwidth expenses ramp up. On top of this, if users end up depending on your service for one reason or another, you're obligated to keep the service running and available if you want to retain users and keep them happy.
00:07:03.130 - 00:08:23.280, Speaker B: Not everyone can afford these costs, and they can be quite daunting, especially if you want to create if you just want to create a fun project, technology or even a blog, a lot of companies, as I mentioned previously, turn to advertising to supplement these costs, too, resulting in much of humanity effectively selling their private information in exchange for free services online. Speaking of content creation, content creators are often completely locked to a few platforms like YouTube, Instagram, and other similar platforms. Those can be subject to a whole host of rules and regulations as well, far more restrictive than what your local government might allow. And you have no say in when or how these rules change. If you want to host any content at all on one of these central entities, you must of course ensure you're abiding by their rules and regulations. This can be quite unfortunate when popular content gets lost to rules changing or updating, as is quite common with video and graphic media today. As a content creator, sometimes it can feel like an impossibly daunting task to solve this problem, especially when people become so dependent on algorithms pushing their content to users.
00:08:23.280 - 00:09:10.124, Speaker B: Sure, one could set up their own services, as I mentioned previously, however, not everyone knows how to do that effectively, and they're not prepared for the time and costs associated with that. From social media to blogs to forums, anything in between, anything on the Web at all. These are all the fundamental issues with the client server model. To summarize, the drawbacks seem to be one, only a few companies decide the rules for what's allowed on the Web and control the infrastructure surrounding the Web itself. Two, the source of truth comes from only the entity you're interacting with. So if that entity or server is ever compromised, there's little, if anything, you can do about it. Three, data is addressed by location.
00:09:10.124 - 00:09:54.670, Speaker B: So for any reason at all, if your data is no longer available in the same location, it's effectively lost. Four, an outage of any of these mentioned servers can not only remove your content from the web, but can also completely prevent you from effectively publishing new content at all. Five, scaling with this model can be very expensive. And six, it's not you or your government that decides what content is allowed to exist, but instead a handful of companies. I think we can do better. Breaking free from the client server model means rethinking how the Web works as we know it today. This is the stage of the presentation where we get to start talking about the fun stuff.
00:09:54.670 - 00:10:37.308, Speaker B: The client server model represents Web Two as we know it today. Love it or hate it, I believe there's a clear and strong argument for how the Web Two model, referring to the client server model can be improved. What do we call this new model? This new model is referring to the Distributed Web. So let's talk about that and how IPFS fits in. But first, for the uninitiated, let's briefly talk what the Distributed Web is and what it means to you. Whether you're a user or developer, you can benefit from the Distributed Web. What I think of as Web Three, the client server model is a centralized model.
00:10:37.308 - 00:11:18.600, Speaker B: This is why we're always talking about central servers in single points of failure. Pictured on the left is this model. You can see a central point in orange. This is the server, and then the spokes are the clients, who are often the users. This graphic should make it easy to see how if that central point is ever removed, then the users can no longer communicate with one another. Before we jump into the Distributed model, though, let's talk about the Decentralized, also known as the Federated model, with another visual. So the Decentralized model is a huge improvement to the centralized model.
00:11:18.600 - 00:12:18.900, Speaker B: It still operates in a client server sort of way, where you end up with hubs that serve users. And if a hub goes out, only a piece of the network is lost. While this is a matrix, when those services have an outage, it only affects a piece of the communities they represent, instead of the whole social network or chat service. While these are awesome, let's see what the Distributed model looks like. It as you can see on the left and on the right. In the Distributed Web, each user is also providing a piece of the network itself. If a user goes offline, the network functions as normal.
00:12:18.900 - 00:13:05.510, Speaker B: If a major node goes down, the network can still function as well by leveraging the local peers. It is the champion of resilience. And the model, I believe will take us forward. Some nodes might be bigger than others, but no single outage can take the entire network down. Let's take a look at how IPFS fits into all of this and also dive deeper into the distributed model itself. We talked a lot about location based addressing, so let's talk about one of the fundamental building blocks to an alternative called Content addressing. IPFS creates mathematically generated fingerprints for data called Content Identifiers, or CIDs for short.
00:13:05.510 - 00:14:07.544, Speaker B: This step relates to something called IPLD or interplanetary linked Data, and is fundamental to how IPFS works to give us content addressing, breaking us free from location based addressing. Pictured on the screen right now is the anatomy of a version one CID represented in binary. Let's break it down real quick. On the far left of the graphic not pictured here would be the multibase prefix. This is actually omitted here because when working with binary, there actually isn't a multibase prefix, so you can save that byte. What the multibase prefix does though, is it allows us to know what base encoding was used to create the CID, at least to represent the CID might be a better way to put it, as IPLD supports many codecs. Next we have the version Identifier, which is simply whatever version number we're working with.
00:14:07.544 - 00:14:58.830, Speaker B: In this case, we're working with a version one CID So, which is a variable sized integer. And the list of supporting encodings are available in the GitHub multiformats repository. And I have a link on there on the right side. It got a little broken. Oh no. Am I freezing? Ah, I'm sorry to hear that's happening. I'll make sure to toss my router out the window after this and get a new one.
00:14:58.830 - 00:15:50.038, Speaker B: Okay, good. I'll continue and hope it gets better. Next up is the multi hash, which includes three things a multi hash algorithm, a multi hash length, and then finally the hash digest itself. You can see here we're working with a Sha Two hash of 32 bytes in length. Then the hash itself trails off the screen. The multi hash specification is also available in the multiformats GitHub repository. I recommend checking it out if you're interested in all the options you have for generating a CID as we went over.
00:15:50.038 - 00:16:58.590, Speaker B: If you're requesting data by its CID, you can also verify it's the correct data, as you have the hash of the data you're expecting baked right into the CID. So you don't have to trust who's sending it to you, as you can simply run the hash function. In the case of the example that Sha Two on the data and verify it yourself. IPFS uses CIDs by either looking up the CID in the distributed hash table, also known as the DHT, or by using bitswap and asking their local peers, do you have the CID? With this? It no longer matters where the data lives, as we know exactly what we want, so it doesn't matter who has it, just that someone has it. We have now broken free from location based addressing. Does multiple files that are the same get cataloged differently? No, actually, if you added a file on your node, let's say using the default settings, and then somebody else across the world adds the exact same file on their node with the default settings, you'll generate the exact same CID. So they don't get cataloged differently.
00:16:58.590 - 00:17:54.612, Speaker B: No problem. So now if you want to share a web page, an image, a video, or an article, you know that if you send that CID to your friend, they can download the exact same version of the data you also saw. As long as at least your node or someone else has a copy, you can share it. It wouldn't matter if the initial hoster removed the data, only that you or someone else has a copy of the data through this data can live as long as there's someone who wants it to through filecoin. You can even pay storage providers to guarantee your data will live for some time, maybe even forever. But that dive is for another talk. In the meantime, if you're looking for a service to persist your data, I recommend checking out web Three Storage, NFT Storage, and Lighthouse Storage.
00:17:54.612 - 00:18:41.000, Speaker B: They're all storage helpers, eager to get your data available over IPFS and backed up the filecoin. Does it get duplicated? We'll still refer to file one, actually. Yeah, that's a great question. So if you have a CID and let's say I think a good example is if you have a file and then two folders, one folder has that file in it, and then you wanted to add the same file to folder two, it won't duplicate the data, it'll just add a reference to the exact same CID. So this makes IPFS? Naturally. It has deduplication as a result, because there is no instance where you can have the same data twice, because you can just refer to the CID and get the same data. So you're really just dealing with handfuls of CIDs.
00:18:41.000 - 00:19:49.026, Speaker B: Yeah, no problem. Oh, I like this question too. Could I PNS be a way to version IPFS files? How would that work? And if so, does it make sense to use that if you want to be able to update an NFT? Or is there something more clean about using the IPFS address? Actually, it sounds like you've already answered your own question. IPNs would be absolutely perfect for that, because with IPFS, you won't be able to update the CID whatsoever. But if you have IPNs, what you're doing is you're publishing and saying like, I have this IPNs key and I want this address to now point to this new CID. So that's really what will give you that mutability that you're looking for. You can create an IPNs key by installing something like Kubo, previously known as Goipfs, and then you run IPFS key gen and then the final parameter is the name of the key that you want.
00:19:49.026 - 00:20:39.794, Speaker B: And then that's how you'll generate a new IPNs key. So now that we've broken free of location based addressing and moved on to content addressing, what does that mean for us? Well, it means that now transparently, the network can find new routes around problems. Problems, in this case, can be outages. We know that if our node goes down for a period of time that we're in the clear if we've gotten our data onto some other nodes, or even if a user running an IPFS node happens to have a copy of our data. With IPFS, your node will cache new data until it's garbage collected. This AIDS in automatically strengthening the resilience of a CID. This can work with entire Internet outages as well.
00:20:39.794 - 00:21:23.986, Speaker B: In certain countries, as long as some node on the local network has a copy of the data, the data can still be served. This shows that in a peer to peer network, no single node outage or event can cause the service to go down. Can multiple people version an IPNs? That might be something to play with. I know if you have two nodes with the same IPNs key, each different node can update it. But I'm not entirely sure all the nuance that comes into play when that happens. But yeah, I would like to see some more delegating kind of stuff to do with IPNs in the future. We're just not quite at totally specking that out yet.
00:21:23.986 - 00:22:02.560, Speaker B: I don't think so. This knowledge is important when you're designing your application. Sure, you can leverage a gateway, and if you're tight on time in a hackathon, that makes perfect sense. However, if you really want to create a resilient Web Three peer to peer application, then it's important to think about how to achieve resilience. And to do that, you must be utilizing the PeerToPeer nature of IPFS. When you rely on a gateway, especially just a single one, then if that gateway slows or goes down, your entire application will go with it. Where if you utilize IPFS directly leveraging its peer to peer capabilities, you're on track to creating a virtually unstoppable service.
00:22:02.560 - 00:22:50.574, Speaker B: So we touched on how IPFS users help reserve data they're interested in to nearby peers. This happens when a user requests data from the network via CID, their node caches it then makes it available to the rest of the network. This AIDS in more than just resilience. You can see it as the network pitching in to help you scale efficiently too. Think about the client server problem of when you create a popular app and your traffic explodes. That situation is effectively flipped on its head with IPFS, resulting in what we call negative bandwidth scaling costs. If a bunch of nodes are attempting to download your CID, they'll be automatically resharing that CID as well.
00:22:50.574 - 00:23:47.534, Speaker B: So while your traffic explodes from people sharing your CID to each other, those very same people are helping send the data itself to their friends or colleagues automatically. So effectively, the more popular your CID is, the easier it is for people to retrieve the data from some other node that might be more local to them. This property is very important to the interplanetary aspect of IPFS. If someone on Mars had a piece of data that they originally retrieved from Earth, then another Mars user shouldn't have to wait to retrieve the same data from Earth. The network should automatically figure out that there's another node on Mars willing to serve that data. Content addressing and the distributed web helps us unlock such a future on the network level, transparently. If any of this presentation has sparked your imagination or gotten any gears turning, then please come join our ecosystem to learn and build.
00:23:47.534 - 00:24:32.926, Speaker B: We have an awesome community comprised of builders who help us foster a positive and productive environment. I highly encourage you to check out a resource we compiled of several tutorials currently available@tinyurl.com. Learn IPFS filecoin. I'm also available in these communities, particularly on our Discord, but also our Forums and Stack Overflow. You'll find myself, other devrels, and tons of knowledgeable people inside these communities. So please don't hesitate to join in and say hi. All right, so when you say local, do you mean geographical? Physical proximity? Is geographical location the primary determination of the proximity of nodes? Sorry, right now I don't think there's any geolocation done whatsoever, and local really depends on the context that I set it.
00:24:32.926 - 00:26:00.300, Speaker B: So when I'm talking about network outages, I'm talking about countries where they might cut, or sometimes there'll be an issue connecting to the greater Internet. So in those instances, local nodes can still communicate with each other just because they have the literal ability to address and find each other where otherwise, if we're talking about publishing something to the DHT, local is determined by key space, not geographic. And then when we're talking about the instance of Mars to Earth, I guess that would be sort of geographical. But it's mostly based on the fact that a Mars node will begin initially serving blocks, theoretically nearly immediately, where communicating with Earth would take so much longer that you would know which one's closer just based on the time to first block. So how does using IPNs to store dynamic data different than using ceramic? Are there trade offs? So I'm not super familiar with using ceramic specifically directly, but I believe ceramic does leverage IPFS, so that would make me think it does leverage IPNs as well. So I think that's like a case by case thing. If you're using IPNs, you're using probably more bare bones, low level type stuff where ceramic is another layer built on top of IPFS to try to help you out, give you extra APIs and SDKs to make things easier for you.
00:26:00.300 - 00:26:20.094, Speaker B: So I guess, yeah, the trade offs are totally, totally up. To you. One might be better than another, but it all depends on what you're trying to build and who you are. No problem. So I hope to see you all around. I'm also available at many hackathons, so please ping me or send me a DM. Even if you're not in a hackathon, I'd love to hear from you.
00:26:20.094 - 00:26:33.670, Speaker B: I'm Discording on related discord servers and the fallacoin slack. So aside from answering any unanswered questions, that's it from me, and I sincerely hope I helped inspire or teach you something, and I hope you all have a lovely day. Happy hacking.
00:26:34.890 - 00:27:01.040, Speaker A: Awesome. That's? It for Discordion's recording of his IPFS filecoin workshop. If you have any questions, dawn from IPFS Filecoin is here to answer. You're more than welcome to come off mute and ask your question live or continue asking your questions in the Zoom Chat. But we do have another, like two, three minutes if you do have any more questions.
00:27:03.090 - 00:27:40.570, Speaker C: Hey, just looking at the Zoom chat, there were questions about making the slides available. I put a message out to Discordion to find out if they are available somewhere that those can be shared. So I'll follow up with you on that for folks that are looking for those. And then we did have a couple questions in the chat they were asking about. Let me scroll back up here. Let's see. So the question that I got was, centralized systems also have underlying distributed systems to make sure that no single node failure will take the service down.
00:27:40.570 - 00:28:18.390, Speaker C: What makes a distributed system better in this case, or is it just more resilient? And yeah, I do agree with that, that it does become more resilient just by nature of being distributed. But we did also talk about how you also want to be thinking about censorship, resistance, and not having vendor lock in as other reasons. And the one more thing I think worth mentioning is you can use your CID or content address and store that on any provider that you want to use, whether that's going on to Filecoin, Rweave, AWS, Google, wherever you like to store your data. It's platform agnostic.
00:28:21.870 - 00:28:56.006, Speaker D: Yeah, I've got a question. Thanks, Don, for that answer. It sounds like with IPNs they were saying that you could basically get a new CID. For example, you needed to update the content that the previous CID pointed to. What does it look like too, in the event that let's say that's a typo in a piece of text, for example, that previous CID, is it just there and it's no longer pointed to directly from that interface, or is it not? Is there a way to remove that?
00:28:56.188 - 00:29:13.718, Speaker C: Sure. Thank you. So when you're using IPNs, you're more working with creating a pointer that then points to CIDs so your original CID still exists. You would just have to re point your IPNs back at it if you wanted to change where it was going or what it was displaying.
00:29:13.814 - 00:29:14.186, Speaker B: Got it.
00:29:14.208 - 00:29:20.080, Speaker D: So basically, it's like in the web, two terms. The URLs there is just not the hot link from the web page.
00:29:21.410 - 00:29:33.346, Speaker C: Yeah, it's in there. Yeah, IPNs, you're just creating a pointer so your old CID still lives. You just have to go point at it again and then you'd be back at square one, if that makes sense.
00:29:33.368 - 00:29:34.782, Speaker B: Thanks a lot. Yeah, totally.
00:29:34.846 - 00:29:36.034, Speaker D: I appreciate it.
00:29:36.232 - 00:29:36.980, Speaker C: Thanks.
00:29:42.610 - 00:30:22.490, Speaker A: Well, I guess that's all the questions we have for now. Well, just like Deskorian said in the recording, the IPFS Filecoin team is available through almost all forms of platforms, but specifically on ETHGlobal Discord, they are available in the IPFS Filecoin Discord channel for you to reach out to. Team is super available and willing to help with your hack, so definitely reach out if you have any questions. Don, thank you so much for taking the time to answer questions for today's workshop and otherwise. Thank you, everyone else, for tuning in today, and I hope everyone has a great rest of your day. Bye.
