00:00:00.250 - 00:00:35.234, Speaker A: GKP accelerations and today I will share some recent progress of our development, our recent findings and our legs per step in the future especially, I want to share with you about something that you can expect in the next year like what kind of hardware you can see on the market. So some background about GK. Smart, honest I think we can skip this part because today everyone is so professional.
00:00:35.234 - 00:00:57.242, Speaker A: So in short, it's a short proof about the sound standard is true. Usually it's about the sound computation is correct and the proof itself should be short and it should be fast to be verified. And GK smarter is like something that you can hide the information behind the computation, and it has severalized properties to make it useful.
00:00:57.242 - 00:01:11.170, Speaker A: For example, it can have some concept of verified computation. You can ensure the desired computation is faithfully evaluated. You can hide the knowledge about a specific piece of information.
00:01:11.170 - 00:01:35.238, Speaker A: Sometimes it is used for, for example, Zcash and the size and the proof of time should be short. And one example of the application is like ZK row app. And the basic idea is like you have a bunch of huge amount of computation which is already happening on the layer one, it's about to verify some computation is correctly evaluated.
00:01:35.238 - 00:01:46.474, Speaker A: And by moving it out of the first layer, you can reduce the competition on layer one. The problem is, like, how can you trust? The result is correct. You have multiple method.
00:01:46.474 - 00:02:04.530, Speaker A: But the ZK rob basically says, like using some cryptography method, you can spend a little computation to ensure that the whole bunch of computation is correctly evaluated. And that's why there are so many. Projects using ZKP.
00:02:04.530 - 00:02:29.414, Speaker A: Some of them are L Two project and some of them are L one. And in general, the system behind Slack consists of three stages. The first stage is you write your applications using some DLCL and then it was transformed to some kind of slack friendly format.
00:02:29.414 - 00:02:46.420, Speaker A: And finally, the encoding, such as R One CS format is sent to slack back end approver, and in the last step you have a huge amount of computation. That computation is so expensive. Of course, it's already much better than 20 years ago.
00:02:46.420 - 00:02:53.394, Speaker A: Maybe ten years ago. But it's still so expensive. That restrict its application.
00:02:53.394 - 00:03:45.490, Speaker A: At least restrict the size of the circuit. You can write and that example is you can see the ideal have at least like 300,000 RTX 30 80 equivalent GPU running on their test late three. How about the future? You have more ZK related project and each project are using larger and larger circuit you need a huge amount of computation power to support the approvers and then we ask how about the specialized hardware? It can immediately bring you several advantage for example, you can have short proof time or equivalently with the same time budget you can work with much larger circuit and you can spend less money compared to GPU like performance per dollar and you may have less electricity bills.
00:03:45.490 - 00:04:12.670, Speaker A: So if we will look at the zig hardware acceleration, what kind of problems we have to look at? So today I will discuss three points. The first is like which part of the system, which part of the computation should we accelerate? And then the second is recent finding about the denomination return after we accelerate the MSM and entity part. And the last part is how can we handle the diverse kernels in the long tail distribution.
00:04:12.670 - 00:04:29.838, Speaker A: So the first part, let's look at the general GKP proof process. This summarization is pretty bad, but sorry for that. So in general, you first write out your witness and then you commit the witness.
00:04:29.838 - 00:05:10.410, Speaker A: When you commit a witness, you see it as a polynomial and then you perform some kind of polynomial commitment and finally you prove that the witness is correct. And if you look at the strip the mathematical way and if you look at the computation only you will find like they involve like large field arithmetic, for example 256 bit additions multiplications, I mean modular addition and modular multiplications. And then when you combine your witness, you have some kind of multiscal multiplications and numerical theoretical transform.
00:05:10.410 - 00:05:28.786, Speaker A: So we start with MSM and NTT. The reason is like first those two are the most time consuming part before acceleration, it takes like 80% of your execution time in some cases. And the second thing is they are loosely coupled with the CPU thread.
00:05:28.786 - 00:05:48.300, Speaker A: So that means if you offload them into a specialized hardware, it will be pretty easy because the overload overhead is small. Just imagine if you want to use accelerator for every single modular multiplication, then the system overhead will be unacceptable. So that's why we started with the first two.
00:05:48.300 - 00:06:11.940, Speaker A: And in the past year we developed several systems for these two actress, three kernels for the MSM. The implementation is kind of similar to the software implementation which is the bucket method. You divide the scalar part into multiple short bit lumber and then you use a bucket to accumulate them.
00:06:11.940 - 00:06:34.470, Speaker A: And the hardware design is consist of multiple independent process elements. Each of them have a standard long AAU which can perform four EC point and double in a pipeline fashion. So each cycle it can finish one that operations and you have all the control logic.
00:06:34.470 - 00:07:34.190, Speaker A: The tricky part in this design is because it is a pipeline design, you have a huge number of easy group add and operations on the fly and they might be added to the same position of the table and you need to carefully handle those kind of potential like right conflict problem. And on top of the PE you have multiple PE located in one FPG board and multiple FPG die are located in one PCB board and that forms our final system. And we reduce the MSM on BN 254 curve the execution time of like 1 billion point evaluation it was reduced to below 200 microseconds.
00:07:34.190 - 00:07:54.130, Speaker A: If you have a sense of what the expected time if you run this on CPU you will know how much difference it is. And we also have some kind of entity implementation. The general implementation follows kind of four parts algorithm.
00:07:54.130 - 00:08:11.690, Speaker A: I would assume this is a common way to do entity. You organize it in a matrix and then you do entity row by row and then column by column. This is also how we do it on hardware.
00:08:11.690 - 00:08:41.874, Speaker A: Some new problem emerge when we do it on hardware. The first is like the memory access pattern is pretty bad especially if you have a lot of data on CPU side. Sometimes we see some older design hardware design that require the CPU to do some kind of tricky data reorganization to permute the data in the entity array and because that part was happening on CPU that makes the CPU overhead pretty big.
00:08:41.874 - 00:09:12.250, Speaker A: And we have customized the intermediate data layout to make every memory access, no matter on CPU on our accelerator card to be much more hardware friendly. The second part is like you really have to store a huge tweeter factor array which is near the size of the entity vector itself. On CPU it's good because CPU have a huge memory but on your FPJ the memory is pretty restricted.
00:09:12.250 - 00:09:35.960, Speaker A: So you may come up with some method that you compute on the fly but the computer on the fly you have to double the computation. That's also bad. So we use some kind of a mixed storing and compute strategy to not only reduce a memory photo parameter but also minimize which means like the compute on the fly overhead is near zero.
00:09:35.960 - 00:09:53.340, Speaker A: So that is some kind of good thing. And with those designs we have massively connected FPD system. Well, the hardware design is kind of boring here but in general we have performance like this.
00:09:53.340 - 00:10:27.846, Speaker A: Okay? So if you have like a 1 billion point entity you can finish it in about 200 microseconds. So the speed up comparing to CPU over here is smaller than the MSM because entity itself the computation is less than the CPU so the transmission time becomes a more important problem. And finally we have like kind of positive mercury in which case the keyboard is a keyboard problem.
00:10:27.846 - 00:10:40.278, Speaker A: Yeah, so like 1 billion import. I think the design is about 64 beta godinox, if my memory is correct. So it can finish in 43 seconds.
00:10:40.278 - 00:10:57.070, Speaker A: So this one is basically PCIe bounded. All the performance that we show here is not a single FPGA die. It can actually come from our FPGA server so it can have multiple accelerator card.
00:10:57.070 - 00:11:30.954, Speaker A: Each card have several process elements on it and this is how it looks like huge machine with customized high bandwidth interconnect, customized PCB design and customized power delivery and water cooling. And the problem is this is too big too heavy. So that's why we are developing Asylum design on top of our implementation of the FPJ design.
00:11:30.954 - 00:11:54.782, Speaker A: Because FPGA itself on hardware it's not so efficient because to make the hardware reprogrammable you lost so much thing. For example, your frequency is restricted to 300 megabytes, that kind of stuff. So what we are going to do in the next step is to squeeze all the performance, all the computation power in that machine into a single accelerator die.
00:11:54.782 - 00:12:28.570, Speaker A: And that's our project. But this is not the end of the story because we found something bad after we accelerated the MSM and entity part. For example, if you accelerate your circuit itself is kind of for positive heart circuit and if you look at the performance before and after acceleration, you will see that like before acceleration it takes 800 seconds and then after accelerations you have five times speed up.
00:12:28.570 - 00:12:42.042, Speaker A: But after acceleration, MSM entity itself added together contributes only 4% of execution time. To make the more clear, you can see this figure. So the orange part is the MSM entity.
00:12:42.042 - 00:13:01.910, Speaker A: So before you accelerate it, it's the dominating part, but after that it's only a fraction. Which means if you continue to build record breaking MSM and Entity implementation, that's just a game. It's no thing useful for end to end performance.
00:13:01.910 - 00:13:30.830, Speaker A: And this is not only a special case or corner case like you can also see it on other things. For example EVM circuit which is widely used in Neo two project and you also see this. So MSM entity 3% GPT-2 like zkgpt two funny and the result leak looks better because MSM entity still contributed to 20%.
00:13:30.830 - 00:13:51.414, Speaker A: But you can imagine in general this one is already the one because GPT is a kind of matrix vector modification. So the stuff is like the proof generation, the vitamix generation part is kind of easier comparing to you do the commitment. So that's why it is very high.
00:13:51.414 - 00:14:09.722, Speaker A: For example 20%. But you can imagine in normal case, in normal application scenario of CKP, you would have that portion smaller than 20%. So what you have to do is like you need to reduce long MSM and entity part, you need to do that part.
00:14:09.722 - 00:14:21.920, Speaker A: But that part is difficult because they are coupled. They are not loosely coupled with the Men's thread, they are tightly coupled with the Men thread. So you needed to come up with better hardware architecture to do that.
00:14:21.920 - 00:14:43.554, Speaker A: And then in order to do that along MSM entity part, you have to handle the diverse kernels in the long tail distribution. And if you look at the evolution of physical algorithms, you can find that they are different in different aspects. But if you look at the lowest level of operations, you can say commitment.
00:14:43.554 - 00:14:59.962, Speaker A: They usually use these four and they are used together with some kind of catalytic composed like modular multiplication stuff. So for these four it's fixed. So you can write hard code hardware to accelerate it.
00:14:59.962 - 00:15:31.266, Speaker A: For those outside of these four, you need to kind of software support and currently our plan is to use Isa for this part. So if you skip the yellow part it is pretty common comparing to common like CPU or GPU Isa. Well, the only difference is you have late support to modular operations, for example modular multiplication and modular additions.
00:15:31.266 - 00:16:14.462, Speaker A: But on top of that you have kind of hard coded units. For example EC group add and addition you have programmable register buffer and these two together make a system strong enough but also flexible enough. So comparing to the traditional CPU or CPU ISO, what is the main thing that it can do but traditional CPU cannot do? That is you can easily offload very fine grained operation to your hardware, especially hardware accelerated hard coded hardware unit on CPU if you want to offload some operations from your software, you have to go through a very complicated system stack.
00:16:14.462 - 00:16:28.914, Speaker A: You have to have a system card. You go to the driver. The driver sends a command through the PCIe and the PCIe goes find the send the data to your accelerator card, wait for the result to be ready, and then interrupt.
00:16:28.914 - 00:16:43.378, Speaker A: And your system takes the data back. And that force your offloading to be restricted to those huge operations that each of them takes. For example, 1GB or at least several tens of megabytes operations.
00:16:43.378 - 00:17:09.958, Speaker A: But this one can be much more final granted and using this way you can handle both those fixed kernels but also flexible kernels. And then we hope that this system can support all this kind of mainstream zero audit proof systems. And with that I think design we are going to build up each of those ASIC die available.
00:17:09.958 - 00:17:45.378, Speaker A: We are going to build a cluster on top of it and we plan to leave the possibility to let it connect with FPG card. So if we are lucky enough we can do everything with our ESC design then we just connect our escape die together to form a unified solution. Otherwise if you are so unlucky that for example a new protocol comes out with some kind of a tricky operations that we have never seen before and nobody expected that, how can we do that? We still have the possibility connected to the FFPJ card.
00:17:45.378 - 00:18:17.382, Speaker A: So even if we have some corner case operations that we cannot cover, we can do it on FPJ and we expect the card to be ready in next year. And finally there are just two QR code. So on the left one I want to remind that tomorrow we have kind of a three prize and actually the track one is like we actually the competition is about end to end performance acceleration.
00:18:17.382 - 00:18:21.040, Speaker A: So welcome to join that part and thank you.
00:18:24.130 - 00:18:26.080, Speaker B: I guess we're open for questions.
00:18:29.570 - 00:18:30.560, Speaker A: Too hard?
00:18:31.570 - 00:18:51.298, Speaker B: It's a bit hard. I probably have one question. So you mentioned that we can achieve some kind of linear scaling by putting several machines on the side and then getting them together into a cluster I was wondering if there are any limits beyond which there will be diminishing returns composing machines?
00:18:51.394 - 00:19:04.154, Speaker A: Yeah. Composing Machine every time you increase the scale of your system, there are two parts that might limit your speed up. The first is the interconnect might be a problem.
00:19:04.154 - 00:19:16.410, Speaker A: For example, if you are two components connected on the die. The bandwidth is huge on the same board. It's still good if it's connected via the PCIe on the same like a motherboard.
00:19:16.410 - 00:19:31.842, Speaker A: The connection is restrict to the PCI bandwidth, which is pretty low I am sure. Like in the PCIe four or five weeks, you have like 30 to 60gb/second. And compared to your computation power, this number is pretty small.
00:19:31.842 - 00:19:49.398, Speaker A: And if you have two machine or two machine and you have to connect it, using Internet, then yeah. God, that transmission time might be even longer than the computation time on a single machine. So that will be the huge problem.
00:19:49.398 - 00:20:20.460, Speaker A: The second problem is, if you scale, you still have to handle those. Sequential part and with the speed up of measurement bigger big because screenshot part is drops becoming huge and sometimes we cannot expect every part of the computation to be paralyzed they will always there so you have to think of some other way to speed up it other than just duplicate your machine. Yeah beautiful.
