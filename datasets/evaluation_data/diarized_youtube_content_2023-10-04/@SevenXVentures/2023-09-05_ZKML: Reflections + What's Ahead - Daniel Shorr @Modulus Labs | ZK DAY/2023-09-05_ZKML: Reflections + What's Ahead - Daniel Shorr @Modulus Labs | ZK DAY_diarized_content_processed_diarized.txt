00:00:00.170 - 00:00:08.426, Speaker A: It. Let me know if this is affecting for you at all. Is it's faithful AI without the faith? Right? So we use cryptography.
00:00:08.426 - 00:00:26.040, Speaker A: So in this case, snarks zero knowledge proofs to prove that the inference was done correctly, without tampering, without any manipulation. But that the consequence of this is faithful AI, right? AI. That doesn't cheat on you, basically, which is something I worry about all the time because I'm a deeply insecure and paranoid person.
00:00:26.040 - 00:00:43.242, Speaker A: So I want all my AI compute to be accounted for. The problem, of course, and really the background of the whole team. We were just like AI researchers at Stanford, no exposure through cryptography, wrapping up our undergrad, getting ready for grad school when COVID happened.
00:00:43.242 - 00:00:58.218, Speaker A: And so we thought, hey, before grad school and we commit the next decade of our lives to academia, maybe we should see what all our smart friends are doing. And this is like 21, 22. So crypto was super hot, so we ended up reading a ton of papers.
00:00:58.218 - 00:01:27.122, Speaker A: And it just became so clear to us, just at least the math nerds within us, because certainly we couldn't discern signal from noise by looking at Twitter, but that these ZK papers were really solid and getting much better, right? So we read Ellie's paper, of course. I mean, we went all the way from Pinocchio in 2008 to plank and everything that's happened since. So it became clear to us that ZK tech is getting much better because of this incidental application of the blockchain that we call succinctness.
00:01:27.122 - 00:01:47.150, Speaker A: But nonetheless, that potentially that we can actually apply this onto AI, right? And this is a super obvious thought to us at that time because at this point, as a recap, we only knew AI and the bit of crypto we have read. So it's not like we had done this comprehensive overview of the crypto space. It's just this is a two discipline that we were interested in.
00:01:47.150 - 00:02:07.460, Speaker A: So what we ended up doing was just a science experiment, right? We used a starcore stack because at that point it was the only thing that was putting out proofs. And we built what became the first Zkml project. It's a project called Rockybot 70,000 Param Neural Network, trying to guess if ETH prices would go up and down, or down, I should say.
00:02:07.460 - 00:02:23.138, Speaker A: Trading with the Unisoft contract on l One. It was cool because it actually put out proofs and put that on chain and it was tamper resistant and fulfilled the promise of ZK and all that. It was so much of that that we didn't put a call function in the contract.
00:02:23.138 - 00:02:46.000, Speaker A: So this bot just traded, right? We put in $500 and there was no way that we could recover our money because it's going to lose all that money. What we didn't expect is that lots of people just started donating money to the bot to quote unquote, keep Rocky alive. We called it the Rockefeller bot, right? And so Rocky bot ended up trading for over a month thousands of dollars in and out.
00:02:46.000 - 00:03:04.806, Speaker A: And we were like, wow, this is bear market now, right? Like dead bear market. Like okay, this is kind of the seed of an interesting potential, right? If we can bring actually performant models on chain using the same kind of thought process, maybe we can do something really interesting. So that was about almost exactly a year ago.
00:03:04.806 - 00:03:19.050, Speaker A: We put out Rockybot just before SBC last year. And ever since then, it's been a journey of trying to figure out what do we need to do to bring really, really performant on chain at high volumes. Like, really expand the kind of design space for smart contracts.
00:03:19.050 - 00:03:34.130, Speaker A: And decidedly. And this is very, very important, the distinction between our way of thinking about this versus the many other very valid way of thinking about this. We are especially focused on AI compute and the reason this is relevant will show up in just a moment.
00:03:34.130 - 00:03:47.574, Speaker A: So we did Rockybot, we thought, okay, that was pretty cool. Let's get deep into the cryptography. Let's go ahead and test very basic AI primitives in every single popular proving system that exists today.
00:03:47.574 - 00:03:56.310, Speaker A: We're talking croft 16 old stuff. I know. Halo two, Plonky two Gemini, Zkcnn.
00:03:56.310 - 00:04:10.346, Speaker A: The Aztec Suite. I don't know if Noir was out back then, but I think we ended up proving across six different proving systems in different schemas. And the TLDR is performance varies a lot of course, but the bottleneck is totally different.
00:04:10.346 - 00:04:37.154, Speaker A: Some proving systems consume a ton of memory, some proving systems don't consume so much memory, but it's very slow. And maybe most farcical of all, there seemed to be no obvious pattern to why one proof system would be really performant when another wouldn't be. And looking back, the reason is actually pretty obvious, at least it's become since obvious, which is that these proving systems were not built to prove AI computations.
00:04:37.154 - 00:05:06.974, Speaker A: If you think about AI computations, it's basically linear transformations in a flavor of matrix multiplication at massive scale with some nonlinearities. This is a very specific subset of mathematical operations within the pantheon of all mathematical operations, which presumably a Zke EVM would have to emulate. And so our thought was that, okay, if an AI model from a compute standpoint is this highly structured and highly repetitive function which is basically map mole a gazillion times.
00:05:06.974 - 00:05:39.554, Speaker A: Even if we build a proving system that was only slightly better at just map mole, conceptually at least it should be significantly more efficient over the scale of an entire AI model, especially big AI models, which is what we cared about because they get better when they get bigger generally. So that was the takeaway from the paper with EF and we thought, okay, we're going to keep pushing this along, we're going to build another proof of concept. In this case we're going to use Halo two because Halo Two had great tooling and it was like super customizable.
00:05:39.554 - 00:06:54.546, Speaker A: We can get down into the muck with the gates and arrange things until it's really efficient, take advantage of cool tools like rotations and things like this, and we put out our second proof of concept called Leela Versus the World. Now, I think the game is live right now, but it's a game on one side, it's human players, all of us, voting on chess moves, right? One global game of chess, voting on chess moves, putting money on different chess moves, and then betting on the outcome of the game on the other side, once all the moves are tallied, once an hour, and the top move gets actually played. On the other side is a proven version of Leela Chess Zero, which is, I believe, the most performant besides Stockfish, on occasion, AI ChessBot, right? Because Stockfish isn't really AI anyways, as much smaller version of it, and you could effectively have confidence that Leela, that Leela you're playing against, or betting against, or potentially betting on, is always the same model and Modulus, with our many connections can never call up Magnus and say, yo, Magnus, how would you play this game? We put a lot of money on, you know, we don't know Magnus, unfortunately, but the game did well, as in, like, it ended up making a little more money than we spent supporting it, which is always a good sign.
00:06:54.546 - 00:07:18.546, Speaker A: But there were big caveats. The first big caveat is, despite the significant amount of time we spent customizing the Halo Two circuits, it took almost an hour to prove, right? And this is a model of 4 million parameters. Objectively, not too bad, but by AI standards tiny, right? This is like really bad when you're talking about billions of params for the language models.
00:07:18.546 - 00:07:37.994, Speaker A: Not that we have to necessarily prove language models, but you're missing out on a lot, right? You're already like renting, in our case, renting a $5,000 per month machine from AWS, eating up a ton of memory. We needed 128 gigs of Ram on the AWS cluster that we were running and just supporting. That required that the game be relatively successful.
00:07:37.994 - 00:08:00.120, Speaker A: Right? And so this was the first big ding from our perspective on the kind of initial orientation, which is, let's just build devtooling and make it super easy to build circuits because you still bump into basically Halo Two, right, this proving system. So that was frustrating. The other thing, of course, is in the actual gameplay of the game, you submit your move and you have to sit there and wait for Leela to respond, and you have to sit a long time.
00:08:00.120 - 00:08:24.958, Speaker A: And people were obviously very frustrated with that. And so there's ways to design around these kind of gameplay mechanics, but it just felt really limiting and it was so punitive, right? Because, again, the game had to be somewhat successful for it to make sense at all in the first place. So that wrapped up our time with Leela and we actually ended up doing one more project in Halo Two which is coming out, I think next week called Zkmon, which is a generative model.
00:08:24.958 - 00:08:42.242, Speaker A: So it's a generative adversarial network that outputs pixel art. So you can think of an AI artist that signs their outputs in crypto ethics signatures, right? And feel free, there'll be a thousand of them. A lot of them are a little blobby, some of them are better than the other ones.
00:08:42.242 - 00:09:26.610, Speaker A: But nonetheless, for us, the big thing was we wanted to really work on our own proving system, right? Something that just from the Silica up was built for the function of AI models, right? Because when we talk about compute regimes that are hundreds of times larger than what you take advantage of in an AMM, it just seemed like this would be the scenario where specialized tool made sense. So we abandon nice things like dev experience, nice things like our sanity and how many papers we have to read in order to just get as much efficiency as possible. And so I guess one of the things that I can say is that Remainder, which is the name of our proving system and for the math nerds amongst UE, that would be funny.
00:09:26.610 - 00:09:45.382, Speaker A: The first version of it will come out at the end of next month and at least to my estimate, it represents about 100 times improvement in efficiency even without any hardware, just software on a single core on a CPU. Right. And we'll obviously be adding in GPU support and Asics and all the rest.
00:09:45.382 - 00:10:14.462, Speaker A: But we think that Remainder is like the first big step change, at least for AI proving, that's going to meaningfully increase the size of the design space for Zkml. And along with that release, we'll be unveiling our first couple of partnerships which will be supporting at volume Zkml proving on the order of 10,000 AI models per hour. And that is a 10,000 x improvement over where we were before with Leela, which is once an hour anyway.
00:10:14.462 - 00:10:33.186, Speaker A: So that's the story of Modulus. I'm happy to obviously unpack whatever you're interested in but I don't really have anything to sell and I don't really have an agenda and I'm big on intellectual honesty so please beat me up intellectually and let me know that I got it all wrong. But yeah, that's modulus.
00:10:33.186 - 00:10:41.606, Speaker A: That's our worldview. Thanks for having me. Questions? Curiosity.
00:10:41.606 - 00:10:43.766, Speaker A: Raise it what's?
00:10:43.798 - 00:10:48.330, Speaker B: It 10,000 times faster at doing that mole.
00:10:48.830 - 00:10:53.998, Speaker A: In this case it's literally or the first model that we're proving is a decision tree. Yes.
00:10:54.084 - 00:10:59.610, Speaker B: And then also are you doing any floating point stuff or is it all like integer?
00:10:59.690 - 00:11:17.478, Speaker A: It's all integer, which makes sense. Yes. For the ZG it's also actually a lot faster and in some sense in this case when we say faster or more efficient, what we genuinely mean like if you unpack that is less sips from the wall in terms of electricity and quantized models is just a lot cheaper to run.
00:11:17.644 - 00:11:27.794, Speaker B: Yeah. And then for activation, as far as activation functions like because I'm assuming you eventually want to get to neural list.
00:11:27.932 - 00:11:28.330, Speaker A: Yes.
00:11:28.400 - 00:11:31.610, Speaker B: So is there like a ZK friendly activation?
00:11:32.430 - 00:12:02.786, Speaker A: Well, certainly some activation functions are friendlier than other like Railu is pretty good batch norm isn't the worst because it's like that and you have to decompose that. Yes. There are a lot of interesting work in the world, I guess about specifically how do we do quantized approximations of these nonlinearities including lookups in the world of Plancks when it comes to the proving schema we use, which is GKR, there is basically no literature on this.
00:12:02.786 - 00:12:25.402, Speaker A: So we are taking a very naive and dumb approach initially but obviously lots of ideas and part of the hope with I guess now I'm selling something but part of the hope with open sourcing our code base is that folks will look at it and go wow, the Modulus guys are so dumb. They do not obviously see this one technique for lookups that would be great in this regime. If that's the case, that's awesome.
00:12:25.402 - 00:12:33.200, Speaker A: Right? Please advance those improvements. But yeah, that's kind of the TLDR great question though. Yeah.
00:12:34.850 - 00:12:38.560, Speaker C: Two questions maybe if you open source it, what license would it have?
00:12:39.010 - 00:12:55.970, Speaker A: MIT Apache, the most permissive stuff. Yeah, the question of defensibility comes up a lot. It's like hey Modulus, how are you going to defend yourself? I don't have anything too intelligent to say on this except to say that we could have stayed in AI.
00:12:55.970 - 00:13:05.746, Speaker A: We came here in large part because we like it here, we like the culture here and we think there is a way to balance the open source ethos and making money. Right. Just being very frank.
00:13:05.746 - 00:13:18.122, Speaker A: The other thing I'll say is in some sense the ZK product that we are building depends on us completing the security story. If I'm giving you a proof exactly. You need to trust that this proof is good.
00:13:18.122 - 00:13:24.378, Speaker A: Right. And not just like our word on then there's no distinction except additional compute costs than the centralized authorities. Right.
00:13:24.378 - 00:13:42.910, Speaker A: And so we think it's really, really key for us to audit our code. So Veradice is doing our first round of audits and then as well putting it out there so other people can poke holes in it and see what's going on. I think there's some economics of scale on hosting the provers in the cloud and obviously offering some volume advantage.
00:13:42.910 - 00:13:50.210, Speaker A: So hopefully we are the preferred API gateway. But that's very theoretical. The business model quite frankly is still very rough.
00:13:50.210 - 00:13:52.598, Speaker A: If you have ideas though, please let me know.
00:13:52.764 - 00:14:03.706, Speaker C: My second question would be building a new group system is really tough to get it right. Did you somehow use parts of other group systems where they are maybe fit for the ML case anyway.
00:14:03.808 - 00:14:04.266, Speaker A: Yeah.
00:14:04.368 - 00:14:06.906, Speaker C: And which one did you Halo Two stick to? Halo Two?
00:14:07.008 - 00:14:23.374, Speaker A: I'm so glad you asked. So the funny thing about everything I just said, including not really shit on Halo Two because it's a very impressive proof system that's been configured for something it wasn't designed for but still has this performance drawback is at the very end of our proof, we settle into Halo Two. Like, that's a recursive step.
00:14:23.374 - 00:14:30.174, Speaker A: And it's kind of sad because the performance is really, really good until we hit Halo Two. And it's like I was telling remind me of your name. Roy.
00:14:30.174 - 00:14:37.614, Speaker A: I was telling Roy this yesterday. It's like punching a beautiful wedding cake just right in the face. Yeah, the performance just like craters.
00:14:37.614 - 00:14:52.374, Speaker A: But we wanted to set even for V Zero, we wanted proofs to go on chain. And quite frankly, there's not a lot of good info right now around ZK proofing of actually bringing proofs on chain. Either because people aren't open sourcing their stuff or just because a lot of these proofing systems are newer.
00:14:52.374 - 00:15:06.298, Speaker A: Right, or they change like every week. So, yeah, table two right now on the final layer. But there's no reason we can't switch it out for something more efficient, right? I guess it was a practical compromise.
00:15:06.298 - 00:15:11.680, Speaker A: Yeah. Good question, though. And the rest, it's like, yeah, bits and pieces of everything.
00:15:11.680 - 00:15:23.780, Speaker A: We are so happy to copy, paste and attribute. Very importantly, right at the beginning, big bold letters. We thank the people who made these proof systems happen before us.
00:15:23.780 - 00:15:45.020, Speaker A: But you are right. Building a proof system is even with the assistance of all the giants before us and all the open source code bases out there, it is very, very difficult and soundness and things like it's not always very binary. There's like gradations, right? And so oftentimes it's like we're building this stuff and we're realizing, oh, wow, it's actually pretty easy to fake a proof here, right.
00:15:45.020 - 00:15:54.942, Speaker A: The kind of dirty side of ZK they don't talk so much about. In that case, we try to hold ourselves to a very high standard of integrity. And obviously the auditing is a big part of that.
00:15:54.942 - 00:15:57.502, Speaker A: Yeah. So don't take our word for it, please. Look at the code.
00:15:57.502 - 00:16:15.726, Speaker A: But if somebody doesn't open source your ZK stack, I'd be very suspicious because every day we're like, man, we can take so many shortcuts and it would be so much faster. But the risk Zero folks, for example, are exceptional thought leaders in this space, right? Because we march the beat of the drum. That is open source.
00:16:15.726 - 00:16:31.446, Speaker A: Right? Yeah, that's true. But also we have to build value generating businesses, right? I don't know if you all have noticed, but in crypto there's this nasty thing that, again, people don't talk so much about, which is like, open source this and that. And then I put a token in, it pops, and everyone's doing great.
00:16:31.446 - 00:16:40.298, Speaker A: And the early team is doing awesome, and the protocol is, like, self sustaining. And boom. The product never evolves beyond that because everybody's rich who started it, right? And it's like, I don't know.
00:16:40.298 - 00:16:58.660, Speaker A: I want Modulus to be a generational company, right? And So How Do We, from an organizational standpoint and from a business model Standpoint, make sure that those goals Are aligned and that we keep being at the bleeding Edge. We never rest in our laurels. We never get fat and spoiled, let's say.
00:16:58.660 - 00:17:26.866, Speaker A: Yeah, but also, I'm Young, so I get to say that for a lot of folks, it's life changing money and the Funds Behind Them As Well. I don't want to shit on that Too Much, but I just Think If we want to move Crypto forward, we have to sustainably, keep Pushing the envelope, and Ask ourselves what we can do to build Products With A Capital P, right? That's kind of what we say internally, which is like, this is not just like, one research paper, and then it's done. No Proofs.
00:17:26.866 - 00:17:32.700, Speaker A: Need to go on chain. We need to carry the weight of real dollars. And, you know, we want to complete the security story.
