00:00:00.570 - 00:00:11.920, Speaker A: Hi everyone, and welcome to this video that accompanies Section 19.5 of the book algorithms Illuminated, part Four. This is a section about a simple recipe for proving that problems are NP hard.
00:00:11.920 - 00:00:36.646, Speaker A: So how can you recognize if one of these NP hard problems shows up in one of your own projects and you definitely want to recognize it because you don't want to inadvertently waste a whole bunch of time trying to come up with some too good to be true algorithm for an NP hard problem. Well, there's two skills that are really necessary. The first one is you should know a bunch of known NP hard problems.
00:00:36.646 - 00:00:55.934, Speaker A: So there's thousands and thousands of NP hard problems out there in the world. And we're going to get you started in the fourth section of this playlist, corresponding to chapter 22, we'll get you started with a list of 19 NP hard problems. In the simplest case, your application will simply include this NP hard problem and then you know that your problem is NP hard as well.
00:00:55.934 - 00:01:22.006, Speaker A: More generally, if your problem isn't exactly map on to a known NP hard problem, you can prove that it's NP hard using a reduction. So this is the second skill that you'll want to hone spotting reductions between different computational problems. Now, as students of algorithms, we're quite accustomed to looking for reductions, right? When we see a new problem, we want to say, oh, does this just reduce to some problem I already know how to solve? That would be sort of the easiest way out.
00:01:22.006 - 00:01:45.566, Speaker A: So reductions, as we know, spread tractability from one problem to another. But actually, if you turn that statement on its head, you realize that reductions also spread computational intractability NP hardness from one problem to another in the opposite direction. So what that means is that there's a very, very simple two step recipe for establishing that a problem that you care about is NP hard.
00:01:45.566 - 00:01:56.738, Speaker A: Step one pick some known NP hard problem. Step two reduce that known heart NP hard problem to the problem you care about. That implies that your problem is also NP hard.
00:01:56.738 - 00:02:18.694, Speaker A: So in the rest of this video, we're going to elaborate on all of these points. For a deep dive, check out the fourth part of the playlist corresponding to chapter 22. So as you might guess, if you have one problem which is NP hard and you have a second problem which is at least as hard as the first, then that second problem is itself NP hard.
00:02:18.694 - 00:02:40.526, Speaker A: Subtle sounds very reasonable. But what does it actually mean? In particular, what does it mean to say that one problem is at least as hard as another? That's a concept we can make precise using the idea of a reduction. So what's a reduction between two problems? Well, we're going to say that first problem A reduces to a second problem B.
00:02:40.526 - 00:02:55.730, Speaker A: If any, algorithm solving B is easily translated into an algorithm that solves A. And this is still not really a precise mathematical definition because I haven't said what I mean by easily translate. So let's consider the following picture.
00:02:55.730 - 00:03:13.354, Speaker A: So here's the way to think about this picture. You want to imagine someone gives you on a silver platter this magenta box, kind of a magic box that can just at will solve the problem b. The question then is, can you build on that to design this light blue box something responsible for solving problem A.
00:03:13.354 - 00:03:35.986, Speaker A: So it's going to be given an input of A. It can query the subroutine or invoke the subroutine for B whenever it wants, solve instances of problem B and using the solutions from the magenta box, along with any additional work that it did outside of its subroutine calls. At the end of the day, the light blue box should output the correct solution to the instance of problem A that it was given in the first place.
00:03:35.986 - 00:03:59.078, Speaker A: Now, what do I mean by easily translate in this definition? Well, the ground rules are in this reduction. We want it to be efficient. So this reduction should only invoke the magenta box, the subroutine for b, a polynomial number of times, meaning the number of invocations is bounded above by a polynomial function of the input size, the input size of the problem A that the light blue box was given.
00:03:59.078 - 00:04:23.646, Speaker A: Moreover, the light blue box can do additional work outside of the subroutine calls to the magenta box, but that again should be bounded by a polynomial function of the input size. So the reduction is a polynomial time algorithm plus a polynomial invocations of the subroutine for the problem capital b. Seasoned algorithm designers such as yourself are accustomed to looking for reductions, trying to spot them between computational problems.
00:04:23.646 - 00:05:02.902, Speaker A: After all, why solve a problem from scratch if it's a special case or a thinly disguised version of some other problem that you already know how to solve? So just to jog our memory, let's actually recall some of the reductions that we've seen previously in this book series. So for example, I think the first reduction that we saw is we saw that computing the median of an array of integers reduces to sorting the array of integers because after all, if you sort the array, the median is just going to be the middle element. So for example, if you use the merge sort algorithm which runs an o of n log n time on arrays of length n, that immediately gives you an o of n log n time algorithm for median finding.
00:05:02.902 - 00:05:23.342, Speaker A: More generally, whatever the running time of the sorting subroutine that you use is, you'll get a median finding algorithm which has the same running time with a larger constant factor. Another reduction we saw is when we first started talking about the all pair shortest path problem. This is where I give you a directed graph and you want to compute the shortest path distance between each ordered pair of vertices in the graph.
00:05:23.342 - 00:05:44.422, Speaker A: And just to draw a line in the sand, kind of the first thing we observed is that if you handed me a subroutine that solves the single source version of the problem. So compute shortest path distances from a starting vertex s to all of the n minus one possible destination vertices. Well, I can use that subroutine to solve the all pair of shortest path problem just by running that subroutine once from each of the n vertices.
00:05:44.422 - 00:06:10.510, Speaker A: So just taking turns with which vertex is serving as the starting vertex. So that's going to translate a single source shortest path algorithm into an all pair shortest path algorithm whose running time is a factor of n larger, where n is the number of vertices. So if you're using like the Bellman Ford algorithm for your single source shortest path problem that has running time O of m times n, where m is the number of edges, n is the number of vertices.
00:06:10.510 - 00:06:37.450, Speaker A: So if you did this reduction, you would get an O of m n squared algorithm for all pairs shortest paths. A third example, which showed up in some of the end of chapter problems in part three, is that the longest common subsequence problem can be viewed as a special case of the sequence alignment problem. So we gave a full blown dynamic programming algorithm solving sequence alignment at O of m times n time, where m and n were the length of the two input strings.
00:06:37.450 - 00:07:22.394, Speaker A: And basically, if you just set the parameters in the sequence alignment problem correctly, so you set the gap penalty to one and you set the cost of mismatching two symbols to be a very large number that actually will just immediately find you the longest common subsequence of the two strings in the same running time. O of m times n where those are the lengths of the input strings. So these are all examples of reductions which we use to spread tractability from one problem to another, to take a fast algorithm that we'd already designed for one problem and to translate it into a fast algorithm for some other problem, right? So taking a fast sorting algorithm, getting a fast median finding algorithm, taking a fast single source shortest path algorithm, and getting not quite as fast but reasonably fast all pair shortest path algorithm and so on.
00:07:22.394 - 00:07:49.602, Speaker A: So in general, if you have a reduction from a problem A to A, problem b and b can be solved by a polynomial time algorithm, then that immediately gives you a polynomial time algorithm for the problem A as well. So for example, the problem A could correspond to all pairs shortest paths, the problem B could correspond to single source shortest paths. And because we have a polynomial time algorithm for the latter problem, that immediately gives us a polynomial time algorithm for the former problem as well.
00:07:49.602 - 00:08:11.594, Speaker A: So reduction spread tractability spread polynomial time solvability from one problem to another. Notice that the direction in which tractability spreads is the opposite direction in which the reduction proceeds. So A reduces to B and then tractability flows in the opposite direction from B back to A.
00:08:11.594 - 00:08:40.350, Speaker A: All of these reductions serve the honorable mission of expanding the frontier of computational tractability, so taking existing fast algorithms and using them to get fast algorithms for further problems. The theory of NP hardness, however, revolves around a much more nefarious use of reductions to spread intractability, spreading NP hardness from one problem to another. Now, in contrast to the spread of tractability, the spread of intractability will go in the same direction as the reduction.
00:08:40.350 - 00:08:59.010, Speaker A: So if A is NP hard and reduces to B, b is going to be NP hard as well. So why is this true? Why is it that a reduction spreads computational intractability spreads NP hardness in the same direction of the reduction? Well, suppose a problem A reduces to a problem B. And suppose problem A is NP hard.
00:08:59.010 - 00:09:14.960, Speaker A: What does that mean? Remember, a problem is NP hard. If a polynomial time algorithm solving it would refute the P not equal to NP conjecture. So now think about B, okay? Suppose you had a polynomial time algorithm for B.
00:09:14.960 - 00:09:27.790, Speaker A: Because A reduces to B, you're going to automatically get a polynomial time algorithm for A. But a was NP hard. So that would refute the P zero equal to NP conjecture.
00:09:27.790 - 00:09:44.966, Speaker A: So putting that together, a polynomial time algorithm for B would refute the P zero equal to NP conjecture. So B itself must be NP hard. So what this means is that we actually have a super simple two step recipe for proving that a problem is NP hard.
00:09:44.966 - 00:09:58.534, Speaker A: So suppose problem B is the problem that you really care about. That's the one that showed up in your own work, and you're trying to figure out whether it's NP hard or whether you should try to design an always correct and always fast algorithm for it. If it's going to be NP hard, here's how you would prove it.
00:09:58.534 - 00:10:24.882, Speaker A: So, first, you need to make a decision of what's going to be your problem A. So what problems are you going to use in the left part of the reduction? Then, once you've settled on the right NP hard, problem A, your second goal is to reduce it to the problem B to show that a polynomial time algorithm for B would allow you to solve A. If you can do that, by this argument, you have proved that your problem B is, in fact, NP hard.
00:10:24.882 - 00:10:38.514, Speaker A: And you should be making some of the compromises either on generality or on speed or on correctness. So carrying out the first of these steps, well, to do that, you need to kind of know some NP hard problems. You'll learn a pretty reasonable list later on in this playlist.
00:10:38.514 - 00:11:17.134, Speaker A: And the videos correspond to chapter 22 we'll talk about 19 different NP hard problems and give you some references for many more if you need them to carry out the second step of the recipe, that just involves kind of building on your already developed skills and identifying reductions between problems. And we will hone these skills further in that same part of the playlist, corresponding to chapter 22. What I want to do in the rest of this video is to give you a quick glimpse of this two step recipe, just to give you the gist with a relatively simple example for a problem that we know and love, the single source shortest path problem when you can have positive or negative edge lengths.
00:11:17.134 - 00:11:33.990, Speaker A: All right, so we've mentioned in passing the single source shortest path problem, which is something hopefully you've seen before. But let me just actually briefly remind you exactly what this problem is. So the input to the problem is a directed graph and one of the vertices is designated as the starting vertex.
00:11:33.990 - 00:12:00.190, Speaker A: Moreover, each of the edges has a length. Now, in this version of the problem, we're going to allow lengths to be positive or negative or zero. So any real value for edge lengths and just in case you sort of scoff and like, well, why does that even make sense to have negative edge lengths in a shortest path problem? Let me remind you that paths and graphs might represent abstract sequences of decisions rather than something physically realizable.
00:12:00.190 - 00:12:13.798, Speaker A: So for example, if you want to compute a profitable sequence of financial transactions that involves both buying and selling, you're looking for a shortest path in a graph with edge links that are both positive and negative. So they do show up in applications. And this is the version of the problem that I want to talk about.
00:12:13.798 - 00:12:35.486, Speaker A: So the goal naturally is to compute the shortest path distance from this starting vertex S to every other possible destination vertex. So the length of a shortest path between this vertex S and every other vertex. Now, it's actually a little bit subtle to define what I mean by shortest path distance, but let's start with a simple example.
00:12:35.486 - 00:12:46.430, Speaker A: So for example, consider this simple three vertex graph. So the shortest path distance from S to itself, well, that's easy. Only the empty path goes from S to itself, so that has length zero.
00:12:46.430 - 00:12:56.882, Speaker A: There's also only one path in this graph that goes from S to V, and that's a path that has length one. So that would be the shortest path distance from S to V. Now there are two different paths going from S to T.
00:12:56.882 - 00:13:12.810, Speaker A: The one that goes through the one, that's just one hop that goes directly there, that would be length minus two. And then there's the two hop path that goes through V that has total length one plus minus five, which would be equal to minus four. So that would actually be the shorter path minus four.
00:13:12.810 - 00:13:22.454, Speaker A: Is more negative than minus two. So maybe no big deal, you say. So what was so hard about defining shortest path distance? It was pretty obvious what they're supposed to be.
00:13:22.454 - 00:13:38.322, Speaker A: But let's look at a second example. So what makes this graph more confusing is that it has a negative cycle, meaning it has a directed cycle for which the sum of the edge lengths is negative cycle. Going from V to U to W to X back to V again.
00:13:38.322 - 00:13:49.786, Speaker A: You'll notice that the overall, the sum of the edge lengths in that cycle is minus two. So consider the shortest path distance, whatever that means, from S to V. On the one hand, there is that one hop path.
00:13:49.786 - 00:13:56.034, Speaker A: So that has length ten. Fine. But then actually, you could tack on a cycle traversal at the end of that one hop path.
00:13:56.034 - 00:14:10.874, Speaker A: So you'd now have five hops in your path, but the length would have dropped from ten to eight. And why stop with one cycle traversal, right? Traverse it again, you have a nine hop path with length six. Again, you have a 13 hop path with length four, and so on.
00:14:10.874 - 00:14:24.266, Speaker A: So by traversing the cycle an arbitrary number of times, you can push this path length to be as negative as you want. So the shortest path distance between S and V, you could define it as minus infinity. You could say that it doesn't exist anyways.
00:14:24.266 - 00:14:41.026, Speaker A: It's confusing what you're supposed to do in the presence of a negative cycle. So one thing you could do that would seem to make a lot of sense is just be clear in the problem specification that you're not interested in these paths that loop back on themselves an infinite number of times. So just say, Look, I want the shortest path distance, but there's no loops in your path.
00:14:41.026 - 00:14:59.846, Speaker A: Your path should be cycle free. So that's a perfectly natural and perfectly well defined algorithmic problem. You might well say, hey, wouldn't it be cool to have a fast algorithm to compute shortest cycle free paths in the presence of negative edge links? Unfortunately, this is a shortest path problem, which we do not think has any polynomial time solution.
00:14:59.846 - 00:15:10.554, Speaker A: The cycle free shortest paths problem is an NP hard problem. So let me now show you how you would actually prove this fact. Prove that it's NP hard to compute, cycle free shortest paths.
00:15:10.554 - 00:15:31.378, Speaker A: I should have mentioned this NP hardness sort of explains the limitations that we saw when we talked about shortest path algorithms in parts two and three, all of our algorithms succeeded only when graphs that did not have negative cycles. So, for example, we had dejer's algorithm that actually made a stronger assumption that assumed all edge lengths are non negative. So certainly all cycles are non negative.
00:15:31.378 - 00:15:41.118, Speaker A: And there we got a near linear time algorithm. We were able to solve that super quickly. We also saw the Bellman, Ford and Floyd Warsall algorithms, those did not assume non negative edge lengths.
00:15:41.118 - 00:15:55.594, Speaker A: They allowed negative edges, but they promised correct shortest path distances only in the absence of a negative cycle. If there was a negative cycle, the algorithm would tell you that, which is good to know, but it wouldn't give you any shortest path distances. So those were all polynomial time algorithms.
00:15:55.594 - 00:16:18.702, Speaker A: And we now see why they didn't do more. Because if they actually were going to compute correct shortest path distances for cycle free paths in the presence of a negative cycle, they would have been solving an NP hard problem in polynomial time, thereby refuting the P not equal to NP conjecture. So how would we prove this? How would we convince ourselves that the cycle free shortest path problem is NP hard? Well, we just use the two step recipe.
00:16:18.702 - 00:16:31.622, Speaker A: So we're just going to pick some NP hard problem A. This is going to be our problem B, and we want to show that this known empty hard problem A reduces to the problem we care about cycle free shortest paths. So let me tell you about each of those steps in turn.
00:16:31.622 - 00:16:45.502, Speaker A: First, I want to tell you what's going to be the known NV hard problem A that we're going to reduce from. Then I want to actually show you the reduction from that problem to cycle free shortest paths. So for our known NV hard problem A, we'll use a famous problem.
00:16:45.502 - 00:17:12.674, Speaker A: You may not have heard of it, but it is a famous problem, the Hamiltonian path problem, specifically in directed graphs. So in the directed Hamiltonian path problem, the input is just a directed graph along with one designated starting vertex S and one designated ending vertex T. And the goal of the problem is simply to identify whether or not this graph G has what's called an St Hamiltonian path.
00:17:12.674 - 00:17:28.666, Speaker A: So that's a directed path from S to T, and it should visit every vertex exactly once. So it's going to have N minus one edges if N is the number of vertices. And it's just going to visit every single vertex once, starting at S ending at T.
00:17:28.666 - 00:17:58.546, Speaker A: So, for example, here are two very similar looking directed graphs, one of which has an St Hamiltonian path and one of which does not. So in the graph on the left, as you can perhaps see, there is an St Hamiltonian path. It snakes back and forth between S and T, whereas in the right graph, I kind of messed up this path by reversing the orientation of the rightmost edge in the second row.
00:17:58.546 - 00:18:14.954, Speaker A: So now if you check, there's actually no longer an St Hamiltonian path in this graph on the right, because I reversed the direction of that one edge. So some graphs have St Hamiltonian paths and some graphs don't. And it can be quite subtle to figure out which of the two cases you're in.
00:18:14.954 - 00:18:20.298, Speaker A: But that's exactly the directed Hamiltonian path problem. So I give you the directed graph. I give you the start and I give you the end.
00:18:20.298 - 00:18:42.718, Speaker A: And I just want to know yes, no, does it have an st hamiltonian path or not? Turns out this seemingly innocuous sounding problem is in fact NP hard. So for this video, I just want to take this on faith. I just want to sort of assume that directed Hamiltonian path problem is NP hard and use that to spread its NP hardness to the problem that we care about cycle free shortest paths.
00:18:42.718 - 00:19:05.862, Speaker A: Now, later on in the playlist in the videos corresponding to chapter 22, we will in fact see why directed Hamiltonian path is NP hard. We will prove that it's NP hard using this exact same two step recipe. But again, for now, let's just take it on faith and spread that intractability to cycle free shortest paths by reducing directed Hamiltonian path to cycle free shortest paths.
00:19:05.862 - 00:19:15.518, Speaker A: Remember, intractability spreads in the same direction of the reduction. We want to spread intractability to cycle free shortest paths. So that's also the direction of the reduction that we want.
00:19:15.518 - 00:19:31.954, Speaker A: All right? So we need a reduction. So remember what that means. It means that if someone gave us a magic box that could solve the cycle free shortest path problem, then we could use that magic box to come up with an efficient algorithm for the directed Hamiltonian path problem.
00:19:31.954 - 00:19:44.454, Speaker A: So we're given this magenta box. So problem b is the cycle free shortest path problem. We're given a subroutine for it, and from that we want to build the light blue box, an algorithm that solves the directed Hamiltonian path problem.
00:19:44.454 - 00:20:09.114, Speaker A: So The Reduction Is Going To Start with An Instance of The Directed Hamiltonian Path Problem that's Specified By A Directed Graph G along With A Starting Vertex S and A Destination Vertex T. So what's sort of The Simplest Thing that We Could Do To this instance so that it Makes Sense to Feed It Into A Cycle Free shortest Path problem? Well, cycle free shortest path, it expects a directed graph. We've already got that it expects a starting vertex.
00:20:09.114 - 00:20:26.980, Speaker A: We've already got that it's not expecting a destination vertex, but so let's just not tell the subroutine about the destination vertex t. The other thing that cycle free shortest pass is expecting is edge lengths. So we need to just put some edge links on the edges of the graph and then we could try feeding it into the subroutine and see what happens.
00:20:26.980 - 00:20:37.398, Speaker A: Specifically here would be the steps that we're going to follow in the reduction. Step one, we're going to give lengths to all the edges. What's that length going to be? The length is going to be minus one.
00:20:37.398 - 00:21:02.458, Speaker A: Why minus one? Well, we're looking for a Hamiltonian path, so that's a really long path. The subroutine computes short paths, so to make it think that short paths, sorry, to make it think that long paths are actually short, we trick it by giving all the edges a length of minus one. The second step is where we're going to invoke the assumed subroutine for computing cycle free shortest path invoke this magic box, which I've drawn in magenta.
00:21:02.458 - 00:21:14.126, Speaker A: So we're going to feed the magic box the same graph g that we were given. We're going to feed it the same starting vertex S that we were given, and we're going to feed it in these edge lengths that we made up, these minus ones. And then we're going to just run it, see what we get.
00:21:14.126 - 00:21:27.270, Speaker A: It's going to tell us the length of a cycle free shortest path between the starting vertex S and everybody else. So in the last step, we need to somehow read the T leaves. We need to look at the answer that our subroutine gave us back and deduce from that answer.
00:21:27.270 - 00:21:45.662, Speaker A: What's up in the original directed graph? Does it have a Hamiltonian path or not? And that's going to be very simple. We're going to ignore distances between S and all vertices other than T, and we're going to look at what the subroutine tells us is the shortest path distance between S and T. And if that is minus quantity N minus one.
00:21:45.662 - 00:21:51.870, Speaker A: So in other words, one minus N, then we'll return. Yes. If it's anything else, we're going to return.
00:21:51.870 - 00:22:06.526, Speaker A: No. So we're claiming that whether or not the shortest path distance is minus quantity n minus one tells us whether or not that graph has an Ft Hamiltonian path. So to understand what's going on, let's sort of revisit our two example graphs.
00:22:06.526 - 00:22:12.870, Speaker A: One was Hamiltonian. One was not Hamiltonian. And let's see what happens in the reduction to those two graphs.
00:22:12.870 - 00:22:28.906, Speaker A: So these are the exact same two graphs that we looked at earlier. The only difference is I've now written a minus one by each of the edges reflecting their edge length. So in the first graph on the left, that was the one that had a Hamiltonian path that snakes back and forth from S to T.
00:22:28.906 - 00:22:43.902, Speaker A: So that path visits all nine vertices exactly once. So as a result, it has eight edges, which means that its length is minus eight. And indeed in the reduction, if this happens, the reduction is going to return the answer yes.
00:22:43.902 - 00:22:56.210, Speaker A: So it's looking for a shortest path distance of minus quantity N minus one. So with nine vertices, when N equals nine, that would be looking for a shortest path distance of minus eight. And that's indeed what it would be.
00:22:56.210 - 00:23:13.846, Speaker A: If you think about it, there's no way you could have a path that was even more negative than minus eight, because if you had nine edges, that means you'd be visiting ten vertices. So you'd have to visit one of the vertices twice, so it couldn't possibly be a Hamiltonian path. Now, in the second graph, there is no St Hamiltonian path.
00:23:13.846 - 00:23:32.286, Speaker A: And for that reason, there is not going to be any path from S to T that has length minus eight. Whatever the shortest path distance is from S to T, it's going to be bigger than minus eight because there's no St Hamiltonian path. And if you think about it, you realize that the best you can do is minus six in this graph on the right.
00:23:32.286 - 00:23:47.940, Speaker A: So indeed, the reduction is going to do the right thing. In this graph, the reduction was given a graph that has no SD Hamiltonian path. When it invokes the cycle free shortest path subroutine, it's going to find that the shortest path distance, cycle free path distance from S to T is minus six.
00:23:47.940 - 00:23:54.466, Speaker A: And then it's going to know, it's going to know there's no SD Hamiltonian path. It's going to return no. And those same arguments work totally.
00:23:54.466 - 00:24:18.010, Speaker A: Generally, we really use nothing about these particular nine vertex graphs to argue the correctness of the reduction. So if the reduction starts with a directed graph that does have an St Hamiltonian path, that path will immediately translate to a cycle free St path. In the shortest path instance, that has length exactly minus times quantity N minus one times the number of hops in that St Hamiltonian path.
00:24:18.010 - 00:24:28.370, Speaker A: So the shortest path subroutine will tell the reduction as much, say, oh, hey, the shortest path distance from S to T is minus times N minus one. And then the reduction is like, oh cool, that meant there was a Hamiltonian path. And so you return.
00:24:28.370 - 00:24:47.894, Speaker A: Yes. In the other case, if the reduction is given a graph that does not have an St Hamiltonian path, well, then all of the St cycle free paths in the graph have fewer hops than a Hamiltonian path. Which means that in the shortest path instance, all of the cycle free paths will have length strictly bigger, less negative than a Hamiltonian path would have.
00:24:47.894 - 00:25:01.754, Speaker A: So it'll have length strictly bigger than minus times quantity N minus one. The subroutine will report that back and that signals the reduction. Okay, I was given a graph that did not have an SD Hamiltonian path and now I'm just going to report as much.
00:25:01.754 - 00:25:21.794, Speaker A: So it doesn't matter whether the reduction is given a graph with a Hamiltonian path or not. Either way it can correctly deduce which case it's in according to the output of the cycle free shortest path subroutine. And that's a reduction that shows that given a magic box solving cycle free shortest path, you could indeed solve the directed Hamiltonian path problem.
00:25:21.794 - 00:25:47.126, Speaker A: So under our assumption that the directed Hamiltonian path problem is NP hard by our two step recipe, this reduction spreads the intractability spreads the NP hardness to the cycle free shortest paths problem as well. So later on in the video playlist, we will see many, many more examples of the two step recipe in action. But I just wanted to give you an early glimpse of it because I think it's really empowering to know how easy the.
00:25:47.126 - 00:26:04.222, Speaker A: Theory of NP hardness is to apply how easy it is to generate your own NP hard problems when you need to. So the last thing I want to discuss in chapter 19 is a collection of rookie mistakes around NP hardness, which you're going to want to avoid. So that's coming up next.
00:26:04.222 - 00:26:05.930, Speaker A: I'll see you then. Bye.
