00:00:00.490 - 00:00:14.190, Speaker A: Hi everyone and welcome to this video that accompanies section 23.6 of the book Algorithms Illuminated, part four. This is the last section from the optional chapter 23 and it's a section about NP completeness.
00:00:14.190 - 00:00:39.954, Speaker A: NP completeness is basically a specific form of NP hardness. So, for example, the three set problem as we know, that's an NP hard problem. What does that mean? That means if gave you a polynomial time algorithm that solved the threesat problem you could, using reductions automatically build polynomial time algorithms for all of the problems in the complexity class NP for all problems with efficiently recognizable solutions.
00:00:40.082 - 00:00:41.674, Speaker B: But in fact, we can say something.
00:00:41.792 - 00:00:49.066, Speaker A: More precise about threesat and most of the other NP hard problems that we've seen, which is that it turns out it's actually NP complete.
00:00:49.248 - 00:00:52.526, Speaker B: And what that's going to mean is that actually it's not just that an.
00:00:52.548 - 00:01:08.926, Speaker A: Efficient subroutine for threesat is sufficient to solve every problem in NP. It's that actually every problem in NP is literally just a thinly disguised special case of the three Sat problem. In other words, NP complete problems like the three Sat problem, they are universal.
00:01:09.038 - 00:01:12.510, Speaker B: In the sense that they simultaneously encode.
00:01:12.670 - 00:01:16.042, Speaker A: Every single problem in the complexity class NP.
00:01:16.206 - 00:01:19.640, Speaker B: Sound pretty amazing. Well, it is. Now let's learn about it.
00:01:19.640 - 00:01:47.726, Speaker B: What do we mean when we say that one problem A is a thinly disguised version of another problem B? Well, we can make that idea mathematical through the use of Leaven reductions. So eleven reduction is a special case of a Cook reduction cook reductions being the reductions we've been using throughout this entire video playlist. So eleven reduction is a special restricted type of Cook reduction where intuitively it's only allowed to do the minimum imaginable amount of work.
00:01:47.726 - 00:02:06.254, Speaker B: All it can do is invoke a subroutine for the problem B once. And the only other thing it can do is preprocess its input to feed it into B and preprocess B's output to return that as its final solution. So let's redraw our usual cartoon to reflect these new restrictions imposed by eleven reduction.
00:02:06.254 - 00:02:24.794, Speaker B: First, I should say at the outset eleven reductions only even make sense when you're talking about a pair of search problems. So think about we're reducing some search problem A to some other search problem B like the three Sat problem and the search version of the Tsp. As usual, we'll be imagining that we're given an efficient subroutine solving the problem B.
00:02:24.794 - 00:02:54.734, Speaker B: The problem that we're trying to prove is hard that's the magenta box. And then the responsibility of the reduction is to build the light blue box to show how to use that magenta box in the service of actually solving problem A as well the known hard problem. Now, unlike a Cook reduction which is allowed to invoke its magenta box any polynomial number of times and is also allowed to use the results of those polynomial many subroutine calls however it wants the Levin reduction has its hands tied much more tightly.
00:02:54.734 - 00:03:18.570, Speaker B: So first of all, it's only allowed to invoke the magenta box once. So the beginning of the reduction, pretty much the only thing it can do is take its input to the problem A it's tasked with solving, and then preprocess it, transform it into some input of the problem B that it can then feed into the magenta box. Moreover, eleven reduction is required to use the output of the magenta box in a very specific way.
00:03:18.570 - 00:03:37.410, Speaker B: Now remember, we're dealing with search problems. So the magenta box will either say here's a feasible solution to the instance of problem B that you gave me or the magenta box will say there was no solution to the instance of problem B that you gave me. And the blue box, an eleven reduction is then required to just copy the answer.
00:03:37.410 - 00:04:36.724, Speaker B: So if the magenta box comes back and says there was no solution, then the light blue box is forced to say, in my opinion, there's no feasible solution to the instance of A that I started with. On the other hand, if the magenta box returns a feasible solution to the instance of B it was given, then with polynomial amount of post processing, the leaven reduction has to at that point transform it into a feasible solution of the instance of A that it was given. So we've been seeing a whole lot of reductions throughout this video playlist.
00:04:36.724 - 00:04:45.144, Speaker B: So the question you should now be asking is, well, were we really using the full power of cook reductions or were we inadvertently just kind of using leaven reductions?
00:04:45.192 - 00:04:46.072, Speaker A: Anyways?
00:04:46.216 - 00:05:02.256, Speaker B: The answer is, technically we weren't always doing leaven reductions, but kind of morally we really were. What do I mean? Well, leaven reduction is only defined for a pair of search problems. And if you go back through the reductions we've seen, many of them involved optimization problems.
00:05:02.256 - 00:05:34.412, Speaker B: However, if you go back to those reductions like, say, the NP hardness of the traveling salesman problem, and instead of looking at the optimization version of the traveling salesman problem, you look at the search version of the traveling salesman problem where you're also given a target tour cost capital t then all of a sudden that reduction does become eleven reduction. One reduction we had that was between two pairs of search problems where I think this format is super clear, was the second of the four big ones that we did in chapter 22. So that was a reduction from the three Sat problem to the directed Hamiltonian Path problem.
00:05:34.412 - 00:05:45.436, Speaker B: And if you go back and look at that, or maybe you sort of remember at least vaguely, we were given the three sat instance. All we did was construct a big, somewhat complicated directed graph. But whatever, we constructed a directed graph.
00:05:45.436 - 00:05:53.996, Speaker B: We just fed it into our directed Hamiltonian subroutine. If it said there's no Hamiltonian Path, we reported that there's no Satisfiable assignment. If it gave us a Hamiltonian path.
00:05:53.996 - 00:06:13.720, Speaker B: We extracted from it a satisfying assignment. So that is an absolutely canonical example of eleven reduction. But again, if you go back to the reductions throughout this video playlist, and I encourage you to do this, and you think about the search version of all of the optimization problems that we discussed, all of the reductions that we've been looking at really are Leaven reductions.
00:06:13.720 - 00:06:30.104, Speaker B: You now know all about Cook reductions and the special case of Leaven reductions. There's a third type of reduction that I'll mention briefly, just because you're likely to see it in pretty much any book on complexity theory and also plenty of books on algorithms, which is something known sometimes. It's called a carp reduction.
00:06:30.104 - 00:06:42.320, Speaker B: That's what I'm going to call it. Or you may see it called a many to one reduction or mapping reduction. So what's a carp reduction? Carp reduction is basically just eleven reduction except for decision problems instead of for search problems.
00:06:42.320 - 00:06:52.336, Speaker B: So remember, in a decision problem, all an algorithm has to do is report yes or no. If there's a feasible solution. The algorithm is actually not responsible for handing one to you on a silver platter.
00:06:52.336 - 00:07:03.304, Speaker B: And so this cartoon on this slide becomes even simpler if you have decision problems. So the magenta box, the subroutine for B is just going to say yes or no. It's not going to give you a solution in the yes case.
00:07:03.304 - 00:07:11.932, Speaker B: And then the light blue box is just going to parrot that answer. If the magenta box said no, the blue box will say no. If the magenta box says yes, the blue box will say yes.
00:07:11.932 - 00:07:37.540, Speaker B: Again, the blue box for a decision problem is not responsible for actually constructing that feasible solution. So for any book that talks primarily about decision problems as opposed to the search problems that we've been talking about here, any book that talks just about decision problems, you're going to be seeing carp reductions instead of Leaven reductions. Again, I'm doing this entire video playlist in terms of search problems because those are much more natural from an algorithmic viewpoint.
00:07:37.540 - 00:07:59.440, Speaker B: We are now ready to formally define NP complete problems. Problems that are the hardest problems within NP, problems that simultaneously encode as special cases all other problems that have efficiently recognizable solutions. NP completeness is really best thought of as a specific kind of NP hardness.
00:07:59.440 - 00:08:23.968, Speaker B: So let me just remind you about that formal definition of NP hard problems that we finally got to three videos ago. Our formal definition of an NP hard problem is a problem B, for which for every NP problem, every problem, every search problem with efficiently recognizable solutions, for every search problem A, there's a reduction from A to B. And again, the entire video playlist we've been looking at Cook reductions.
00:08:23.968 - 00:08:46.152, Speaker B: So a problem is NP hard. If given a polynomial time subroutine solving B, you would automatically get polynomial time algorithms for all of the problems in the class NP. To qualify as NP complete, that problem B has to satisfy some additional properties.
00:08:46.152 - 00:09:08.710, Speaker B: So first of all, as we'll see, only search problems are going to be eligible to be NP complete. So while the Tsp in its optimization version that's an NP hard problem, the Tsp in its optimization version is not going to be an NP complete problem, it is true that the search version of the Tsp will in fact be an NP complete problem. So NP complete only refers to search problems.
00:09:08.710 - 00:09:31.724, Speaker B: Next, it should be the case that not only is B algorithmically sufficient to solve all the problems in NP, actually all the problems in NP are literally just thinly disguised versions of B. And remember, we've expressed thinly disguised versions through eleven reduction. So for NP hardness, we just wanted a cook reduction from every NP problem to B.
00:09:31.724 - 00:09:54.764, Speaker B: For NP completeness, we're going to insist on eleven reduction from every NP problem to B. This first condition is basically requiring that the problem B is simultaneously encoding all problems of NP, all problems that have efficiently recognizable solutions. The second condition is so that we can interpret an NP complete problem as the hardest problems among NP.
00:09:54.764 - 00:10:32.104, Speaker B: And so for that to make sense, we're going to require that B is in fact a member of NP, that B in fact is a search problem with efficiently recognizable solutions. So this definition of an NP complete problem, that's one of the absolute most important definitions in the entire history of the field of computer science. So I want to make sure that it's clear that you will, if you look in some books, see a slightly different definition of NP completeness.
00:10:32.104 - 00:10:55.572, Speaker B: And I don't want you to get confused. So again, what we're working with here is search problems where an algorithm is responsible for handing back a feasible solution when one exists and leaven reductions where you preprocess an input, feed it to the subroutine and post process its output to get a feasible solution when one exists in many books. Instead they will use decision problems rather than search problems.
00:10:55.572 - 00:11:12.956, Speaker B: And a decision problem is a yes no problem. So you don't have to construct a feasible solution, you just report whether one exists. And then if you're using decision problems, the analog of eleven reduction is one of these carp or many to one reductions where you don't even need to bother with the post processing step.
00:11:12.956 - 00:11:34.092, Speaker B: You just ask the magenta box yes or no and then you just parrot that exact same answer as your own. So I say all this just to ward off any confusion should you go read about NP completeness from another source. And actually at this point you might be kind of irritated, right? Because we have these three different types of problems decision and search and optimization.
00:11:34.092 - 00:11:52.468, Speaker B: We have these three different types of reductions cook and leaven and carp. And it seems like you can mix and match and it's not really clear which pair you should use. But unless you're going into complexity theory full time, if you're focused mostly on the algorithmic side, don't worry about the fact that there's multiple kinds of problems, that there's multiple kinds of reductions.
00:11:52.468 - 00:12:19.760, Speaker B: As far as the algorithmic implications, as far as the guidance the theory gives you about how to tackle different problems, it's exactly the same no matter which of these definitions that you use. How cool is the definition of an NP complete problem? A single problem with efficiently recognizable solutions that simultaneously encodes every such problem. It's kind of amazing that an NP complete problem could really exist.
00:12:19.760 - 00:12:40.360, Speaker B: Wait a minute, I actually haven't shown you an example of an NP complete problem yet. So do they really exist? There are such universal problems. And in fact, the theorem that we've touched on a couple times already shows that the Cook Levin theorem, when I first showed you this theorem, I kind of shortchanged it.
00:12:40.360 - 00:12:49.720, Speaker B: I said that it proved that the three sat problem is NP hard. It actually proves something stronger. It proves that the three sat problem is in fact NP completes.
00:12:49.720 - 00:13:12.304, Speaker B: And the reason this is true, the reason the Cook Levin theorem actually says something stronger, it's actually kind of evident if you go back and review the proof sketch of the Cook Levin theorem that I gave to you a few videos ago. So back then, the proof was a reduction. It was a reduction from an abstract NP problem, which we were calling A, a reduction from that problem A to the three sat problem.
00:13:12.304 - 00:13:30.564, Speaker B: And at the time, we were only worried about having using a Cook reduction because those were the only reductions we knew about up to that point. But if you go back and look at the sketch of that reduction, it is a canonical example of eleven reduction to remind you how that reduction worked. So you fabricate this three set instance.
00:13:30.564 - 00:13:53.040, Speaker B: It has two sets of decision variables. One set of decision variables encodes candidate solutions to the instance of the problem A that you were given. And then there's this sort of two dimensional table of state variables that are encoding the computation performed by the verification algorithm associated with that abstract NP problem, capital A.
00:13:53.040 - 00:14:19.844, Speaker B: And we also had a bunch of constraints to enforce the intended semantics across the state variables. Point being, all the reduction did is take the given instance of the problem A, construct this big threesat instance and construct it in a way that there's a correspondence between satisfying assignments to that three sat instance and feasible solutions to the instance of the problem A that the reduction started with. And so then what did it do? It literally just invoked its assumed subroutine for solving three Sat.
00:14:19.844 - 00:14:31.324, Speaker B: Once on the three sat instance. It concocted. And if the three subroutine said there's no satisfying truth assignment, the reduction concluded that there was no feasible solution to the instance of problem A it was given.
00:14:31.324 - 00:14:46.352, Speaker B: On the other hand, if the subroutine for three set came back with a satisfying assignment, you could read off a feasible solution to the instance of A we were given just from the values of the solution variables. And that's exactly what eleven reduction does. Preprocess I e.
00:14:46.352 - 00:15:06.148, Speaker B: Transform an instance of this problem into a three set instance invoke the assumed subroutine once that's what we did, and then just basically copy the results. And for the case where there is a feasible solution, do some post processing to translate it to a solution of the problem that you started with. That's the stronger version of the Cook Levin theorem and a sketch of why it's true.
00:15:06.148 - 00:15:24.508, Speaker B: So that's not an easy observation. There's a reason that major prizes were awarded to Cook and Levin for this work. But now, again, the good news is that once we have one NP complete problem, we get to stand on the shoulders of these giants and use reductions to generate further NP complete problems.
00:15:24.508 - 00:15:37.330, Speaker B: So we already used reductions to spread NP hardness and there we were working with Cook reductions. Because NP completeness is all about Leaven reductions. We're going to use Leaven reductions to spread NP completeness from one problem to another.
00:15:37.330 - 00:15:58.528, Speaker B: So what this means for us is that we have a very simple three step recipe for proving that a problem is NP complete, very much in the spirit of our two step recipe for proving that problems were NP hard. So remember the net two step recipe. How did it work? You choose known NP hard problem A and then you reduce it to the target problem B using a cook reduction.
00:15:58.528 - 00:16:18.092, Speaker B: So to spread NP completeness, we're going to need to make a couple changes, but not much. So first of all, why do we need a third step? Well, it's because NP completeness, remember, that means not only are the reductions from all the problems in NP to you, but you better yourself be a member of the class NP. You're supposed to be one of the hardest problems in NP if you're an NP complete problem.
00:16:18.092 - 00:16:33.024, Speaker B: So the extra step is just checking that the problem B that you're trying to prove NP complete really does belong to NP because that's a prerequisite. Otherwise it's the same. You choose a known NP complete problem A and then you reduce it to the problem that you're interested in B.
00:16:33.024 - 00:16:52.424, Speaker B: And if you want to spread not just NP hardness but NP completeness, then it's important that you use eleven reduction rather than a more general cook reduction. But that is, it prove that your problem is in NP, choose your favorite NP complete problem. You can start with threesat and then reduce that problem using eleven reduction to your target problem.
00:16:52.424 - 00:17:13.040, Speaker B: If you can do those three things, boom. Your problem B is in fact NP complete. Now, this simple three step recipe has been applied many times over, and as a result, we now know that thousands of natural problems are MP complete, including problems from all across engineering, the life sciences, and the social sciences.
00:17:13.040 - 00:17:44.730, Speaker B: For example, the search versions of almost all of the optimization problems we've discussed, including the Tsp, the Know Maximum Coverage, Minimum Make span. The search versions of all of those optimization problems are in fact, not just NP hard, but NP complete. If all of the NP complete problems from chapter 22 aren't enough, well, then you can check out that classic book I mentioned earlier by Gary and Johnson for hundreds of more examples of NP complete problems.
00:17:44.730 - 00:18:25.220, Speaker B: That concludes these videos that accompany the optional chapter 23 on PNP and all that. Thanks very much for checking them out. I hope you now, having watched them, feel much more solid mathematically on exactly what does it mean for a problem to be NP hard? What's the rigorous definition? What's the formal definition of the P versus NP conjecture? What's the difference between a problem being NP hard or NP complete? What are these fancier conjectures like, the exponential time hypothesis and so on? These videos were directed at those of you who were motivated to up your level of expertise with NP hard problems up to the highest level that we mentioned, level four.
00:18:25.220 - 00:19:04.164, Speaker B: So at this level, you can actually have your colleagues gather around you at a whiteboard while you regale them with tales about what the P versus NP conjecture actually is. So I hope after spending some quality time with these videos, you feel like you've reached that level, or at least are quite a bit closer to that level than you were when we started. Coming up next are going to be the videos for the last chapter of the book, chapter 24, which is all about a big and exciting case study on something called the FCC incentive auction, which was a big, complex algorithm involving tens of billions of dollars that reallocated a bunch of wireless spec in the United States just a few years ago.
00:19:04.164 - 00:19:15.890, Speaker B: It turns out under the hood of the FCC Incentive Auction, you can find an amazingly wide swath of the algorithmic toolbox that you've learned in this video playlist. So don't miss it. I'll see you there.
