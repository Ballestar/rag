00:00:00.250 - 00:00:00.798, Speaker A: You.
00:00:00.964 - 00:00:26.530, Speaker B: Okay, so what are we going to do this lecture? We're going to keep talking about maximum flow. So last week we talked about a few maximum flow algorithms, and all of them are what's known as augmenting path algorithms for the obvious reason, you know, each iteration of those algorithms, you find some path in the residual graph, and then you augment it by routing additional flow along that path. So in the first lecture we saw the Ford Folkerson algorithm.
00:00:26.530 - 00:00:45.894, Speaker B: So that's where you just pick an arbitrary path from st t in the residual graph to augment that's not even polynomial time algorithm, that's only a pseudo polynomial time algorithm. Then on Thursday we talked about the Edmunds Carp specialization of Ford Folkerson. That's where amongst all the st paths in the residual graph, you pick the shortest one, the one with the fewest number of hops.
00:00:45.894 - 00:01:01.354, Speaker B: That gave us a polynomial running time bound. It wasn't very good, just o of m squared times n, but it was a start. And then we also talked about denix algorithm briefly, although that'll be developed more on problem set number one, which was this idea of augmenting through blocking flows.
00:01:01.354 - 00:01:08.530, Speaker B: And with the implementation on the problem set, we'll get a little bit better of a runtime bound of n squared m. So that's the story. So far.
00:01:08.530 - 00:01:36.540, Speaker B: The goal for this lecture is to teach you a different paradigm for maximum flow algorithm design known as the push relabel paradigm. And so to this day, in fact, push relabel algorithms are often the method of choice in practice, they're really very fast in practice, even if they've never sort of quite won the grand championship of worst case running time bounds, in practice, they do really, really well. That's why I want to devote a whole lecture to it, because it's really something you might use in your own work in the future.
00:01:36.540 - 00:01:56.020, Speaker B: So to introduce push for label, let me just sort of give you a motivating example of why you might want an algorithm which does something other than augmenting paths. So for some parameter k, think of k as like 10,000 or something, something big. Imagine you have a network that looks like this.
00:01:56.020 - 00:02:07.400, Speaker B: So you have a bunch of edges with capacity k. And let's just say you have k of these edges. So k will mean several different things in this example.
00:02:07.400 - 00:02:27.820, Speaker B: So you have a path of k edges, 10,000 edges each with a capacity 10,000. Here's your sink, and then x is connected to the sink by lots of disjoint paths that are only unit capacity each. And let's say the number of these paths is k.
00:02:27.820 - 00:02:48.820, Speaker B: So this obviously is some flow network. You might be given this as input for all you know. Okay, so what can you say? So how long would it take, say, the Ford Fulkerson algorithm to compute a max flow in this instance as a function of k.
00:02:53.970 - 00:02:54.938, Speaker A: Squared?
00:02:55.114 - 00:03:03.454, Speaker B: What was it k squared. That's absolutely right. So each iteration of Ford Ferguson takes linear time.
00:03:03.454 - 00:03:27.034, Speaker B: And indeed it's going to have to when you do breadth first search or depth first search, you're going to have to search down this whole long path of length, roughly k. So each iteration takes k time. And unfortunately, you'll only be able to route one unit of flow each time because you'll be able to these don't bind, but you have these unit capacities on the right, so you have a full k iterations with k time each.
00:03:27.034 - 00:03:43.546, Speaker B: So that's k squared. The max flow value here is k. So intuitively you're like, okay, how could we do better, at least on this network? Well, you look at this and you kind of say, well, what we really kind of want to do is just like flood this path, just route k units.
00:03:43.546 - 00:04:02.278, Speaker B: So spend linear time linear in k, getting flow to x, and then spend linear time just distributing it amongst the theta of k different edges. If we could do that, that would be a linear time algorithm for MaxFlow. In this particular instance, we wouldn't waste all this work just sort of recomputing these long paths over and over and over again.
00:04:02.278 - 00:04:15.640, Speaker B: So that's sort of the high level intuition behind what push relabel algorithms are trying to do. All right. On the other hand, it's not so simple, right? I mean, this is just one example.
00:04:15.640 - 00:04:44.180, Speaker B: If there were less than K paths over here and we tried to flood this path, we'd push K units of flow to X, and then we actually wouldn't have enough capacity to push it the rest of the way to T. So then we'd have some excess flow here and we'd have to sort of send it back to the source. If there were less than K paths, So the question is, is there any way to implement this idea systematically so that it always works no matter what the graph is, whether it looks like this graph or whether it looks like something totally different.
00:04:44.180 - 00:05:09.850, Speaker B: So simple but important definition for us is that of a preflow. So this is the same thing as a flow, except the conservation constraints are a bit relaxed. Remember, the conservation constraints was our way of saying that the flow in should equal the flow out at every vertex other than s and T.
00:05:09.850 - 00:05:28.500, Speaker B: And if you think about this idea of sort of flooding the network before this flow is reached all the way to T, you're not going to have conservation. You might have sort of excess, say, at the node x if you had fewer than k paths. So a preflow just relaxes the conservation condition in a natural way.
00:05:28.500 - 00:05:48.310, Speaker B: Okay, so it's a vector indexed by edges, and it satisfies two things. First of all, it satisfies the usual non negativity and capacity constraints. So these are exactly the capacity constraints that we had for flows.
00:05:48.310 - 00:06:06.510, Speaker B: And then instead of flow in equals flow out. We're going to allow the flow into a vertex to be possibly Larger than the Flow out, okay, but not smaller. So only more flow can go into a vertex than Go Out of a vertex.
00:06:06.510 - 00:06:30.678, Speaker B: And this is except at S. So of course at S, there's going to be zero flow in and some amount of Flow out. Okay, and Again, why this relaxation? Think about this case where we're sort of trying to push flow through this network and we're at some intermediate point, right? So whatever the frontier of our pushing the flow is going to be, there's going to be more coming in than there is going out.
00:06:30.678 - 00:06:33.350, Speaker B: So that's Why we want this relaxation.
00:06:35.850 - 00:06:40.120, Speaker A: Clear? Okay.
00:06:42.170 - 00:06:49.270, Speaker B: So remember, there is this totally key concept of a residual graph. We talked about this at length last week. So you have a flow in a network.
00:06:49.270 - 00:07:10.926, Speaker B: Then you get a corresponding residual graph where you have forward Edges with Residual Capacity UE minus Fe, and reverse arcs with residual Capacity F sub E. Notice that with a preflow, we can just as well form exactly the same residual network as we could with a Flow. Again, we just edge by Edge, do the forward arc, do the reverse arc with the appropriate residual capacities.
00:07:10.926 - 00:07:24.360, Speaker B: This relaxation doesn't screw up our residual network construction. So for preflow F, we define the residual network or residual grab GF as before.
00:07:26.890 - 00:07:27.640, Speaker A: Okay?
00:07:30.490 - 00:07:43.686, Speaker B: All right, so the next concept is we want some way at the end of the day. So we're going to have algorithms which in their intermediate stages maybe have a pre flow which is not A Flow. The conservation constraints will be violated.
00:07:43.686 - 00:07:47.734, Speaker B: Of course, by the time one of these Algorithms Terminates, it Better give us A Flow.
00:07:47.782 - 00:07:47.946, Speaker A: Right?
00:07:47.968 - 00:08:02.450, Speaker B: In the maximum flow problem, you can't output a preflow, you have to output a flow. So we need to work toward arriving at a flow, work toward conservation constraints. So we want a measure of how Badly we're violating the conservation constraints.
00:08:02.450 - 00:08:34.166, Speaker B: So for a fixed flow F, the excess of a vertex is just the difference between These Two quantities flow in minus Flow out. Okay, and for a pre Flow, this Translates to saying that all of these excesses will be non negative. Okay, again, except we're not Thinking about the source and the sink.
00:08:34.166 - 00:08:44.426, Speaker B: We're thinking about the rest of the vertices. Okay, so flow just means all the excesses are equal to zero. With a preflow, you might have strict inequality.
00:08:44.426 - 00:09:01.474, Speaker B: Here you might have some positive excesses. Preflow is a flow if and only if it has no vertices with positive excess. Everything clear so far? All right, so to turn a preflow into a flow, we basically have to kind of squash these excesses.
00:09:01.474 - 00:09:35.050, Speaker B: We have to get rid of the excesses. Okay, so next we need to understand, what does it mean? How are we going to modify preflows in an algorithm? Okay, when we were working with flows, our hands were kind of tied, right? We had to maintain the conservation constraints at all times. So that naturally motivated pushing flow on an st path because that maintains the invariant, the conservation constraints are satisfied.
00:09:35.050 - 00:09:51.480, Speaker B: Now that we're actually allowing ourselves to violate the conservation constraints, we have more flexibility in how we augment one of these preflows. We don't have to use a path. So in fact, what we're going to do is we're only going to push flow along a single edge of the network in one step.
00:09:51.480 - 00:10:12.170, Speaker B: So they're called push relabel algorithms. So let me tell you about push. So again, you want to think there's some fixed graph, there's some current flow in that graph, and then there's a corresponding residual graph like usual, sorry, some pre flow f in the graph.
00:10:12.170 - 00:10:25.770, Speaker B: So the subroutine, I'm going to unveil the whole algorithm step by step. So this will be a subroutine in the whole algorithm push. So suppose you look at a vertex v, and you're thinking of V as being a vertex with excess.
00:10:25.770 - 00:10:38.206, Speaker B: And you're trying to get rid of the excess, you're trying to move it somewhere else in the network. Okay, so what do you do? Well, you hope there's some outgoing edge in the residual graph that has positive residual capacity.
00:10:38.318 - 00:10:38.980, Speaker A: Okay?
00:10:41.190 - 00:10:55.794, Speaker B: So you start at v. You look at the outgoing edges and the residual graph. And again, so for this whole lecture, unless I specify otherwise, when I say an edge in the residual graph, I always mean an edge with positive residual capacity.
00:10:55.842 - 00:10:56.054, Speaker A: Okay?
00:10:56.092 - 00:11:11.040, Speaker B: So all those zeros, just think of them as deleted from the residual graph. Okay, so you want to find an outgoing edge from V in the residual graph with positive residual capacity. And then you just want to route as much flow as you can on the edge from V to W.
00:11:14.930 - 00:11:15.246, Speaker A: Now.
00:11:15.268 - 00:11:34.198, Speaker B: If you think about it, there's actually two different constraints on how much flow you can push from V to W. The one which you're very used to is the capacity constraint. So if the residual capacity is six, you'd only push six units of flow along that edge just like before.
00:11:34.198 - 00:12:00.670, Speaker B: But now remember, we need to have this flow in as at least flow out or equivalently excesses need to be non negative. So if you have a vertex and its excess is only three, even if the residual capacity of this outgoing arc is six, you can only send three units of flow that drives the excess down to zero, and we're not allowed to make it negative. So delta is just the smaller of these two possible bottlenecks, the most we could possibly push without violating any constraints.
00:12:00.670 - 00:12:24.630, Speaker B: So the minimum of V's excess and the residual capacity of e, okay, where this is defined in the usual way in residual networks. So there's the forward arcs, there's the backward arcs, et cetera. And now what do you do you push delta units on this edge.
00:12:24.630 - 00:12:47.034, Speaker B: And so this augmentation works exactly the same way as in path augmentation. If this edge e corresponds to a forward edge of some network, then you increase the flow on that edge in the original network. If this is a backward edge, then increasing by delta really means you reduce the flow in the original network on this edge.
00:12:47.034 - 00:13:15.474, Speaker B: Okay, so just like an augmenting paths so that's the push subroutine. So notice that if there wasn't already an excess at w, this is going to create an excess at w, right? Because when you push on VW, you increase the flow going into w, but you're not increasing the flow going out of w. Okay? So the excess will definitely be positive after the push.
00:13:15.474 - 00:13:17.474, Speaker B: It might have been positive already beforehand.
00:13:17.602 - 00:13:18.280, Speaker A: Okay?
00:13:22.570 - 00:13:33.754, Speaker B: All right, so that's a basic subroutine, the push subroutine. So you change a preflow just by increasing or decreasing the flow on a single edge at once. Now, this isn't going to be good enough.
00:13:33.754 - 00:13:58.210, Speaker B: We need some more ideas to get a bona fide max flow algorithm. And one thing we could be concerned about is just pushing flow around in cycles forever. So think about the following network, say, with all unit edge capacities.
00:13:58.210 - 00:14:17.500, Speaker B: So if you push some flow here and you get an excess of one here, you can then push on this edge that just moves the excess over here. You can push on this edge that moves the excess down here. Of course you want to push it to t.
00:14:17.500 - 00:14:23.130, Speaker B: But conceivably, if you don't know what you're doing, you could just push this around the cycle over and over again.
00:14:23.280 - 00:14:23.786, Speaker A: Okay?
00:14:23.888 - 00:14:31.722, Speaker B: So just because you zero out the excess at one vertex, it doesn't automatically mean you're making progress. It might just be like that excess reappears at some other vertex.
00:14:31.786 - 00:14:31.966, Speaker A: Okay?
00:14:31.988 - 00:14:51.222, Speaker B: So it could be an infinite game of whack a mole, if you know what that is. All right, so we need some principled way of figuring out how to apply pushes so that this doesn't happen. So that first of all we terminate, but then also second of all, of course we want to be correct.
00:14:51.222 - 00:14:52.886, Speaker B: We want a max flow at the end of the day.
00:14:52.988 - 00:14:53.640, Speaker A: Okay?
00:14:54.490 - 00:15:44.022, Speaker B: So here's the nice idea we're going to use to argue all the correctness properties of this algorithm for each vertex. We're going to give it a height it and I really encourage you to think about a graph now as just living in 3D, okay? So all the vertices have an x y coordinate, and then the height just corresponds to a z coordinate. So there's a notion of some vertices being higher than other vertices, okay? So you maintain a height for each vertex subject to three invariants, and the first two invariants are sort of very simple.
00:15:44.022 - 00:16:01.100, Speaker B: The third one is the interesting one. So first of all, the height of the source is just n. So remember, n is the number of vertices, okay? And it never changes, okay? That's an invariant source is always at height n.
00:16:01.100 - 00:16:14.560, Speaker B: The sync is always at height zero. And again, it never changes. So all the action, all the heights, they only change on the other vertices other than S and T.
00:16:14.560 - 00:16:34.006, Speaker B: And so here's the interesting invariant. Interesting invariant says that at all times in the residual network, the current residual network, the arcs of the network, again, arcs with positive residual capacity. Let's see.
00:16:34.006 - 00:16:37.574, Speaker B: So basically, they can't go downhill too quickly.
00:16:37.772 - 00:16:38.230, Speaker A: Okay?
00:16:38.300 - 00:17:06.430, Speaker B: So let me write that in math. So for all edges in the residual graph from V to W h of V, so this is the tail is almost one more than the height of the head. So for a picture, imagine you have a vertex v, and it has three outgoing neighbors.
00:17:06.430 - 00:17:40.368, Speaker B: And suppose it just so happens that the three neighbors are at the heights three, four, and six. Then for this invariant to be true, v better be at most what's the biggest height I can give to V and have the invariant still hold? Four. Good, right? So basically, it can only be one more than any of the outgoing edges.
00:17:40.368 - 00:17:49.850, Speaker B: So it has to be at most one more in particular than the minimum of the vertices on outgoing edges. So this three constrains V's height to be at most four.
00:17:50.220 - 00:17:50.970, Speaker A: Okay?
00:17:52.060 - 00:18:13.810, Speaker B: So what we see from this example is that edges in the so if this is satisfied, then there's three cases for edges in the residual network, they can go uphill, like from V to w three. They can stay flat like from V to w two. And if they go downhill, they only go downhill one step.
00:18:13.810 - 00:18:17.776, Speaker B: That's the case of v from w to w one.
00:18:17.958 - 00:18:18.690, Speaker A: Okay?
00:18:19.700 - 00:18:28.870, Speaker B: Now, I don't ask you to have any intuition for these invariants yet. I'll explain where they come from in a second. But can everyone sort of parse them clearly? Someone understand what they're saying.
00:18:28.870 - 00:18:44.070, Speaker B: It all right. So how would you ever think of these invariants? Where would these come from? Well, here's the point.
00:18:48.120 - 00:18:49.070, Speaker A: You it.
00:19:01.380 - 00:19:22.180, Speaker B: So I claim that if the invariants hold, then a familiar condition is true. Namely, the residual graph at this moment in time does not have a path from S to T. S and T are disconnected in the current residual graph.
00:19:22.180 - 00:19:50.120, Speaker B: These invariants one through three for a little bit and see if you can see why this is true, why they imply that S and T have to be disconnected in the residual graphic.
