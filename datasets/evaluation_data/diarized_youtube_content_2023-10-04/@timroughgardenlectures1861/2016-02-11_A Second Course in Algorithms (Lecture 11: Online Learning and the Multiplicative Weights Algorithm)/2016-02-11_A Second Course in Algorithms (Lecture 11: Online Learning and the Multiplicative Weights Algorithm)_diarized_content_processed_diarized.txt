00:00:00.410 - 00:00:22.570, Speaker A: Okay, so the plan for this week is it's going to be sort of a segue between the second module of the course on linear programming and the third module out of four of the course on online algorithms. So we are going to start talking about our first online problem, our first online algorithm. Today we're going to cover a famous algorithm called the multiplicative weights update method.
00:00:22.570 - 00:00:54.320, Speaker A: But then also I'll show that there are consequences for our last couple of topics, linear programming and the Min max theorem that are derived from the multiplicative weights algorithm. So we're going to both introduce some new concepts but also reinforce the things we talked about last week. All right, so what do I mean by an online problem? Well, this term was invented in the 80s, so it sounds a little anachronistic at this point, has nothing to do with the internet or with social networks, anything like that.
00:00:54.320 - 00:01:22.920, Speaker A: What an algorithms person means by online is that the input to the problem arrives piecemeal one at a time in some sense. So the input arrives one by one. Now what this means sort of depends on the problem, right? But so for example, if you're scheduling jobs on machines, you could imagine that the jobs arrive online, meaning one by one, or maybe you're solving some graph problem and you get a new vertex of the graph at each time step.
00:01:22.920 - 00:01:48.158, Speaker A: And what makes it hard is that your algorithm has to make some kind of irrevocable decision at each time step. So algorithm makes a revocable decision each time step. Okay, so maybe you have to decide which machine the new job goes on.
00:01:48.158 - 00:01:52.846, Speaker A: Maybe you need to decide whether or not to match the new vertex that just showed up, this kind of thing.
00:01:52.948 - 00:01:53.600, Speaker B: Okay?
00:01:54.850 - 00:02:12.120, Speaker A: And if you think about it for a second, you realize actually like a lot of problems in real life, both computational and otherwise, really are online problems. So it doesn't really get any airtime in CS 161. But if you think about it, the kinds of problems you might design algorithms for in the future, likely some of them will be online problems.
00:02:12.120 - 00:02:36.746, Speaker A: So that's what we're going to talk about in for a couple of weeks. And the subject of today is a very simple model of making decisions online. So this is a model that was actually first studied in the 50s in game theory, but it's been rediscovered many times, in particular in machine learning theory.
00:02:36.746 - 00:03:00.040, Speaker A: They also have sort of independently studied this model. This problem, I think it's interesting in its own right, but as we'll see there's also implications for both linear programs and for the minimax theorem. So what's the setup? So there's a set of actions that's just fixed and we know what they are.
00:03:00.040 - 00:03:13.030, Speaker A: In today's lecture, n will denote the number of actions and this problem is already interesting. If n equals two, that's still a nontrivial case. So what might this mean? This can mean a lot of things, but for example, maybe every action is a stock.
00:03:13.030 - 00:03:23.310, Speaker A: You're trying to choose how to invest or maybe you're trying to figure out how to get to class from home. And there's different routes you could take. So the different actions could be the different routes.
00:03:23.310 - 00:03:57.526, Speaker A: Okay, that's the first thing. So what's the online aspect? Well, there's going to be reward vectors r one up to R capital T. So each reward vector specifies for each action at a given time, step T, the payoff that you will gain from picking that action at that particular day.
00:03:57.628 - 00:03:58.280, Speaker B: Okay?
00:03:58.670 - 00:04:06.380, Speaker A: And so these rewards can be positive or negative. We can scale with Alice of generality for our purposes. So they're between minus one and one.
00:04:06.380 - 00:04:48.166, Speaker A: So these arrive online, meaning what? At a time in a sequence, every time you have to make some decision. And what makes it really tricky is that at time T, the algorithm has to choose an action to play on day T at knowing only the reward vectors that have shown up in the past. So knowing only R one up to R T minus one.
00:04:48.166 - 00:05:02.622, Speaker A: Okay, so when you have to pick an action at day T, you don't actually know what the reward of every action is. Of course, if you knew all the rewards up front, you just pick the action with the highest reward. But if you don't know what the rewards are going to be, it's really not clear what to do.
00:05:02.756 - 00:05:03.440, Speaker B: Okay?
00:05:05.810 - 00:05:19.246, Speaker A: And I should say when I say chooses, randomizations allowed. And actually, as we'll see, randomization is necessary for good guarantees in this problem.
00:05:19.348 - 00:05:19.578, Speaker B: Okay?
00:05:19.604 - 00:05:32.658, Speaker A: It's sort of important that you hedge your bets given that you don't know what the rewards of different things are going to be. Okay, so that's the setup. So each day you choose an action from a, you know, the reward vectors from the previous time steps.
00:05:32.658 - 00:05:57.294, Speaker A: But what really governs your reward is what is the reward unknown reward vector for right now for day T. And if you think about this a little bit, should seem kind of unfair, right? I mean, you have this algorithm in effect, you have to make this action and you know absolutely nothing about the rewards of the different actions. So you have no clue what the consequences of the different actions are and somehow you got to pick.
00:05:57.294 - 00:06:15.258, Speaker A: So if you could get any kind of nontrivial guarantee in this very unfair setting, one should be pleased. On the other hand, you should also be thinking, okay, well, is anything nontrivial even possible? Maybe you're just sort of always there'll be reward. However smart your algorithm is, there'll be instances where you do terribly.
00:06:15.258 - 00:06:42.400, Speaker A: So that's the next thing I want to talk about. What kind of guarantee should we be shooting for with this unfair setup to the problem? So I'm going to show you some limitations on what we could possibly expect. But then I'm going to give you an algorithm which actually, given these limitations, is as good as could possibly be.
00:06:42.400 - 00:07:20.882, Speaker A: So what are the limitations? So, first of all, well, how would you try to convince somebody that your algorithm was really good, that you did a great job of sort of choosing actions over time? Well, probably the most compelling statement you could make would be to say, well, suppose it were the case that even if I were omniscient and I knew all these reward vectors up front, I couldn't have done much better than how my algorithm actually did. So my algorithm, even though it didn't know the reward vector at the current time step, was almost as good as if I had clairvoyance and I knew what it was going to be. Okay, so that would be the strongest imaginable guarantee.
00:07:20.882 - 00:07:49.890, Speaker A: And it's too strong so it's too strong a benchmark in this setup. So the first thing you might want to try to do, you can't do can't compete with. Okay, so if you were omniscient and you knew everything up front, what would be your reward? Well, on each day from one to capital T, you would just pick the action that at day T gives you the largest reward.
00:07:49.890 - 00:07:59.510, Speaker A: Okay, so do you agree? So in hindsight or with foreknowledge, this would be the total reward that the obvious algorithm would get.
00:07:59.660 - 00:08:00.360, Speaker B: Okay.
00:08:01.050 - 00:08:13.926, Speaker A: And it'd be nice if we could come up with an algorithm that's online and does almost as well. So why can't we do that? Well, pick your favorite algorithm, even. Just suppose there's two actions.
00:08:13.926 - 00:08:27.710, Speaker A: Okay, so pick your favorite algorithm. It can be randomized. And now I'm going to play the role of an adversary and I'm going to look at the code of your algorithm and I'm going to feed you kind of really nefarious reward vectors.
00:08:27.710 - 00:08:35.458, Speaker A: How would I do that? Well, I know the code of your algorithm, so I just look. Okay, it's day 17. I know what happened in the last 16 days.
00:08:35.458 - 00:09:02.860, Speaker A: What is your probability distribution over the two actions on day 17? And if it's say, 60 on the first action and 40 on the second action, I'll just give you a reward vector, which is minus one on action one and one on action two. Okay, so every day, whatever you're more likely to play, I'll feed your reward vector where you get minus one. Then, but if you sort of get lucky and choose the action that you're less likely to pick, then you get a plus one.
00:09:02.860 - 00:09:31.430, Speaker A: So with an adversary like that, which always feeds in a vector, which is either plus one minus one, or minus one plus one, what is this number? T. Right. Each day, little T, there are two actions.
00:09:31.430 - 00:09:43.226, Speaker A: One has reward plus one, one has reward minus one. So the max is certainly plus one summed over capital T days. Okay, so in hindsight, however, the adversary did this as long as it only did plus minus one vectors.
00:09:43.226 - 00:10:02.606, Speaker A: You can get T in hindsight. So the adversary can ensure that this notion of opt equals capital T. Well, what does our algorithm get? Well, the best case for us is that we were picking an action 50 50, uniformly at random.
00:10:02.606 - 00:10:15.560, Speaker A: Then we have a half a chance to get plus one, a half a chance to get minus one. If we're picking anything other than 50 50, we're only more likely to pick up the minus one. But in any case, the expected reward earned by our algorithm is going to be zero or less.
00:10:15.560 - 00:10:35.390, Speaker A: So that's a quite big gap between how well any algorithm could do. Notice the algorithm was arbitrary. So the claim is, for every single algorithm, no matter how smart, just by virtue of being online, you're stuck with this huge gap between how well you might do and the best action sequence in hindsight.
00:10:35.390 - 00:10:53.410, Speaker A: Any questions about that? So this is the first limitation. So the first super strong benchmark you'd dream up is too strong. So here's a key idea, and this is sometimes known as regret minimization.
00:10:53.410 - 00:11:05.110, Speaker A: So we're going to compare to a weaker benchmark in which we swap the max and the sum.
00:11:05.530 - 00:11:06.326, Speaker B: Okay?
00:11:06.508 - 00:11:32.910, Speaker A: So in English, what we're going to ask, the way we're going to try to convince somebody that our algorithm did really well is we'll say, you know what, suppose we at least had foreknowledge not of the reward vectors, but of the best action. Suppose little birdie told me that if you were just going to pick one action, you should pick action seven all the time. Okay, so let's see how well the best fixed action does and let's compare our algorithm to the best fixed action in hindsight.
00:11:32.910 - 00:12:08.390, Speaker A: And so again, what is that? That's really just exchanging the maximum and the sum. This is what we'd like to get. We'd like to do as well as the best fixed action in hindsight.
00:12:08.390 - 00:12:29.614, Speaker A: So we're going to examine the difference between our benchmark and how well we actually do. Okay, so we have some algorithm, it's choosing, perhaps randomly, some action at, and this is our total reward over all of the days. So this is what we're going to be focusing on for the rest of the lecture.
00:12:29.614 - 00:12:46.674, Speaker A: This is also called the regret of an action sequence or of an algorithm for a given sequence of reward vectors. Okay, yeah, no, which is important. It's one action.
00:12:46.674 - 00:12:56.026, Speaker A: So let me actually write A in capital A for emphasis. So fixed action. So by fixed I mean independent of the time t.
00:12:56.026 - 00:13:14.082, Speaker A: And why? Well, there's a few reasons, but the first reason is just you can't allow these sequences. You can't maximize over someone who does something different on the different days. You're going to get no nontrivial advice from the theory about what kind of online algorithm you should use.
00:13:14.082 - 00:13:22.786, Speaker A: Okay, good question. All right. So we'd like to get close to this.
00:13:22.786 - 00:13:28.902, Speaker A: So we'd like our regret to be as small as possible. We'd love it, for example, to be zero or maybe even negative. That'd be great.
00:13:28.956 - 00:13:29.174, Speaker B: Okay.
00:13:29.212 - 00:13:56.186, Speaker A: But as the regret gets really big, it means our algorithm is doing poorly with respect to this benchmark. Okay, so why this benchmark? So that'd be a good question. Where does this come from? And in some ways will justify the ends, will justify the means in the sense that by looking at this benchmark, we'll be able to really sort of focus on some cool algorithms that have a lot of other applications.
00:13:56.186 - 00:14:06.554, Speaker A: So first of all, this one, as we'll see, is basically achievable. You actually can have an algorithm that competes with this benchmark. And secondly, a lot of algorithms don't compete with this benchmark.
00:14:06.602 - 00:14:06.766, Speaker B: Okay?
00:14:06.788 - 00:14:34.810, Speaker A: So it's not trivial to get this number close to this number, but if you have a sufficiently clever algorithm, you can do it. Okay, so that's a win for the theory, right? It's sort of a framework which sort of guided us to an interesting algorithm for this problem that we might not have come up with otherwise. And then the other reason why I sort of justified is even if you can compete with this weaker benchmark, that's sufficient for the applications I mentioned in linear programming and game theory.
00:14:34.810 - 00:15:07.890, Speaker A: Okay, it's any questions to the goal is going to be to design an algorithm so that the regret is small, ideally close to zero. Okay, again, just for emphasis, I want to emphasize that when we design our own algorithm, we are perfectly allowed to choose different actions on each day. But in the benchmark, in the left term, we're thinking about the best fixed action in hindsight.
00:15:07.890 - 00:15:27.020, Speaker A: All right, so I said that even getting this regret close to zero, it's not trivial. So let me show you more concretely what I mean. So first of all, you really need randomization to have any hope of getting the regret to be small.
00:15:27.020 - 00:15:46.970, Speaker A: So deterministic algorithms, not good enough. And I think most people when they first start trying to think about how they might what should an algorithm do? I think you'd probably start with a deterministic algorithm. I actually think probably a lot of people would start with what's called follow the leader.
00:15:46.970 - 00:16:05.762, Speaker A: So what is this? So you're at day 17, you have no idea what the rewards of the different actions are going to be. All you know is what happened in the first 16 days. Okay, well, sort of a natural heuristic would say, well, which action has been the best historically, meaning summed over those 16 days has the highest cumulative reward.
00:16:05.762 - 00:16:11.682, Speaker A: Let's pick that one. Okay, so that's the follow the leader algorithm. That's a deterministic algorithm.
00:16:11.682 - 00:16:22.860, Speaker A: Okay, so given the past, you put all your eggs in one basket and you play the best action. You but the claim is not just follow the leader, but any to.
