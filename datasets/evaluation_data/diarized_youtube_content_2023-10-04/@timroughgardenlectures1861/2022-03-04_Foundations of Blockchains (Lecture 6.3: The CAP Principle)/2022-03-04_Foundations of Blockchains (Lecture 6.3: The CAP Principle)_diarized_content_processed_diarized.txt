00:00:00.490 - 00:00:50.026, Speaker A: All right, so I want to wrap up lecture six by connecting what we've been discussing thus far to a famous principle in distributed systems known as the Cap Theorem, which is an articulation of fundamental trade offs that come up over and over again in the design of distributed systems. By virtue of being called the Cap Theorem, maybe you think this was something that was proved by authors with last names starting with C-A-P like the FLP theorem is for Fisher, Lynn and Patterson. That's in fact not true. So the Cap theorem was first stated by Eric Brewer and then was proved not long after by Seth Gilbert and Nancy Lynch. And that's all stuff from sort of the early two thousand s. All right, so if CA and P don't stand for the last names of authors, what do they stand for? Let me walk you through them one by one. They stand for properties that you would really like a distributed system to have.
00:00:50.026 - 00:01:29.898, Speaker A: And to interpret these three properties, it might be helpful to just have that same old old school consensus application in mind where you have some sort of big company like IBM just trying to replicate a database. You can have multiple servers, each with its own copy of the database to try to get really high uptime. All right, so the C stands for consistency. It plays a similar role that consistency has played for us in our discussions of state machine replication. So for example, this would mean that if you issue a database query, the answer you get should not depend on which of the replicas your query gets sent to. All of the replicas should agree on what the answer to some database query is. More generally, in a distributed systems context, you want the user experience.
00:01:29.898 - 00:02:18.642, Speaker A: So from the user or client perspective, it should be indistinguishable from interacting with a centralized system, like a single database residing on a single server. The A stands for availability, and that's going to play the similar role that liveness plays for us in state machine replication. So for example, in the database example, if some user inserts a new row to one of the tables in the database, eventually the queries to the database should reflect that new insertion. So more generally, any command that a client issues to the system should eventually be carried out. And finally, the P stands for partition tolerance. And that's very much like having the Asynchronous phase in our partially synchronous model. So partition tolerance means you would like these other two properties, consistency and availability.
00:02:18.642 - 00:03:08.150, Speaker A: Ideally you'd like them to hold even under a network partition, that is, even when you actually have two sets of nodes with all communication in between the sets completely blocked off. Let me just draw a quick cartoon of a network partition in the lower right part of the slide, so it's clear what I mean. So the assumption is you have two sets of nodes, A and B. Maybe A is like 90% of the nodes and B is 10% of the nodes that have been cut off. And again, so that no two nodes in different parts of the partition are able to communicate with each other. All messages trying to cross the partition are blocked. You could imagine this kind of network partition arising, for example, because of a denial of service attack, right? So somebody just sort of rents out a botnet, they all blast all of the nodes in the set capital B, to effectively cut them off from the rest of the network from the nodes in A.
00:03:08.150 - 00:03:45.966, Speaker A: So all three of these are obviously desirable properties for a distributed system to have. And the Cap theorem is an impossibility result which says you cannot have all three simultaneously out of CA and P, you must pick only two. And for us, the most relevant way to think about the Cap theorem is that if you have P, then you cannot have both CNA in the presence of a network partition. You must give up one of consistency or availability. And that is a fundamental trade off you need to think about in designing your system. The intuition behind the Cap theorem, I think it's fairly straightforward. So let me just walk you through sort of a proof sketch.
00:03:45.966 - 00:04:24.734, Speaker A: It would not be difficult to turn this into a formal proof, as was done by Gilbert and lynch. So imagine the distributed system is keeping track of some variable x. So like maybe x represents the number of times that the San Diego Padres have won the World Series. So currently x would be set equal to zero. Now imagine one of these years the Padres do win the World Series, and a Padres fan very excitedly wants to update the database to reflect that. So a client issues a command saying actually now set x equal to a new value one. And let's say that this command gets routed to a node that belongs to the bigger light blue set A.
00:04:24.734 - 00:04:54.086, Speaker A: So we'll call that node i. That's the node that receives this command from the client to change the value of the variable x. And now, future clients, let's assume that all anyone does is ask the distributed system to return the current value of the variable x. Well, now, if you think about it, if we've been in a network partition all along, so the picture is like it is in the lower right part of the slide. Pity poor node i. Node I is in a total Catch 22. Damned if it does, damned if it doesn't.
00:04:54.086 - 00:05:38.498, Speaker A: So one option is at some point, after enough sort of clients ask it about the current value at x. Maybe at some point it reports what it knows to be the new value of x x equals one. Then of course the problem is going to be a consistency violation. So because during this network partition, all communication is blocked between nodes of A and nodes of B. And because only this node I was informed about this update setting x equal to one, all of the nodes in B are unaware of it. So if you asked a node in B what's the value of x, they would say zero. So if I ever says one that violates consistency, the alternative of course is that node I just very stubbornly, always answers zero, never acknowledging the update commands it received.
00:05:38.498 - 00:06:12.894, Speaker A: That of course would be a violation of availability. A client issued a command to update a value and it's never going to happen. So this argument may strike you as a bit trivial and to be honest, it kind of is. But that doesn't change the fact that the Cap theorem is kind of a super helpful articulation of the fundamental trade offs that come up in system design. It sort of really clarifies what design decisions it is you need to be making. So for example, the Cap theorem makes it clear that you should stare at your application and decide whether or not you want to worry about network partitions. If you're running something sort of at the internet scale, probably you do want to worry about network partitions.
00:06:12.894 - 00:07:04.494, Speaker A: If you're just running sort of like a small private internal network in your company, maybe you're not so concerned about network partitions. And the Cap theorem says that in the latter case, if you don't actually expect network partitions, then you have no excuse to fail to provide either consistency or availability. If you don't have to deal with P, you really should be responsible for providing both CNA, both consistency and availability. This is very much how when we talked about the synchronous model, we insisted on consensus protocols that had both kind of safety and liveness properties. On the other hand, if in your application, you really do want to worry about network partitions, like this is something you're running kind of at a global scale, well, then the Cap theorem makes it very clear. You need to stare at your application and decide which would hurt more giving up on consistency or giving up on availability. Right? And different applications are going to be making different trade offs on this curve.
00:07:04.494 - 00:07:45.498, Speaker A: So for example, traditional databases have prioritized consistency over availability. So under a network partition they would just kind of basically stop operating. But they'd never be inconsistent. Things like NoSQL databases makes the opposite choice. They want to always be available at the cost that sometimes sort of different parts of the distributed system will have slightly different views of the world. So for example, you could imagine something like Google, they would probably be much more interested in availability than consistency. Suppose two people on different parts of the globe issue the same search query at the same time and get slightly different results because the local copies aren't 100% in sync.
00:07:45.498 - 00:08:23.478, Speaker A: Who cares, right? No harm, no foul. Presumably both got useful sets of search results. Everybody's happy. On the other hand, if you had a violation of availability, if you typed in a search query and just it hung for 30 seconds, the end user would be not so happy. That would actually be a pretty major fail. On the other hand, you can imagine how an institution like, I don't know, say like a bank would be much more interested in consistency than in availability, right? So for a bank, kind of the epic fail would be if kind of different branches of the bank actually had different views of what your account balance was. That's the situation that has to be avoided at all costs.
00:08:23.478 - 00:09:30.866, Speaker A: Meaning if there's a network partition, it has to give up on availability and it's just going to be the case that customers won't be able to do certain transactions for some period of time. Definitely not ideal, but probably from a bank's perspective, much more palatable than actually giving away free money that didn't exist in the account due to a mistake in consistency. So ultimately that's the takeaway from the Cap theorem. So step one, decide whether or not network partitions are important for your application. If they're not, then you should demand both consistency and availability. If you are worried about network partitions, know that you have to choose between CNA and make a smart decision based on your application of which one you want to keep. All right, so to wrap up lecture six, let me answer a question you may have, which is what's sort of the difference between these two famous results we've discussed, the FLP impossibility result concerning Byzantine agreement in the Asynchronous setting and this Cap theorem that we just discussed, after all, they seem to have pretty similar takeaways, right? I mean, they basically both say that when you're under attack, which in FLP meant you're in the Asynchronous model.
00:09:30.866 - 00:10:08.538, Speaker A: In the Cap theorem, it meant there's a network partition. But in any case, when you're under attack, you can't have both safety and liveness, where again, in the Cap theorem, liveness corresponds to availability. So either way, these results seem to be saying in the event of an attack, choose which one you want to favor. Favor safety or favor liveness. Okay, so very similar takeaways both from the FLP impossibility result and from the Cap theorem. Oddly, you don't usually see both of these results covered in the same university course. So the FLP theorem sort of shows up in distributed computing courses that have a fairly strong theoretical bent.
00:10:08.538 - 00:11:02.202, Speaker A: The Cap theorem shows up in sort of more systems y type courses, but obviously they are closely related, at least conceptually, saying very similar things. So given that something you might be finding mysterious, you might be thinking for the same conclusion. It sure seems like we had to work a lot harder for one of these than the other. The FLP impossibility result approved, consumed most of lectures four and five, whereas the sort of proof sketch of the Cap theorem that occupied, what, like one little corner of one slide? So I want to wrap up lecture six just by sort of explaining why it is FLP required so much more work to prove than the Cap theorem. In effect, it's going to be because the adversary is weaker. And so that makes any impossibility result both sort of more impressive and harder to prove. So let's just sort of compare and contrast the main features, the main assumptions in each of the two settings.
00:11:02.202 - 00:12:00.238, Speaker A: Now, first of all, for the FLP impossibility result, we made a big deal about how the adversary controlling message delivery is very powerful, largely unconstrained. But I also made a big deal the fact that there was actually one constraint on message delivery, which is every message is eventually delivered, whereas the argument in the Cap theorem does not respect this requirement. So in the Cap theorem, when we talk about a network partition, we mean a network partition that is permanent. So I put this distinction first, and this really is sort of the biggest distinction. This is the biggest reason why the FLP impossibility result is quite a bit harder to prove than the Cap theorem. If you remember the proof of FLP, you might even remember we had that notorious Lemma Two, right? So we had Lemma One, which gave us an ambiguous configuration to start in, and then repeated applications of Lemma Two extended the sequence of ambiguous configurations. And remember, we had this clunky version of Lemma Two which included like a target message that had to eventually be delivered at the end of that part of the sequence.
00:12:00.238 - 00:12:52.466, Speaker A: And the whole reason we did that is so that we could meet this constraint, so that we could make sure that every message is eventually delivered and there's no starvation. So if you go back to that proof, you'll see this really was what forced us to work hard. And in the Cap theorem, it just says, here's a network partition, it lasts forever, there's nothing you can do. So that's the sense in which the adversary that you face the message delivering adversary is much stronger in Cap than in FLP because it doesn't have to deliver messages, it can just throw them away. On the other hand, in the Asynchronous model, while it's true the adversary has to eventually deliver any message subject to that, it can do whatever it wants. So certainly one option for the adversary on the FLP side is to do a network partition can't last forever, but if it wanted, it could do a very, very long network partition. On the other hand, the adversary and the Asynchronous model could do anything else as well.
00:12:52.466 - 00:13:41.902, Speaker A: But in hindsight, this distinction is not so important. It really does seem like network partitions is the good kind of canonical attack that an adversary might carry out so really the big difference between the adversaries is in this first row having infinite message delays versus finite, if unbounded, message delays. Now, going back to the Tap theorem setting, I mean, if you think about it, not only is that argument more straightforward, but you don't even need faulty nodes, right? I mean, you do have this adversary controlling message delivery, but each of the nodes running the protocol could be honest. And the Cap theorem still applies. The FLP impossibility result, on the other hand, really does assume that there is faulty nodes. Okay, granted only one faulty node, but still one is more than zero. And if you think about it, this is not an accident.
00:13:41.902 - 00:14:28.058, Speaker A: I mean, the FLP impossibility result is just not going to be true. With little f equal to zero, you can in fact solve Byzantine agreements. Even in the Asynchronous model, when little f equals zero, what would that solution look like? Well, just have every node broadcast its private input to all of its N minus one colleagues, right? I mean, because little f equals zero, I mean, we're not worried about any nodes lying about their private inputs. We're also not worried about any nodes sort of not sending the messages that's supposed to be sending. So all of the right messages are going to get sent. The only worry would be like, will they ever get to where they're going? But that of course, is the constraint on the adversary. In the Asynchronous model, all of those messages that honest nodes are sending to each other will eventually arrive once everybody has all of their messages.
00:14:28.058 - 00:15:13.786, Speaker A: Everybody has exactly the same data at that point. Everybody knows what all of the private inputs were. And then all of the nodes are going to wind up making the same decision, for example, taking majority vote amongst the private inputs. And you think this through and now you realize that sort of the infinite message delays on the Cap side that really is very powerful for the adversary, right? The adversary does not even need any control over any of the nodes because its control over the message delivery is so strong. So to tie these results together, just even still a little bit more tightly, let me remind you that when we stated improved the FLP impossibility result, we were working with one Byzantine node, so little f equals one. That one was Byzantine. But I mentioned that you can tweak the proof a little bit so that it holds even with the most benign type of faulty node, a crash fault.
00:15:13.786 - 00:15:53.454, Speaker A: A crash fault, remember, means that the adversary controlling the faulty node. All it can do is pull out the plug of the machine at some point in the protocol. So the node is going to run the protocol honestly. Then at some point it goes away and never sends or receives any further messages. And if you think about it, a crash fault does at least a little bit resemble kind of infinite message delays, right? Because maybe the machine was about to send out a bunch of messages honestly following the protocol, and then it suddenly crashed. Well, that's kind of as if the messages it was about to send are delayed for an infinite period of time because the crash is permanent and never comes back online. So, in my opinion, really kind of the way to think about the FLP proof, really.
00:15:53.454 - 00:16:44.254, Speaker A: The essence of the argument is they show that a single crash fault already captures enough of the power of infinite message delays, of dropped messages to trigger the same conclusion that we have in the Cap theorem. Namely that when you're under attack, you must choose between safety and liveness. Okay, that concludes lecture six, where we introduced the partially synchronous model, maybe the most important model for the discussion of blockchain protocols. We showed the fundamental limitations of what you can hope to achieve in the partially synchronous model. And in particular, we learned where is this magical 33% you always see in kind of various protocol white papers. Where does it come from? We learned where it comes from and then we related that to the lessons of the Cap theorem. So the next order of business is to prove a matching positive result.
00:16:44.254 - 00:17:20.130, Speaker A: So to show that there actually do exist protocols that as long as less than a third of the nodes are Byzantine, do actually kind of solve consensus, like Byzantine agreement or state machine replication in the partially synchronous model. Now, there's been solutions to that problem all the way back to the 1980s when partial synchrony was first introduced. But we're going to prove the positive result in lecture seven using an actual real Blockchain protocol known as the tendermint protocol, which, for example, is the basis of blockchains you might have heard of, like cosmos and terra. So that's coming up in lecture seven. I'll see you there for tendermints. Bye.
