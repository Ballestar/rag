00:00:08.250 - 00:00:55.230, Speaker A: Good afternoon everyone. So I hope you can hear me well. So it is my immense pleasure to have with us today Tim Ravgarden, who will present his research in the talk entitled Beyond Worst Analysis. So let me first say first about team. So I'm quite sure that all of you are familiar with teams research. He's not really the person that needs a lot of introduction, but he is currently a professor in the Computer Science department, Columbia University. Prior to joining Columbia, he spent 15 years as a computer science faculty at Stanford, following a PhD at Cornell and a postdoc at UC Berkeley.
00:00:55.230 - 00:01:26.870, Speaker A: His main interest for many years have been on the boundary of computer science and economics. He has been also doing some amazing research in the design, analysis, applications and limitation of algorithms. He is a winner of lots of prizes. It's too long list really mention it. I think that we all know about his Kalai Prize in Computer science and game theory. Tucker's Prize. Goodel prize.
00:01:26.870 - 00:01:41.520, Speaker A: ACM grace Moray, hopper prize. And today we'll have real pleasure of Tim presenting to us his recent research on beyond worst case analysis. Tim?
00:01:42.850 - 00:01:59.582, Speaker B: Thank you, Arthur. So can you see the slides okay and hear me okay? Yes. Okay, great. Okay. Thanks very much Arthur, for the invitation to speak and the kind introduction. It's kind of funny. Just a quick story for the audience.
00:01:59.582 - 00:02:45.066, Speaker B: Arter and I actually go way back because we were two of the earlier people working on The Price of Anarchy. So it's really, I think, been about 20 years ago when I was a pretty junior graduate student that I first met Arter. So it's kind of cool just to fast forward two decades. And here we are. I also wanted to thank everyone in the audience for joining me. I know there's a lot of zoom fatigue in the world these days, so I'm happy you decided to spend an hour hearing about beyond Worst Case analysis. So what do I mean by beyond worst case analysis? Well, to motivate this area of algorithms, let me point out a kind of discrepancy between the way we teach algorithm design and the way we teach algorithm analysis in most undergraduate courses.
00:02:45.066 - 00:03:32.074, Speaker B: So we're all very comfortable with the idea that there's no bullet algorithm, there's no like, one algorithmic technique which is going to be the right one for every problem that you ever encounter. Right? We all sort of know that. So given that we don't have one, what do we teach our students? Well, we sort of give them a toolbox. So we teach them multiple algorithm design, paradigms thinking, like greedy algorithms, dynamic programming, et cetera. So techniques that work not for all problems, but for a healthy chunk of problems. So we equip them with these tools and then we also give them guidance about which sorts of problems tend to call out for which sorts of tools. On the other hand, the way we talk about algorithm analysis again, at least in the tip of algorithms course, is we act as if there actually is a silver bullet way to analyze all algorithms.
00:03:32.074 - 00:04:29.380, Speaker B: In other words, we focus almost single mindedly on worst case analysis. If you're lucky, might see an average case analysis of sort of hashing or of quick sort, but for the most part outside of advanced classes, the focus is entirely on worst case analysis. But the reality, which I think people do research in algorithms are all sort of well aware of, is there's equally no silver bullet analysis framework that is sort of the right tool for the job. No matter what problem and no matter what algorithm you're looking at, worst case analysis is not that and no other analysis framework is going to always be the right one either. So for example, maybe the most famous failure case of worst case analysis would be the running time of the simplex method for linear programming. So famously, instances called claymanti cubes show that the simplex method has worst case exponential running time. But it's basically impossible to ever discover instances in practice where the simplex method runs in anything other than very quickly.
00:04:29.380 - 00:06:09.218, Speaker B: A more modern example might be in the rise of deep learning, where the training problems that people solve to train deep neural networks theory will tell you that in the worst case those are hard problems and yet practitioners routinely seem to be able to solve them using really quite simple algorithms, even just things like gradient descent. So beyond worst case analysis is really about looking at sort of more nuanced or alternative analysis frameworks to align how easy or hard problems are in practice, or how good or bad algorithms are in practice to align the theory with what we observe. So what goes wrong with worst case analysis? Why are there these failure cases? Well, it's generally because first of all, it can have extremely pessimistic performance predictions. So the simplex method does not really almost ever run in exponential time, despite what worst case analysis tells you about it. And then that's particularly problematic when it just translates to terrible algorithmic advice, right? So like if you really wanted to solve a linear programming practice, you really would want the theory to tell you to use the simplex method over the ellipsoid method, whereas worst case analysis, if you take it completely at face value, actually tells you the opposite. And the reason this happens is because the strength of worst case analysis is in some sense also its Achilles heel, right? So the strength being that there's no assumptions about inputs at all, so you don't need to have any model of data to commit to. But then that also means you don't have the vocabulary to sort of articulate what might be special about sort of realistic sometimes.
00:06:09.218 - 00:07:09.746, Speaker B: If worst case analysis has a data model, it's something I would call the Murphy's Law data model. So Murphy's Law is the map that says anything that can go wrong will go wrong. And worst case analysis, it's a mathematically equivalent to the belief that the only instance of the problem that you might possibly care about is an adversarially chosen function of the algorithm you choose to use to solve the problem, which in a cryptographic application, maybe that makes sense. But for most applications of algorithms, that's a very paranoid and sort of logically inconsistent way to think about how to solve a problem. So what does it mean to go beyond this? Well, it basically means you do need some kind of model of data, right? We would still like to keep the assumptions as weak as possible, but you're going to need to have the courage to sort of deem some of the inputs of the problem more relevant than others. And beyond, worst case analysis offers a number of different tools for doing that. We'll see a couple of examples today.
00:07:09.746 - 00:07:48.974, Speaker B: Now, as I said, there's not going to be like one right way to do this that always makes sense for all kinds of problems and all kinds of algorithms. And that's why it's sort of rich. It's really a toolbox of analysis frameworks in parallel to the toolbox of algorithm design principles that we already have. So that's really what worst case analysis refers to. All right. And it's not a new idea. I mean, really, already in the 1970s, for sure, very well known theorists like Dick Carp and many others were looking at alternatives to worst case analysis with an eye toward sort of more meaningful algorithmic guarantees.
00:07:48.974 - 00:08:28.590, Speaker B: So this idea but I do think it's really kind of gathered new momentum over the past decade or so. I think it's just the pace of advancement has increased, the connections to practice have increased. And there's really kind of the last decade extremely exciting for progress along these lines. And so I'm pleased to announce that there's actually a new book which sort of covers all the latest developments. This literally just came out last month, published by Cambridge University Press. Because it's such a sprawling topic, it's a pretty sprawling book. So it's over 700 pages, sort of 30 chapters, 40 different contributing authors who you see here on the left.
00:08:28.590 - 00:09:00.422, Speaker B: As you can see, it's a stellar group of algorithms researchers. 30 chapters. Here they are today. I'll talk about results that touch on two different parts of the book. The results themselves are too new to have made it into the book, but most of the talk will be about smooth analysis. And there's three chapters on that in the book, chapters 13 through 15. And at the end, I'll say a bit about distribution free models of social networks, which again corresponds to a chapter 20, chapter 28 of that book.
00:09:00.422 - 00:09:28.180, Speaker B: But if any of this sort of piques your interest, I definitely encourage you to check it out. I think the last ten years have been really exciting, and I'm really optimistic that the next ten years are going to be really exciting. Plenty of open questions are stated in the vast majority of these chapters. Okay, so that's sort of what I wanted to say by way of preamble, arthur, how did you want to handle questions? Did you want to do them all at the end or what's sort of the norm?
00:09:29.350 - 00:09:38.550, Speaker A: If anyone has any question during the presentation, please put this in the chat, and I will just forward this to team. Otherwise, we'll have questions at the end of the presentation.
00:09:39.370 - 00:10:04.698, Speaker B: Perfect. Okay, great. Then let's move on to really the meat of the talk. I want to talk two recent results in Bianca's analysis that I've been involved in. Mostly I'll be talking about online learning. And so this is joint work with Nika Hagtalab, who's a professor at UC Berkeley, and her PhD student Abhishek Shetty. He's a PhD student at Cornell.
00:10:04.698 - 00:10:52.674, Speaker B: And then I'll also say a little bit about a beyond worst case approach to community detection in social network analysis. That's joint work with Eden Hussick, who's a PhD student at London School of Economics, advised by La C. All right, so the main result of the talks about an application of smooth analysis in online learning, and I don't want to assume that anyone necessarily knows what any of these phrases really means. So I want to start out with sort of a gentle introduction to smooth analysis. Then I'll give a gentle introduction to online learning, and then we can start talking about what are the main risk going to look like. Okay. All right, so smooth analysis in beyond worst case analysis, many, not all, but many of the analysis frameworks are sort of hybrids interpolations between average case and worst case analysis.
00:10:52.674 - 00:11:48.014, Speaker B: These are also known as semi run models, wherein an adversary and nature collaborate to come up with an algorithm, to come up with an instance, which then gets passed on to an algorithm. And one semirandom model, maybe the most famous one, one semirandom model is smooth analysis, which was invented by Men and Tang 20 years ago now with the context of analyzing the running time of the simplex method. And so the way Spielman and Tang sort of described it, they said, let's think about a two step process whereby the adversary goes first. The adversary picks whatever nasty input it wants. So, for example, if you're dealing with linear programming, the adversary might pick one of those clay minty cube instances, and then nature goes second. Nature is going to flip some random coins and apply a small perturbation to the input. And the goal then is to argue that the algorithm does well no matter what the adversary does in step one.
00:11:48.014 - 00:12:39.082, Speaker B: So no matter how nefarious the choice of the algorithm and choosing its initial input, once you involve a small perturbation by nature in expectation, an algorithm will form well. So, for example, run in polynomial time in expectation for any fixed choice by the adversary in step one. That's how Spielman and Tang described it. These days, people think about it, it's basically equivalent, but it's a slightly different way of talking about it is you think about the adversary as choosing not an input but an input distribution. Now, we want to rule out worst case adversaries, so we don't want to allow the adversary to just deterministically choose a single input. So we assume a lower boundary amount of entropy in the input distribution. A fancy way of saying it might say the adversary can do whatever it wants as long as its input distribution has sufficient anticoncentration.
00:12:39.082 - 00:13:09.970, Speaker B: And a canonical example you might want to think about is like, okay, the adversary doesn't deterministically pick one input. It chooses kind of like a really small set of inputs and then picks a uniformly random input from that super small set. That would be sort of the way to interpret this. And then again, the goal is the same. You'd like to say no matter what the adversary does, no matter what sufficiently diffuse input distribution it is, your algorithm performs well. It has low running time in expectation. Okay? So that's smooth analysis.
00:13:09.970 - 00:14:19.214, Speaker B: Now this is one of many, many different ways of going beyond worst case. And so in general, when you have many options like this, it's sort of my responsibility or the responsibility of all researchers to give advice about when might this be the right tool for the job? So when would you want to analyze an algorithm this way rather than some other way? So let me tell you about sort of two signatures of a problem and or algorithm that suggest smooth analysis could be a good idea. Okay? Signature number one is just you have an algorithm, you've identified some bad inputs for that algorithm, but just by inspection, you look at those bad instances and they're super brittle, right? So if you perturbed them a little bit, you suspect they probably flip from being bad instances for the algorithm to being good instances for the algorithm. And so if any of you have ever seen the construction of the claimant e cubes the four simplex method to run an exponential time, you look at those examples and intuitively they feel extremely delicate, extremely brittle. Like if you messed with the numbers just a little bit, probably the whole house of cards would collapse. Okay? So that's a good sense that maybe smooth analysis is a good tool to use. Here's a second sanity check.
00:14:19.214 - 00:15:29.906, Speaker B: This really applies more generally to all semirandom models, not just smooth analysis, but because it's wedged in between the two extremes of average case analysis with respect to the uniform distribution and worst case analysis. A necessary condition for sod analysis to be interesting is that there should be a significant gap between what is true in the average case and what is true in the worst case, it should be the case that there are much stronger positive results under the stronger assumption of average case analysis. Otherwise, if the almost the same, there's no wiggle room for smooth analysis to be interesting. Now secondly, when you have a problem where you do have this really big gap between the average case and the worst case, now it's totally clear exactly what you should be trying to prove when you do the analysis. You should prove that you get positive results that are as close to as good as you get in the average case as possible. So ideally, you basically extend the average case results under the strong assumption of, say, uniformly distributed inputs. You extend those positive results to the much weaker assumption of an adversary who just has to inject a little bit of randomness into its inputs.
00:15:29.906 - 00:16:19.454, Speaker B: So best case scenario is the second two boxes on the slide more or less collapse. That's what you're sort of hoping for. So that's what smooth analysis is and that's when you might think about using it. Okay, there's now a pretty nice literature that's developed over the last 20 years. Let me just sort of tell you what I regard as the top two killer patients. First one is the original application sort of pioneered by Spielman and Tang, which is proving that the simplex method runs in expected polynomial time in the smooth case. Specifically, the model here that Spielman and Tang considered an adversary picks a worst case linear program and then each entry of the constraint matrix of that LP is perturbed by an independent Gaussian random variable with small variance.
00:16:19.454 - 00:17:08.622, Speaker B: And so they showed that as long as the variance of each of those Gaussian sort of perturbations is at least one over polynomial in the dimension, then the expected running time of the simplex method is indeed polynomial. That's a result that has been sort of simplified and improved over the years. And if you want to do a deep dive on this, I highly recommend chapter 14 of the BWCA book by Daniel Ditch and Sophie Juliel. That may be the most well known application of smooth analysis, but one that I really believe is just as interesting has been developed over the past 15 years or so. There's a lot of papers here. I've only shown you some earliest ones on the slide. There's a lot more recent ones as well, which is smooth analysis of running time of local search algorithms, like a canonical one might be, say, the two opturistic for Tsp.
00:17:08.622 - 00:17:53.570, Speaker B: And so to be clear here, when I talk about smooth analysis, I'm not talking about the quality of the local optima that you converge to. I'm just talking about the number of iterations of local search you need to run before you converge to some local equilibrium, local minimum or maximum, which may be a good one or maybe a bad one. I'm not going to worry about that. Just like with the simplex method, in practice local search pretty much always converges very quickly. It sometimes converges to a bad equilibrium, a bad optimum. We're not worrying about that, but it converges quickly. Like the simplex method, there are extremely delicate constructions showing that most local search algorithms of interest can require an exponential number of iterations to converge to a local optimum.
00:17:53.570 - 00:18:33.790, Speaker B: Finally, just like with the simplex method, smooth analysis comes to the rescue. And it turns out that for most of the combinatorial local search algorithms that we care about the most, we now actually understand that in a smooth model with slightly perturbed inputs, the expected number of iterations till convergence actually is polynomial on the input size, not exponential like it is in the worst case. Here. There's a nice survey in the book by Bodomanthi. That's chapter 13. I also don't want to give a short shift to chapter 15 of the book that's by Heiko Roglin and that's about smooth analysis of the size of pareto curves, which then has implications for certain enumeration algorithms. So this is kind of what's out there.
00:18:33.790 - 00:19:07.654, Speaker B: And the result I want to tell you about I really think is a fundamentally different, fundamentally new type of killer application for smooth analysis. In particular, I will not be discussing running time analysis. Unlike everything else I told you about in online learning, the primary focus is on the number of prediction mistakes that you make. The foremost objective is regret minimization. And then once you've got that right, then you start worrying about computational efficiency. So it's really sort of totally different of algorithm performance. We will show that smooth analysis again comes to the rescue.
00:19:07.654 - 00:19:52.140, Speaker B: Worst case analysis is not helpful, but you can recover average case results even in a smooth analysis setting. Okay, so that's where we're going. So that's introduction to smooth analysis. Let me give you an introduction to online learning minimizing, number of prediction mistakes, and at least some of you out there I'm sure, are aware that online learning, regret minimization, that's a really well studied topic. There are literally dozens of papers, if not hundreds of papers published every single year on this model and its generalizations. These next couple of slides are for those of you that maybe aren't so familiar with online learning. And I just want to give you a very gentle introduction to you before we move on to kind of sort of the general version of the problem.
00:19:52.140 - 00:20:27.578, Speaker B: So I'm going to introduce this through a game which is going to take place on the unit interval, where again, basically you want an algorithm that makes predictions and you want to minimize the number of prediction mistakes. So here's the way it's going to work. It's going to seem pretty unfair and that's because it is going to be pretty unfair. So an adversary will choose a point that they want you to predict. Okay, let's say they say they pick a point from the unit interval between zero and one. Say they say one half. And now secretly, each point of the unit interval is colored either red or blue.
00:20:27.578 - 00:20:52.790, Speaker B: All right? And your algorithm then needs to guess. Do you think one half is actually colored red or do you think it's actually colored blue? After seeing your guess, the adversary then reveals the actual color of the points. Okay, so it says one half. You say either red or blue. And then the adversary says either blue. And the goal is to play this multiple times. You want to minimize the number of mistakes.
00:20:52.790 - 00:21:15.940, Speaker B: So here on the right, this is after the game has been played three times. As you can see, the adversary has asked about three different points, the XTS or the little x's on the line. On the bottom are the algorithms guesses. So it guessed red, red, and blue. And on top are the actual labels. So red, blue, and blue. So you can see that there's exactly one mistake the algorithm made, and that's on the middle point.
00:21:15.940 - 00:22:03.406, Speaker B: Now, many of you have already noticed that this is a totally impossible game, right? Because the adversary gets to reveal the color after we've made our guests. So the adversary can literally just say blue whenever we say red and say red whenever we say blue and force us to make a mistake at every single time state. So let's restrict the adversary a little bit beyond that to try to make this nontrivial. Let's say, you know what, I'm going to promise you that whatever the coloring of the unit interval is, it's very well structured. So it corresponds to what's called a one threshold. So I promise you the unit interval is a red segment followed by a blue segment. So the only thing you don't know in advance is exactly where the transition point is, where it turns from red to blue.
00:22:03.406 - 00:22:46.386, Speaker B: Okay. The adversary is going to be restricted so that whatever it's told you, whatever sort of points and colors it's given out, those are consistent with one of these 1D thresholds. All right? But you think about this a little bit and you realize, actually we're still toast. There's still nothing we can do, right? Why? Well, obviously we might make mistakes early on. The hope is that we learn from our mistakes and therefore get accurate over time. So the adversary maybe just asks us about one half and who knows? We know nothing about the coloring, so maybe we guess blue. Who knows? And the adversary says, ha ha, no, sorry, it was red.
00:22:46.386 - 00:23:18.780, Speaker B: Okay, so at this point and that's totally consistent with any 1D threshold where the transition point is to the right of one half. Okay, notice we have learned from this mistake, right? We do now know that the entire left half of the unit interval has to be red. So if you asked us about any point less than one half, we would definitely know what the label is. It would be red. We just don't know about to the right of one half. So naturally, the adversary will next ask us about something to the right of one half, say three quarters. Again, it could be red or blue, we don't know.
00:23:18.780 - 00:23:42.546, Speaker B: Maybe we guess red. Maybe we guess that the threshold is even further to the right. Then the adversary can say, no, it's actually to the left. The threshold is somewhere between one half and three quarters. So now again, we know that anything to the right of three quarters is blue, but we don't know what's up between one half and three quarters. So the adversary can ask us about five, eight. Again, maybe we guess that it's blue and the adversary says, no, it's actually red.
00:23:42.546 - 00:24:24.770, Speaker B: And so now we know the threshold is between five 8th and three quarters. And clearly we can continue doing this till the cows come home. We can do this for as many steps as we want. We do keep learning with every single mistake, we keep narrowing the window of ambiguity. However, the window remains always non empty. So for an arbitrarily long period of time, the adversary can force us to make a mistake every single time step. The same example shows that if we wanted to use randomization so if our z two was a guess, a distribution over red and blue rather than a deterministic guess, the adversary could still force us to be wrong half of the time, right? There's nothing we could do that would be better than random guessing if we used a randomized algorithm.
00:24:24.770 - 00:24:58.394, Speaker B: And so that's a total disaster, right? It's a bummer. We can't say anything, really, as far as if we're doing analysis, trying to learn about what might a good algorithm be for making predictions. Worst case analysis tells us literally nothing. It says literally, no matter what algorithm you use, it is as bad as it could possibly be. You will make the maximum possible number of mistakes. So literally, there's zero information content about what a good algorithm might be. If you do worst case analysis, that's the bad news.
00:24:58.394 - 00:25:37.586, Speaker B: The good news is, remember what's the signature where smooth analysis might come to the rescue. The first signature is that you have very brittle bad inputs. Okay? And boy, does this example seem brittle. Right? The adversary needs to keep making use of higher and higher precision as this process goes on, okay? And so it seems like any perturbation what the adversary is doing would actually, the example would break down. And it would be easy to prove this formally, but let me just sort of hand wave through it. So imagine we were in a smooth analysis where the adversary can, again, can try to pick a data point. It picks XT however it wants.
00:25:37.586 - 00:26:27.010, Speaker B: But then nature will intervene and perturb the data point chosen by the adversary by a little bit. Okay, so a little bit of anticoncentration. Well, then again, with every mistake you make, you're narrowing the window of ambiguity. And eventually the window of ambiguity will be so small that no matter what the adversary does, the perturbation will almost surely take it out of the window of ambiguity. It'll either sort of perturb it to the left of it, in which case you know that the answer is red, or it'll perturb it to the right of the window of ambiguity, in which case you know that it's blue. So it will be very unlikely that you don't just know the correct answer once the adversary's choice gets perturbed. Okay? And so as a result, you will not be making mistakes kind of forever.
00:26:27.010 - 00:26:56.980, Speaker B: Okay? So with every mistake you learn stuff. Once you've learned a lot, you'll almost never make a mistake again. And so formally, the way we'll state the goal is that the fraction of time steps where we make a mistake will go to zero as T goes to infinity. Or equivalently, the number of mistakes we make grows only sublinearly with capital T as the time horizon. Capital T goes to infinity. Okay? And that's going to be our goal moving. So good predictions for us will mean that as time goes on, we're basically never making mistakes in this sense.
00:26:56.980 - 00:27:44.500, Speaker B: All right, so that's hand waving, but I hope it's convincing that actually for this 1D thresholds learning problem, worst case, say anything. In the smooth adversary, it seems like you can totally get positive results, right? We haven't worked out sort of the parameters, but it just seems clear that this is true. Now, this is obviously just a very toy model, just sort of this game on the line with 1D threshold. So for an actual research kind of result, what we'd really like is sort of a general positive result, saying that you get something similarly positive for kind of all online learning problems, provided you have a smooth adversary, provided the adversary's choices are highly perturbed. This is another good place to pause for questions if there are any.
00:27:45.590 - 00:28:00.070, Speaker A: Yes, any questions. If anyone wants to ask any question now, please ask in the chat or chat there is one question. Is this an asymptotic notion?
00:28:02.250 - 00:28:06.550, Speaker B: By this I take it you mean the sort of sublinear and kelt?
00:28:08.430 - 00:28:09.180, Speaker A: Yes.
00:28:12.510 - 00:28:54.150, Speaker B: The way I'm going to state it in this talk, yes. So I'm going to imagine so think of it this way for each fixed, finite T and talk about sort of the minimum number of mistakes that you can make with any algorithm, and then you can look at that bound and take capital T to infinity. Okay? And then that's what should be sublinear. So concretely, we'd like to say something like the way it usually typically works is you say something like for all capital T, here's an algorithm such that no matter what capital T is, I will make at. Most like ten times the square root of T mistakes. And that would be a way that you would satisfy the goal, as I put it, slide.
00:28:55.690 - 00:29:03.290, Speaker A: There is one more question. Can you use randomization here instead of smooth analysis?
00:29:03.950 - 00:29:39.010, Speaker B: Yeah. So good question. So if you only have randomness in the algorithm, it doesn't help. And basically that exact same example we did with kind of 20 questions on the line sort of shows that. So basically that example shows that even with randomization, if you have a worst case adversary, you can't do better than 50 each label. So you're not going to make a mistake every time step, but you'll make a mistake at half of the time steps, and that doesn't satisfy our goal of wanting a sublinear number of mistakes. So we're going to wind up having randomness in the algorithm also in our solution.
00:29:39.010 - 00:29:44.950, Speaker B: But it's provably necessary to have randomness in the adversary or some other way of restricting the adversary.
00:29:46.010 - 00:29:53.654, Speaker A: The next question of amipass, does the adversary knows the smoothest result? The answer to the previous queries?
00:29:53.782 - 00:30:08.240, Speaker B: Excellent question. Excellent question. The answer is yes, and that's a big part of the challenge in the analysis. So the adversary will be totally adaptive. It can condition what it does on anything that's happened in the past. It's a great question.
00:30:09.730 - 00:30:11.678, Speaker A: Okay, so far so good.
00:30:11.764 - 00:30:12.154, Speaker B: Excellent.
00:30:12.202 - 00:30:14.126, Speaker A: No more questions. Tim, go ahead.
00:30:14.148 - 00:30:46.700, Speaker B: Okay, very good questions. Thanks for that. So I told you what I mean by smooth analysis. I told you what I mean by online learning. What would I mean by general results? So we're not just going to look at the unit interval, we're now going to look at an abstract domain. I will assume it's a Lebeg measurable subset of Euclidean space, but so think of it maybe as the unit cube, if you like. And then just like before, we had these 1D thresholds, this promise on what the reds and the blues looked like, that's described through what's called a hypothesis class, capital H.
00:30:46.700 - 00:31:19.886, Speaker B: A hypothesis here is really just the coloring here's a binary function from the domain to red or blue, kind of abstract. So let me tell you about the types of capital H's people tend to think about in learning theory. So first of all, rather than a 1D threshold, you could think about linear thresholds in higher dimensions, so that would correspond to a half space. So maybe capital X is just the unit cube. You slice it in half with a half plane and then that separates the reds from the blues. Or want to get fancier than linear functions. You could use a bounded degree n variate threshold function.
00:31:19.886 - 00:31:52.826, Speaker B: So in other words, you have a bounded degree polynomial on RN, and you would just color a point red or blue according to the sign of the evaluation of the polynomial at that point. So when I'm talking about hypothesis classes capital H, these are the types of things. I need lots of other examples as well, but these are good running examples to have in mind. Okay, and then the game with a worst case adversary works just like before. So instead of choosing a point in the interval, the adversary chooses a point from main. The adversary has to guess red or blue or maybe randomize over them. And then the adversary reveals the actual coloring.
00:31:52.826 - 00:32:33.498, Speaker B: And again, there's this promise that the adversary can only color things in a way that's consistent with one of the legal colorings, one of the hypotheses in capital h. And the goal remains exact same. So we'd like to get what we got in the 1D thresholds case. We would like the number of mistakes to be sublinear in the time horizon, capital t. So we'd like the fraction of time steps that we make mistakes to be going to zero as we get enough experience. And then the research agenda is to understand when is this goal possible and when is it not possible? We already know sometimes we know sometimes it's impossible. Like for 1D thresholds with a worst case adversary, we know sometimes it's possible, like with 1D thresholds for a smooth adversary.
00:32:33.498 - 00:33:14.534, Speaker B: But really, which h's and which adversaries can you handle? All right, so what do I mean by a smooth adversary in the context of a general online learning problem? Well, maybe, let me tell you what the extremes would be. Okay, so worst case on analysis would just mean that the adversary can do whatever it wants. It can pick any input distribution. In particular, it can pick a single point. The sort of easiest adversary would be an average case adversary where we force it to actually pick a point uniformly at random from the domain capital x. So in general, we will have a parameterized interpolation between these two extremes. The parameter will be sigma.
00:33:14.534 - 00:33:55.554, Speaker B: Sigma equals zero will correspond to the worst case adversary. Sigma equal one will correspond to the average case adversary. And the formal definition is just that, we insist. So the adversary is going to pick an input distribution over capital x, and we insist that it has a density function. And that density function is point wise upper bounded by one over sigma times the form distribution. So think of sigma as maybe like n to the ten, where n is some notion of input size, right? Then we're basically saying, like, it's basically l infinity bound on the density function, saying it's never more than n to the 10th times the density of the uniform distribution. Again, zero is the worst case adversary.
00:33:55.554 - 00:34:30.974, Speaker B: So we want our positive results to have sigma as small as zero as possible. We're not going to get it for sigma equals zero, but we'd like it to be for sigma as close to zero as possible. Okay, so that's what we're going to mean by a smooth adversary. This is kind of exactly the same notion that's used all throughout the literature on local search that I mentioned earlier. The new game now proceeds as you'd expect. The only difference is that nature inserts herself after the adversary chooses their sigma smooth distribution. Now nature supplies the randomness to actually realize a point drawn from that distribution.
00:34:30.974 - 00:35:29.302, Speaker B: Then the adversary again guesses a color or distribution over colors and the adversary reveals the true color subject as usual to the promise that it's consistent with one of the legal colorings we agreed upon in advance. And the goal again, we would like the expected number of mistakes to be growing sublinear with capital T. And just to reiterate the really good question that was asked a few minutes ago, hopefully from the way I phrased the game, but the adversary can be as adaptive as you could imagine. So when it makes its decision in step one of timestep T, so when it chooses its distribution capital D sub T, it can condition that choice on everything that happened in the T minus one time steps. Its own chosen its previous capital DS. Nature samples the X's and the algorithm's guesses the Z's and the Y's. For that matter, all of that information from the first T minus one time steps is in play when the adversary chooses a distribution of time T.
00:35:29.302 - 00:36:23.670, Speaker B: And in particular, what really complicates things is the choice of capital D sub T depends on the realization of the X's from previous time steps. The reason that makes things complicated is it means that you lose independence across time steps. All of a sudden, there's a sort of nasty dependence about how future points depend on the realizations of previous points. And that really complicates the analysis. All right, so let's go back to our goal. So when can you achieve this? For which adversaries, for which H's? And let me remind you the second clue that you should be looking into smooth analysis or some other semirandom model, which is that you want to look for problems or algorithms where there's a huge gap between what's true in the average case and what's true in the worst case. So we should talk about what are the extremes for these general online learning problems before we worry about smooth adversaries.
00:36:23.670 - 00:36:52.366, Speaker B: So I'll give a little bit of detail here. But the main point is just that the takeaway is going to be that in the worst case you can do basically nothing. You basically can never get this goal. And in the average case you can basically always get this goal. Okay? So the rate at which this expectation goes to zero will be different for different choices of capital H. But for almost any capital H that you might think about, you actually can achieve this goal, the average case. So there's kind of the biggest imaginable gap between average and worst case, you can never do it.
00:36:52.366 - 00:37:27.686, Speaker B: Worst case, you can always do it. In some sense, average case so specifically, I mean 1D thresholds, we already know you can't do it in the worst case and almost any other hypothesis class people tend to think about has embedded in it 1D thresholds. So technically it's characterized by having finite Littlestone dimension. Definition of that is not important for this talk. Point is just that kind of everything has infinite Littlestone dimension. On the other hand, in the average case all you need to assume is that H has finite VC dimension. Okay? So hopefully some of you know about the concept of VC dimension.
00:37:27.686 - 00:38:26.640, Speaker B: I'm not going to tell you the definition of this talk, I'll just tell you about some consequences of having finite VC dimension. But if you haven't seen this before, the takeaway is just the kind of almost everything has finite VC dimension. Okay? So, like for example, if you have degree d polynomials in RN, the VC dimension of that family is roughly N to the D. So again, as long as you have a finite dimension and a finite degree bound you have finite VC dimension and you can actually get the number of mistakes to the fraction of mistakes to go to zero with capital T. Okay? So that's definitely an opportunity. There's a big gap and the hope would be that it actually dramatically weaken the assumption. Okay, I should say so what do I mean by the average case version, an average case adversary? What I mean is that the adversary is allowed to make only one choice once ever, which is it gets to settle on a single input distribution capital D over the domain capital X once and for all at the beginning of the game.
00:38:26.640 - 00:39:13.230, Speaker B: And then every single time step the data point x sub T is an IID draw from that opera adversarially chosen distribution capital D. So, huge restriction on the adversary. That's the bad news for this average case work. Okay, so now it's also really clear what kind of result we want. We want to have the same conclusions we get in the average case that you can basically always have vanishing prediction errors, but we want it under the radically weaker adversarial assumptions that we use in the smooth case where all we know is that the adversary's choices get slightly perturbed. Okay? So the result I run a report on is such a result. So again, this is joint work with Nikah Hagtala, the nabi shetty.
00:39:13.230 - 00:40:24.180, Speaker B: So we show, in my opinion, exactly what you'd want, right? We get exactly the same conclusion, modulo a tiny bit of fine print around the parameter sigma, which I'll tell you in a second, but for sigma bounded away from zero, we get exactly the same conclusion we had with the average case adversary, we get it with the smooth adversary. And so again, you can achieve this prediction goal pretty much for any capital H you might care about as long as capital H has finite VC dimension. Let me just make a couple of comments in case there's any learning theory experts in the audience. For simplicity, I've been phrasing everything in terms of the realizable case that corresponds to this promise that the adversary has to consistent with one of the legal colorings we agreed upon up front. But actually this theorem carries over to the agnostic case and regret minimization, so you do not actually have to have that promise. So the adversary can actually output whatever colorings it wants you to change the benchmark that the number of mistakes that you make is not much larger than the minimum number of mistakes that would have been made by any of those allowable colorings in capital h. If that didn't mean anything to you, don't worry about it.
00:40:24.180 - 00:41:44.640, Speaker B: Second thing, so this thing that's satisfying about this result, which for the most part is not the case for the known smooth analyses of running times that I mentioned earlier, which is that you get quantitatively extremely satisfying bounds as well. So if you care not just about ratio in blue going to zero in the limit, but you really want to understand, how many times steps do you need to play this game before the probability of you making a mistake is going to be less than epsilon? Well, it turns out the rate at which this goes to zero is almost as good as the rate at which it's known to go to zero. With an average case adversary that just picks one fixed distribution up front. Now of course that can't quite be true because our bound has to depend on sigma. Remember, sigma equals zero corresponds to a worst case adversary and we know everything blew. Okay? So there is an extra factor that depends on sigma, but it depends only on the square root of log one over sigma, which means that actually we can take sigma to be exponentially big and still get reasonable kind of regret minimization. By contrast, most of the local search smooth analyses of local search algorithms and of the simplex method, while they give expected polynomial running time bounds, the polynomials are often really quite big just because the analysis is difficult.
00:41:44.640 - 00:42:30.890, Speaker B: So the upper bounds there are presume not very close to tight. Our upper bound definitely is quite close to tight because it's quite close to the known to be tight average case bounds. Also in running time analyses, you actually need the dependence on one over sigma to be polynomial in one over sigma. But here in this different application for regret minimization or prediction error minimization, you actually only have a logarithmic dependence on one of her sigma. So that was a very nice surprise that we discovered when we proved this result. Okay, and just a couple of precursors. So Rachland's, Truderin and Tawari actually proved the special case of this result when capital H are half spaces, basically using bare hands approach.
00:42:30.890 - 00:43:39.794, Speaker B: Whereas in a previous paper with and Abhishek we proved a special case in which we need an extra hypothesis that capital H has a low bracketing number which is satisfied by many capital H's of interest, but it's definitely a stronger assumption. So this theorem here, this is the first result that achieves the sort of minimizing prediction mistake goal with really the minimal possible assumption of bounded BC dimension. Okay? All right, I'm a little bit running out of time and I do want to say a little bit about the social analysis. So let me just say a tiny bit about the proof of this, but not very much. I'm happy to sort of stay on afterwards for those of you that are interested in hearing more details, but I do want to at least address sort of questions that probably many of you have in mind. So question number one might be, this is an algorithm seminar, so you might be wondering like, okay, what is the algorithm that achieves this guarantee? Right? Very natural question. Remember, a big reason of why we're doing this whole analysis is to get algorithmic advice about how to tackle, and that's exactly what worst case analysis failed to give us.
00:43:39.794 - 00:44:18.526, Speaker B: Secondly, thinking as a mathematician, you're probably wondering about the role of the Hypotheses in proving this theorem, specifically the hypothesis that H has bounded BC dimension. How does that help? And secondly, we're assuming who the adversary instead of a worst case adversary. How does that help? And this theorem would be false without either of those assumptions. So clearly they must play a big role in the analysis. So let me just briefly talk about those questions. So how does bounded VC dimension help? Well, let me just focus your attention on answer number one a here. Basically, what's important is that even though capital H is generally infinite, even the family of 1D threshold is an infinite class.
00:44:18.526 - 00:45:15.262, Speaker B: If you have bounded VC dimension, then you can really approximate the hypothesis class with a finite subset of it. Okay? So the size of the subset you need grows exponentially with the VC dimension. But still it is going to be a finite subset where at least with respect to the uniform distribution for any of your original Hypotheses, you hypothesis in H prime, which does almost exactly the same thing, meaning colors, almost all of the points, one minus epsilon measure of the points in exactly the same way. Okay? So that's sort of the major thing with bounded BC dimension is you can have these finite approximations of the hypothesis class. So now as far as the algorithm, so we're going to make use of that fact and then we're going to rely on off the shelf randomized algorithms for regret minimization. And you may have heard of some of these algorithms before. Examples would be like hedge, multiplicative weights, follow the perturbed leader, et cetera.
00:45:15.262 - 00:45:58.110, Speaker B: So these are randomized algorithms which give the kinds of guarantees we want even for adaptive worst case adversaries. So these work really well. The cat is they only work when you have a finite number of different options, which for us corresponds to a finite number of different Hypotheses. Our original hypothesis class, capital H, is infinite generally. So it does not make sense to talk about running these algorithms directly on capital H, but it does make sense to talk about running them on the finite subset capital H prime that well approximates capital H. Okay? And so that's what the algorithm is going to do. It's going to reduce to the finite case exploiting the bounded BC dimension hypothesis.
00:45:58.110 - 00:46:33.418, Speaker B: All right, this algorithm is something you could imagine using. You could imagine sort of sampling Hypotheses to construct your family H prime. And then these are very lightweight algorithms multiplied of weights, et cetera. You could just run this and that could your prediction algorithm. So this is a sort of plausible method for how to actually make these predictions in this problem, which is what we really wanted all along. Okay? And then you need to do the analysis. And basically the analysis boils down to how much do you lose by using H prime instead of H.
00:46:33.418 - 00:47:09.910, Speaker B: Okay? So the worry is that basically you're doing fine. If the real problem only had the Hypotheses in capital H prime, you'd be doing fine. You would just inherit the no regret guarantee from multiplicative weights. But unfortunately, there's all these other colorings capital H also, which the adversary is free to make use of. And so the worry is that the adversary somehow conspires to choose its little x's, conspires to choose its data points. So that actually just finding the finite subset capital H prime, you totally missed out on what the real coloring actually was. That's what can go wrong in the analysis.
00:47:09.910 - 00:47:52.910, Speaker B: In fact, with a worst case adversary, it literally can go wrong. So this is where the smooth adversary assumption gets used to argue that actually you do not lose much passing from capital H to capital H prime. The way we do that is through a coupling method. So again, what makes this difficult to analyze is the non independence. So first of all, the x's are non uniform because the adversary can choose any sigma smooth distribution it wants. Secondly, as we discussed, they're not independent either, because the adversary can condition its distribution on one time step, on the realizations of the x's of previous time steps. So we have a trick for dealing with that, which is a coupling argument.
00:47:52.910 - 00:48:49.430, Speaker B: So we basically show that it's good enough to analyze a uniform IID adversary because that's a much stronger assumption, right? Uniform IID rather than seductive. So it's really just a uniformly random point every single time step. You can get away with analyzing just the uniform adversary as long as you blow up the number of time steps by a one over sigma factor. So again, as sigma is going to zero, you're heading to the worst case. So this is sort of an unbounded blow up, but for sigma bounded below, bounded away from zero, this is some finite blow up in the number of time steps you need to worry about. Okay, so that's the sort of generic reduction we managed to show. Sort of a sufficiently large number of just uniform IDs actually wind up dominating, at least from the perspective of the R analysis approach, dominates the smaller number of points generated by the smooth adversary.
00:48:49.430 - 00:49:15.520, Speaker B: Okay, this coupling approach leave is general and in the paper, which is on Archive, we give other applications, including one to online discrepancy minimization I'm not going to talk about here in the interest of time. And so instead I will take a short pause to see if anyone has a question about the online learning part. And then after taking questions, I'll say a little bit about the social network analysis part.
00:49:16.050 - 00:49:26.130, Speaker A: Okay. Any more questions in the chat or we can post on them after the talk as you wish.
00:49:27.110 - 00:49:33.582, Speaker B: Yes, because I will stick around after the talk concludes for a while for people who want to dive deeper.
00:49:33.726 - 00:49:37.638, Speaker A: Yeah. Okay, so there are no more questions, so please go on.
00:49:37.724 - 00:50:35.842, Speaker B: All right, what do we just saw? I really think a totally new type of killer application, a smooth analysis, the semirandom model to a fundamental model in learning theory, a really, really well studied model in learning theory. Now, I want to totally switch gears. I want to talk about a totally different application of algorithms, namely social network analysis and sort of graph algorithms that are sort of specialized to work well on social networks. Now, where does the beyond worse case sort of lens come in on that problem? The issue is that what do we mean when we say a social network? Right? So what would a theorem look like where you would say, like on social networks, our algorithm runs in o of n log n time or achieves a two approximation. What would that even mean? Well, on the one hand there are these sort of features of social networks that are pretty non controversial. Like you've probably heard of a power law distribution, degree distribution. My work is going to focus on triadic closure.
00:50:35.842 - 00:51:28.040, Speaker B: So this is the property in social networks that friends of friends tend to be friends in their own right. So in terms of graphs, paths tend to close into triangles rather than just be sort of induced to hot paths. So intuitively there should be hope of having a mathematical model that captures at least some of what's special about social networks. And in fact, there's massive literature just about generative models that is sort of probability distributions that people have offered suggesting that samples from these models should be good representations of social networks, preferential attachment being probably the one that you may have heard about the most is a zillions others. In fact, 15 years ago chakrabardi and Felizos wrote a survey with literally 23 different models of social networks comparing their pros and cons. That was 15 years ago. So now, as you can imagine, there's many more than 23.
00:51:28.040 - 00:52:30.234, Speaker B: So if you want to do really nice kind of stockfox soda style algorithms work motivated by social networks, there's this question of like, what should you do, right? Because you have these like 23 or more models to look at. You could pick one of those models and then design an algorithm and prove that it performs well with higher probability on that model. But I don't know that I really believe any of those models, right? So there's too many to choose from. And for all I know, they're kind of all wrong in some sense. So that brings us very much into sort of actually what we were just talking about with hybrid models gain average case interpolations. So here, given that there's so many different proposed average case analyses for social networks in the form of all these generative models, intuitively what I want is I want to take like a worst case over all those proposed generative models of then with high probability or average case performance for the given model. So worst case over sort of reasonable distributions for social networks and then average case with respect to any such model.
00:52:30.234 - 00:53:32.042, Speaker B: Okay? Now practically the way that you carry this out is you instead impose combinatorial restrictions on a graph. So it's just going to be a restricted graph class, just like planar graphs, bounded tree width, all that kind of stuff. It's going to be a combinatorial restriction on graphs, which is going to hold high probability from samples from any of these sort of generative models that we were motivated by. And so there's been actually, I think, a really sort of exciting sequence of papers by a bunch of different people over the recent years. There's a chapter that I wrote with Shadri in the BWCA book, chapter 28, that's all about that surveys all of these results. I'm just going to flash quickly sort of the latest results that we have extending this last point, this Ical paper with Jacob Fox, CshA, Shadri Fanway and Nicole Wine on so called C closed apps. So just to be clear, theorems I'm going to be giving you are basically fixed parameter tractable type theorems for computing the maximum clique and generalizations of the maximum clique.
00:53:32.042 - 00:54:30.654, Speaker B: Okay? So max clique, I mean, is a really hard problem, right? It's approximate, but it's even hard to get fixed parameter tractability results for max clique except in extremely restricted classes of graphs, right? So like for h free graphs for almost all H's, you still can't get FPT results for max clique. And we're going to be looking at generalizations of that. So to get any kind of FPT results for these problems, we need to make a pretty non trivial restriction on the graph. The good news is I'm going to give you a restriction which is reasonably well motivated by the special structure that we know social networks have, namely closure that if people have lots of mutual friends, they tend to be friends in their own right. Okay? So formally we're going to call a graph C closed C. Here is the parameter that we're going to be FPT with respect to a graph is C closed. If whenever two vertices that have C neighbors in common, they must be themselves directly connected by an edge.
00:54:30.654 - 00:55:23.380, Speaker B: Okay? So you can't have two vertices that are not neighbors that have C or more mutual neighbors. So once you have enough friends in common, you have to be friends yourself. Okay? And it turns out that actually so again, this comes from tratic closure social networks. This is sort of a coarse approximation of what you do see in real world social networks. And happily, lots of graph optimization problems which are generally not FPT solvable, do become fixed parameter tractable with respect to this parameter C. And we've done some experimental results as well showing that many of standard social network benchmarks are C clothed for reasonably modest values of C. Okay? So what can I say here? Let's just check for a second.
00:55:23.380 - 00:56:11.710, Speaker B: Okay, good. So just a couple more things and then I need to wrap up. So in the work with Eden Hughesick, we look at a sort of very generic version of a sort know subgraph maximization problem. In fact, we look at the more general enumeration problem. So by F graph enumeration problem parameterized by a family of graphs capital F for concreteness, you can think of capital F for now as being just family of all cliques, okay? By capital f graph enumeration What I want you to do is given an input, an undirected graph, capital G. I want you to enumerate all of the induced subgraphs of G that are maximal with respect to membership in the stanley capital F. So if capital F is the cliques, then the goal here is just to enumerate the maximal cliques of the graph.
00:56:11.710 - 00:57:07.446, Speaker B: If capital F is something different, you'd be enumerating the maximal things from capital F. And moreover, I would like an algorithm which is output efficient, which uses polynomial time for each such subgraph. Of course, this is more general than finding the max cardinality subgraph member of F because if not, let's just enumerate all of them and remember the biggest of them. So enumeration is a sufficient condition for maximizing number of vertices in an induced subgraph from capital H from capital F. Excuse me. So in the work with Eden Hughesik, what we do is we show that for many different classes of dense graphs capital F, you actually can solve the enumeration problem and therefore the cardinality maximization problem in fixed parameter tractable time, again fixed parameter respect to the parameter C in the definition of C closed. Okay? So for example, as long as.
00:57:07.446 - 00:58:14.218, Speaker B: It's the case then whenever two vertices have ten neighbors in common, they must themselves be directly connected as long as your graph satisfies that property, then in fact we will be giving you a polynomial time algorithm for computing the maximum clique or the maximum dense subgraph for a number of other possible definitions of dense subgraphs. And we also show that these results are in some sense the frontier of you could expect kind of in two senses. So we identify several capital F's where you could do this notions of dense subgraphs. And we show that kind of like the next most relaxed definition of subgraph you might look at like, for example, having bounded average co degree are not good enough. You probably cannot get FPT enumeration if you weaken the assumptions much compared to what we did. Okay, so I know this was fast, but I mean, the takeaway from this part is that in principle, one could have arrived at sort of defining the graph class of C closed graphs just from first principles. That is not how we did it.
00:58:14.218 - 00:59:08.410, Speaker B: We really did it exactly through the methodology. I told you we were taking a beyond worst case lens to what social network analysis had been doing through all these competing generative models. And we came up with the definition of C closed graphs because we wanted to have theory that spoke simultaneously to all of those models. That's much a beyond Worst case way of thinking. And so happily, we now have this new definition of a graph class, C closed graphs, which you can do all kinds of fun things with. And it is sort of a reasonably strong assumption, but actually we're getting significantly stronger algorithmic results than are possible for most other graph classes, in particular FPT algorithms for both maximum clique and generalizations thereof. All right, I'll skip the techniques here and conclude with two open questions.
00:59:08.410 - 00:59:53.180, Speaker B: So again, in general, most of the chapters in the Beyond Worst case book point out concrete open questions and directions for future research. Let me just highlight two that sort of come up in the two results I talked about today. So the first one is, as I discussed, if you look at the learning algorithm we used for minimizing the number of predictions, that was not computationally efficient. And that's because in the first step you have to somehow construct this exponentially big family capital H, prime of Hypotheses that well approximates Kel H. And so an open question is whether you can get analogous sort of mistake minimization guarantees for an algorithm which is computationally efficient. I think probably you can. And that would be quite a nice result.
00:59:53.180 - 01:00:54.718, Speaker B: In the second part that I went through very quickly, I did mention that really we focused on enumerating maximal subgraphs from some family. And then of course, as a consequence, you can also maximize the size of a subgraph from them just by enumerating and remembering the one with the largest number of vertices. And what's interesting is a possible conjecture here is that in fact, the only way you can solve these enumeration problems, these optimization problems very generally, is basically enumeration. So in other words, whenever enumeration is hard, there's no FPT enumeration algorithm. Perhaps there's no FPT algorithm either for merely finding the maximum size maximal subgraph graph from capital F. And you could imagine a proof that would show some kind of like this. So you could imagine a generic reduction that takes as input a family of instances that are bad for enumeration, meaning there's just way too many maximal subgraphs that belong to capital F.
01:00:54.718 - 01:01:21.222, Speaker B: So that would be bad for enumeration because there's just too many to enumerate. And then generically transforming that into a different family of instances which were w one hard with respect to the optimization problem. So that, I think, would be a super cool, super cool result. Okay, so I think I'm over time. Thanks again for joining me. Happy to take questions. Know I can definitely stick around for at least a half an hour, so very happy to answer any questions that you have.
01:01:21.222 - 01:01:22.680, Speaker B: So thanks very much.
01:01:24.250 - 01:01:41.950, Speaker A: Okay, so there is a question from Bart Jansen. Can you say something about the values of C for which a typical N vertex graph from popular generating models is C closed? Is it merely sublinear in N or even Polarithmic?
01:01:43.810 - 01:01:56.290, Speaker B: Good question. Let me make sure I understand the question. The question was what is C for standard network benchmarks? Or is the question samples from the Gen models, what is their C?
01:01:56.440 - 01:02:00.100, Speaker A: Bart Kotura. What is C?
01:02:01.830 - 01:02:02.242, Speaker B: Okay.
01:02:02.296 - 01:02:03.954, Speaker A: The question is what is C?
01:02:04.152 - 01:02:53.540, Speaker B: All right, so, good. There's two versions of the question, one of which actually haven't studied, but one we have, and I even have a slide to illustrate that one. So 1 second, when did I give this last? Probably Fox 19. Okay, good. So one version of the question is, let me tell you one thing we haven't done. What we haven't done is taken, say, like the preferential attachment model, and probably we'd start with more modern generative models. So the latest and greatest generative model take random samples and compute for what? C? Are they C closed? Okay, so that we have not done.
01:02:53.540 - 01:03:01.878, Speaker B: What we have done is we've taken the most well studied benchmarks for social networking. Yeah.
01:03:01.964 - 01:03:03.622, Speaker A: Showing us a slide or not?
01:03:03.756 - 01:03:04.966, Speaker B: I was just about to.
01:03:05.068 - 01:03:07.802, Speaker A: Okay, good, because we still see the previous one.
01:03:07.856 - 01:03:31.760, Speaker B: See the old one? Yeah. Okay, right. So I'll stop the old one. We haven't actually studied samples from generative models. Admittedly we should, but we did at least study the benchmarks that social network analysts really use. And so let me show you some. The paper has more, but here's a slide that has some of it.
01:03:31.760 - 01:03:41.300, Speaker B: So if you look at the bottom of this slide, do you see a slide that says Weekly? See closed at the top?
01:03:41.830 - 01:03:42.482, Speaker A: Yes.
01:03:42.616 - 01:04:04.726, Speaker B: Great. That table at the bottom. So we were using the Snap data set, which is the Stanford network something. So those are the benchmarks that people tend to use in social network analysis. The rows correspond to four of those graphs. The first column corresponds to the number of vertices. The second column to the number of edges in each of those graphs.
01:04:04.726 - 01:04:50.038, Speaker B: And then the third column, that is the smallest C with respect to which the graph is C closed. Now the trivial bound on C n minus one. Once C is n minus one, you automatically satisfy it. So as you can see, these values of C are sort of, well, less than the number of vertices. But something I didn't get into is sort of to keep things short and sweet, I just gave kind of the easiest to state but most restrictive notion of a C closed graph. We have also investigated relaxed definitions of it. And so there is this notion of weekly C closed graphs defined in that same ical paper with so in the fourth column here.
01:04:50.038 - 01:05:23.540, Speaker B: And so it turns out all of our results for Max Clique hold equally well for weekly C closed and C closed. So it actually doesn't matter. And so here in the fourth column, you start getting really compelling results. So this is the C for which the network in question is weekly C closed. And here you see, you really start getting down into the single digits even for graphs that have thousands and thousands of vertices. And so the running time of our analyses here is sort of exponential in C over know C gets down to like eight or not. That's not so crazy.
01:05:24.790 - 01:05:33.650, Speaker A: Here is, I guess, a follow up question from Barth. Is there a large gap between the degeneracy and weak closure number for these graphs?
01:05:33.810 - 01:05:48.890, Speaker B: That's a good question. It's not even I'm not even sure. It's obvious that they're let's see. Is it clear they're related?
01:05:51.490 - 01:06:00.080, Speaker A: I don't know. Yes, they are. Weak closure is almost degeneracy. Vij ali saying.
01:06:02.210 - 01:06:06.274, Speaker B: Let'S see. It's a good question to sort of compute that in these sorry.
01:06:06.312 - 01:06:09.090, Speaker A: Is at most degeneracy.
01:06:09.510 - 01:06:59.522, Speaker B: Okay. That's good, I guess. Is degeneracy even lower bounded? This may speak to then. Right? So I'm sort of rusty on this, but if the claim is that degeneracy is at least C for C closed, what you can see here is that weak C gets you way smaller than C, which then in turn degeneracy is even worse than that. Right. So by having a big gap between the C and the week C columns, that would at least imply a big gap between what you get from degeneracy and what you get from weekly C closed. Is that reasonable? Okay, I agree.
01:06:59.522 - 01:07:14.674, Speaker B: We should add another column to this table, which is compute this. You know, it's not hard to compute the degeneracy of a graph. So maybe we even did that at some point and it's been lost at the time. But I think that's an excellent question. That would be a more refined comparison.
01:07:14.802 - 01:07:40.590, Speaker A: Here'S Vaishali saying click is in degenerate, but one, we closed. So maybe this but, but let's maybe move to because there is still one question from Uri Mayer that I didn't ask. Can we relax the assumption to approximately see closed, say, close in humming distance or when the closeness property holds for most pets?
01:07:41.090 - 01:08:21.818, Speaker B: That's a great question. Yeah, good question. We did not look at that. I think that's a really no question. Like how brittle? I think if you look at the proof, the feeling is if all you did was mess up a C closed graph by o of one edges or something, just thinking about the proof, my feeling is not could go wrong. If you were able to change an epsilon fraction of the edges, I'm sure would, it be a disaster. But that's not something we ever worked out.
01:08:21.818 - 01:08:34.980, Speaker B: So I think this is actually a really nice research agenda. Under what notions of distance from C closedness do you get kind of graceful degradations in these algorithmic results? That would be cool to see.
01:08:35.910 - 01:08:53.910, Speaker A: One more question from Rohit Chatterjee for the online learning result. You mentioned that you have an overall time step overhead of about lock one over sigma in runtime. Where does the one over sigma blow up from the coupling Clemma get absorbed?
01:08:59.930 - 01:10:23.380, Speaker B: I should go back to the other slides, close, Sense, screen share, blah, blah. Okay, so, right, so just to be clear, so the one over sigma is not about the running time, it's really about so when you do online learning, you do not worry about the running time upfront. Traditionally, the concerns in online learning have focused on the number of mistakes you make, or more generally, the regret. And so when I talked about I'm trying to pull up the theorem here's the theorem. Yeah, good. So this one over sigma, that's in the mistake bound, right? So so theorem here is going to look something like there's going to be some concrete bound which says if you use this algorithm, the expected number of mistakes you've made after capital t times steps is ten times square root of t. And then there's going to be a times square root log one over sigma or something like that.
01:10:23.380 - 01:10:47.500, Speaker B: So that's sort of where it shows up. So if sigma goes to zero, it becomes vacuous because it just says you're making less than an infinite number of mistakes, but for any sort of fixed sigma, you're getting the sublinear in T growth of the number of mistakes that you so that's just a clarification point. Now, Archer, remind me sort of the second part of the question.
01:10:48.110 - 01:10:52.910, Speaker A: Where does the one over sigma blow up from the coupling Clemma get absorbed?
01:10:53.330 - 01:11:49.722, Speaker B: Yeah, let's see. So that was this part I skipped, unfortunately. Let's see later right. So basically so it's here, so it's in the bottom here, basically. So I guess I mentioned this very briefly that you can run an off the shelf algorithm like multiplicative weights and then you know you're going to be doing fine with respect to capital H prime, right? So, in other words, if the adversary was nice enough to actually make sure that it was consistent with the hypothesis of capital H prime, and the total number of mistakes you make would grow, as it says here, square. Root of T times log times the size of H prime. Okay? Which would then boil down to, like, kind of a remember, the size of the subset is like one of Epsilon raised to the VC dimension.
01:11:49.722 - 01:12:39.390, Speaker B: So there's going to be like a root t times D log one over epsilon. But then there's the second thing, which is like the adversary only promised it's going to color things consistent with the hypothesis in capital H. It did not promise it was going to color things consistently with a hypothesis of H prime. And what you'd like to show is that, well, it doesn't really matter because anything that it could have been doing with respect to capital H, you had something in capital H prime that was almost the same and you'd be fine. And if you had just like a uniform adversary, that would definitely be true by the uniform convergence results you get for Banner BC dimension classes. But the problem have that and indeed with a worst case error, it could be that you wind up coloring things that's consistent with something in H prime. So you think you're good, but then actually the adversary is secretly coloring things with respect to some other capital H and you make a ton of mistakes.
01:12:39.390 - 01:13:24.118, Speaker B: So that's where it goes in. And so let's get the next slide up there. And so basically what happens is, let's see, there's going to be this dependence so you see already here, right, there is this dependence on sort of square root t as far as how many mistakes you make. So the bound does keep growing, it just is growing sublinearly. So the coupling is sort of almost like scaling up. Or one way to think about it is like you take each old time step and chop it into one over sigma timesteps. So this is not quite how it works, but roughly you can imagine that this capital T gets replaced by a capital T over sigma.
01:13:24.118 - 01:13:52.920, Speaker B: Now that doesn't give the logarithmic that I promised on one over sigma, right? So that's not how we do it. We really sort of open up the analysis. We kind of have to use Bernstein's inequality type trick somewhere to sort of get what we want. But that's sort of intuitively where it comes from. The number of mistakes does scale with time steps. And if you're de facto blowing up the number of timesteps you will be blowing up the number of mistakes that you suffer, or at least the bound of the number of mistakes certainly. Did that answer the question?
01:13:53.930 - 01:14:25.420, Speaker A: I hope so. Right now, my suggestion is that let's just move questions to more questions, because Amy Pass also asked one more question. Let's move the questions, we'll move to the table rules. So first of all, let me just pause the mention about the next talk. The next talk in two weeks will have Carl Brinkman talking about fine grained complexity of optimization problems. And as for now, I would like to thank Tim for a great talk.
