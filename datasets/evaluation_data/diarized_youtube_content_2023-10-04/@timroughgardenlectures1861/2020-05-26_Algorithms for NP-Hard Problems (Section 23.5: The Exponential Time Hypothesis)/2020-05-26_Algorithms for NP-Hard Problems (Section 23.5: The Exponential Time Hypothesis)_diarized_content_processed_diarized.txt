00:00:00.490 - 00:00:14.506, Speaker A: Hi everyone, and welcome to this video that accompanies section 23.5 of the book Algorithms Illuminated, part Four. This is a section about a strengthening of the p zero equal to NP conjecture known as the exponential time hypothesis.
00:00:14.506 - 00:00:33.762, Speaker A: We will also discuss its more ambitious cousin, the strong exponential time hypothesis. We often conflate NP hard problems with problems that require exponential time to solve in the worst case. In fact, if you remember all the way back to our section on rookie mistakes, that was one of our culturally acceptable inaccuracies.
00:00:33.762 - 00:00:48.058, Speaker A: But the P not equal to NP conjecture. It doesn't actually assert that NP hard problems take exponential time in the worst case. It leaves open the possibility even if that conjecture is true, even if P is not equal to NP, it leaves open the possibility that every problem in NP can be solved in sub exponential time.
00:00:48.058 - 00:01:04.146, Speaker A: A running time like, say, n to the o of log n, or maybe two to the o of square root of n. Few experts believe in this possibility. However, most experts think not only does P not equal to NP, but also that typical NP hard problems do require exponential time in the worst case.
00:01:04.146 - 00:01:22.294, Speaker A: So that belief is codified by something known as the exponential time hypothesis or ETH. I'm going to state the exponential time hypothesis specifically for the three sat problem, and what it's going to assert is that there is some constant C bigger than one. So maybe 1.1,
00:01:22.294 - 00:01:36.278, Speaker A: maybe 1.1, maybe 1.2, whatever some constant C, so that every algorithm that correctly solves threesat has worst case running time at least C to the n, where n is the number of decision variables.
00:01:36.278 - 00:01:53.194, Speaker A: So every correct three set algorithm has time exponential in n. A couple of comments so first of all, I did not state the ETH as saying that every NP hard problem requires an exponential amount of time. And the reason I didn't state that is because that statement's actually false.
00:01:53.194 - 00:02:12.146, Speaker A: It does turn out they're sort of contrived NP hard problems, which you can definitely solve in not polynomial but subexponential time. And there's an example at the end of the chapter exercises, if you're interested. The exponential time hypothesis, then, is really about natural NP hard problems, the ones you're actually probably going to encounter in your own projects.
00:02:12.146 - 00:02:36.142, Speaker A: And you get basically equivalent results even if you stated the ETH for any number of different natural NP hard problems. But why not use the mother of all NP hard problems? Why not use threesat and have that be the starting point where we assert that every correct algorithm requires exponential time in the worst case. Once you've assumed that for one problem, you can use reductions to push the exponential lower bound to other natural problems as well.
00:02:36.142 - 00:03:07.058, Speaker A: And indeed, it's known via appropriate reductions that, for example, all of the graph problems that we've studied in this video playlist if the ETH is true, if three set requires exponential time, then so do all of those NP hard graph problems that we've been considering, like independent set, traveling salesman problem directed Hamiltonian Path, et. Cetera. One thing the exponential time hypothesis does not assert.
00:03:07.058 - 00:03:19.402, Speaker A: It does not assert that there's no algorithm for the three side problem that's provably better than exhaustive search. Exhaustive search that would involve enumerating the two to the N possible truth assignments. So that would run in time scaling with two to the N.
00:03:19.402 - 00:03:46.274, Speaker A: Whereas the ETH, all it asserts is that every correct algorithm needs to have running time scaling exponentially within, but possibly with a base C that is less than two. And there's a reason that this is the version of the ETH, which is that there are, in fact, algorithms for the three sat problem that are provably faster than exhaustive search that have running time of the form C to the N, where C is indeed a constant less than two. It's a constant bigger than one.
00:03:46.274 - 00:04:05.462, Speaker A: So these better algorithms still run in exponential time, but they're considerably better than the two to the N running time you'd get from exhaustive search. There's been a long sequence of research papers over the 21st century that keep designing faster and faster algorithms for threesat. So all have running times C to the N, but the constant keeps getting a little bit smaller, a little bit closer to one.
00:04:05.462 - 00:04:12.506, Speaker A: The current record is C, something like 1.308. So no one's breaking through C equals 1.3 yet.
00:04:12.506 - 00:04:41.308, Speaker A: And ETH asserts that, in fact, the diminishing returns are necessary, that there will be some base below which you cannot solve the three set problem correctly. In all cases, we've now seen two different conjectures. We've seen the P not equal to NP conjecture, which asserts that an NP hard problem like three set requires super polynomial time.
00:04:41.308 - 00:05:06.980, Speaker A: And now we've seen this stronger exponential time hypothesis, which asserts the stronger statement that natural NP hard problems like three set actually require not just super polynomial, but actually exponential time. Now, when you strengthen an assumption, there's an upside, and there's a downside. The upside is the stronger assumptions you're willing to make, the cooler conclusions you're going to be able to derive from those assumptions.
00:05:06.980 - 00:05:25.368, Speaker A: The downside? Well, the stronger your assumption, the more likely it starts being that it's actually false. For example, for all we know, the P zero equal to NP conjecture is true, but the stronger exponential time hypothesis is false. Not many people believe that that's the actual state of the world.
00:05:25.368 - 00:05:40.568, Speaker A: But that is completely a logical possibility that the stronger ETH is false, even though the P not equal to NP conjecture holds. All right, but in any case, ETH, I mean, you have to believe PNE equal to NP to believe ETH. But most people who do believe PNE equal to NP do believe ETH.
00:05:40.568 - 00:05:58.070, Speaker A: I actually want to tell you about a third, even stronger assumption. The strong exponential time hypothesis or the seth, which is actually so strong that it's no longer true that everybody believes in it. There's actually about a 50 50 split among experts about whether the Seth is true.
00:05:58.070 - 00:06:20.700, Speaker A: So informally, the strong exponential time hypothesis is going to assert that there's a problem that you cannot solve faster than exhaustive search. Now we know that that statement is not true about the three set problem. So it's got to be a harder version of the three sat problem.
00:06:20.700 - 00:06:35.404, Speaker A: But actually we don't have to look very far. We're just going to look at the KSAT problem for bigger positive integers k. So again, remember an instance of KSAT that just means you have your boolean, variables, constraints as usual, disjunctions of literals at most k literals per constraint.
00:06:35.404 - 00:06:48.772, Speaker A: So the bigger the k is, the more general the problem. So here's an interesting fact about the state of the art in algorithms for solving KSAT. It is known that for every positive integer, k equals ten, k equals 100.
00:06:48.772 - 00:07:05.470, Speaker A: Whatever. There exists an algorithm which improves at least a little bit over exhaustive search. So there's an algorithm, no matter what k is, there's going to be an algorithm that has running time bounded above by c raised to the n, where n is the number of variables and c is a constant less than two.
00:07:05.470 - 00:07:27.376, Speaker A: However, as k gets larger and larger, as you take k to be ten or 100 or 1000, the improvement that the best known KSAT algorithms give you over exhaustive search gets smaller and smaller. So like, if you're looking at K equals 1000 and you're looking at algorithms for 1000 Sat, you're not going to get a running time from the state of the art. That's better than 1.99
00:07:27.376 - 00:07:32.580, Speaker A: raised to the N. Yeah, 1.99 is less than two, but it's really devolving to two.
00:07:32.580 - 00:08:16.522, Speaker A: As K, the number of disjunctions per constraint, increases, grows large. What the strong exponential time hypothesis asserts is that this deevolution of the running time of KSAT algorithms with k is inevitable, that as you take k large, there's really no significant improvement you can make over exhaustive search in the worst case. So how do we say that in math? Well, now we say that for every choice of a constant c less than two.
00:08:16.522 - 00:08:31.934, Speaker A: So this is going to be the base of our exponential lower bound. So think of c as maybe like 1.99. So for every constant c less than two, we can then take k, the number of literals per disjunction to be large enough, right? So if you set c equal 1.99,
00:08:31.934 - 00:08:37.634, Speaker A: maybe I take k equal 1000. If you set c equal 1.99, maybe I set k to be 10,000.
00:08:37.634 - 00:09:18.190, Speaker A: But whatever constant less than two you set, I can pick k large enough so that for KSAT with that value of k, it is the case that every correct algorithm requires running time at least c, your constant raised to the n the number of decision variables in the worst case. If that is in fact the case, if the set is true, then that means the general Satisfiability problem, the version that we talked about when we were talking about Sat Solvers where there's no limit at all on how many literals you have in each disjunction then that's a problem where essentially you can't make any improvement over exhaustive search. That would be a consequence of the strong exponential time hypothesis.
00:09:18.190 - 00:09:47.546, Speaker A: Now, again, unlike the other two conjectures, other like P not equal to NP and the ETH, which most people believe, expert opinion on a strong exponential time hypothesis is split, and frankly, everybody working with it is fully prepared any day for there to be a resolution in either direction. But if nothing else, falsifying the strong exponential time hypothesis would require a major theoretical advance in algorithms for the Sat problem. Perhaps that advance is coming, but that's what it's going to take.
00:09:47.546 - 00:10:20.898, Speaker A: To refute the Seth in this video, I've introduced you to two new conjectures, the exponential time hypothesis, or ETH, and the strong exponential time hypothesis, or Seth. And I introduced you to each of these two conjectures for different reasons. So the ETH I wanted to tell you about because it codifies the common belief among experts that natural NP hard problems like threesat Tsp, et cetera, that they don't just require super polynomial time, as is asserted by the P not equal to NP conjecture, but actually they require exponential time.
00:10:20.898 - 00:10:33.858, Speaker A: So that's the point of the exponential time hypothesis. It codifies that belief that exponential running time is required for natural NP hard problems. Now, the strong exponential time hypothesis, like I said, some people believe it, some don't.
00:10:33.858 - 00:11:08.546, Speaker A: So why am I telling you about it when it might well be false? Well, what's really remarkable about the strong exponential time hypothesis is that it implies striking impossibility results, even for easy problems, even for problems that we were tackling in earlier parts of the book that were polynomial time solvable, right? So think back to earlier books in this series and earlier video playlists, right? We were going through all these polynomial time solvable problems, and we certainly were not content to just run in polynomial time. We wanted algorithms that were blazingly fast. We wanted algorithms that ran in linear time or near linear time.
00:11:08.546 - 00:11:20.946, Speaker A: Enjoyed the beginning of the book series, part one, part two. We were usually successful. So sorting connected components, dejkstra's algorithm, we were getting running times that were quite close to linear.
00:11:20.946 - 00:11:43.182, Speaker A: Something happened in part three. However, once we did our dynamic programming bootcamp and we had a number of problems where we did successfully beat the pants off of exhaustive search, we got algorithms that ran in polynomial time, but we weren't very often getting linear running times anymore in our dynamic programming case studies. Let me give you one example, the sequence alignment problem.
00:11:43.182 - 00:12:05.022, Speaker A: So that was covered in chapter 17 of part three just to remind you of the problem. So you're given two strings as input and they're over some alphabet like ACGT, and you're responsible for inserting gaps into the two strings so that they have the same length. And you want to do it so that they nicely align, which means you minimize the cost of an alignment.
00:12:05.022 - 00:12:18.050, Speaker A: Now, also part of the input, in addition to the two strings, I'm going to give you a penalty. So what you're going to have to pay every time a gap gets inserted. I also am going to tell you what you have to pay whenever you mismatch two different symbols in your alignment.
00:12:18.050 - 00:12:31.014, Speaker A: So all of those penalties are part of the input and the sequence alignment problem. You want to have the minimum cost alignment of the two input strings. So that was one of our six dynamic programming case studies, and we gave a quadratic time algorithm for it.
00:12:31.014 - 00:12:43.170, Speaker A: We used a quadratic number of subproblems and we solved each in constant time. So quadratic time overall. So running time O of N squared, where n was the longer length of the two input strings.
00:12:43.170 - 00:13:30.770, Speaker A: Throughout this book series, I've trained you whenever you see an algorithm for a problem, you always want to ask, can we do better? So we had a quadratic time algorithm for sequence alignment. That's a lot better than exhaustive search, but why stop there? Can we do better than quadratic time? So what's amazing about the strong exponential time hypothesis is it actually provides evidence of why we have not been able to do better than quadratic time for the sequence alignment problem. Let me now give you the formal statement, a pretty difficult result that was proven just a few years ago in a paper by Backers and Indik.
00:13:30.770 - 00:13:47.146, Speaker A: The formal statement is that if you had a sub quadratic time algorithm for the sequence alignment problem, so an algorithm running in time, big O of N to the two minus epsilon for some constant epsilon. So think of epsilon as say, maybe zero one. So if you had an O of N to the 1.99
00:13:47.146 - 00:14:16.974, Speaker A: time algorithm for sequence alignment, then the Seth would be false. In other words, you could extract from a sub quadratic time sequence alignment problem a better than exhaustive search algorithm for satisfiability. Said another way, the only hope, the only avenue for improving over that needleman wundtz algorithm and beating quadratic time for sequence alignment, the only way available to us is to make major progress on Satisfiability.
00:14:16.974 - 00:14:46.310, Speaker A: And that is a stunning connection between two seemingly radically different problems, sequence alignment and satisfiability. So how would you ever prove this result? Well, again, we're not going to go through all the details, but at a high level, you prove this result just using a reduction, just like the kinds of reductions we used in all of our NP hardness proofs as usual, the reduction will be from a problem we already know or already assuming is hard. And then we're going to reduce that to the target problem to spread intractability.
00:14:46.310 - 00:15:10.974, Speaker A: So for us, our known hard problem is going to be KSAT, right? That's what the strong exponential time hypothesis asserts running time lower bounds for. So we're going to have a reduction from the known hard problem KSAT to the problem that we care about right now, which is sequence alignment. So in the cartoon, we're going to have a reduction it's mapping instances of the KSAT problem to instances of sequence alignment.
00:15:10.974 - 00:15:39.520, Speaker A: The reduction, of course, doesn't know whether or not the instance it was given was satisfiable or not, but we're hoping the satisfiability status will be reflected in the best possible alignment of the sequence alignment instance that we construct. Specifically, whenever there's a satisfying truth assignment, we're hoping that there's a low cost alignment from which we can extract a satisfying truth assignment. And then whenever we have an unsatisfiable three side instance, we're hoping that should be reflected in the optimal alignment having relatively high cost.
00:15:39.520 - 00:15:47.866, Speaker A: But this doesn't seem to make sense, right? The KSAT problem. It's NP hard. We know that the sequence alignment problem we know is polynomial times solvable.
00:15:47.866 - 00:16:45.682, Speaker A: So how could we ever reduce this NP hard problem, KSAT to an easy problem like sequence alignment without refuting the P not equal to NP conjecture? Well, this reduction is going to look just like all the other reductions that we've seen, except with one difference, which is that it's going to take its three Hassat instance that it's been given and it's going to construct an exponentially large instance of sequence alignment. Specifically, if our reduction is given an instance of KSAT that has n decision variables, it is going to construct an instance of sequence alignment where both of the input strings have length ballpark two raised to the n over two. Why two raised to the n over two? Well, notice that if you had a sequence alignment instance where that was the input size two to the N over two, and you ran a quadratic time algorithm for it, something that ran in the input length squared that would have running time two raised to the N over two squared, which would be two to the N.
00:16:45.682 - 00:17:00.278, Speaker A: So if we just had our normal needleman wound sequence alignment quadratic time algorithm, then we'd only get a two to the N time algorithm for the KSAT problem. Which we have anyways, because of exhaustive search. However, for the same reason.
00:17:00.278 - 00:17:21.710, Speaker A: If hypothetically, we had a sub quadratic time algorithm for sequence alignment, say, with running time big O of n raised to the 1.99, that would, for the exact same reason, give us a better than exhaustive search algorithm for the KSAT problem for arbitrarily large positive integers k. And that's exactly falsifying what the strong exponential time hypothesis asserts.
00:17:21.710 - 00:17:54.510, Speaker A: As I mentioned, this result is just a few years old and in fact, these types of results, I think has been one of the most interesting developments in computational complexity theory over the last ten years, with really meaningful algorithmic implications. So for decades, we've had not just sequence alignment, but lots of other problems where we knew they were polynomial time solvable. We didn't have a linear time algorithm, and we were annoyed.
00:17:54.510 - 00:18:25.078, Speaker A: We seemed stuck at higher than linear time, quadratic time, or worse, but we lacked any explanation explaining why we were stuck at quadratic time or worse. The theory of NP hardness originally did not seem helpful because it was about problems that seemed to require exponential amount of time. But with this trick, with using reductions that blow up the input size by an exponential amount, you can actually transfer assumptions about the difficulty of NP hard problems into assumptions about the difficulty of polynomial time solvable problems.
00:18:25.078 - 00:18:36.000, Speaker A: So this relatively new area of complexity theory has its own name. It's known as fine grained complexity. So that's a good term to search on if you want to learn more.
00:18:36.000 - 00:18:45.610, Speaker A: Coming up next is the last video of this optional series of chapter 23, where I'll tell you about NP completeness. I'll see you then. Bye.
