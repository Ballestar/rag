00:00:00.090 - 00:00:38.274, Speaker A: As far as what we're going to do today, we're going to start talking about parameterized analysis. So that'll be a theme for the next couple lectures and it'll recur throughout the course. And we're going to apply it specifically to the exact same online paging problem that we talked about on Monday. We'll talk about how to parameterize the performance of Caching algorithms via the locality in the data. And that'll allow us to finally say in a rigorous way that the least recently used policy is better than alternatives like first in, first out. So let me just sort of remind you conceptually what we're in the middle of doing and what we did on Monday. So we talked about online paging and I introduced you to the dominant paradigm traditionally of competitive analysis.
00:00:38.274 - 00:01:25.314, Speaker A: And competitive analysis got a pretty bad report card when we scored it according to our goals for algorithmic analysis. So the first goal of either predict or explain the empirical performance, it was a total disaster because the competitive ratios, remember growing with the cash size, were sort of laughably huge. There was no way we could take them literally as performance predictions. That was the first issue. The second issue is that while it was a partial success and that it did identify least recently used in some sense our ground truth solution as an optimal online algorithm, other things were also optimal, including relatively clearly inferior solutions like flush when full. So Monday I also introduced you to the idea of resource augmentation. And so what we did there is the good news is we made absolutely no compromises as far as doing worst case analysis.
00:01:25.314 - 00:02:06.738, Speaker A: We still got guarantees, input by input. But by changing our comparison, changing the benchmark, comparing to optimal solutions while still offline with smaller caches, we got much more sort of meaningful and interpretable guarantees. So for example, if we had doubled the size of the offline optimal, we were getting within a factor of two and again, even in the worst case. So that was really good for our first goal. It gave us sort of bounds that we could perhaps interpret as some kind of guarantee on real instances. But you'll see on the homework number two, we actually made no progress Monday on differentiating LRU from other algorithms. So on the comparison goal of getting a nice ranking of algorithms, we haven't really done anything yet and that's going to be the purpose of today.
00:02:06.738 - 00:02:29.890, Speaker A: So intuitively the reason we believe LRU is better than alternatives is because of properties of real world data. So we shouldn't expect to see a theorem which singles out LRU without somehow articulating those properties of real world data. So that's what we're going to do today. We're really going to parameterize data according to the locality and that'll allow us to prove that LRU is better than alternatives.
00:02:29.990 - 00:02:30.640, Speaker B: Okay?
00:02:31.090 - 00:03:00.814, Speaker A: So that's what we're doing. Any questions? Makes sense? Motivation clear. Okay, so just to sort of couch this as part of a bigger picture or a bigger theme of the course. Let me tell you what I mean by parameterized analysis. And in fact, we already saw a really nice example of this exactly a week ago. So fix some problem. It could be paging, it could be 2D maxima.
00:03:00.814 - 00:03:39.358, Speaker A: We'll see many other examples of problems in this class. And think about the set of inputs, okay? So there's this big rich set of all the different inputs you might have for this problem. And so parameterized analysis is a way of trying to think at a pretty detailed but still analyzable level of granularity about algorithm performance. So the first thing you want to do is you want to have some kind of natural parameterization of all inputs. And intuitively you want to think about this parameter as somehow measuring the Easiness or the difficulty of this particular input.
00:03:39.454 - 00:03:40.100, Speaker B: Okay?
00:03:44.230 - 00:04:24.042, Speaker A: So for example, we might hope that paging sequences in some sense get easier as they in some sense have more locality, and we'll have a formal definition of that. Basically you wind up having this ranking of inputs according to this parameter, which of course you have to make up, you have to sort of come up with a good one. And another way of thinking about this, for those of you who have seen it, is we can sort of interpret this as a condition number, if that's a phrase that you're experienced with. Again, a sort of measuring sort of input difficulty in some sense. Now you want to somehow connect the Easiness of an input to the performance of algorithms.
00:04:24.106 - 00:04:24.286, Speaker B: Okay?
00:04:24.308 - 00:04:39.458, Speaker A: So again, anyone can just sort of propose numbers about inputs for them to be relevant for algorithm analysis. It should be the case that you can express the performance of some algorithm, or maybe many algorithms in a sensible way in terms of this parameter.
00:04:39.554 - 00:04:40.200, Speaker B: Okay?
00:04:40.570 - 00:05:38.930, Speaker A: So you want to talk about the performance analysis of an algorithm, the number of page faults, the running time, whatever as a function of the parameter. And what we somehow have in mind is that as the input grows easier, the performance of our favorite algorithm or every algorithm is going to get better and better and better, okay? Performance should improve with the Easiness. So we've already seen a nice example, a parameterized analysis, which was last week the Kirkpatrick Seidel algorithm for 2D Maxima. Now, the point of that lecture was not parameterized analysis per se. The point of that lecture was instance optimality, proving that this algorithm was best possible in some sense, input by input. But as we said many times, that lecture a prerequisite to proving instance optimality result. You need to have a lower bound that applies to every single algorithm, which is tight.
00:05:38.930 - 00:06:26.194, Speaker A: And certainly a minimum prerequisite is that you at least understand completely the running time of your protagonist algorithm, in that case Kirkpatrick sideel. So we needed tight input by input upper bound on Kirkpatrick sideel. And then we had this sort know, hierarchy of finer grained guarantees, right? So first we said, oh, well, what if we only parameterized by the number of points n the input size? Well, then it was easy to prove an upper bound of n log n, and there exist endpoint data sets where that's tight, but it wasn't tight input by input. It was only tight in the worst case for each value of n. So then we said, okay, let's have a more sensitive, a more fine grained parameterization. Let's also throw in the output size, the number of maximum H, and then we're allowed to have some running time analysis, which depends on both N and H. And on the homework.
00:06:26.194 - 00:06:59.490, Speaker A: You gave an upper bound of N log h, and that's also tight in the worst case, but it still wasn't good enough. For instance, automatic result. So we parameterized it at an even finer level of granularity. Okay, so all of these are examples of parameterized analysis. What's changed is just sort of how many parameters do you have to work with and so how tightly can you fit your analysis to the actual performance of the algorithm. The first parameterization was just by n, the number of points, then it was by n and h. And then the final one was this sort of crazy expression where you minimize over all the legal partitions and you have this entropy term, the sum over the i, the cardinality of si log n over si.
00:06:59.490 - 00:07:21.654, Speaker A: And so that actually was sort of such a fine grained parameter. It actually characterized up to a constant factor, the running time of the Kirpatrick Seidel algorithm. So that was sort of a just wildly successful paramerise analysis of a very nice algorithm. And again, remember, this is always just in the analysis, right? Usually when we're doing this, the algorithm we have in mind is dirt simple, or at least small lines of code.
00:07:21.692 - 00:07:21.894, Speaker B: Okay?
00:07:21.932 - 00:07:29.286, Speaker A: Kirkpatrick Seidel, it's very clever algorithm, but it's very succinct to describe, very elegant, least recently used, very simple algorithm.
00:07:29.318 - 00:07:29.514, Speaker B: Okay?
00:07:29.552 - 00:08:13.414, Speaker A: We want to do the same kind of approach there, all right? So remember, all of the kind of work that we're doing in parameterized analysis is just in hindsight trying to figure out when the algorithm works well and when it doesn't work well. All right, so why do this? So what's the motivation? Well, there's two motivations. Well, at least two motivations. The first one is just if there's an algorithm you really care about, that for example, you already have it implemented and it's sort of at your ready disposal and you can use it on problems if you want. You want theory to kind of tell you when is it going to work. Well, okay, so again, the best case scenario is you just have some great worst time bound, it always runs in linear time, then great use, it always right. But for lots of algorithms, either the running time or for heuristics.
00:08:13.414 - 00:09:02.498, Speaker A: The solution quality will be good on some inputs and bad on some other inputs. And if you have a very fine grained understanding of its performance and you know something about your inputs, you know its condition number, its parameter value, then you can reason about whether this is the right algorithm to use in that setting or not. To give a very simple example, which you should be familiar with, just think about graph algorithms. So when we studied graph algorithms in undergraduate in 161, we actually parameterized the running time not just by one thing, but by two things the number of vertices usually called N, the number of edges usually called M. Okay? And so you depend on both. And if you start looking at the running time bounds of different algorithms for a common problem, say, all pair shortest paths, you see, you know, there's some combinations of NMM where you want to use one thing, like maybe you want to use dijkstras and times. And then there's other graphs, like, say, dense graphs.
00:09:02.498 - 00:09:13.830, Speaker A: Or you want to use something else, like Floyd Warshaw. So depending on the edge density already that's sort of a very coarse parameterization that tells you whether you should use algorithm A or algorithm B for a problem based on some easily computed statistic of the input.
00:09:13.910 - 00:09:14.250, Speaker B: Okay?
00:09:14.320 - 00:09:50.114, Speaker A: So that's reason number one. Fine grained understanding of an algorithm performance tells you when to use it and when to not use it. The second reason, and this is going to show up in this class, especially in the smooth analysis segment, but it's also going to be really kind of the overarching narrative today is suppose you have an algorithm and it works great in practice. So meaning on certain benchmarks or certain classes of inputs and you want to have an explanation. So again, here we're going to be the natural scientist explaining this observed phenomenon of good empirical performance. An approach to doing that which is exactly what's going to play out today is. So first of all, you do this kind of parameterized analysis.
00:09:50.114 - 00:09:55.750, Speaker A: So you say, okay, well, some inputs are easy, some inputs are hard. For example, some inputs have a lot of locality, some inputs don't.
00:09:55.830 - 00:09:56.460, Speaker B: Okay?
00:09:57.470 - 00:10:07.854, Speaker A: And so what you might want to then do is say, well, as long as the input is easy, as long as it has the sort of small parameter value, then this algorithm that's used in practice provably performs really well.
00:10:07.892 - 00:10:08.094, Speaker B: Okay?
00:10:08.132 - 00:10:34.358, Speaker A: So maybe the worst case analysis says it's not so good. Like think about the simplex method, but at least on easy inputs according to some condition number, according to some parameter, it is good. So that's step one. Step two is you make some argument about why real world instances actually have small parameter values are easy in this sense, okay? Maybe this is just something you compute on your benchmarks and you just say, look, I computed the parameter and it's small, for example, or maybe you have some theorem which says, oh, if it's a randomly generated instance, then it's easy in this sense.
00:10:34.444 - 00:10:34.838, Speaker B: Okay?
00:10:34.924 - 00:10:44.460, Speaker A: So it's a two step approach to explaining good empirical performance on real world instances, okay? Prove that the algorithm is good on easy instances. Prove that real world instances are easy.
00:10:45.630 - 00:10:46.380, Speaker B: Okay?
00:10:46.990 - 00:11:31.354, Speaker A: So like I said, I'm giving you this high level discussion now because there'll be several parts of the point that we'll sort of be able to slot into this kind of philosophy, all right? But now let's just do this kind of very literally for a problem we're very comfortable with at this point, which is the online Paging problem, okay? All right, so now zooming in to Paging. So what should the parameterization be? So again, the intuition. We know what we want. We want over here. We want there to be page sequences which exhibit a lot of locality, and over here are page sequences which don't. So we need some way to measure or quantify the locality of a page sequence. And there's not a unique way to do that.
00:11:31.354 - 00:12:50.000, Speaker A: I'll show you there'll be others on the homework, but I'm going to show you a very natural one here. This is an old idea not invented by theorists, it's been around for a long time called the working set model, almost 50 years old. So we're going to parameterize locality by a function from the natural numbers to the natural numbers. And the point of this function is basically to say how many repeats, how many repeated page requests you're going to have in a given window, a given subsequence of a page sequence, okay? And so in some sense, the more locality you have, the more page repeats you're going to see. All right? So formally we'll say that a page sequence sigma conforms to this function f if for all window length W and for all windows of size W. Okay? So by window here, I just mean somewhere in this page sequence w requests in a row, then the number of distinct pages requested should be at most F of.
00:13:00.010 - 00:13:00.760, Speaker B: Okay?
00:13:02.090 - 00:13:09.254, Speaker A: So F of W is an upper bound on how many different I mean, you're going to see W page requests total, but the question is how many different pages are those?
00:13:09.292 - 00:13:09.734, Speaker B: Four.
00:13:09.852 - 00:13:14.902, Speaker A: And it's going to be a most F of W four sequences that conform to a given function.
00:13:15.036 - 00:13:15.720, Speaker B: Okay?
00:13:16.410 - 00:13:17.140, Speaker A: So for example.
