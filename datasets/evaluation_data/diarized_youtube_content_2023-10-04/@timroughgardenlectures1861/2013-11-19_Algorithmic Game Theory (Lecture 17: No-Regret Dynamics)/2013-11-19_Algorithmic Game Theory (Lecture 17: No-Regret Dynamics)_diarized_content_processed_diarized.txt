00:00:00.090 - 00:00:29.394, Speaker A: You something to remind you where we are in the course of topic, we started on Wednesday, we started asking the question do players actually reach equilibria in games? We've been spending all this time assuming that systems are at equilibrium and when and how can that be justified. And not only can they reach an equilibrium in principle, but can they do so quickly? And again, the reason we're doing that is to justify equilibrium analysis. So one special case of that, when you care about the objective function value, that's a price of anarchy analyses.
00:00:29.394 - 00:00:41.766, Speaker A: Of course there's other reasons you might focus on equilibria as well. So on Wednesday what we did is we talked about one very important types of dynamics, best response dynamics. That's where players take turns going one at a time.
00:00:41.766 - 00:00:55.626, Speaker A: While you're not at a pure Nash equilibrium, a player updates to its best response given what the other K minus one players are currently doing. So best response dynamics make a lot of sense in potential games. That was the focus on Wednesday where they're guaranteed to converge eventually to pure Nash equilibria.
00:00:55.626 - 00:01:12.574, Speaker A: And we studied when do they converge quickly and when do you converge to outcomes that are almost as good as Nash equilibria. And so today the plan is to discuss a second, equally important type of learning dynamics called no regret dynamics. So these dynamics also make very good sense beyond potential games.
00:01:12.574 - 00:01:23.174, Speaker A: Potential games cover a lot of the applications that we care about, but not all of them. So no regret dynamics have very broad sweep. And we'll also see that a bonus is they converge to an approximate equilibrium very quickly.
00:01:23.174 - 00:01:40.534, Speaker A: Now it's not going to be a Nash equilibrium, it'll be something else we mentioned in passing and we'll talk about again today called coarse correlated equilibrium. So those are some of the reasons to care about no regret dynamics. Now the sort of foundations of regret minimization happen first just in a single player context.
00:01:40.534 - 00:02:00.690, Speaker A: And so the bulk of the lecture we're just going to be thinking about. There's one player trying to make decisions, playing against an adversary or playing against nature. So at the end of the lecture, I'll tie in all of this single player discussion to the meaning in games where you have lots of players playing against each other, okay, but for the next 45 minutes plus, we're just going to focus on a single player.
00:02:00.690 - 00:02:16.120, Speaker A: So here's the setup. So there's a set of actions and for this entire lecture, N will denote the number of actions. And these are always the same over time.
00:02:16.120 - 00:02:30.140, Speaker A: Even the binary case is interesting. So at times in the lecture if you like, think about there even just being two actions. And so then this player will have to make decisions, meaning pick an action from a over time.
00:02:30.140 - 00:02:45.154, Speaker A: So there's going to be a time horizon, capital T. In this lecture we're going to think of capital t as known to the player on the exercise set. I'll ask you to extend the guarantees today to where the time horizon is unknown as well.
00:02:45.154 - 00:02:57.906, Speaker A: But for today, think of capital T as known. And here's what happens at each time step. So you have to pick an action, a strategy you are allowed to randomize, and as we'll see, it's essential that you randomize.
00:02:57.906 - 00:03:47.682, Speaker A: So at each time step you pick a mixed distribution, say PT over the actions A, and then an adversary, having seen the distribution that you've chosen, now picks a cost vector which indicates for each action that you might have chosen or that you might choose randomly what cost you'll incur for that action. And we're going to assume that the costs are bounded. In particular for today, we'll be always assuming that the costs are real numbers between zero and one.
00:03:47.682 - 00:03:56.230, Speaker A: So if you want to think about a routing game or something like that, imagine we've scaled all of the payoffs excuse me, all of the costs, so that they lie between zero and one.
00:03:56.380 - 00:03:57.080, Speaker B: Okay.
00:04:00.670 - 00:04:18.618, Speaker A: So then an action is chosen at random from the mixed strategy that you committed to and then your cost is, as promised, just the part of the cost vector corresponding to the action you wound up picking.
00:04:18.794 - 00:04:19.520, Speaker B: Okay.
00:04:21.650 - 00:04:48.434, Speaker A: So the freedom, of course, is how do you choose the mixed strategy PT at each time step T. How do you even reason about what a good strategy might be? Okay, now, right, so what are some examples? So you could imagine these actions being sort of different investment strategies, maybe even in the binary case for a single stock. It could be to buy or sell a stock and the cost indicates how much you win or lose depending on which action that you chose.
00:04:48.434 - 00:05:14.590, Speaker A: If you want, you can imagine these actions as being routes from home to work. And these are different sort of ways you might drive to work in the morning and the costs indicate the delays during rush hour traffic that particular morning. Eventually, when we go back to the multiplayer case with games, the action set is just going to be the strategy set of a single player and this cost vector is going to be induced by the strategies S minus I chosen by the other K minus one players.
00:05:14.590 - 00:05:55.340, Speaker A: So eventually that's how we'll instantiate this in the game setting. All right, so the first time you see this, I think you might rightfully think, well, if this is a game, it seems a little unfair, seems a little stacked against us, right? So we've got to go first, which is kind of annoying, right? So we go first and the adversary goes second and can penalize the strategies we're likely to take, right? So we need to understand what can we even hope to try to accomplish in this regret minimization setting? Question so we'll get to it. Yeah.
00:05:55.340 - 00:06:36.730, Speaker A: So what can we hope to prove observations that govern the best case scenario for what we might be able to accomplish. So the first observation is that what's not realistic as a comparison is the best sequence of actions we could have taken in hindsight. Or if you like, the best sequence of actions, meaning minimum cost sequence of actions we could have taken had we known all of the cost vectors in advance.
00:06:36.730 - 00:07:45.220, Speaker A: So I'll let you think about, while I write on the board, think a little bit about if we wanted to prove a statement that says we have an algorithm that does almost as well as if we knew all the CTS up front, why does that seem hard to pull off? So that is if we sum over all time steps, on each time step, we look at the minimum over the options, over the cost. So I claim this is way too strong a benchmark to measure the performance of our algorithms, which of course do not know the future, do not know the cost vector now or later when it has to make a decision. So it seems like anyone have a sort of simple suggestion of why this would be hard to do or impossible to do.
00:07:47.910 - 00:07:48.660, Speaker B: Yeah.
00:07:51.030 - 00:08:17.594, Speaker A: So what if there's only like two actions? Yeah, I mean, isn't part of it that the adversaries, like the cost is dependent on what strategy you select? Sure. So concretely, so what I'm looking for is how could the adversary make this really small and yet every algorithm could be very big, have very big cost. So the example where all the costs are won all the time is actually an easy example in a sense because there's nothing you can do.
00:08:17.594 - 00:08:30.702, Speaker A: Everything is equally well. So I mean, if the input is a bad input, you'll never regret any decisions, right? All decisions were bad. So what you're worried about with an algorithm is that there was some awesome way to make decisions and you totally missed it, you blew it.
00:08:30.702 - 00:08:45.782, Speaker A: And so if this is our benchmark, then there will be examples where that's the case. No matter how smart you are in crafting your algorithm, you can be very far away from the best action sequence in hindsight. So imagine there are even just two actions on an every day.
00:08:45.782 - 00:08:56.840, Speaker A: So you have two ways of going from home to work and every day there's going to be a traffic jam on one of them and not the other. So one will have cost zero and one will have cost one. And the rub, of course, is you have no idea which is which.
00:08:56.840 - 00:09:10.460, Speaker A: And you can imagine that the adversary even just flips a coin. So every day the adversary flips and if it's heads, the top route has a traffic jam and house costs one. If it's heads, then the bottom route costs one and the other route is always zero.
00:09:10.460 - 00:09:22.080, Speaker A: So any sequence of cost vectors generated in this way always has a zero on every day. So this is going to be zero at every day t. There was always an action in hindsight with cost zero.
00:09:22.080 - 00:09:30.786, Speaker A: But as an algorithm designer, there's nothing you can do. So you have to make a decision. The cost vector is equally likely to be 10 or zero one.
00:09:30.786 - 00:09:41.894, Speaker A: So actually, no matter what decision you make, your expected cost will be one half every day. So your cost overall T days in expectation will be T over two. But in hindsight there was something with cause zero.
00:09:42.092 - 00:09:42.840, Speaker B: Okay?
00:09:46.170 - 00:10:16.850, Speaker A: So the reason is that the adversary can enforce simultaneously this very strong notion of opt zero, but your cost is at least T over two. Okay? And we want to do a lot better than that. So we're not going to be satisfied with being T over two away from some benchmark.
00:10:16.850 - 00:10:39.350, Speaker A: So here's the most important definition of the lecture, which is that of regret. It's a kind of regret called external regret. And the idea is basically to exchange this sum and this minimum.
00:10:39.350 - 00:10:58.080, Speaker A: So rather than comparing to the optimal action sequence in hindsight, which can optimize separately for each day t, we're going to compare ourselves to the performance of the best fixed action. So someone who has knowledge of all of the cost vectors but plays exactly the same action day after day.
00:11:15.370 - 00:11:15.878, Speaker B: It.
00:11:15.964 - 00:11:24.520, Speaker A: So we're going to look at the time average, although that's not especially important, and we say, well, let's look at how well an algorithm does.
00:11:29.610 - 00:11:30.214, Speaker B: It.
00:11:30.332 - 00:12:17.990, Speaker A: So this is your cost and then with the benefit of hindsight so in order the mins on the outside. Now let's ask how well could we do with a fixed strategy? So this quantity is called the regret of an algorithm on a given sequence of cost vectors. So the adversary generates the CTS, your algorithm generates the actions, the ATS.
00:12:17.990 - 00:12:31.790, Speaker A: And this difference, the extent to which your cost is higher, then the best fixed action is called the regret. Whenever I say regret in lecture, this is what I'll mean. Let me just mention a couple of modifiers.
00:12:31.790 - 00:12:50.370, Speaker A: So there are various notions of regret. We'll look at a different one on Wednesday. This version is called external regret and obviously I'm time averaging and I'll do that throughout the lecture.
00:12:50.370 - 00:13:17.574, Speaker A: You could also speak about the cumulative regret if you wanted by hiding the normalization factor, but it wouldn't make a big difference. So rather than comparing to this overly strong benchmark, we're going to compare the goal is going to be to get the regret as low as possible, ideally very close to zero. Now you might ask why is this a good thing to do? Why is this a good definition, a good benchmark? There's a few reasons.
00:13:17.574 - 00:13:41.074, Speaker A: First of all, it's sort of the sweet spot where it's tractable. I'll give you an algorithm that achieves very low regret today, but secondly, it's really nontrivial and we'll go through a couple more lower bounds governing what we could hope for, and we'll see that there's a sense in which you can't do much better than getting close to this benchmark. So it's the right benchmark for giving you algorithms to make smart decisions in the face of an unknown future.
00:13:41.192 - 00:13:44.322, Speaker B: Okay, yeah.
00:13:44.376 - 00:14:02.490, Speaker A: Question. Do you mean a fixed specific action or like a fixed distribution over cost? So in this minimizer without loss, you can focus on a pure action by linearity. So you could make it mixed strategies, but it wouldn't change the definition.
00:14:02.490 - 00:14:37.526, Speaker A: All right, so this is the benchmark. Second thing is, in our algorithm, even for this weaker benchmark, we're going to need to be randomized. So deterministic algorithms, by which I mean choosing the distribution PT to put all of its mass on a single action, that's not going to work.
00:14:37.526 - 00:15:06.640, Speaker A: At least it's not going to work as well as we'd liked. In fact, it's deterministic algorithms that really kind of expose how harsh this model is to the algorithm designer. So if your algorithm is deterministic, so that means you broadcast you're going to play action three at this time step.
00:15:06.640 - 00:15:18.580, Speaker A: So it's actually pretty obvious how an adversary should respond with a cost vector. Well, if you're going to play three, I'm going to make the cost of action three one, and I'm going to make the cost of every other action zero.
00:15:19.270 - 00:15:19.874, Speaker B: Okay?
00:15:19.992 - 00:15:41.434, Speaker A: So I'm going to ensure that you pick the unique worst action every single time step that'll result in the cost of the algorithm being T. You pay one every single time step. Now, again, that wouldn't be a big deal if the benchmark was also T, but it's not because all but one action is going to have cost zero every single time step.
00:15:41.434 - 00:15:55.150, Speaker A: If you have N actions, the largest this could possibly be is T over N for N actions. So even for the binary action case, you're going to be off by at least a factor of two. So you're going to have one half time average request.
00:15:55.150 - 00:16:33.260, Speaker A: All right, so summarizing adversary can force your cost to be equal t, while best fixed action, is no worse than T over N. And again, we're not going to be happy with guarantees of that form. We're going to want to actually get that thing very close to zero, whereas here, it's much closer to n.
00:16:38.590 - 00:16:38.858, Speaker B: All.
00:16:38.864 - 00:16:59.346, Speaker A: Right, so last observation governing what we might hope for, even if we just try to compete with external regret, and even if we use randomized algorithms. And this is sort of the sense in which I meant that this benchmark is nontrivial. It's not too easy.
00:16:59.346 - 00:17:33.390, Speaker A: In this certain sense, it so I claim it's actually impossible for any algorithm. So I claim every algorithm suffers regret. First of all, positive regret in the worst case, and it grows like the square root of log n remembers the number of actions over capital T, where T is the time horizon.
00:17:33.390 - 00:18:03.094, Speaker A: Now, to be clear, this is not that bad. As T grows large, this quickly goes to zero, which is great, and that's what we're going to be shooting for. But it just still points out that even with respect to the regret benchmark, even using randomized algorithms, the best case scenario is to have that go to zero.
00:18:03.094 - 00:18:07.590, Speaker A: And this governs how fast we could even possibly hope for it to go to zero.
00:18:07.740 - 00:18:08.440, Speaker B: Okay?
00:18:09.290 - 00:18:23.654, Speaker A: So let me just I'm not going to prove this in detail, but let me just give you the idea. And I'm going to give you the idea for the binary strategy case, but exactly the same idea. So for the binary strategy case, we're only shooting for an omega of one over root T lower bound.
00:18:23.654 - 00:18:41.854, Speaker A: That's what I'm going to give you. But the exact same idea for general N gives you the more general lower bound. And it's actually exactly the same idea as in the first observation, where we're going to think about the adversary.
00:18:41.854 - 00:18:47.446, Speaker A: Again, binary strategies. And the adversary is just going to pick cost vectors at random. It's either 10 or zero one.
00:18:47.548 - 00:18:48.150, Speaker B: Okay?
00:18:48.300 - 00:19:00.102, Speaker A: 50 50 each. So we argued before that any algorithm has expected cost T over two because you don't know if the cost vector is zero one or 10.
00:19:00.236 - 00:19:00.726, Speaker B: Right?
00:19:00.828 - 00:19:15.070, Speaker A: Now, before, the reason that killed us is because our benchmark was too strong. And in hindsight, there was this optimal action sequence with cos zero, and that's certainly not true anymore. So now that we have a more sensible benchmark that's fixed action, in hindsight, it's not the case that the right hand side there is zero.
00:19:15.070 - 00:19:21.806, Speaker A: Certainly not. Okay, but it's not T over two either. It is going to be smaller than T over two.
00:19:21.806 - 00:19:30.514, Speaker A: All right, because what am I doing? I'm basically just flipping a sequence of capital T coins. If it's heads, the cost vector is 10. If it's tails, the cost vector is zero one.
00:19:30.514 - 00:19:44.566, Speaker A: Now, if you flip T coins, you're very unlikely to get exactly T over two heads. If I got exactly T over two heads, then both action one and action two would have cost T over two each. Fixed action would actually be bad T over two.
00:19:44.566 - 00:19:59.754, Speaker A: But if you flip T coins, there's a standard deviation involved. The expectations T over two, there's a standard deviation involved, and it's basically a root T. Okay? Law of Large numbers binomial approximations, however you want to think about it.
00:19:59.754 - 00:20:18.820, Speaker A: So you're going to get T plus minus root T heads when you flip T coins. So the better of these two actions, so you have T plus root T of one of them and T minus root T of the other. So if you have T minus root T tails only, that says the action corresponding to the tails being zero, that will have cost in hindsight only T over two minus root T.
00:20:18.820 - 00:20:48.300, Speaker A: So the right hand side the better. Of the two possibilities will be with high probability root t better than the expectation t over two, which is the expected cost of any algorithm adversary. Randomizes CT equals 10 or zero one.
00:20:48.300 - 00:21:17.734, Speaker A: The expected algorithm cost t over $2. Expected best action cost equals T over two minus theta of root t. So inside the bracket, it's going to be theta of root t.
00:21:17.734 - 00:21:40.634, Speaker A: Then once we divide by t, we get theta of one over root t. Okay? So that's going to be the regret. So that's why we're not going to do better than this benchmark.
00:21:40.634 - 00:22:01.086, Speaker A: And indeed, our regret is not going to approach zero any faster than at this rate, this function of n and T. Okay? All right. So questions? Okay, so that's on the lower bound side, let's switch to what we that's what we can't do.
00:22:01.086 - 00:22:34.038, Speaker A: Let's switch to what we can do. So an algorithm and to satisfy this definition is necessarily, as we now know, a randomized algorithm is called no regret. If no matter what the cost vectors are, the expectation we're here the expectation is over the coin flips in the algorithm.
00:22:34.038 - 00:22:44.720, Speaker A: It's a randomized algorithm. So if the expectation goes to zero sorry, the expected regret goes to zero as T grows large.
00:22:46.130 - 00:22:46.880, Speaker B: Okay?
00:22:47.330 - 00:23:06.462, Speaker A: So this is not ruled out by any of our three observations. So if you have a randomized algorithm and we use regret, it's conceivable that our regret goes to zero as quickly as this with T to be no regret. All it insists is that this is the low of one, that this is going to zero as T goes to infinity.
00:23:06.606 - 00:23:07.300, Speaker B: Okay?
00:23:09.590 - 00:23:33.100, Speaker A: So in fact, I've stated this for a fixed set of cost vectors that the adversary in some sense chooses up front before anything, before the ball gets rolling. In fact, the no regret guarantee I'm going to show you holds even if these cost vectors are chosen adaptively by the adversary having seen what you played in the past. So that's our goal, to design no regret algorithms in this sense.
00:23:33.100 - 00:24:10.920, Speaker A: So here's the main theorem. It's a theorem that's been discovered many different times, many different people in many different communities. So certainly one key name is Hanan, who is a game theorist, studied this in 57.
00:24:10.920 - 00:24:34.478, Speaker A: Another key reference is Littlestone and warmouth, and there are many others. So the conceptual takeaway here is actually you can achieve this goal. There are algorithms for which the regret goes to zero as you play the game sufficiently long.
00:24:34.478 - 00:25:19.002, Speaker A: Moreover, they're simple, very lightweight, easy to implement, easy to think about, and something which I don't necessarily expect you to remember a month from now, but at least for today is extremely cool, is that you can even get an optimal regret bound with one of these simple algorithms. So even with regret going to zero as the square root of the logarithm of the number of actions over the time horizon, t. And I will give you an algorithm that does indeed achieve this bound.
00:25:19.146 - 00:25:19.840, Speaker B: Okay?
00:25:21.570 - 00:25:30.930, Speaker A: So again, let me remind you when I say regrets, so this is that expression there. It's time averaged. And it's this notion of external regret with respect to the best fixed action.
00:25:30.930 - 00:25:50.870, Speaker A: Also, don't forget, n is the number of actions for this whole lecture. So maybe a better way to think about this is, well, suppose you had some target on the regret. Suppose you wanted to know, wanted to know how long you have to play a game and experiment before your regret is meeting some guarantee that you had in mind, some epsilon.
00:25:50.870 - 00:26:33.320, Speaker A: So immediately from the form of this is if you want your regret to be at most epsilon, then you actually don't have to play the game that many rounds at all, considering as soon as you play it, log n, logarithmic in the number of your actions over epsilon squared, your regret will have dropped to epsilon. So I need T, I'm going to drop the constant, but basically log n over epsilon squared to get regret at most epsilon, okay? And as a bonus, the algorithm itself is very natural, very simple.
00:26:39.930 - 00:26:40.680, Speaker B: Okay?
00:26:44.110 - 00:27:07.410, Speaker A: So it's an algorithm that has a lot of applications outside of algorithm game theory. Some of you may even have seen it in an algorithms course. It's an algorithm called these days you hear a lot of people call it multiplicative weights.
00:27:07.410 - 00:27:38.006, Speaker A: Other names you sometimes hear are hedge or randomized weighted majority, almost all no regret algorithms follow two key principles. The first is, well, look, we know it got to be randomized, but it's clear we shouldn't just choose an action uniformly at random every day. That'd be kind of stupid.
00:27:38.006 - 00:27:52.380, Speaker A: You want to somehow be sensitive to how well actions have performed in the past. So you're going to choose different actions with various probabilities. And in some sense, the obvious thing to do is the better something's done in the past, the lower its cost in the past, the higher the probability you're going to play that with now.
00:27:52.380 - 00:28:07.140, Speaker A: And it turns out many reasonable implementations of exactly that idea play with probability higher as the past performance is better work and gives you a no regret guarantee. So that's point number one. Pay attention to past performance to figure out what to do right now.
00:28:07.140 - 00:28:10.400, Speaker A: Point number two, which is important, not.
