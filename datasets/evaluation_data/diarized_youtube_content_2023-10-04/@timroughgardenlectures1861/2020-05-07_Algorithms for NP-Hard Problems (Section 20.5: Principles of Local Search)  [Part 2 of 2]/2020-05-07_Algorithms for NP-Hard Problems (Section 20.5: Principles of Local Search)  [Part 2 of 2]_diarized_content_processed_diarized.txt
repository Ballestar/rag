00:00:00.410 - 00:00:47.846, Speaker A: So there are different ways to apply local search to the same problem. In other words, even once you know the answers to step one and two, so you know what your problem is and you know what the global optima are, there's different reasonable choices of step three. To illustrate this, let's go back yet again to the traveling salesman problem. So we looked at two changes as allowable local moves where we take out two edges and then put two back in. But who says we can only swap pairs of edges at a time? Why not three edges at a time? Or maybe even more? But let's start with the three opturistic, where you take three edges out of a tour and then put three back in in each iteration of local search. So, for example, we can look at this cartoon in the light blue tour. So here I've identified three different edges, all with distinct endpoints.
00:00:47.846 - 00:01:28.678, Speaker A: The edge v comma w, y comma z, and U comma X. So those are going to be the three edges that are removed. So we're still going to be left with a path from U to V, a path from W to Y, and a path from X to Z. But now we reconnect those six vertices in a different way so that we get a new tour. What I've shown in the cartoon is we connect V and Z directly by an edge, and similarly W and X and U and Y. And as you can see, this gives us a tour, and it's certainly a different tour than the one we started with. Now, interestingly, with three changes, unlike two changes, with two changes, whatever pair of edges you removed, it uniquely forced what your new tour was going to be.
00:01:28.678 - 00:02:16.306, Speaker A: And that is not the case with three changes. So even after you've committed to which three edges that you want to remove, there's actually seven different ways to reconnect those six vertices that will lead to different tours than what you started with. So when you're using three changes in your local search algorithm, you actually have seven possibilities for a three change for each triple of edges. So your improving move is just going to be any way to remove three edges and reconnect things into a tour so that the total cost goes down. So no prizes for guessing the formal definition of the three opt heuristic. It's the same as two opt, except you allow three changes in addition to two changes. Now we have two bona fide local search algorithms for a common problem for the Tsp, right? We've got the two opt heuristic and the three opt heuristic.
00:02:16.306 - 00:03:06.428, Speaker A: They're both local search algorithms, but they are not the same because in three opt, you're allowed to use this richer family of three changes to make improvements. So let's, in the next quiz, develop a better understanding of how are these two different local search algorithms going to compare to each other. So fix any instance of the traveling salesman problem. So fix your N vertices and fix all the edge costs. We've got these two local search algorithms, two opt and three opt, each with its own metagraph. So which of the following statements are true about those metagraphs? And let me just warn you that it's possible more than one of these answers is correct. All right, so the correct answer is the first one A and also the last 1D.
00:03:06.428 - 00:03:40.692, Speaker A: So to see why, remember that the three opturistic has only more options of local moves than the two opturistic. Two opt has to only use two changes. Three opt can use a two change, or it could also use a three change if it wants. So because edges of the metagraphs correspond to the allowable local moves, and the three opt has only more of them, it also has only more edges. So that's why answer A is correct. If you're an edge of h two for two opt, then you're also an allowable local move for three opt. And then if you think about it, that means that answer D is correct as well.
00:03:40.692 - 00:05:00.288, Speaker A: Because if you have a vertex of the metagraph that has a neighbor in h two with better objective function value, meaning that if you have a vertex which is not a local minimum in h two, then this exact same local move, this exact same neighbor shows that the vertex isn't a local minimum of the graph h three either. So turning that around, that means every local minimum of h three is definitely also a local minimum of h two. Said another way, if you look at all the tours which might plausibly be the output of the three opturistic, all the places where it might get stuck, those are also all tours where the two opturistic might get stuck as well. There may be other ToRs which are locally optimal for two opt, so there's no improving two change, but on the other hand, are not locally optimal for three opt because there is an improving three change. So whenever someone gives you more than one algorithm for the same problem, you want to sort of press that person for guidance about which algorithm should you use and when. So that's what I've done here, right? I've shown you you could apply local search to the Tsp in multiple ways. And so if you were going to attack Tsp with local search, which is a pretty good idea, you might be asking, does it make sense to allow three changes or should I not bother? Sort of what are the tradeoffs? And honestly, with local search, usually those kinds of questions are best answered empirically by trying out several options on data that you think is representative for your application.
00:05:00.288 - 00:06:09.060, Speaker A: But this quiz and specifically answer D, it does sort of indicate a general advantage of the large neighborhood sizes, which is as you allow more and more local moves, there are fewer and fewer local optima. And so local search is less prone to halting at a local optimum that is much worse than a global optimum. The primary downside of larger neighborhood sizes is that it slows down the sort of test in the while loop when you're checking for an improving local move. So for example, in the traveling salesman problem, checking for an improving two change takes quadratic time of the number of vertices, while checking for an improving three change takes cubic time because there's a cubic number of potential three changes you could make. So one approach you could take as a heuristic to balancing the pros and cons of large neighborhood sizes is you could imagine using the largest neighborhood size possible, subject to having a target per iteration running time. So, like, maybe you want to say, okay, I want every iteration of my local search algorithm to run almost always in at most a second, or almost always in at most 10 seconds. And then subject to that budget, just sort of use the biggest neighborhood that you can so that you'll have the fewest poor local optima.
00:06:09.060 - 00:07:17.192, Speaker A: So now that we've gone very thoroughly through the modeling decisions, steps one through three of applying local search, let's proceed to the algorithmic decisions in steps four and five, where we need to finish specifying exactly how the generic local search algorithm is going to work. Specifically, how is it going to be initialized? And secondly, when there are multiple improving moves, how are you going to choose among them? So let's start with the first question. How do you initialize? Well, in our Tsp example, we kind of already suggested what might be a natural thing to do, which is just initialize with the output of a greedy heuristic like the nearest neighbor heuristic. And it's not just Tsp that we can apply this idea to. So for example, if you want to tackle make span minimization using local search, you could initialize it with the schedule output by the LPT longest processing time first algorithm, because then at least you're starting from a schedule whose make span is almost 33% more than the minimum possible. And after local search, it's only going to get better. Similarly, if you wanted to do maximum coverage using local search, you could initialize it with the output of the greedy algorithm that we studied.
00:07:17.192 - 00:08:01.160, Speaker A: So greedy initialization is often worth trying. Actually, a second often really good idea for initialization is just to choose the feasible solution randomly. And at least for the problems we've been using as running examples, it's quite straightforward to see what a random solution would mean. So for example, in the Tsp, rather than doing anything greedy or clever, you would literally just pick a random ordering of the vertices and then look at the toy that visits the vertices in that order coming back to where it started. It's even simpler in makespan minimization, right? So for each of the end jobs, just independently assign it uniformly at random to each of the M machines. So each job is equally likely to be initially on each machine and you do it independently for all N jobs. Same thing with maximum coverage.
00:08:01.160 - 00:08:43.204, Speaker A: The feasible solutions are all subsets, are all collections of k of the subsets. So you just pick that collection of k subsets at random. So I sympathize if this kind of bothers you a little bit, sort of. We worked really hard to analyze these greedy algorithms and prove they have these really nice approximate correctness guarantees. Why would we want to throw them out and just do something sort of silly and random instead? But it's important to realize that just because you start local search at a better solution, that doesn't mean that local search will end at a better solution. So in fact, an ideal initialization procedure is one that quickly finds a starting solution that is not too bad, but also has lots of room for local improvement. And in many cases, random solutions fit the bill.
00:08:43.204 - 00:09:22.404, Speaker A: So the other algorithmic decision is, as we've seen, there may be multiple competing improving local moves from a given feasible solution. And for your local search algorithm to be fully specified, you need to say which one you're going to use. And there's several approaches, we've touched on a couple. So the first thing, which is what we did in our running example is you could just sort of enumerate all of the possible local moves and as soon as you find one which is improving, you take it. So that rule makes a lot of sense if you want to make sure that each iteration of the main while loop is as fast as possible. Right, because if you're enumerating all of the local moves and if you find an improving local move early on in that enumeration boom, you can stop. You can just move on to the next feasible solution and start all over again.
00:09:22.404 - 00:10:28.516, Speaker A: If you wanted to prioritize how rapidly the objective function improves iteration to iteration as opposed to the running time spent per iteration, a different thing you could do is you could be patient. You could look at all of the local moves. For example, in two opts you'd be looking at all roughly N squared of the possible two changes and you would pick the local move that is the best, that is, that improves the objective function by the largest amount. So the second rule will definitely give you a slower per iteration running time than the first rule. But you might hope that you wind up executing fewer iterations because you're aggressively making progress in the objective function iteration to iteration. One third thing you might want to do that we didn't mention before, especially if you want to encourage your local search algorithm to kind of explore the solution space, is amongst all the improving local moves you could pick one at random. So I get it if this third rule kind of strikes you as the worst of both worlds of the first two, right? It seems like it might be as slow per iteration as with the second rule and then as slow with the improvement in objective function as with the first rule.
00:10:28.516 - 00:11:13.248, Speaker A: So that seems kind of bad. It'll make more sense in a couple of minutes when we start talking about avoiding local optima by injecting randomization into your local search algorithm and running multiple independent trials. That's kind of the context in which this third rule makes the most sense. How about performance? So if you run a local search algorithm, should you expect it to run quickly? And should you expect it to output high quality solutions? Well, for many local search algorithms, the answers and trade offs are basically the same as what we already saw in the two opturistic for the Tsp. So let me just recap what those properties were. So first, we're not worried about local search getting into an infinite loop. And that's because each feasible solution it considers is strictly better than the previous one.
00:11:13.248 - 00:12:07.828, Speaker A: So if you only have a finite number of feasible solutions, like in all of the applications we're talking about, that means eventually local search is going to halt necessarily at a locally optimal solution. And like with the two opt heuristic, most local search algorithms unfortunately do not have a provable running time guarantee there are going to be pathological cases where they need a tremendous number of iterations before they halt at a locally optimal solution. There are some exceptions, there's some local search algorithms which you can guarantee will halt in a polynomial number of iterations, but they're really the exceptions that prove the rule. So the good news is that's not actually much of an impediment to applying local search to real world problems. And that's because the instances that tend to show up in realistic applications, local search tends to halt not necessarily super quickly, but in a tolerable amount of time. Like too opt. We said it often takes a super linear but sub quadratic number of iterations to reach a locally optimal tour.
00:12:07.828 - 00:13:00.440, Speaker A: And that's roughly characteristic of what you see with lots of other local search algorithms as well. Another reason it's not that big a deal that local search, at least in principle, could take a lot of iterations to halt at a local optimum is you can always stop the algorithm early. So you can set a timer after an hour or after a day, and when the timer goes off, you just say, hey, local search algorithm, give me the most recent and therefore the best solution that you ever managed to find. So let's move on to the quality of the solution you can expect from the output of a local search algorithm. So like with the two app heuristic, it's very common that local search algorithms have no provable approximate correctness guarantees like the ones that we saw for the first three greedy algorithms in this section. Again, there are a few exceptions. There are local search algorithms that do have provable approximate correctness guarantees, but they're again, sort of the exceptions that prove the rule.
00:13:00.440 - 00:13:41.816, Speaker A: Again, the good news is that empirically local search algorithms seem to do surprisingly well. It is very common for local search to return a really quite good locally optimal solution. It's not too much worse than the global optimum. That said, while for the running time you pretty much will never in your life see iterations where local search needs an exponential number of iterations to converge, you certainly will see in your life cases where local search outputs a really crappy locally optimal solution. That can happen. And we're going to talk next about how do you tweak local search to minimize the chance that you get stuck at these bad local optimal. So very frequently local search gives you a high quality solution, but you cannot count on it.
00:13:41.816 - 00:14:30.634, Speaker A: It will sometimes give you poor ones as well. Low quality local optima really can be an impediment to applying local search in real applications. So it's definitely worth knowing various bells and whistles that you can layer on top or inject into the basic local search algorithm with the goal of making it less common that you will wind up at these bad local optima. So let's look at a number of ways you might go about that. So one thing you can do we've already touched on, which is that if you're finding too many lousy solutions or locally optimal, just allow more local moves and some of them will stop being locally optimal. Remember, just because you're a local minimum for the two opturistic, there's no improving two change. You may not be locally optimal for the three opturistic, there may be an improving three change.
00:14:30.634 - 00:15:11.030, Speaker A: So in general, increasing the number of local moves decreases the number of local optima. So you're less likely to be in any of the poor local optima. But actually, the first thing you should try, and the very simplest thing you can do that can sometimes make a big difference, is to inject randomization into your local search algorithm. So we've already touched on two kind of very easy places to inject randomization into your local search algorithm. One is in the initialization. So for example, instead of using the nearest neighbor heuristic to pick an initial tour, you could just pick an initial tour uniformly at random. And then the other place is when you're choosing among multiple improving moves, you could pick one of those at random.
00:15:11.030 - 00:15:40.500, Speaker A: Now, once you have a randomized version of your local search algorithm, this is great. You can start exploring the space of local optima. You might just run your local search algorithm over and over again. Independent trials run it 100 times. You'll get back 100 local optima. There'll be some duplicates but generally speaking, you will see different local optima across the different runs of your local search algorithm. And in most applications, all you need is one of them to be good, right? If 99 of them are bad local optima, but one of them is a good one, that's the one you're going to be using.
00:15:40.500 - 00:16:16.766, Speaker A: If you're really desperate to inject more randomization into your local search algorithm, you can even consider allowing with some probability the algorithm to take moves that make the objective function worse. So for example, here's one simple way you could go about this. This will sort of loosely correspond to simulated annealing if that's something that you've heard of. So imagine you're at some feasible solution. So like in the Tsp, you're at some traveling salesman tour. The first thing you do is you're going to pick a local move uniformly at random. It may or may not be improving, right? So like in the Tsp, there's N times N minus three over two different two changes you could make.
00:16:16.766 - 00:17:03.194, Speaker A: You're going to pick one of those uniformly at random. Then you say, okay, so what's going to happen to the objective function if I actually make this local move? If the objective function value stays the same or goes down, then there's no reason not to just do this move. So just go for it. The issue is, what if this is a local move that would actually make the objective function worse? Well, now you're going to flip a coin and you're going to decide probabilistically whether to make this move. And if the move only makes the objective function value a little bit worse, then the probability that you'll make the move is going to be pretty close to one. Again, in the interest of random exploration. But if the move would make the objective function value a lot worse, then it's going to be a very low probability that you'll actually sort of have the courage to execute that local move right now.
00:17:03.194 - 00:17:38.914, Speaker A: If you don't execute that local move, you stay at exactly the same feasible solution in the next iteration where you're going to make a new random choice about which local move to consider. Let me point out that when you allow non improving moves like this, a local search algorithm is generally not going to halt. It's going to run forever. So that's definitely a kind of algorithm you want to interrupt after a target amount of computation time. So there's any number of additional bells and whistles you can add to your local search algorithm. Let me just mention sort of two genres of them that at least in some corners are pretty popular. So the first idea is to use neighborhoods that are history dependent.
00:17:38.914 - 00:18:40.220, Speaker A: That is, rather than fixing the allowable local moves once and for all, the allowable local moves could depend on the trajectory so far of the local search algorithm. Why would you want to do that? Well, for example, you might disallow local moves that seem to partially reverse the previous move and undo what you just did. So for example, in a Tsp context, you might want to rule out using a two change that uses some of the same endpoints as the previous one. So if you've heard of either taboo search or the Lynn Kernigan heuristic, both are related to this idea. And maybe one of the strongest motivations for history dependent neighborhoods comes from the previous point for local search algorithms that allow moves that make the objective function worse in addition to better. Because as soon as you allow non improving local moves, you have to be worried about cycles and that your local search won't make any progress and won't explore a different part of the solution space. And so history dependent neighborhoods can be particularly effective at sort of preventing the local search from sort of immediately going back to where it just was.
00:18:40.220 - 00:19:44.362, Speaker A: Finally, while the local search algorithms we've discussed so far just maintain a single feasible solution throughout the entire execution, there's also a variance where you maintain a population of feasible solutions. So for some parameter k at least two, the algorithm will maintain k feasible solutions at all times. Each iteration of the algorithm is now responsible for generating k new feasible solutions from k old ones, for example, by keeping only the k best neighbors of the current k solutions, or maybe by combining pairs of current solutions to produce new ones. So if you've ever heard of genetic algorithms or beam search, both of those are based on this kind of idea. So, as someone who's made it this far into Algorithms illuminated chapter 20 of part four, you're someone who knows a lot of algorithm design paradigms, and here I'm giving you yet another one. So the question you should be asking me is, when is local search the first thing you should try? So let me give you a list of reasons why you might want to use local search. If your application checks several of these boxes, I would give it a shot.
00:19:44.362 - 00:20:30.058, Speaker A: So first of all, local search is relevant when you don't have enough computational power to solve the problem optimally. Like maybe it's an NP hard problem and the instances are of a reasonably large size. That is, you should consider local search primarily when you're committed to the approach of fast heuristic algorithms. So second, provable guarantees are not really the strong suit of local search algorithms as we've discussed. In many cases, you can't prove that they're guaranteed to halt quickly, because there will be pathological examples that you'll never see. But there will be pathological examples where they don't converge quickly. And you won't be able to prove guarantees generally about the solution quality, because even empirically you will see cases where local search algorithms output very low quality solutions.
00:20:30.058 - 00:21:26.338, Speaker A: The one exception to that second point is we mentioned that you can use local search as a post processing step after using some other heuristic algorithm. So if that heuristic algorithm you start with has an approximate correctness guarantee, like the first three greedy algorithms that we studied in this chapter, then of course tacking on local search at the end that only makes things even better. So you inherit the approximate correctness guarantee from whatever heuristic generated the starting solution for your local search algorithm. But still, generally speaking, if you're looking for provable running time guarantees, provable correctness properties generally, this is not going to be the first design paradigm you look to. One nice thing about local search algorithms is at least in their most basic version, they're quite simple, quite easy to code up. So if you need a quick and dirty heuristic stat for a problem, local search is often a good place to get started. Now, mind you, to sort of squeeze all the possible performance out of local search, you generally have to do a lot of experimentation and that can actually take quite a bit of time.
00:21:26.338 - 00:22:22.100, Speaker A: But just to get basic versions up and running, that's a relatively easy implementation project. Then, as we've mentioned, one really almost no brainer use for local search is to improve on solutions that you may have gotten from a different heuristic algorithm. Again, as long as you have some additional computation time you can throw at making the problem better, why not? Why not use local search and see how it does? So another unusual benefit of local search algorithms is that you can stop them at any time. They are quote unquote anytime algorithms. And a lot of algorithms are not like this. So for example, later when we talk about state of the art solvers for mixed integer programming, say it is not the case that if you just terminate it after five minutes, it gives you something useful, whereas a local search algorithm does. Speaking of state of the art solvers for mixed integer programming and satisfiability, which we'll talk about in quite a bit more detail a few lectures from now, those are probably the stiffest competition for local search when tackling NP hard problems.
00:22:22.100 - 00:22:57.642, Speaker A: In practice, if you're in a scenario where those solvers are working for you, then great. Use them. If you can specify your problem in a format that they can handle and your instances are small enough or structured enough that they can solve them to optimality, more power to you. So local search becomes really relevant when those solvers are not working for you. So one reason for that might be that your inputs are just too big so the solvers are choking on them. Meanwhile, local search is kind of a simpler algorithm, has the potential to scale to bigger input sizes. A second reason might just be if your optimization problem is kind of weird.
00:22:57.642 - 00:23:49.434, Speaker A: Like you can't just write down the objective function in a very simple way, which is what, say, a mixed integer programming solver would expect notice. Local search, it needs very little information about your objective function. All you need to be able to do is evaluate the objective function efficiently for a given feasible solution. If you can do that, you can run local search, whereas the state of the art solvers require much more much stricter formats for objective functions. So those are some of the features of an application where if you see them, kind of a light bulb should go off in your head and you should say, well, this kind of seems like the classic scenario. While Local search is a good technique to use, the final thing I'll say on the topic is that to get the most out of the local search algorithm design paradigm, it's crucial that you experiment. As we've seen, local search is not really just an algorithm.
00:23:49.434 - 00:24:16.678, Speaker A: It's really a whole collection of algorithms. There's a million bells and whistles you can throw in, and which bells and whistles are the right ones are going to depend on the details of your application. So I really encourage you to get some representative instances for your application. Code up a bunch of different versions of local search, see which one works the best and go with that. So that wraps up chapter 20. That's everything I wanted to tell you about fast heuristic algorithms. So coming up next are the videos corresponding to chapter 21.
00:24:16.678 - 00:24:47.140, Speaker A: And so now we're going to switch gears and instead of compromising on running on correctness, we're going to be compromising on speed. So we're going to be looking at algorithms which are exact, which really always solve the problem correctly. For NB hard problems, that means you got to be expecting the running time to be exponential in some cases. But you'd still like to have algorithms which beat something naive like exhaustive search by as much as possible, as much of the time as possible. So there's a lot of cool technique weeks for designing those kinds of algorithms. That's coming up next. I'll see you then.
00:24:47.140 - 00:24:47.870, Speaker A: Bye.
