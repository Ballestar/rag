00:00:00.490 - 00:00:05.054, Speaker A: Hi everyone and welcome to this video that accompanies section 19.1 of the book.
00:00:05.092 - 00:00:17.518, Speaker B: Algorithms Illuminated part four. It's a section about an algorithmic mystery between two similar seeming but computationally very different problems the minimum spanning tree problem and the traveling salesman problem.
00:00:17.604 - 00:00:19.854, Speaker A: This video is the first one from.
00:00:19.892 - 00:00:23.310, Speaker B: Chapter 19 about what is NP Hardness?
00:00:23.890 - 00:00:26.402, Speaker A: Now, many introductory of books on algorithms.
00:00:26.466 - 00:00:54.086, Speaker B: Including the first three parts of Algorithms Illuminated, they suffer from a sort of selection bias. They focus on problems where there are always correct and always fast and usually quite ingenious algorithms. Because after all, what's more fun and empowering to learn than a clever algorithmic shortcut? Unfortunately, this cherry picked collection of problems misrepresents the reality where the specter of computational intractability haunts the algorithm designer.
00:00:54.086 - 00:01:10.046, Speaker B: So, even though there are a lot of problems that do have these always fast and always correct algorithms, you've seen many examples sorting and searching, computing connected components of a graph, shortest path, sequence alignment, et cetera. There are a lot of important problems with always fast, always correct algorithms. There's a lot of other problems, including.
00:01:10.078 - 00:01:11.346, Speaker A: Ones that are likely to show up.
00:01:11.368 - 00:01:19.842, Speaker B: In your own work where that seems to not be the case. Problems that are fundamentally unsolvable by always fast and always correct algorithms.
00:01:19.986 - 00:01:22.534, Speaker A: Newly aware of this stark reality, two.
00:01:22.572 - 00:01:24.902, Speaker B: Questions come immediately to mind.
00:01:25.036 - 00:01:26.406, Speaker A: So first of all, how can you.
00:01:26.428 - 00:01:29.894, Speaker B: Recognize when a problem is one of these hard problems so that you don't.
00:01:29.942 - 00:01:31.898, Speaker A: Inadvertently waste time trying to come up.
00:01:31.904 - 00:01:34.858, Speaker B: With an always fast, always correct algorithm for it?
00:01:35.024 - 00:01:37.178, Speaker A: Secondly, when you do know that a.
00:01:37.184 - 00:01:41.066, Speaker B: Problem is intrinsically computationally difficult, how should.
00:01:41.088 - 00:01:42.630, Speaker A: You revise your ambitions?
00:01:42.710 - 00:01:56.000, Speaker B: And what tools do you have in your algorithmic toolbox to achieve those relaxed ambitions? So the point of this video playlist and the point of Heart Four of Algorithms Illuminated is to equip you with very thorough answers to both of these questions.
00:01:56.470 - 00:02:13.874, Speaker A: So hard computational problems can actually look a lot like easy ones and you do need appropriate training to be able to tell them apart. Let's start with a problem which I hope is familiar to you, the famous minimum spanning tree problem. So the input in the minimum spanning tree problem is an undirected graph.
00:02:13.874 - 00:02:23.174, Speaker A: The graph should be connected, meaning it's all in one piece. So you can go from any vertex to any other vertex in the graph using a path. That is you do not have disconnected pieces.
00:02:23.174 - 00:02:43.930, Speaker A: So a connected underactive graph where each edge has a real valued edge cost. We're going to denote that by C sub E for an edge E. The responsibility of an algorithm for the MST problem is to compute a spanning tree of the graph and among all spanning trees that you compute one that minimizes the sum of the costs of the edges in the tree.
00:02:43.930 - 00:02:52.334, Speaker A: So just to remind you, a spanning tree, it's what it sounds like it's a tree. So there's no cycles and in their hand it's spanning. So it covers all of the vertices of the graph.
00:02:52.334 - 00:03:07.560, Speaker A: In other words, for each pair of vertices V and W, there should be a path in the spanning tree capital T from V to W. So for example, let's consider a graph with four vertices and five edges. Each of the edges is labeled with its cost.
00:03:07.560 - 00:03:29.150, Speaker A: So if you stare at this graph for a little while you realize that the minimum cost spanning tree has the edges of cost one, two and four. So if you think about it, because you have four vertices you need to connect together, you're really going to need three edges to do it. And the only triple of edges with less cost than the one, two and the four is the one, two and the three.
00:03:29.150 - 00:03:39.618, Speaker A: But they form a triangle so that would be neither a cyclic nor would it span. So the best you can do is the one and the two and the four. So the minimum spanning tree in this example has total cost seven.
00:03:39.618 - 00:04:11.450, Speaker A: So how hard is the minimum spanning tree problem? Now on the one hand, a graph can have an awful lot of spanning trees. So for example, there's a famous result in combinatorics known as Kaylee's Theorem which says that if you have a graph with N vertices and it's the complete graph, meaning all n choose two edges are present, the complete graph on N vertices has N raised to the N minus two different spanning trees. So at this stage of the book series, you should be very sort of familiar with the idea that exponential numbers get very big very quickly.
00:04:11.450 - 00:04:18.686, Speaker A: So for example, an N to the N minus two already, if you plug in n equals 50 that winds up being more than the estimated number of.
00:04:18.708 - 00:04:20.474, Speaker B: Atoms in the known universe.
00:04:20.602 - 00:04:37.086, Speaker A: So what does that mean for us? Graphs can have a ton of spanning trees. Well that says that the most naive algorithm you could think about, exhaustive search, where you literally just go spanning tree by spanning tree and remember the best one that you've ever seen. That is a totally hopeless algorithm to implement except in the tiniest of graphs.
00:04:37.086 - 00:04:54.326, Speaker A: So there's a lot of spanning trees. Exhaustive search is definitely not going to be a fast algorithm. But the other thing that you hopefully do know is that despite the exponential number of possibilities, there are fast algorithms that sort of very quickly home in into the best of all of the spanning trees.
00:04:54.326 - 00:05:13.902, Speaker A: So two that we talked about at length in previous videos were Prim's algorithm. That's the one that's a little bit like Dijkstra's algorithm where it sort of slowly grows a tree to spread over the whole graph. And then we also talked about Kruskal's algorithm which sorts the edges and goes through them in a single pass and sort of grows the spanning tree like in little pieces that fuse together at the end.
00:05:13.902 - 00:05:36.594, Speaker A: And we saw both of those algorithms if you use the appropriate data structures so heaps for Prim's algorithm or a Union Find data structure for Kruskal's algorithm, both of those have blazingly fast implementations. They can be implemented to run an almost linear time, linear plus an extra logarithmic factor. So barely more time needed than to read the input, which is pretty amazing and even more amazing when you think about the needle in the haystack that they're finding.
00:05:36.594 - 00:05:54.062, Speaker A: This is huge, huge number of spanning trees and somehow they very quickly identify which is the best of them all. Now let's look at a second famous problem, the traveling salesman problem. So we actually have not even mentioned this problem in the previous playlists, in the previous books, but as we'll see in part four, the traveling salesman problem.
00:05:54.116 - 00:05:55.786, Speaker B: Will really play a starring role.
00:05:55.818 - 00:06:06.942, Speaker A: It's one of the most famous NPRD problems. What's funny is the problem sounds an awful lot like the minimum spanning tree problem. So the input like in the MST problem is going to be an undirected graph.
00:06:06.942 - 00:06:27.506, Speaker A: In the MST problem, we assumed it was connected for the Tsp, it's convenient to assume that it's a complete graph, meaning all N choose two edges are present, where N is the number of vertices. Like in the MST problem, each edge is going to have a real valued edge cost. Now, don't get put off by the fact that it was connected in the MST problem and complete in the Tsp.
00:06:27.506 - 00:06:47.882, Speaker A: Really, that's a totally superficial distinction. So for example, the regular MST problem is totally equivalent to the MST problem on complete graphs because if you gave me an incomplete graph, I could just add in all of the missing edges and give them super high costs. So the MST would never use one of these super high cost edges, it would just find the MST in the original graph.
00:06:47.882 - 00:07:13.890, Speaker A: So it's more or less without loss of generality when we say the graph is complete. The responsibility of an algorithm for the Tsp is to return not a spanning tree, but a tour or traveling salesman tour, again with a minimum possible sum of edge costs. So what is a traveling salesman tour? Well, it's just a walk through a graph that visits every single vertex exactly once and ends where it began.
00:07:13.890 - 00:07:24.262, Speaker A: So for example, imagine a complete graph with four vertices. One tour would just follow the perimeter so the four edges around the boundary. A different tour would use the zigzag edges.
00:07:24.262 - 00:07:57.298, Speaker A: So just as a sanity check that you see what I mean by a tour of a complete undirected graph, let's move on to a quiz. So the question is in a traveling salesman problem instance that has N vertices, where N is at least three, how many distinct traveling salesman tours are there? And here in the answers, the N with the explanation part that's the N factorial function so that's the product of all of the integers between one and n. So for example, three factorial is six, four factorial is 24, five factorial is 120, and so on.
00:07:57.298 - 00:08:26.970, Speaker A: Note that the factorial function grows even faster than the function two to the n. So as usual when I give you a quiz, I encourage you to pause the video here and think about the quiz for a while, then come up with your guess of the answer and then unpause the video and we'll discuss the solution. Okay, so the correct answer is b one half times n minus one factorial.
00:08:26.970 - 00:08:36.190, Speaker A: So for example, if n was equal to four, this would be three different tours. So on the last slide, we looked at the complete graph on four vertices. We looked at two of the tours.
00:08:36.190 - 00:08:58.854, Speaker A: There's also a third tour, which again uses the zigzag edges, but this time uses the edges on the side rather than the edges on the top and bottom. In general, the formula is one half times n minus one factorial. So why is the answer b? You may sort of see that there's an intuitive correspondence between traveling salesmen tours and orderings of the vertices, right? A tour kind of feels like, oh yeah, you just choose an ordering in which you visit the vertices.
00:08:58.854 - 00:09:15.098, Speaker A: So that would sort of suggest d might be the right answer. But actually all of the vertex orderings that double counts tours actually counts each tour a total of two times n times. First of all, it overcounts a tour because it counts at n times for each choice of the starting vertex.
00:09:15.098 - 00:09:34.818, Speaker A: Doesn't matter what starting vertex is, you wind up with the same tour at the end of the day. Also, a tour can be traversed in either direction, so those give you two different orderings, so each distinct tour gives you two n different orderings. There's n factorial orderings, so that leaves you with one half times n minus one factorial distinct tours.
00:09:34.818 - 00:09:49.018, Speaker A: So that's a lot of tours, but it does at least show that the traveling salesman problem can be solved in a finite amount of time. Finite, but large amount of time. If nothing else, you could solve the traveling salesman problem by exhaustive search.
00:09:49.018 - 00:10:26.886, Speaker A: Systematically enumerating each of these one half times n minus one factorial tours, and remembering the best one, try exhaustive search out yourself in a four vertex example. Okay, so the correct answer to this quiz is the second one, b 13. And again, one way you can see this is just to enumerate the three different tours you have in this four vertex graph.
00:10:26.886 - 00:10:41.546, Speaker A: So there's the one around the sides that does indeed have 13. There's the zigzag path that uses the top and the bottom edge that has overall cost 15. And then there's the one that uses the zigzag and the edges on the side and that has overall cost 14.
00:10:41.546 - 00:11:00.354, Speaker A: So the cheapest of those options is 13 the tour that goes around the perimeter. So that's fine if you have a tiny instance like this with four vertices or maybe five or six vertices, no big deal to just enumerate the exponentially many tours and remember the best one. But this is going to work out only in the smallest of instances.
00:11:00.354 - 00:11:47.166, Speaker A: And as algorithm designers, it is our duty to ask the question, can we do better than naive exhaustive search? Could there be analogous to the MST problem, an algorithm that magically homes in on the minimum cost needle in the exponential size haystack of traveling salesman tours? And despite the superficial similarity of the statements of the two problems, the traveling salesman problem actually appears to be fundamentally much more difficult than the minimum spanning tree problem. So I could tell you a cheesy story about the traveling salesman problem involving a traveling salesman, but that would really do a disservice to the problem, which is actually quite fundamental whenever you have a whole bunch of tasks that you want to get solved. And the cost of completing a task depends on the preceding task that you completed.
00:11:47.166 - 00:11:59.814, Speaker A: Boom. You've got an instance of the Tsp. So, for example, you could imagine that you're assembling a bunch of different cars in a factory, and you could imagine that the factory needs to be kind of in a certain configuration in order to assemble a particular car.
00:11:59.814 - 00:12:32.994, Speaker A: And you can also imagine that there might be a setup cost, transition cost when you reconfigure the factory for a car of type A with a car of type B. So if you have a bunch of different cars you need to assemble and you're trying to figure out the right sequence in which to sequence them so that they're all assembled as quickly as possible with the minimum setup time in between, that's exactly the traveling salesman problem. Or for a very different application in computational genomics, you could imagine that maybe you have a bunch of short fragments of a genome that are partially overlapping, and you would like to reverse engineer the most plausible ordering in which those.
00:12:33.032 - 00:12:34.894, Speaker B: Fragments appear on the genome.
00:12:35.022 - 00:13:07.034, Speaker A: So if I give you a sort of pairwise plausibility measure sort of for each pair of fragments, how likely they are to be adjacent, for example, maybe based on the length of their longest common substring, then to reverse engineer the most plausible ordering, that, again, is exactly a traveling salesman problem. So if you want to learn more about the many applications of the traveling salesman problem and also a lot about its fascinating history, there's a nice book from 2006 by Applegate, Bixby, Katal and Cook, which you could check out. So the Tsp has always had a lot of natural applications.
00:13:07.034 - 00:13:33.958, Speaker A: It also evidently has tremendous aesthetic appeal. And so because of those two reasons, many of the greatest minds in optimization have thought long and hard about algorithms for the Tsp, dating back at least to the 1950s, there's been very, very serious people thinking hard about how to solve the Tsp. And despite 70 years of effort with many brilliant minds involved, at this day, currently it's 2020.
00:13:33.958 - 00:13:50.970, Speaker A: There is no known fast algorithm for the traveling salesman problem. No one has yet come up with anything close to as good an algorithm as the prims and Kruskal's algorithms that we have for the minimum span entry problem. So to be a little more precise, I should say what I mean by a fast algorithm.
00:13:50.970 - 00:14:13.646, Speaker A: And remember way back sort of in part one of the book series, or early on in these video playlists, we agreed that by a fast algorithm, we should mean an algorithm whose running time scales linearly or close to linearly in the size of the input. That would be a blazingly fast algorithm. Now, here I'm actually just talking about a very relaxed notion of a fast algorithm.
00:14:13.646 - 00:14:29.238, Speaker A: I'm talking about an algorithm with any polynomial running time. So forget about blazingly fast running times. No one even knows an algorithm for the traveling salesman problem guaranteed to run an end of the 100 time, or n is the number of vertices, or even end of the 10,000 time.
00:14:29.238 - 00:14:45.514, Speaker A: We don't even know that. Now, there are two possible explanations for this dismal state of the art the fact that we don't know any algorithm with guaranteed polynomial running time solving the Tsp. Explanation number one would be, actually, there is a fast algorithm out there, and just we haven't been smart enough to figure out what it is.
00:14:45.514 - 00:14:53.786, Speaker A: It's waiting to be discovered. It's possibility one. Possibility number two is that actually the reason we haven't found such an algorithm is because there just literally is no algorithm.
00:14:53.786 - 00:15:12.866, Speaker A: That type of algorithm does not exist. So to this day, we don't know which of these two situations is the real one, whether there is an algorithm and we haven't found it, or whether there's no algorithm. But most experts believe in the second of those explanations that's equivalent to what's known as the P not equal to NP conjecture.
00:15:12.866 - 00:15:42.586, Speaker A: And actually all the way back to 1967, which is actually before the P versus NP question was formally identified, jack Edmonds, in a famous 1967 paper called Optimum branchings He, conjectured that, in fact, there is no good algorithm for the Tsp, whereby good Edmunds meant with running time scaling, like a polynomial function in the input size. So the Tsp is an example of an NP hard problem. And that means, under this famous mathematical conjecture, the P not equal to NP conjecture.
00:15:42.586 - 00:15:54.130, Speaker A: If that conjecture is true, then indeed, Edmunds is right. There is, in fact, no guaranteed polynomial time algorithm for the Tsp. And what we'll see as we go on is that it's not just the Tsp that's NP hard.
00:15:54.130 - 00:16:16.902, Speaker A: Unfortunately, computational intractability is actually a widespread phenomenon. Many, many problems are NP hard, just as computationally difficult, as the traveling salesman problem. So, moving on to the next video, we'll talk about the different levels of expertise in hardness that you might want and which part of the playlist is best suited for that desired level of expertise.
00:16:16.902 - 00:16:18.870, Speaker A: I'll see you then. Bye.
