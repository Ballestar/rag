00:00:00.410 - 00:00:56.574, Speaker A: So what I want to talk about this lecture is, okay, so remember, you know, the quest for this first part has always been to have simple ascending options. And it was always the case that a sanity check would be, well, there better at least be a polynomial time dominant strategy implementation. And of course, for welfare maximization that means CG. So we always pause for like 2 seconds to say, okay, well, VCG is in polynomial time, at least right now. It turns out once we get to the generality of gross substitutes, it is actually quite a bit of work to prove that you can implement VCG in polynomial time. And that's the goal of this lecture. And what's interesting is we're actually going to use the stuff we talked about last lecture, the fact while Waz and equilibrium are guaranteed to exist, we're going to use that directly in proving that we can implement this in polynomial time.
00:00:56.574 - 00:01:41.950, Speaker A: So hopefully this will give you sort of some indication of all the intricate connections between properties of gross substitutes valuations. Because again, these two seem like to have very little to do with each other. Waz and Equilibrium was just an existence question, seems to have nothing to do with computation. And then the payments weren't the same as the VCG payments. And so now here we're switching to computation, we're switching to VCG, and yet somehow the same condition is sort of the right one. All right, so polytime implementation of VCG, okay, right. So a couple comments.
00:01:41.950 - 00:02:30.966, Speaker A: So we're going to do this using linear programming both to sort know reason about how this might work, and then the actual algorithm will use linear programming as well. It will be not practical at all. So the point is just to show today in principle, you can do welfare maximization and therefore the BCG mechanism in polynomial time with gross substitutes. I mean, keep in mind we're going to be using pretty heavy machinery. But I mean, keep in mind this is somehow a pretty big generalization of Bipartite matching, which is already not the most trivial problem to put in polynomial time. We're really going significantly beyond that. Speaking of Bipartite matching, I want to basically develop all the relevant linear programming in the context just of scenario number three, just in the context of unit demand bidders, where we can think about welfare maximization as Bipartite matching.
00:02:30.966 - 00:03:28.826, Speaker A: The reason I'm going to do that is because the general proofs are exactly the same proofs. But it's going to be, I think, quite a bit easier to just understand what's going on if we keep it concrete for Bipartite matching. Okay? So we're going to go retro for a little bit back to scenario three. So warm up unit demand knew if you knew some linear programming and you never knew that there were common choice algorithms for Bipartite matching, let's give an argument about why the VCG mechanism can be implemented in polynomial time. Via linear programming. Along the way we'll get a very satisfying interpretation of Walra's equilibria in terms of optimal dual solutions, which is actually a good way to think about them. All right, so let's start with an integer programming formulation of welfare maximization in this setting.
00:03:28.826 - 00:04:20.880, Speaker A: And again, welfare maximization is just bipartite matching, where you have the bidders on one side. Again, this is for the unit demand case items on the other side, and it's a complete graph. And the weight of an edge is vij bidder I's value for good j. So welfare maximization is exactly the same as max weight bipartite matching in this graph. So let's encode this as an integer program. So we're going to have one decision, variable xi j either zero or one for each bitter i. And item j one just denotes that j gets assigned to i.
00:04:20.880 - 00:05:14.080, Speaker A: Zero means j is not assigned to i. So we should talk about what we want, what's our objective, and then what are the constraints? So we want a max welfare, right? So the welfare is just well, let's look at each bidder I in turn. Let's look at the various, let's look at the goods that I might have gotten. And whenever xij equals one, meaning in fact item J goes to bidder i, that contributes vij to the welfare. So this is just the welfare objective. Now, for this to make sense, two constraints have to be true. So first of all, an item of course, can only go to one person.
00:05:14.080 - 00:06:09.326, Speaker A: So we have it at most one, and this is summing over all the bidders to whom it might be assigned. And then second of all, in a unit demand case, we want to focus on just bidders getting a single item because remember, they don't want more than one. So we have another set of constraints for all bidders i. And again, this is at most one. And now it's a sum over all of the items J that the bidder might get. Okay, so the assertion, which I can elaborate on if you want, but I won't if you don't ask, is that this integer program exactly encodes the maxweight bipartite matching feasible zero one solutions to this exactly correspond to bipartite matchings, and the objective function exactly corresponds to welfare. So any questions about that? All right, now, so lots of integer programs are hard to solve.
00:06:09.326 - 00:06:33.926, Speaker A: This one is not. Many of you have seen bipartite matching algorithms, which, whether they know it or not, are producing optimal solutions to this integer program. That said, I still want to think about the linear programming relaxation. Often linear programming relaxations are for problems that are empty hard, which this one is not. But still, I want to think about duals. That's useful interpreting all Rossian equilibrium. Duals of what? Duals of this program's.
00:06:33.926 - 00:07:32.222, Speaker A: Linear relaxation. So what's a linear relaxation? It just means that we remove the only thing which is not linear, namely this this is a binary constraint, and we replace this with a linear constraint. So LP relaxation, you take each constraint of this form and you replace it with the constraint xij is non negative. Your first thought might be to also have a constraint that xij is at most one notice that would be redundant with these constraints. So we're just going to have X-I-J non negative. Now, this is a linear program, and linear programs can be solved in polynomial time. That's not an obvious fact, and I'm certainly not going to prove it in this class, but it's a true fact, which you should know.
00:07:32.222 - 00:08:15.610, Speaker A: So linear programs can be solved both in theory and in practice. Okay, good. Now let's think about the dual linear program. So let me actually just do a little review of duals and what they mean and where they come from. So we'll call this the primal linear program in this context. So the point of a dual, what it's really doing is it's encoding a family of upper bounds on how big the optimal solution to this linear program can possibly be. So this linear program has some optimal objective function value 123 or whatever.
00:08:15.610 - 00:09:17.194, Speaker A: And I'm going to give you a way to encode proofs that this optimal value is at most something. Okay, so let me be more concrete. So let's do a thought experiment. So imagine, so how would I convince you that the optimal solution to this linear program is at most whatever you know has value of most 123? Well, here's an approach I could try to use to convince you that you can't do better than 143. Suppose I dream up some non negative weights both for the items and for the Bidders. So it doesn't matter where these come from. So non negative weights to the Bidders.
00:09:17.194 - 00:10:23.602, Speaker A: So I'm going to call these the UIs. Eventually we'll be able to interpret them as utilities and the items. So these are the PJs, which eventually we'll be able to interpret as prices. Okay? So inside, so if we think of this as a bipartite graph, the nodes corresponding to the Bidders get these weights UIs, and the nodes corresponding to the items get these weights the PJs. And suppose I cover the values of all the edges, so that all edges are covered in the sense that if I sum up the weights on the edges endpoints. So think about just a pair IJ, and I compare the sum of the weights of its two endpoints, namely UI and PJ, with the weight or the value of that edge vij. The node endpoint weight should be at least as big as the edge value.
00:10:23.602 - 00:11:21.298, Speaker A: So that's what I mean by these weights covering all of the edge values. Suppose the sum of these weights, all of the UIs and all the PJs. Suppose the sum of all of these is at most 143, I claim. Then you have to admit that the max value of any matching is at most 143. You. So why is that true? Well, for any matching so when you pick an edge, I j. Of course, the contribution to the welfare is vij.
00:11:21.298 - 00:11:38.730, Speaker A: But you know what? I'm going to be a nice guy. I'm not going to give you vij. I'm going to give you UI plus PJ. Instead of vij the edge value, I'm going to give you the sum of the edges, the endpoints weights, which is at least as large. So I'm doing you a favor. I'm giving you a larger number. Now, it's a matching.
00:11:38.730 - 00:12:07.080, Speaker A: So each node on each side only participates in one edge. So this node I with weight UI, that's only going to contribute to the sum once. And this node J with weight PJ, that's only going to contribute to the sum once. So the best case scenario is you pick up each of these weights from every possible node once. That's the sum of the PJs and the sum of the UIs. Okay, so that's definitely an overestimate on the value of any matching. Great.
00:12:07.080 - 00:12:34.910, Speaker A: Notice for the matching problem. If you wanted to convince me that the optimal solution is at least some value, like 111, it's clear how you'd do that. Show me the matching, I'd inspect it, count up the values. I'd be like, Yep, that's 111. The best matching is at least as good. So this is how we get bounds on the other side from above. We're putting node weights that cover the edge values and then summing up all of the node weights.
00:12:34.910 - 00:13:15.826, Speaker A: Okay? All right, so this is a generic method you could use to upper bound the value of the max weight matching. The dual linear program by construction, by definition, is just the best the smallest upper bound you could ever prove. This way. Okay. Oh, something I forgot to say. So notes the upper bound argument I just gave. It doesn't just apply to matchings.
00:13:15.826 - 00:13:41.500, Speaker A: It also applies to so called fractional matchings. That is, it applies to any feasible solution of this linear program for exactly the same reason. So if you pick an edge at zero five again, instead of giving you zero five times its weight, I'll be a nice guy. I'll give you zero five times the sum of the weights at its endpoints. This says that counting up.
