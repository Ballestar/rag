00:00:00.330 - 00:00:40.450, Speaker A: Last lecture, I made something sound more complicated than it was. So I apologize if you just like a couple more minutes about this. So it was the part where I was talking about, you know what, how does the separation oracle work? So we're applying the Ellipsoid algorithm to this dual linear system, okay? And at the end of the day, we know we're going to actually prove that this is infeasible. But it's the part of the proof where we're trying to argue, we're trying to implement the separation oracle. So the first thing I said said was correct, which is just this last constraint. Okay, let's remember what everything is, all right? So at the very beginning, long ago, there are these real valuations that you care about, the vis. You solve the linear probing, relaxation for welfare maximization.
00:00:40.450 - 00:01:21.958, Speaker A: For the real valuations of the vis, y star was the best fractional solution for the original ones, the vis. And for the rest of the proof, you're just taking the Y stars as targets, just as fixed, given constants. And so in this dual system, the Z's are what are the decision variables? So you check this last constraint, and then obviously, if this is violated, you can go ahead and return that. And then the claim is, if this last constraint is not violated, well, then actually it's super easy to generate a violated constraint from this family. So that's the step that I made. It sound way more complicated than it is to first order. Here's how it would work.
00:01:21.958 - 00:01:39.226, Speaker A: This assumes you're okay with like, a randomized solution. So suppose this passes, this passes the test. This is actually true. So now what do you do? You just run randomized rounding from last week, meaning you scale down by a small factor, one nice epilone factor, and you apply that to Y star.
00:01:39.408 - 00:01:40.138, Speaker B: Okay?
00:01:40.304 - 00:02:11.782, Speaker A: Now, semantically, what we're doing is very strange because Y star, this is an optimal fractional solution for the vis. The real evaluations right now, we have these totally fictitious pseudo valuations, these ZIS, and it's not clear that if you actually resolved an LP for the ZIS, you'd probably get something totally different. You wouldn't get y star. Again, Y star was optimized for the vis. But nevertheless, if you want, you can still apply randomized rounding to any feasible LP solution. Y star is one of them. So go ahead and do that.
00:02:11.782 - 00:02:40.730, Speaker A: And what we know is that if you apply randomized rounding to a feasible LP solution, you get an integer solution, which is almost as good. That's what we proved. So if this constraint is satisfied, it means you have a fractional solution, Y star, which for the valuations, these pseudo evaluations, the Z eyes is good. It's bigger than Z Star. If you do randomized rounding, you're only going to lose a one minus epsilon, and you're going to get an integer solution, and that's going to in fact, be an allocation, an integer solution, which violates this constraint.
00:02:40.890 - 00:02:41.646, Speaker B: Okay?
00:02:41.828 - 00:02:55.870, Speaker A: I mean, think about dividing the one minus epsilon here, okay? So this fractional solution is way better than z star. It's better than z star divided by one over epsilon. You do randomized rounding, you get integer solution, which is still strictly better than z star. That violates this. So you just return that as your separation Oracle.
00:02:55.950 - 00:02:56.206, Speaker B: Okay?
00:02:56.248 - 00:03:13.820, Speaker A: And that's it. That's the whole separation oracle implementation. Okay, so what we need from last week is just the simple idea that randomized rounding scaled down slightly does almost as good as the expectation. That's kind of the only thing we're using. So any questions about that? I hope that sort of cleared up that point.
00:03:15.150 - 00:03:15.900, Speaker B: Okay.
00:03:17.150 - 00:04:17.502, Speaker A: All right, so now for basically the culmination of part two, I want to circle all the way back to where we started, which was sub modular evaluations. So just to remind you, at the time, we had just sort of finished our victory lap with all the tractable special cases. We'd sort of knocked off gross substitutes. We'd seen we could implement the VCG mechanism and so on, and we went what seemed like just a very small step beyond gross substitutes. So we have a set U of items, not identical, and each VI is submoduular. And this is just sort of a set theoretic version of diminishing returns. Your value for something, your marginal value for something is going down.
00:04:17.502 - 00:04:58.842, Speaker A: The more stuff you have. So the more you have, the less you want more stuff. It if I look at how much extra value J gives me, that's less for the bigger set than for the smaller set. And okay, but we saw already here welfare maximization is NP hard. So that was sort of the can of worms we opened when we passed from gross substitute to submoduular. We also showed that it's just an approximation problem. Welfare maximization is not that bad.
00:04:58.842 - 00:05:38.730, Speaker A: All you do is run the Kelso Crawford auction, and that already gives you a one half approximation. There's other one half approximations as well. And then we asked the question, okay, well, how can we get a good approximation and also get Dsick? And I've given you no answers to that. Instead, we took these detours to multi unit auctions and to these things with multiple copies, and we've developed our toolbox. And so now in particular, we understand MIDR mechanisms, and we're going to apply that to solve at least a special case of scenario number six. Okay, so no full blown solution to scenario number six exists, or none is known. And I'll tell you probably a little bit tomorrow about the sense in which we think there isn't any for the full class of submodulular valuations.
00:05:38.730 - 00:06:19.490, Speaker A: But here's the special case that we'll be able to solve. So this is a subclass of submodulular valuations called coverage valuations. A given valuation VI is coverage if it has the following form. So for a bidder i, there's going to be some ground set. So you could think of this as being cities or something like that. And for each item, there corresponds a subset. So these are going to be AIGs.
00:06:19.490 - 00:07:25.834, Speaker A: And by definition, the valuation of a bidder is going to be the area covered by the sets correspond to the items that it's given. So let me give you a couple examples. So one thing is, you could imagine that Xi are a bunch of cities, maybe cities in the US. And items know licenses to broadcast something or permission to do something else. And so Aij could be the set of cities that a particular license enables you to reach. And your goal is to have coverage, if you like, of as many cities as possible. So you can think of these AIJS as broadcast radio, okay? And so it doesn't do you any good if you have two different you have permission in two different ways to broadcast to the same city.
00:07:25.834 - 00:07:49.038, Speaker A: It only counts once. So given a bunch of licenses, you look at the union of everybody you can reach, and that's what you get credit for. That's your value. Or if you prefer, maybe you're a firm an Xi is a number of skills you're trying to hire, or maybe you're a sports team. And those are positions that you need to fill. And so then basically, these AIJS are going to correspond. It's going to correspond to the skills of a particular worker.
00:07:49.038 - 00:08:29.920, Speaker A: So a worker maybe can do multiple skills. They cover multiple needs, and maybe you don't need more than one worker for the same thing, so you get credit for the union. So that's how you should think about coverage valuations. It's pretty easy to see that every coverage valuation is, in fact, submoduular. So basically, what's the added value of a new person? Well, it's just the things in Xi that you cover that were not covered before you got there. And so the more other stuff there was, the less new things you cover. So the less your marginal value, the bigger the set of other people that are already there.
00:08:29.920 - 00:09:20.030, Speaker A: So subset of submoduular valuations, okay? Now, whenever you pass to a special case, you should, of course argue, well, but is this even still a hard problem? Now, maybe this is now a subset of gross substitutes or something like that, and we already know how to solve it. But in fact, welfare maximization is actually still hard even for coverage valuations. So here's the best impossibility result known for approximating welfare with coverage valuations. So there's no polynomial time approximation.
00:09:23.910 - 00:09:24.274, Speaker B: For.
00:09:24.312 - 00:09:34.660, Speaker A: Welfare maximization better than one minus one over e, which is roughly 63%.
00:09:35.590 - 00:09:36.340, Speaker B: Okay?
00:09:37.030 - 00:09:53.686, Speaker A: So I'm going to assume so what's true in this negative result and what I'm going to assume for the positive result is that the input is just given explicitly. So I just literally tell you what my AIJS are, okay? And then that's the valuation. And I'm going to tell you an Aij by just, like, listing the elements in the Aij.
00:09:53.798 - 00:09:54.460, Speaker B: Okay?
00:10:01.870 - 00:10:25.780, Speaker A: So assume the Aij is provided as explicit input. This certainly says it's not part of gross substitutes, or at least assuming P is different than NP, it's not part of gross substitutes because that we can solve in polynomial time.
00:10:26.790 - 00:10:27.394, Speaker B: Okay?
00:10:27.512 - 00:11:01.150, Speaker A: So it's a subclass of sub modular, which still motivates everything we've been talking about, which is when welfare maximization is empty hard, what do you do? How do you get Dsick approximation mechanisms? And so the point of this lecture, so this is a result by a couple former PhD students of mine, sheddon Dugmi and Chi Yan, is that using an MIDR mechanism, and hence a DSIC mechanism, that gets a matching approximation guarantee.
00:11:05.110 - 00:11:05.860, Speaker B: Okay?
00:11:07.030 - 00:11:35.146, Speaker A: One minus one over E for coverage valuations. And so, again, this is sort of the happy case where, first of all, notice this in a proximity result. This is nothing about incentives. This just says, forget incentives, just polynomial time algorithms can't do better than 63%. So we're doing as well as any polynomial time algorithm. Incentives are not. So that's the first piece of good news.
00:11:35.146 - 00:12:14.120, Speaker A: The second piece of good news is that as approximation algorithms go, 63% is a pretty good number. There's a lot of problems where you can't get this close to 100% in the worst case. So, again, there's not tons of positive results in the Dsik approximation world, but this is another interesting one. Well, I mean, not per se, because I'm just assuming it's part of the input. So if Xi is big, you're forced to write it down. And I'm allowed to be poly. So, in effect, the algorithm is allowed to be polynomial on the size of the Xi, whatever that may be.
00:12:16.090 - 00:12:22.780, Speaker B: So if you're given any submodule evaluation, can you rewrite it as a coverage complicated enough?
00:12:23.150 - 00:12:35.600, Speaker A: It's a good question, no? Yeah, I don't think so. I see. So maybe with an exponential ground set, you're thinking that I haven't thought about what do you think?
00:12:37.170 - 00:12:38.240, Speaker B: Seems like.
00:12:43.460 - 00:12:52.676, Speaker A: Yeah, maybe with exponential maybe you need x. Yeah, maybe with an exponential size ground set. I haven't thought about that, actually.
00:12:52.778 - 00:12:55.830, Speaker B: But you will involve the size of the set.
00:12:56.840 - 00:13:23.170, Speaker A: Well, the claim is it's polynomial in the size of the input, where the input is defined to be the AIJS, which are explicit lists. Yeah. So in that sense, it has a pseudopolynomial flavor. In that sense. But that's another reason why sort of this is important. This hardness result is for this version of the problem. So in the hard instances here, of course, the XIs aren't going to be ridiculously big.
00:13:23.170 - 00:13:50.824, Speaker A: It's going to be a reasonable encoding of things. But yeah, I'll have to think about that. Whether given kind of unbounded sets, you can encode what you can encode. It's a good question. Okay. Right. I should say so.
00:13:50.824 - 00:14:26.020, Speaker A: In particular, you'll notice this is a one minus one over approximation algorithm, right? So even for getting incentives, I actually haven't told you a positive result this good. I've told you a one half approximation, a 50% approximation for general submoduular functions. But there were so in 2006, there was a one minus one over E approximation, nondesick for coverage. And then in 2008, there's one minus one over E for general submoduular functions. So prior to this, it was known you could get this without incentives. Not a lot prior to this, but prior to this, it was known you could get this. So it's clear that this would be the Holy Grail for an approximation mechanism.
00:14:28.760 - 00:14:29.316, Speaker B: Okay?
00:14:29.418 - 00:15:21.284, Speaker A: Any other questions? All right, so it's going to be an MIDR mechanism, but it's not going to be one of these scaling algorithms. It's not going to be a special case of the Levy Swami framework. So let me just comment on why do we need to do something different. So there's a few reasons, actually, but let me tell you the main reason. So what was the idea in Levi Swami? The idea in last lecture know, okay, you give me an approximation algorithm, which I can't lift into a mechanism, but then I'll somehow smooth the approximation error over all of the inputs without any loss so that I can get the incentives. So I'm telling you that for this problem, there are these good approximation algorithms. They get 50% or 63% or whatever.
00:15:21.284 - 00:16:15.110, Speaker A: So maybe we can just, again, do the same thing where we solve an LP and we smooth that error over everything, and then we're good to go. So why can't we do that? Okay, so let's suppose as a hypothesis that I gave you an algorithm, which, let's say, for every submodular valuation function got a good approximation, let's say, got 50% for every submodulular valuation, like Kelso Crawford. And that's the guarantee. Here's the problem. So in Levi Swami, where did we actually use the existence of a good approximation algorithm? So it didn't show up until buried inside the separation oracle that we used when proving the infeasibility of the dual here. So we used it to say, oh, well, if we have this solution with good fractional pseudo welfare, we'll invoke our approximation algorithm to get an integer allocation with almost as good pseudo welfare. Here's the problem.
00:16:15.110 - 00:17:00.144, Speaker A: So what instance do we invoke the algorithm on? We invoke it on the instance thinking of the ZIS as valuations, okay? And the problem is, we have no control at all over what these ZIS are, okay? Remember, we're just like running the Ellipsoid algorithm, and it's just sort of generating these ZIS for us. It's like, oh, here, deal with this, here, deal with this. And we're having to sort of parry these zices by feeding back violated inequalities these ZIS. They're not going to be submoduular. They need not even be monotone. They can be anything. So just having an algorithm which does well for submoduular valuations, which are the valuations we care about, the vis totally useless once we're faced with these arbitrary ZIS that the Ellipsoid algorithm gives to us.
00:17:00.144 - 00:17:54.416, Speaker A: Okay, so it really doesn't seem to help having a good approximation algorithm for subclasses of valuations. In this Levi Swami framework, you really seem to need general valuations, because the zices could be anything. Let me just write that. So ziss in D two could be anything. So approximation algorithm for special cases, not directly useful. So, full disclosure, there's some other kinds of incomparabilities or reasons why this particular problem doesn't fit into their framework. For the Levy Swami result, they need evaluations that support demand queries.
00:17:54.416 - 00:18:25.820, Speaker A: The reason they needed demand queries is in the very first step. So what they start they start by solving the LP relaxation to get this fractional allocation, maximizing the welfare over fractional solutions. There the separation oracle is a demand oracle. Turns out it's actually not hard to prove reduction from set cover. If I give you a coverage valuation, it's actually NP hard to answer a demand query. So way back when I started talking about queries, I said, Value queries pretty uncontroversial, right? You got to tell me what your value is for a set. Seems like a minimal query.
00:18:25.820 - 00:19:01.610, Speaker A: Demand queries. We said, Well, I said that sort of the mathematical results are more mixed, and it's more that it's natural to ask for demand queries. So here's a concrete example where the mathematics are mixed for demand queries, it's a pretty natural valuation class for which answering one is MP hard. So that's another reason why Levy Swami doesn't apply here, because these don't support those in polynomial time. That said, a value query for a coverage valuation is easy. You just enumerate through the XIs and just check which of the XIs are contained in one of the given sets. All right.
00:19:04.060 - 00:19:04.810, Speaker B: Okay.
00:19:05.980 - 00:19:35.700, Speaker A: So the upshot is, all right, so Levy Swami doesn't seem to work. At least it's not clear how to use it. So we want to do some MIDR mechanism. And still, if you think about as far as all the algorithms in the world, which ones seem like they might yield MIDR mechanisms, still seems like randomized rounding is kind of the closest to it. Again, because you're doing this optimization over these fractional solutions, and the randomized rounding generates these distributions. It kind of feels like on the right track. But we need to go beyond scaling algorithms.
00:19:35.700 - 00:20:23.060, Speaker A: If you restrict yourself to scaling algorithms, it feels like you're basically it really is. This Levy Swami framework, it's hard to see. They seem like more or less the same thing. So the big question then is, what else could we do other than a scaling algorithm and get an MIDR randomized rounding algorithm? That's kind of the conceptual hurdle we have to climb over. So question how to go beyond scaling algorithms. So let me just remind you, sort of the blueprint that I had in mind. So the way we're thinking about it is the vis are fed as input to some LP solver.
00:20:23.060 - 00:21:52.144, Speaker A: The LP solver generates the best fractional solution y star for the given input, the vis. This is the input into a rounding algorithm, an oblivious rounding algorithm R, and then that generates some distribution. Over allocations and scaling algorithms were by design so that optimizing over the output of the rounding algorithm, optimizing over the distributions was exactly the same problem as optimizing over the input to the rounding algorithm, optimizing over the LP solutions. And so the challenge, but differently, is that for most routing algorithms so, like I said, MIDR is not the kind of property you ever get accidentally. If you're not designing for it, you're not going to get it. So for most R, it's just not going to be the case. As we talked about last lecture, that the optimal y, where this is over the LP solutions corresponds to the optimal R of y scaling algorithms has this property most rounding algorithms will not even clear.
00:21:52.144 - 00:22:30.168, Speaker A: I really don't have any ideas how you would ensure this property other than through scaling algorithms. So we need a different idea. Okay, so here's the sort of crazy idea, and it's kind of motivated by the idea, okay, so we want an algorithm with a bunch of properties, right? So we're thinking MIDR because that's about the most powerful class of mechanisms we know. We want it to be run in polynomial time. We want to have a good approximation factor, okay? And so what we've been doing is we're saying, well, we know how to get polynomial time and a good approximation factor, so let's try to sort of tack on MIDR somehow.
00:22:30.264 - 00:22:30.764, Speaker B: Okay?
00:22:30.882 - 00:23:46.880, Speaker A: And that very successfully led us to the scaling algorithm idea, which was great, but so now we're kind of stuck, right? So now let's actually go back and ask, well, is there some alternative approach where MIDR is the property that seems really hard to get, like the hardest to get? So can we have some high level approach that would guarantee the MIDR property and then we can sort of investigate what are the conditions, extra conditions, do we need to recover polynomial time and good approximation? Okay, so let's shift our focus. Rather than thinking of MIDR as an add on to the algorithms we already have, let's actually take that as the initial constraint and then go from there. And that'll lead us to think about a very different or somewhat different kind of algorithm. What would it look like? So what could we do that by definition would give us MIDR? So here's what we could do. So the idea is, instead of optimizing, as we've been doing all along, over the LP solutions, instead of optimizing over the input to the routing algorithm, let's optimize directly on the output of the routing algorithm. And again, for a scaling algorithm, those are the same thing for a non scaling algorithm. These are different things.
00:23:46.880 - 00:24:17.360, Speaker A: So instead of optimizing over these, these are still going to be our decision variables. But let's optimize over this objective function. So here's what I mean. So let's see if we could directly solve, okay, so here's what we have. So we're still maximizing some notion of welfare. We still have some same decision variables as before. This is still going to be subject to y feasible for some LP.
00:24:17.360 - 00:25:07.730, Speaker A: But instead of up here writing as I've always been doing some over I, some over s yis vis, I'm going to write something different. I'm going to write let's actually just sort of look ahead to the expected welfare we're going to get from the routing algorithm if we feed in y as input. So, in other words, how do I want to write this? I want to write this like this. So, expected value. So for a given choice of y, there's some corresponding distribution R of y over allocations. This is just a random allocation from that distribution I e. This is just the output of the rounding algorithm if you feed it input y.
00:25:07.730 - 00:26:13.588, Speaker A: And I just look at the expected welfare. So relative to before, basically what I've changed is the objective function. And again, rather than measuring the quality of an LP solution y by the usual linear objective function, I'm measuring the quality of a solution y to these constraints by how good is the resulting random allocation after I apply the randomized algorithm R to it. So, again, the point is to optimize directly over the output of capital R, not over the input. Another thing to notice, so this optimization is only defined with respect to a rounding algorithm capital R. So you should think of capital R as fixed. So we fix some algorithm that takes a linear programming solution and somehow generates randomly an allocation R is fixed, then this optimization problem, at least it's well defined to write it down.
00:26:13.754 - 00:26:14.470, Speaker B: Okay?
00:26:15.800 - 00:26:34.460, Speaker A: Okay. So in effect, optimize over the output rather than the inputs.
00:26:37.040 - 00:26:37.790, Speaker B: Okay?
00:26:40.320 - 00:27:38.412, Speaker A: And so I told you. So the only reason you'd ever come up with this is if you were trying to guarantee some very challenging property like MIDR. So the whole reason we thought about this is this gives you MIDR by definition, okay? So in other words, consider the allocation rule which solves this optimization problem, thereby computing A-Y-A-Y star and then apply the rounding algorithm capital R to it. Okay, so therefore randomly outputting an allocation. So the claim is that allocation rule is MIDR for sure. Why? Well, so what's the range of this rule? So the range of this rule is just all output distributions of the rounding algorithm for solutions that meet the LP constraints.
00:27:38.556 - 00:27:39.250, Speaker B: Okay.
00:27:40.420 - 00:27:59.524, Speaker A: So the range is just imagine any input to the rounding algorithm. Imagine the corresponding distribution and the objective function literally says pick the best of those distributions to maximize expected welfare. Okay, so really the definition of this objective function means it's an MIDR allocation rule.
00:27:59.652 - 00:28:02.810, Speaker B: Okay, all right.
00:28:04.940 - 00:28:54.676, Speaker A: So not clear. This is sort of a crazy idea. Okay, let me just point out that, like I said, this optimization problem is defined with respect to a rounding algorithm, R. So let me just point out, if R happens to be a scaling algorithm, then actually this is a very normal optimization problem. And in fact, this is sort of one way you can sort of interpret the previous lecture. So scaling algorithms are the ones where this problem becomes a linear program. So if R is alpha scaling, then the objective, right? So if R is a scaling algorithm, then this is always just the expected welfare.
00:28:54.676 - 00:29:23.260, Speaker A: But if it's a scaling algorithm, you know, the expected welfare is just alpha times the LP solution value. So the objective would just be equal to alpha. And this was just our computation before. So when we first introduced alpha scaling algorithms, we noticed that given an LP solution of Y, the expected well for the energy allocation is the same scaled by an alpha factor.
00:29:23.340 - 00:29:23.632, Speaker B: Okay?
00:29:23.686 - 00:29:33.696, Speaker A: So that's just we're plugging in that computation here. This, you'll notice, is linear. That should be just a Y. This is linear in the decision variables, the Y's.
00:29:33.808 - 00:29:34.132, Speaker B: Okay?
00:29:34.186 - 00:29:54.532, Speaker A: So this would just be a linear program if capital R is a scaling algorithm. So at least it captures the special case that we used successfully last time. So the hope then again, remember, the whole point here is how do we go beyond scaling algorithms subject to MIDR?
00:29:54.676 - 00:29:55.044, Speaker B: Okay?
00:29:55.102 - 00:30:56.050, Speaker A: So this at least is sort of now like a concrete proposal. So we can say, okay, here's an optimization problem no matter what capital R is, it's MIDR. So now what we want is we want say, for what, rounding algorithms. Can we solve this in polynomial time and get a good approximation? If it's a scaling algorithm, we can solve it in polynomial time. Is there anything else where we can solve this in polynomial time? Well, the first question will be, is there anything else for which this is linear? And that seems like that's basically only scaling algorithms. But we can solve nonlinear, some nonlinear programs in polynomial time as well, right? If this is a concave function in Y, then it's still a tractable problem, right? So concave maximization subject to linear constraints, that's something we can do. So a pathway to go beyond scaling algorithms to say, for which rounding algorithms are, is this objective function concave and the decision variable is Y.
00:30:56.050 - 00:31:25.800, Speaker A: Okay, so hope is that this optimization problem remains tractable for some non scalar R's. Of course, we're also hoping R gives us a good approximation. Okay, e g if objective function is concave.
00:31:26.700 - 00:31:27.450, Speaker B: Okay.
00:31:30.400 - 00:31:33.832, Speaker A: So that's the dream.
00:31:33.976 - 00:31:34.670, Speaker B: Okay.
00:31:38.000 - 00:31:51.280, Speaker A: So where I want you to be is I want you to agree that if star is tractable. Then we have an MIDR mechanism, namely, you solve it, you get the Y, you apply the rounding algorithm capital R to it.
00:31:51.350 - 00:31:51.632, Speaker B: Okay?
00:31:51.686 - 00:31:59.140, Speaker A: That's an MIDR allocation rule. So what you should be wondering is, are there any interesting non scalar capital R's?
00:32:01.160 - 00:32:01.636, Speaker B: All right?
00:32:01.658 - 00:33:30.864, Speaker A: So let me show you a rounding algorithm, a capital R, which doesn't work, but it will be very instructive in the way it doesn't work, okay? And I guess I should say for what I've said up till now, this also has nothing to do with evaluations, okay? So it's just always true that this will be MIDR, whatever the valuations, whatever the rounding algorithm capital are. So really what we're going to eventually come up with is we're going to come up with joint assumptions on, first of all, the valuations, namely the assumption that they're coverage valuations. And then also we're going to judiciously choose this rounding algorithm capital R so that we do get concavity and a good approximation because that's what's eventually going to happen. All right? So nonexample. I'm going to use a simpler linear program than we've been using so far, okay? And that's sort of well motivated because if I want to prove that the objective function is concave in the decision variables, kind of the fewer decision variables I have, the better. So I'm going to switch. So the linear programming constraints will be very simple.
00:33:30.864 - 00:34:11.084, Speaker A: They'll just be A-Y-I-J for a bitter, I getting an item j. And we're just going to say that no item can be allocated more than once. That'll be it. So Y-I-J non negative for all IJ, and then also each get allocated at most once. So we haven't really seen something this simple since the unit demand days. And actually this is even simpler because I've dropped the constraint that every bidder gets at most one good, because they're not unit demand, so that wouldn't make sense. So our set of decision variables and the constraints are going to be simple.
00:34:11.084 - 00:34:59.192, Speaker A: So that's nice. Now consider the following rounding algorithm r. Okay, we're just going to do the simplest randomized rounding imaginable, because remember, a rounding algorithm, it just takes as input some Y's and it outputs randomly an allocation. So if you give me yijs that satisfy these properties, just can do randomized rounding. Look at each item J independently. I'll think of the yijs as a probability distribution and Y-I-J gets j with probability y, sorry, I gets j with probability yij. Any leftover probability, if this is less than one, it just goes to nobody's.
00:34:59.192 - 00:35:21.460, Speaker A: So independently for each good j, assign j according to the probability distribution induced by the bidders. Again, leftover probability, it just winds up unassigned.
00:35:22.280 - 00:35:23.030, Speaker B: Okay.
00:35:26.280 - 00:36:06.150, Speaker A: So this is a particular proposal for capital R. We're going to assume that they're coverage valuations. And what I want to investigate is whether or not our objective function, the expected welfare after the rounding. Algorithm. After that rounding algorithm, whether or not that expected welfare is concave and those decision variables the wise. All right, so just a preliminary observation, sort of a sanity check if you think about it for a second. Um, this could never work.
00:36:06.150 - 00:36:49.090, Speaker A: By which I mean there's no way that this rounding algorithm could give us a concave objective function. Let me explain why. So notice this goes back to a comment I made last lecture when I was talking about the imperfect analogy. I said, why do MIDR? Why allow distributions? Well, maybe it gives us tractability, like linear programs give us tractability. But then I pointed out this distinction that linear programs, you get sort of extra stuff, you optimize and it's easier, but then you get things that are sort of even better than integer solutions. But if you're dealing with distributions, you never get something that's better than all the integer solutions, right? Because a distribution is dominated by the best solution in its support. So we're going to see that come up here.
00:36:49.090 - 00:37:57.796, Speaker A: So think about this rounding algorithm. What if I fed this rounding algorithm capital R? What if I fed it an integer solution, a zero one solution? So, like the y's, just say the first item goes to bidder three, the second item goes to bidder five, the third item goes to bidder one. What's the rounding arithmet going to do? Just going to be like, okay, you already know what you want. What do you need me for? Right, so R of y is just going to be y'sort of an abusive notation, but basically a zero one vector is a characteristic vector for an allocation, and this thing will be a point mass at that allocation. All right, so what would it mean to be so now think about being MIDR, right? So that means so you need to look at all the distributions in your range. So this says in particular, every point mass is in the range of this rounding algorithm, right? You give me an allocation, integer allocation, there's a corresponding zero one vector y. If I feed it into R, I get it right back.
00:37:57.796 - 00:38:42.352, Speaker A: So every single allocation, every point mass is in the range. So if you're going to optimize over this range, you have to in particular optimize over all of the integer allocations. And the whole point was to attack problems for which that's a hard problem using MIDR. Okay, so this can't work. There's no way that a rounding algorithm that leaves integer solutions untouched could possibly be computationally tractable unless B equals NP. Okay, so we know this is doomed to fail because the point masses survive in the range, but it's instructive to still try to prove it concave even though we know it's not. To see where the concavity proof fails, it's going to actually lead us quite directly to something else, which is concave.
00:38:42.496 - 00:38:43.190, Speaker B: Okay?
00:38:45.240 - 00:39:39.908, Speaker A: All right. So again, the point here is optimizing over the R of y's as hard as the original problem. Okay, but let's try to prove it's concave anyways. All right, so is it clear what I'm about to try to do? So we have this space of N times M decision variables Y-I-J for every bidder and every item. So this is kind of the feasible region we're dealing with. We're looking at this objective function. So remember what this is.
00:39:39.908 - 00:39:53.790, Speaker A: This is the expected welfare after the randomized rounding algorithm gets applied to y. So this is some real valued function on the y's, and we can ask whether or not this is concave for this particular rounding algorithm or.
00:39:56.400 - 00:39:57.196, Speaker B: Okay.
00:39:57.378 - 00:39:59.980, Speaker A: All right, so proof attempt.
00:40:02.560 - 00:40:04.070, Speaker B: And it.
00:40:05.240 - 00:40:22.740, Speaker A: All right, so, first observation. So we want to prove this is concave. Sum of concave functions is concave. So let's just try to prove this separately for each bitter I. That's sufficient.
