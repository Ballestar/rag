00:00:00.410 - 00:00:55.760, Speaker A: So that brings us to the final topic in part two of lecture twelve, which is going to be one more approach to implementing an approximation of a randomness beacon. And specifically in the last video we talked about the idea of taking Rs of T as being derived from a VRF output or signature from one or multiple recent block proposers. As discussed, discussed in that video, we saw that the R sub T's are not perfect, they're not impossible to manipulate, but there's many less degrees of freedom from manipulating the pseudoraminous R sub T than there would be with a lot of other possible implementations. But in research you never want to be satisfied, you always want to do better. So it's natural to ask could we have an alternative approach to implementing an approximation of a randomness beacon that actually cannot be manipulated at all? So that's what we're going to explore in this video and the next.
00:00:56.210 - 00:00:57.678, Speaker B: And at a very high level, the.
00:00:57.684 - 00:01:43.910, Speaker A: Plan is going to be the shift the burden of generating randomness from the protocol, which is how we've been treating it thus far, to the nodes themselves. So we are going to now be thinking about crowdsourcing randomness from the nodes running the protocol. If you think about it, this is what actually happens in a proof of work protocol like Nakamoto Consensus. I mean the nodes there through trying to solve those hard crypto puzzles just under the random oracle assumption that external mining process has the effect of the nodes supplying the randomness to the protocol. So we'd like some analog of that here in a proof of stake context. The challenge of course is that there's no obvious external process to rely on, we kind of have to rely on the nodes themselves. And so presumably we need some amount of cooperation from at least some of the nodes.
00:01:43.910 - 00:02:44.670, Speaker A: So our goal here is going to be aspire toward what would seem to be kind of the least trust assumption you'd be able to get away with, which is that of all the nodes running the protocol, at least one of them is supplying randomness according to the way we want them to. All of the protocols we discuss are going to ask each of the nodes to generate randomness or pseudorandomness locally and then contribute the outcome of their local process to the protocol. So in this context, a node being correct just means using a reasonable process to generate pseudorandom bits and then report those to the protocol. Like maybe you use the output of a verifiable random function. So in effect, the goal of these protocols is to produce an output which is at least as random as the most random input that the protocol receives from one of the nodes. So let's build up from more naive to more sophisticated approaches to achieving this goal. Our first sort of very simple attempt, which to be clear is a bad idea, is to just use the XOR, the exclusive or of all of the supplied inputs.
00:02:44.670 - 00:03:39.166, Speaker A: So if you're an honest node that owns one of these public keys in the Staking contract, your responsibility is to at each round, generate, for example, using a good pseudorandum generator, some randomness and report that to the protocol. And of course you'll also be signing your contribution so that the protocol knows to associate that contribution, that R sub I with your public PK sub i. Now, from the protocol's perspective, there's going to be some period of time within the round where it's accepting these contributions and the protocol can't count on hearing from everybody, right? If there's 20 public keys registered in the Staking contract, maybe only 17 of them wind up submitting sort of signed proposals for the randomness, right? Those other three, maybe they're Byzantine, maybe they're sort of network failures, who knows? So the protocol, when sort of the time expires, it's just going to take whatever proposals it got. So like 17 R sub I's, it's going to XOR them all together and then use that as the pseudorandum seed.
00:03:39.198 - 00:03:41.394, Speaker B: R sub T. So let me say.
00:03:41.432 - 00:04:10.666, Speaker A: A little bit more about what I mean by XOR or exclusive or that's just going to be a bitwise operation. So assume all of these ris are just bit strings of a common length, like maybe all the R sub I's are 256 bits long. And so now by XOR I mean do an XOR bit by bit. So for example, imagine you're taking the XOR of two different strings. So you just look at the first bit in isolation. If both of the inputs have a zero, the output has a zero. If both of the inputs have a one, the output has a zero.
00:04:10.666 - 00:04:13.738, Speaker A: If exactly one of the two inputs has a one, then the output is.
00:04:13.744 - 00:04:17.026, Speaker B: Going to be a one. So for example, if you XOR 101.
00:04:17.028 - 00:04:35.686, Speaker A: And one 10, then in that first bit both of the arguments have a one. So that gives us an output of a zero in the first bit. In the second bit, exactly one of the two inputs has a one, the second one. So we're going to get a one as the output. And similarly, in the third bit, exactly one of the two inputs, namely the first one, has a one. So we're also going to get a.
00:04:35.708 - 00:04:38.566, Speaker B: One in the third bit if you're.
00:04:38.588 - 00:05:17.590, Speaker A: Xoring a bunch of bit strings. Well, one way to think about it is just that you do this kind of pairwise application over and over and over again, mixing in one new input each time. And each time you take an XOR, you can sort of think of it as like a toggling operation. Like for this example in Magenta, the one 10 in the second argument kind of says toggle the first bit, toggle the second bit, don't toggle the third bit. So you take the first input, you toggle the first bit, you get a zero. You toggle the second bit, you get a one, you don't toggle the third bit, so you keep that one. Another way to think about it, which is exactly the same, is to just stack all of the strings on top of one another and do column wise addition modulo two without any carries.
00:05:17.590 - 00:05:43.114, Speaker A: That's the same thing as XOR. Now that we're all on the same page about what the XOR of a bunch of strings is, let's talk about the pros and cons of attempt number one. On the positive side, if the R sub eyes are chosen independently, meaning nobody sees anyone else's R sub eyes before they choose their own, then in fact we get the property that we want. As long as one of the nodes submits a random string, then the output is the property of the XOR function.
00:05:43.232 - 00:05:45.686, Speaker B: The output will also be random.
00:05:45.878 - 00:06:16.130, Speaker A: So maybe the easiest way to think about this is to view XOR as a toggling operation bit by bit, as we discussed, and to think about the XOR of N strings as just N minus one applications of this pairwise operator. Now imagine that there's, I don't know, like 20 public keys registered in the Staking contract. Imagine we own one of them. We own the 17th public key. We're honest. So we actually submit, let's call it perfectly random bits as the 17th contribution. Now let's think about the output of the XOR of all 20 contributions.
00:06:16.130 - 00:06:50.414, Speaker A: Now the other 19 contributions, one through 16 and 18 through 20, they're arbitrary. They could all be chosen by Byzantine nodes in cahoots with each other. Doesn't matter. Okay, take the XOR of those other 19 contributions. You're going to get some fixed string, right? 110110, whatever. Now the final thing that's going to happen is that string 110110 or whatever, is going to get toggled by the perfectly random string that we chose. If the first bit of that string is a one, well, with a 50% chance we're going to toggle it, turn it into a zero.
00:06:50.414 - 00:07:17.750, Speaker A: With 50% chance we're going to leave it alone, keep it a one. If that bit is a 00:50 percent chance, we toggle it to a 150 percent chance we leave it a zero. Either way, the first bit of the output is equally likely to be a zero or a one. And that's going to be true for all the other bits. And moreover, because the coin flips in our contribution are independent, the different bits of that output are also going to be independent. In other words, because our input is uniformly at random, so is the output of the XOR.
00:07:18.170 - 00:07:18.566, Speaker B: All right?
00:07:18.588 - 00:07:31.838, Speaker A: So that should seem pretty great. So why can't we just stop and declare victory? Well, it's because of this hypothesis. In the good news, if the ris are prepared independently, if each ri is prepared without knowledge of any of the.
00:07:31.844 - 00:07:34.874, Speaker B: Rest of them, and the problem manifests.
00:07:34.922 - 00:07:55.140, Speaker A: In the extreme opposite situation, where there's a node that actually knows all of the other contributions before it chooses its own. And if that's the case, if you have an attacker that's going last, that attacker can force this output, force the XOR to be absolutely whatever it wants.
00:07:56.150 - 00:07:57.918, Speaker B: This should hopefully be clear if you're.
00:07:57.934 - 00:09:04.650, Speaker A: Again thinking about the way XOR works, is this sort of repeated toggling operation because this attacker, it's going to know the other 19 contributions, so it knows their XOR, maybe their XOR is 10110 and now the attacker gets to choose what toggles happen at the end. For example, if it wants to force the all zero string and everybody else's XOR is to 110110, the attacker will also pick 110110 because that'll make the XOR equal to the all zero string. If it wanted to pick the all one force the all one string it would use and so on. So literally, the attacker gets to pick exactly what the output is going to be, which is obviously a total disaster. So is this a realistic scenario? Well, I mean, you can definitely imagine an attacker that has a sort of great network connection and waits till like just before some time expires before submitting its own contribution. So maybe it's an extreme case, but to be honest, it's kind of hard to rule this out. So we definitely need to iterate on this idea to reduce or eliminate this last mover advantage.
00:09:04.650 - 00:09:58.122, Speaker A: So that brings us to our second approach, which is going to use cryptography, unlike the first approach, but in a fairly straightforward way. So let's look into a commit reveal approach to the problem. So the idea here is instead of just having one sort of phase one, sort of period where people submit reports, we're going to have two. So in the first one, people propose not randomness, but commitments to randomness, generally implemented as taking a hash of them, some allegedly random bits. Then once all of those have been collected, and only once all those have been collected, do the participants reveal what was actually the randomness that led to that hash that led to that commitment. And so the intuition here is that unlike in the first attempt, everyone is forced to commit to their randomness before they know everybody else's randomness. Right, because you have to commit to your randomness in phase one, no matter who you are.
00:09:58.122 - 00:10:05.950, Speaker A: And during phase one, you do not know other people's choices. You only know a hash, a cryptographic hash of their choices.
00:10:06.690 - 00:10:08.126, Speaker B: So here in the notation, little left.
00:10:08.148 - 00:11:10.718, Speaker A: Denotes some cryptographic hash function like, say, shot 256. And the property we really need that hash function to satisfy is that in phase two, no node can sort of go back and sort of reinvent some different Ri prime that would lead to the same hash as the Ri that they actually chose. So to see why that's so important, let's just be clear on what the output of this protocol is, which is that in phase two, whatever Ris wind up getting revealed, the protocol is just going to output the XOR of the revealed R subis. Now, from what I've written right there, there's actually no I haven'tied together phase one and phase two at all. I'm not sort of using the commitments. So of course, I mean, it's going to XOR together the legitimately revealed R sub eyes, which means the protocol is actually going to check that when the owner of, say, public key number 17 submits R sub 17, that does in fact hash to the commitment that node 17 submitted in phase one. And the intuition is that if you try to lie and report something in phase two inconsistent with what you did in phase one, you're going to get caught.
00:11:10.718 - 00:11:54.110, Speaker A: So if you report some R sub I prime, it's going to have some different hash. It's not going to match the hash you reported in phase one. The protocol is going to catch that and just ignore you. It'll be as if you didn't participate in phase two at all. Now that intuition is not quite right because all the protocol is in a position to do is compare hashes, right? So first of all, the hash of whatever string you report in phase two, and then secondly the hash value that you reported in phase one. So if those match, then the protocol is going to accept your phase two contribution. Which means that if a node was somehow capable of producing some alternative report, some Ri prime consistent with the hash of Ri that it computed in phase one, that would be a way to trick the protocol.
00:11:54.110 - 00:13:01.462, Speaker A: So second, pre image resistance just says that for all practical purposes, nodes just cannot do this, right? So even if you know the input and you know the hash function's output on that input, you cannot then find some second distinct input that leads to the same output that has exactly the same hash. And this is now starting to sound like a pretty good idea, right? So with a good cryptographic hash function, commitment really means commitment and a potential attacker really is stuck committing to its string R sub I in phase one. It cannot change its mind in phase two by the properties of commitments at the time that it chooses Rsibi in phase one. It may know other participants commitments, it may know the hashes of their own random strings. But again, assuming as usual that you can't just invert this cryptographic hash function, in practice that means during phase one there's no way for the attacker to know what other people's random strings are, only the commitments. Therefore, it doesn't know what it should pick for its string to force some output of the eventual x or. So that intuition is correct in the sense that there's no last mover advantage in phase one.
00:13:01.462 - 00:13:30.830, Speaker A: It's literally no help at all knowing everybody else's commitments at the time that you're choosing your own input r sub I. There is however, a somewhat subtle last mover advantage in phase two. Now wait a minute, I hear you say I thought the whole point of these commitments, I thought the whole point of this second pre image resistance to the cryptographic hash function is that actually a node's hand is forced in phase two. Its only option is to reveal the string that it had in mind in phase one, the string on which it computed its commitment.
00:13:31.330 - 00:13:33.082, Speaker B: And so what's true there is that.
00:13:33.156 - 00:14:36.194, Speaker A: If a node chooses to participate in phase two, then by second pre image resistance there's only one possible report it could make but it can still choose whether or not to participate in phase two. You want to again here be thinking about some attacker that has a super good network connection and they're waiting up until just the very last moment at which they're still eligible to reveal the string that they committed to in phase one. So suppose in that time the clock's about to strike twelve. Suppose 16 other random strings have been revealed contributed by owners of other public nodes, public keys. So the attacker node now has two options, right? It can look at the 16 strings contributed by everybody else, compute their XOR, look at the result and it could say, you know what, I'm pretty happy with that choice of the output of this protocol. Like maybe I'm going to be the node selected as a leader and in that case I can just remain silent. Okay? I just won't bother to reveal the string I committed to in phase one.
00:14:36.194 - 00:15:34.870, Speaker A: I'll just allow the protocol to use the XOR of the other 16 strings because I like that output anyways. If I don't like that output, like maybe it doesn't select me as a block proposer in the next round, well then I have sort of a free option, right? There's some string I committed to in phase one and I have the option of revealing that in phase two. And if I do that will then get mixed in to the previous XOR of everybody else's 16 contributions. So that'll give me a second effectively independent chance of getting an output that I'm happy with, for example of resulting of the result of the XOR being a string that selects me as the next block proposer. So that's the big drawback you need to know about. With this approach to crowdsourcing randomness, whoever goes last in phase two basically gets to pick between two possible outputs of this randomness generation protocol. So maybe there are some applications where this sort of limited bias ability wouldn't be a deal breaker, wouldn't be that big a deal.
00:15:34.870 - 00:16:28.600, Speaker A: But if you're using this pseudorandomness, these Rs of T's to for example select block proposers in a proof of stake blockchain protocol, then this is not going to work. This is just not good enough to see that. Think about an attacker that has maybe 20% of the overall stake that's locked up in some staking contract, which is supposed to be not that much, right? All our consensus protocols tolerate at a minimum, something like a third of the participation being controlled by Byzantine nodes. And indeed, if the randomness was generated perfectly, then this 20% attacker, it would be chosen as the block proposer one in five times. And that's something all our consensus protocols would be able tolerate. Now, suppose that the attacker not only controls 20% of the overall stake in the staking contract, but is also able to manipulate the randomness generation in the way we've described here. In effect, in each round, T getting a free option to choose between two possible values of what R sub T is going to be.
00:16:28.600 - 00:17:12.390, Speaker A: Now, for any particular random value for R sub T, this node has a four and five chance of not being selected as a block proposer, but with two different independent chances. Now, the probability it's selected as a block proposer and neither one is four fifths squared, also known as 16 over 25, which means the probability that it would be selected with at least one of these two options for R sub T is the complementary probability, nine over 25. And in that case, if at least one of the two choices would lead to the node being selected as the block proposer, this node is good to go. It's obviously going to manipulate the randomness procedure so that the output is the one that results in it being elected as a block proposer.
00:17:12.730 - 00:17:14.198, Speaker B: So it will be in a position.
00:17:14.284 - 00:17:54.014, Speaker A: To make sure it's the next block proposer nine out of 25 times, which, if you think about it, is strictly more than a third. So that then would actually allow this Byzantine node to break our BFT type consensus protocols. So we thought we were going to be robust to 33% Adversaries, but because we have this manipulable randomness generation, the protocol is going to wind up not even being robust to 20% Adversaries. So hopefully that convinces you. We just cannot possibly stop at attempt number two. We need to push onward, we need to work harder. Maybe we wind up using fancier cryptography, but we need to go beyond attempt number two and have some way of crowdsourcing.
00:17:54.014 - 00:18:30.654, Speaker A: Randomness which cannot be biased, cannot be manipulated in this way. So the third and final approach I'm going to tell you about is based on something known as a VDF, which stands for a verifiable delay function. Don't worry, you're not supposed to know what a VDF is. We'll talk about them at length in the next slide in the next video. So let me just kind of here show you what the structure of the protocol is going to be, assuming this kind of like magical cryptographic primitive I'm calling a VDF. So this will also be a two phase protocol. The first phase, honestly, is going to look kind of just like our attempt number one.
00:18:30.654 - 00:19:44.102, Speaker A: We're just going to ask all of the owners of the public keys in the Staking contract to contribute some string arsabi, which again, if the note is honest, is going to be chosen more or less uniformly at random. The difference from attempt number one is that rather than just doing this simple XOR computation, we're going to do a much more involved computation of the contributions and the VDF little F that's going to be exactly this more involved computation. And let's say for concreteness, let's say the input to the VDF is just going to be the XOR of everybody's contributions. So that's going to be attempt number three. Everybody just reports their own proposals and then to figure out what the actual string R sub T that's going to be used by the protocol at round T, then you have to do this nontrivial computation, which is the VDF. Now, the question you should all have for me is why isn't this just as stupid as the idea we used in attempt number one? But again, if you think about an attacker with a really good network connection, just before the conclusion of phase one, they're going to know all of the other Rsubis that have been submitted and it would seem that then they'd be able to force the output of the function little F to be whatever they want. Well, if you think about it, that's going to depend on the choice of little F.
00:19:44.102 - 00:20:32.754, Speaker A: And what I just said is certainly false. For example, if little F is a cryptographic hash function like, say, shot 256, right? Because if I wanted to try to force, say, the all zero string, I wanted to figure out how to pick my R sub I given everybody else's R sub j's so that I get zero out of the function little F that's basically equivalent to inverting a cryptographic hash function on a given output. And that's going to be, for all practical purposes, impossible. So that's the first reason why attempt number three is looking quite a bit more promising than attempt number one. At the very least, we can choose some function little F which, unlike XOR, is unpredictable. So a function little F where you have no idea what its output is going to be on a given input until you actually carry out the work of explicitly evaluating little F on that input. Now, unpredictability is not going to be good enough for our purposes.
00:20:32.754 - 00:21:47.594, Speaker A: And indeed, VDFS, they're not merely cryptographic hash functions, they're really their own cryptographic primitive with their own defining properties and their own proposed constructions. And to see the issue, think again about an attacker who right at the conclusion of phase one is aware of everybody else's contributed Rsubies. Now, because of the unpredictability of little F, the attacker is not in a position to force its favorite output, like to force the all zero string. But if the function little F is quick to evaluate, the attacker is in a position to generate a zillion different possibilities for its choice of R sub I. And again, given knowledge of everybody else's R sub J's, the attacker can then evaluate what little F would be for each of those zillion different choices of its R sub I. And that means that while this attacker cannot force its favorite output of the function little F, it can choose among a zillion different possible outputs of the function little F, which is certainly going to be good enough, for example, to ensure that it gets selected as a block proposer in the next round. So that should sound like a fatal flaw in this approach until you remember something I said along the way, which is that if little F is cheap to evaluate, right? So with this attack we were sort of using that assumption.
00:21:47.594 - 00:22:52.914, Speaker A: We were assuming that the attacker just in these last moments before the sort of closing of phase one, was able to try a zillion different possibilities for the output of little F. That's obviously only possible if it can evaluate little F quite quickly. So why not design little F so that it cannot be evaluated quite quickly? That then, is one of the key motivating ideas behind verifiable delay functions. So in a little more detail, we really want a function little F where we have pretty tight control over how long it takes anybody to evaluate whether that anybody is an honest node or whether that anybody is a well funded attacker. Think for example, about an honest node who, if you remember our protocol, they're responsible for actually evaluating little F to figure out what r sub T the protocol is going to be using. So an honest node better be able to evaluate little F in the time it has available to it, which is the time of phase two. On the other hand, let's think about the attacker right now, the attacker's contribution, its arsebi, it has to be submitted during phase one.
00:22:52.914 - 00:23:38.286, Speaker A: So what we're worried about is the attacker having time to evaluate little F a bunch of times during phase one. So let's just make sure that the time to evaluate little F, even once by the attacker is bigger than the length of phase one. So we want to be robust to attackers, ideally that have a pretty big budget. So we want to be thinking of an attacker as having potentially more computational resources than an honest node. So maybe both faster machines and maybe also like lots of machines that they can run in parallel. So because of that, because the attacker we're thinking of as being faster than the honest nodes, we're definitely going to need the duration of phase two to be longer than the duration of phase one. The extent to which it has to be longer is going to correspond to the extent to which we think a well funded attacker can be faster than an honest node.
00:23:38.286 - 00:23:39.330, Speaker A: In evaluating the function.
00:23:39.400 - 00:23:41.986, Speaker B: Little f. So the trick then is.
00:23:42.008 - 00:23:51.654, Speaker A: To design a little f where even if you have an attacker with 10,000 times the computational resources of an honest node, nonetheless, they can't get that much of a speed up of the function.
00:23:51.772 - 00:23:54.918, Speaker B: Little f. If we could actually design.
00:23:55.004 - 00:24:12.410, Speaker A: Such a function little f, then this attempt number three would be looking pretty good. It would definitely be off to the races. If the attacker doesn't have time to evaluate little F even once during phase one, then it has no choice but to commit to its choice of R sub I before it learns the output of little F on literally any input whatsoever.
00:24:12.990 - 00:24:14.238, Speaker B: On the other hand, you should be.
00:24:14.244 - 00:24:36.214, Speaker A: Asking yourself, this seems like a pretty strong property, right? You want kind of the evaluation time of little F to stay roughly constant, even under massively sort of asymmetric differences in computational power. Isn't that not how computers work? So a good question would be like, do we think a function with these properties even exists? So next video is going to be a deep dive on that question.
00:24:36.332 - 00:24:38.050, Speaker B: I'll see you there. Bye.
