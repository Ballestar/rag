00:00:00.250 - 00:00:13.578, Speaker A: All right, so today we're going to do two topics. I want to give you an application of strong linear programming duality, which we covered on Tuesday. I'm going to show you how a famous resulting game theory called the Minimax theorem follows from strong linear programming duality.
00:00:13.578 - 00:00:29.026, Speaker A: Minimax theorem, in addition to being basic and game theory also has applications in computer science. I may have a chance to tell you about those later. Second topic is to actually survey some of the algorithms that are really used used for solving linear programming and a discussion of their efficiency.
00:00:29.138 - 00:00:29.846, Speaker B: Okay?
00:00:30.028 - 00:00:47.962, Speaker A: All right, so first, zero sum games. So for example, you guys all know rock, paper, scissors, right? Or row shambeau, right? So, okay, we're going to play a couple of rounds. Are we going four, right, all right, four.
00:00:47.962 - 00:00:51.710, Speaker A: Okay, sorry. No, sorry, it's four. I'll get it right this time.
00:00:51.780 - 00:00:52.800, Speaker B: All right, ready?
00:00:59.970 - 00:01:10.660, Speaker A: All right, so he beat me two out of three. So I have a proposal for you. Let's now play it where you go first and then I go second.
00:01:10.660 - 00:01:24.562, Speaker A: So pick one. All right, I won. So how about that? I agree, that's very unfair.
00:01:24.562 - 00:01:45.226, Speaker A: So what if I only made you commit to a probability distribution over your strategies over rock, paper, and scissors? So you just said I'm going to randomize in the following way. Okay, then I have to choose a response and then nature flips some coins for you. Now, actually right in Rock, Paper, Scissors it's not so bad if you have to commit up front to just a way of mixing.
00:01:45.226 - 00:02:13.526, Speaker A: Because if you just commit up front to choosing a strategy uniformly at random, each one equally likely, then no matter what I do, the probability that you win, lose or tie are all equal all equal to a third. Okay, so in rock, paper, scissors, if you're allowed to randomize, you can go first and it's not a big deal, you can still protect yourself. The Min max theorem says that in any so called zero sum game, similarly, a player can go first and protect themselves by randomizing in an appropriate way.
00:02:13.526 - 00:02:47.410, Speaker A: Okay, so what is a zero sum game? So it's specified by matrix M by n matrix, one chooses a row and the other chooses a column. So they're naturally called the row and column players. So one player chooses a row.
00:02:49.270 - 00:02:49.586, Speaker B: The.
00:02:49.608 - 00:03:23.120, Speaker A: Other chooses a column. And so what are the semantics of the matrix entries? So the matrix entries could be positive or negative, and we interpret Aij as the payoff earned by the row player if the row player chooses row I and the column player plays column J. Okay, so payoff of the row player in the outcome I comma j.
00:03:23.120 - 00:03:48.790, Speaker A: And why is it called zero sum? It's because we also define this to be the negative payoff of the other player. So in other words, you should think of Aij as the amount of money that the column player has to pay to the row player. Now AIG might be negative, in which case the payment goes in the reverse direction.
00:03:48.790 - 00:04:11.760, Speaker A: So for example, rock, paper, scissors. How would you encode this as such a matrix? Well, the diagonal is where the two players choose the same action. So that's a draw.
00:04:11.760 - 00:04:23.610, Speaker A: So that's going to be zero. Nobody wins, nobody loses. Now here paper beats rock, right? So in this entry, it's the row player that wins.
00:04:23.770 - 00:04:24.478, Speaker B: Okay?
00:04:24.644 - 00:04:40.146, Speaker A: So we put a one there, okay, so the row player chooses paper and the column player chooses rock. Then the column player should pay a dollar to the row player. Down here it's the opposite, right? So if the row player plays scissors, then it's a loss to a column player playing rock.
00:04:40.146 - 00:04:43.426, Speaker A: So in that case, the row player pays a dollar to the column player.
00:04:43.538 - 00:04:44.200, Speaker B: Okay?
00:04:44.810 - 00:05:01.214, Speaker A: And similarly, okay, so any questions about the basic formalism? So the difference is just that in a general zero sum game, you can have any number of rows m, any number of columns n, and the entries can be whatever you want.
00:05:01.332 - 00:05:04.960, Speaker B: Okay? All right.
00:05:06.770 - 00:05:24.606, Speaker A: So I mentioned the idea of choosing a strategy at random. So suppose x and y are probability distributions over the rows and columns respectively.
00:05:24.718 - 00:05:25.282, Speaker B: Okay?
00:05:25.416 - 00:05:48.300, Speaker A: So think of x and y, they're probability distributions, but think of them as vectors x indexed by the rows, y indexed by the columns. What does it mean that their probability distributions? So the semantics are, is just each entry of X has the probability with which the row player will choose that particular row. So probabilities should be non negative and they should sum to one.
00:05:53.550 - 00:05:54.300, Speaker B: Okay?
00:05:57.230 - 00:06:12.690, Speaker A: So for now, just fix a way in which the row player randomizes over the rows and the column player randomizes over the columns. They randomize independently. Notice again, this is all sort of building up to the statement of the MinMax theorem.
00:06:12.690 - 00:06:41.550, Speaker A: So the expected payoff of the row player, okay? So whenever you see expected, you always just do a sanity check and say, oh, what's the expectation over what's the randomness? The expectation is over which row gets chosen and which column gets chosen from the distributions x and y, respectively. So what's the average payoff of a row player when the players choose strategies according to these distributions? Well, by the definition of expectation.
00:06:43.650 - 00:06:43.966, Speaker B: You.
00:06:43.988 - 00:07:02.180, Speaker A: Just look at everything that might transpire. So the probability that we wind up in the outcome I comma j, the row player picks i, the comp player picks j times the payoff that the row player receives in that particular case when that's the outcome. And then we just sum over all of the things that could happen.
00:07:02.180 - 00:07:32.510, Speaker A: So that's the definition of expectation. Now the row player and the column player, they're picking strategies independently, which means that the probability that the ith row and the jth column both get selected is just the product. Okay? So this is by independence of the random coins of the row and the column player in our vector notation.
00:07:32.510 - 00:07:49.746, Speaker A: This is xi and this is y j. And so if we really want to be succinct to writing this in matrix vector notation. So sum over I-J-I-J times X-I-Y-J.
00:07:49.746 - 00:08:01.586, Speaker A: So the x belongs to the rows, it's multiplying the rows. So we should have an x transpose to the left of the matrix. A y is the one that corresponds to the columns.
00:08:01.586 - 00:08:05.254, Speaker A: Multiplying the columns. So we have a y on the right hand side.
00:08:05.372 - 00:08:06.040, Speaker B: Okay?
00:08:06.490 - 00:08:11.958, Speaker A: So again here x is going to be an m vector, a and m by n matrix and y and n vector.
00:08:12.054 - 00:08:12.314, Speaker B: Okay?
00:08:12.352 - 00:08:14.650, Speaker A: Distribution of the rows, distribution of the columns.
00:08:19.550 - 00:08:20.010, Speaker B: You.
00:08:20.080 - 00:08:36.130, Speaker A: So this is how the row player evaluates how well he or she is doing when the row player randomizes according to X, comp player, randomizes according to y. This is the payoff of the row player. X transpose a y an expectation.
00:08:36.130 - 00:09:07.530, Speaker A: So now let's go back to this issue about whether you play simultaneously or whether you have to go first or whether you get to go second. So question is it better to go first or second? And I mean intuitively in a zero sum game, it can't help to go first. There's only a first mover disadvantage.
00:09:07.530 - 00:09:16.046, Speaker A: Why? Because the second player gets to adapt to what the first player chose. And you saw this when I forced you to pick a strategy up front.
00:09:16.148 - 00:09:16.462, Speaker B: Okay?
00:09:16.516 - 00:09:36.658, Speaker A: So I was able to respond and win the game every time. But there's this question about, well, what if you only commit to a randomized strategy? So maybe we make the row player go first, commit to an x and then the column player gets to respond with a Y, or maybe vice versa. So then what? It still can never help.
00:09:36.658 - 00:09:48.326, Speaker A: You can never be better off going first than going second. Why? Well, consider what you would have done if you had to go first. You're still perfectly free to do that if you go second.
00:09:48.428 - 00:09:48.742, Speaker B: Okay?
00:09:48.796 - 00:10:19.540, Speaker A: You can choose to not adapt if you go second, if you don't want to, okay? So you have only more options if you go second, so you're only going to be better off. But the Min Max theorem is the sort of stunning statement that it actually doesn't matter, doesn't matter if you go first or second. Okay, what do I mean by that exactly? Here's what I mean.
00:10:19.540 - 00:10:31.990, Speaker A: So remember, this is just the expected payoff to the row player when the row mixes according to x. Sorry, these are sometimes called mixed strategies. So when I say mix, I just mean randomize.
00:10:31.990 - 00:10:37.910, Speaker A: Row is chosen according to X. A column is chosen according to Y. This is the expected payoff of the row player.
00:10:37.910 - 00:10:56.554, Speaker A: So what does it mean for the row player to go first? I E for the column player to go second. Well, naturally we're going to assume that the column player, it's a zero sum game, right? So whatever the column player wins, we lose. So we assume the column player is going to do the optimal thing given whatever we chose in the first round.
00:10:56.682 - 00:10:57.360, Speaker B: Okay?
00:10:58.450 - 00:11:19.750, Speaker A: And so remember, the semantics of these entries are the row player prefers bigger numbers. The column player prefers smaller numbers. So when the column player goes second and knows the row player's choice of a strategy, choice of x, it's going to pick the best response as it's called.
00:11:19.750 - 00:11:29.080, Speaker A: It's going to pick the Y, which minimizes the row player's payoff I e. Maximizes the column player's payoff given that the row player chose x.
00:11:29.690 - 00:11:30.306, Speaker B: Okay?
00:11:30.428 - 00:11:36.006, Speaker A: So this is for a fixed X, what we expect to happen when the column player responds.
00:11:36.118 - 00:11:36.780, Speaker B: Okay?
00:11:37.150 - 00:12:00.814, Speaker A: Now from the row player's perspective, if you have to go first, you of course are expecting the column player to respond in this way. Well, you want to make your payoff as high as possible. So you will play your best strategy, the strategy that maximizes your expected payoff, assuming an optimal I E, worst case response by the column player.
00:12:00.942 - 00:12:01.330, Speaker B: Okay?
00:12:01.400 - 00:12:22.566, Speaker A: So mathematically, this is what it means to say that the row player goes first and the column player goes second. If you reverse their roles, you just reverse the Min and the max. So if the column player is forced to go first, they will choose whatever probability distribution gives them the optimal payoff.
00:12:22.566 - 00:12:47.460, Speaker A: And again, remember, the column player wants to minimize under, worst case play by the opponent, which would be a maximization of a transpose Ay, okay? So that's the formal statement of the Min max theorem. Here A. Here is any matrix you want, it doesn't matter.
00:12:47.460 - 00:12:53.966, Speaker A: And x and y are ranging over all possible probability distributions over the rows and columns respectively.
00:12:54.158 - 00:12:57.218, Speaker C: So currently we are choosing the probability.
00:12:57.314 - 00:13:00.280, Speaker B: Distribution instead of the fixed strategy, right?
00:13:01.450 - 00:13:04.840, Speaker C: After we shoot that distribution, then everything happens.
00:13:05.450 - 00:13:11.640, Speaker A: We can't actually control what so nature flips coins is what happens.
00:13:12.030 - 00:13:12.940, Speaker B: Go ahead.
00:13:14.830 - 00:13:20.390, Speaker C: So first, the first player will shoot the distribution, right? The second player shoots distribution.
00:13:20.470 - 00:13:20.874, Speaker A: That's right.
00:13:20.912 - 00:13:23.982, Speaker C: And then the first player like, I mean, something happened.
00:13:24.036 - 00:13:38.626, Speaker A: But think of it this way. Think of it like each player writes a computer program which flips random coins inside, okay? And so the row player first has to submit their code. The column player gets to inspect the code and then submit some code.
00:13:38.626 - 00:13:49.650, Speaker A: And then I'm just going to run their two randomized algorithms and see what happens. Makes sense. All right, so that's the formal statement of MinMax theorem.
00:13:49.650 - 00:13:57.106, Speaker A: So we argued that going first can only hurt you. So we'd certainly expect this to be at most this, but in fact, they're equal.
00:13:57.218 - 00:13:57.880, Speaker B: Okay?
00:13:58.190 - 00:14:02.294, Speaker A: And spoiler alert, this just follows from strong linear programming duality.
00:14:02.422 - 00:14:03.194, Speaker B: Okay?
00:14:03.392 - 00:14:20.670, Speaker A: So any questions before I explain why is the statement clear. All right, I guess I guess a little backstory. So this was first proved by von Neumann in the 1920s.
00:14:20.670 - 00:14:43.266, Speaker A: So Borrell, one of the co founders of modern probability theory, thought he had disproved the Min Max theorem, but then von Neumann realized it's actually true. Von Neumann proved it using arguments kind of related to fixed point theorems, if you know what those are. Then actually, in the years later, von Neumann proved it again by using arguments that are basically equivalent to linear programming duality.
00:14:43.266 - 00:14:58.854, Speaker A: They are arguments I'm about to show you. And so that's why when George Danzig, who I'll say more about later, but he's the inventor of linear programming and the simplex method, so when he went and met von Neumann, and von Neumann at this point was really a big shot. Danzig was quite young, so he was quite nervous.
00:14:58.854 - 00:15:22.580, Speaker A: And so he nervously told von Neumann about this crazy linear programming thing he'd just come up with. And von Neumann instance was like, oh, and he just kind of wrote duality theory on the board in front of dancing, because he'd basically been thinking about it in the context of these different types of proofs for the Min Max theorem. Okay, so here's how we're going to start.
00:15:22.580 - 00:15:34.646, Speaker A: We're going to adopt the role of the role player when they have to go. First, we're going to consider just the computational problem. Suppose I actually wanted to figure out what to do.
00:15:34.646 - 00:15:50.640, Speaker A: Suppose I wanted to actually figure out what is my best strategy if the other player is going to respond in the optimal way. Okay, so for Rock, Paper, Scissors, it's not that hard to convince yourself that the best thing to do is to randomize uniformly. That guarantees you expect to pay off zero.
00:15:50.640 - 00:16:01.358, Speaker A: But what about in a general game? So we're going to show you is that we can actually encode this as a linear program.
00:16:01.524 - 00:16:01.854, Speaker B: Okay?
00:16:01.892 - 00:16:17.110, Speaker A: So computing the left hand side can be formulated as a linear program. Now, initially, you should say it shouldn't necessarily be very obvious. I mean, there's the nested max and Min, which you don't have in linear program.
00:16:17.110 - 00:16:29.810, Speaker A: So that makes you nervous. Also, if you remember, what is this expression? This expression is multiplying x's and y's together. And we're sort of thinking of both the x's and Y's as being decision variables.
00:16:29.810 - 00:16:53.690, Speaker A: So it has a nonlinear kind of quadratic flavor if you just stare at these expressions. Nevertheless, using the tricks I've already showed you, you can formulate this as a linear program. So to see that, here's a preliminary observation, which is that actually, if you get to go second life's, pretty straightforward, you actually don't have to randomize.
00:16:53.690 - 00:17:09.038, Speaker A: All right, so suppose the row player went. So I know the distribution with which the row player is randomizing over their choices. I can just say, okay, if I choose column one, what's my expected payoff? If I choose column two, what's my expected payoff.
00:17:09.038 - 00:17:23.450, Speaker A: Again, the expectation here is over the random choice of a row by the row player. So I can do this thought experiment for each of my columns, each of my strategies. I can just look at which column gives me the best expected payoff, and I may as well just play that deterministically.
00:17:23.450 - 00:17:38.318, Speaker A: Okay, so in rock, paper, scissors, if you do something where you skew toward one strategy at all. So if you're strictly more than a third, say on rock, then I'll just deterministically respond paper. So if you're going one third, one third, one third.
00:17:38.318 - 00:17:50.466, Speaker A: Well, if I want, I can just always play rock, okay? I'll win a third of the time, lose a third, and tie a third. Okay, if you're randomizing uniformly, I could randomize uniformly. That's just as good.
00:17:50.466 - 00:17:56.680, Speaker A: But without loss as the second player, I may as well just pick a best action, breaking ties Arbitrarily, and play it.
00:17:57.050 - 00:17:59.000, Speaker B: Okay. Clear.
00:17:59.770 - 00:18:09.366, Speaker A: All right, so that's the first simplification. That actually for the second player. We don't have to think about optimizing over all of these mixed strategies, all of these distributions.
00:18:09.366 - 00:18:30.560, Speaker A: We just have to think about optimizing over the finite set of possible strategies. So no need for a second player to randomize. So that means we can rewrite this thing.
00:18:30.560 - 00:18:54.200, Speaker A: Let me unpack it again in the summation notation AI J-X-I-Y-J. So we've just said that the second player, rather than minimizing over all distributions, y can just minimize over all of the columns. All of the columns j.
00:18:54.200 - 00:19:05.834, Speaker A: So what would it mean in this notation for the column player to just always choose column one? It would mean that Y one is equal to one, and the other entries of Y are zero.
00:19:06.032 - 00:19:06.778, Speaker B: Okay.
00:19:06.944 - 00:19:28.530, Speaker A: Similarly, if it always played column two, y two would be equal to one, and all the rest would be zero. Okay, so when the column player just chooses some column deterministically, this expression simplifies. Okay, so now I'm just going to look over all possible strategies, all possible columns.
00:19:28.530 - 00:19:50.902, Speaker A: Now I'm going to sum only over the rows, and I'm going to get Aigxi. Okay, so for a fixed j, I've just plugged in the vector y, which is one in the jth coordinate, and zero everywhere else. Again, that corresponds to just always playing column j.
00:19:50.902 - 00:20:06.526, Speaker A: So this is what the column player will get. Or the column player will get the opposite of this if it always plays J, given that the row player is playing x, what's the column player going to do? It's going to pick the best choice of J over all of the options. Okay, so I claim these are exactly the same number.
00:20:06.526 - 00:20:07.840, Speaker A: Everyone follow that.
00:20:11.300 - 00:20:12.050, Speaker B: Okay.
00:20:13.380 - 00:20:29.864, Speaker A: All right, so what progress have we made? Okay, the big progress that we've made is we took a term which used to look like sort of a quadratic term, and we've replaced it with something which looks like a linear term. Remember, the AIJS are just constants. And so now this is a linear function of x.
00:20:29.864 - 00:20:34.180, Speaker A: Okay, there's still the other problem we mentioned, which is you have this weird nested minute.
