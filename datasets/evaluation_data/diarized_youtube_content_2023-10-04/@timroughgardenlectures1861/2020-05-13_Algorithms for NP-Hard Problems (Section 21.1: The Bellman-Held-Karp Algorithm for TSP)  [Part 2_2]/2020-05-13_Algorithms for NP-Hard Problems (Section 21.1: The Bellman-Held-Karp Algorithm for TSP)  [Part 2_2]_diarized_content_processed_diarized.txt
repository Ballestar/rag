00:00:00.570 - 00:01:14.046, Speaker A: So what's the upshot of that quiz? The upshot is that we now know what optimal solutions have to look like. So if you're telling me you have a minimum cost path from one to J that visits every vertex exactly once, I know there are N minus two and only N minus two candidates for what your path could possibly look like. As soon as you tell me the penultimate vertex k, as soon as you tell me that the last hop of your path is k comma J-I-I know what the rest must look like. It must look like a minimum cost path starting at one ending at K, visiting the vertices of V minus j and only the vertices of V minus j. So that is the sub problem that the path prefix p prime solves optimally. So now that we understand that there's only these N minus two possibilities for what the one to j path looks like, we can write down a recurrence which expresses the cost of that optimal solution in terms of the costs of the N minus two smaller optimal solutions. So let me introduce a little notation so we can specify the recurrence succinctly by capital C, subscripts capital S, comma j, I'm going to mean the minimum cost of a path that satisfies the following four properties.
00:01:14.046 - 00:01:30.666, Speaker A: Number one, the path should start at vertex one. Number two, the path should end at the vertex j. Number three, the path should be cycle free so it doesn't visit any vertex more than once. And number four the vertices that it visits exactly once should be precisely the.
00:01:30.688 - 00:01:34.154, Speaker B: Vertices in the set capital S. So.
00:01:34.192 - 00:02:13.590, Speaker A: For example, in this cartoon, on the upper right part of the slide, imagine that the sort of outer magenta circle is the set of all of the vertices and then the top set is S, and then the bottom is the other vertices, v minus S. And this light blue path is meant to sort of indicate that this path is visiting each vertex of S exactly once. So we want to know. So this notation is the minimum cost of any path that looks like that. And what did we learn from the quiz? Once you know K, you know what the rest of it looks like. The path prefix must be a minimum cost path from one to K that's cycle free and visits exactly the vertices of V minus j. And then of course, the original path that went all the way to J.
00:02:13.590 - 00:03:06.350, Speaker A: It also pays for that additional hop. So the cost of the edge from K to J. So again, if you prefer to think about dynamic programming recursively, this is basically saying to solve the original problem, you need to make N minus two recursive calls to compute the optimal solutions to these different subproblems. And then you take the best of all of the solutions returned by those N minus two recursive calls. Now, of course that recursion is going to continue, right? So you need to solve these N minus two smaller problems on the right hand side. But how do you solve that? Well, you just apply the exact same recurrence again, this time to the smaller set of vertices. So more generally, if we replace capital V in this equation with any subset capital S of vertices, we get exactly the same recurrence.
00:03:06.350 - 00:03:41.956, Speaker A: So in words, if you show me an optimal path that visits exactly the vertices in capital S, it goes from one, it goes to j and it's cycle free. If you tell me the penultimate vertex on that path, I know what the rest has to look like. It's going to be a subpath. Now it visits all of the vertices of S except for that last endpoint, J, and it visits the other vertices S minus J exactly once in a cycle free way while going from one to K. So that's the general version of the recurrence for any vertex subset.
00:03:41.988 - 00:03:45.064, Speaker B: Capital S. So this now tells us.
00:03:45.182 - 00:04:14.348, Speaker A: Exactly what our subproblems should be. Basically we need a subprobleblem for every term that might show up in one of these recurrences. So for every choice of the vertex subset capital S, and for every choice of that last vertex little j, we're going to need a separate subprobleblem to compute that capital C s comma j. So which of these terms do we need to worry about? Which choices of capital S and little j actually make sense? Well, remember, capital S, those are the vertices that the path is supposed to.
00:04:14.374 - 00:04:15.796, Speaker B: Visit and the path is supposed to.
00:04:15.818 - 00:04:33.316, Speaker A: Start at vertex one. So the set capital S better include the vertex one. Also remember j, that's where the path ends, so capital S better also include the endpoint j. So you're going to have one of these terms for each choice of j and each choice of capital S that includes both vertex one and that choice.
00:04:33.348 - 00:04:43.954, Speaker B: Of the endpoint j. So the bad news is this is.
00:04:43.992 - 00:05:31.314, Speaker A: A lot of subproblems, an exponential number, right? Because there's N vertices, so there's two to the N, different subsets of vertices. Now capital S here, it can't be any vertex subset, there's a couple mild constraints, but still there's an exponential number of different capital S's that you need to worry about. Plus then there's again another linear and n number of choices of j. So that's a bummer that there's an exponential number of subproblems. But remember, we were expecting this, right? The Tsp is NP hard, so if we're going to apply dynamic programming, we need to expect something exponential to show up somewhere either in the number of problems or in the time required to solve each subproblem or in the post processing step. And just looking at the many examples that we've seen, it seems like the extra complexity always shows up in the number of subproblems. So we were actually expecting to see an exponential number.
00:05:31.314 - 00:05:32.718, Speaker A: So this actually is telling us we're.
00:05:32.734 - 00:05:33.970, Speaker B: Probably on the right track.
00:05:34.120 - 00:05:59.606, Speaker A: I also want to point out that while exponential it's a lot better than N factorial, it's more like two to the N than N factorial. And the reason where that savings is coming from is that these subproblems don't worry about the order in which the vertices of Capital S are visited. So it tracks which subset of vertices a path is visited, but not the order in which they were visited. And that's why the factorial goes away and is replaced by the simple exponential.
00:05:59.638 - 00:06:02.058, Speaker B: Function two to the N. So we've.
00:06:02.074 - 00:06:40.726, Speaker A: Got almost all our ducks in a row, right? We have our sub problems. We came to them by this thought experiment of what optimal solutions have to look like that led us to our recurrence. And so now we just want to solve all these sub problems systematically from smallest to largest. There's a very natural ordering from smallest to largest depending on how many vertices the path visits, depending on the size of the set capital S. Remember, there's one final ingredient, dynamic programming, which is you need to be able to extract the final solution from the sub problem solutions. Most commonly the original problem literally is one of the subproblems. That's not true here, right, we want a tour and here all of the subproblems are computing paths.
00:06:40.726 - 00:07:27.878, Speaker A: But now just we can do that exhaustive search over the N minus one choices for the last vertex visited by the tour. Do exhaustive search over those N minus one choices and then just plug in the values of our largest subproblem solutions. So with all the ingredients in place, the dynamic programming algorithm now just writes itself. We're first going to solve the base cases so the smallest subproblems that'll correspond to vertex subsets, capital S, with only two vertices, vertex one and some other vertex. Then we'll move on and use the recurrence to solve the next largest subproblems with the visit past the visit, three vertices, then subsets of size four and then five, et cetera. And once we're through with all of the subproblems, we'll use that final equation to compute the final solution. This algorithm is sometimes called the bellman held carp algorithm.
00:07:27.878 - 00:07:32.666, Speaker A: It was proposed independently in 1962, on the one hand by Bellman and on.
00:07:32.688 - 00:07:34.970, Speaker B: The other hand by Held in Carp.
00:07:35.310 - 00:08:21.250, Speaker A: So we start by initializing our array that's going to keep track of all of the subproblem solutions. Our subproblems are parameterized by two different parameters, capital S and little J. So it's going to be a two dimensional array. There's a roughly exponential number of choices of capital S two to the little N minus one, quantity minus one to be precise. And then there's the most N minus one choices for j all vertices other than vertex one. The base cases correspond to the vertex subsets of size two, so it has to contain vertex one, and then there's going to be some other vertex, little j, and then little j is the only option for the endpoint as well. And so then the shortest path that goes from one to j envisits only one in j.
00:08:21.250 - 00:08:24.246, Speaker A: That's got to be the direct one hop path. And we know that's cost, that's just.
00:08:24.268 - 00:08:26.706, Speaker B: The cost of the corresponding edge.
00:08:26.898 - 00:09:07.670, Speaker A: So now we solve all the sub problems systematically, working from smaller subproblems to larger subproblems. And again, the natural notion of problem size here is the number of vertices that the path is supposed to visit. So the cardinality of the set capital S. So we start from size three subsets, and then we work up to size four, et cetera, culminating with when capital S is equal to all N vertices. So now we have two more for loops which are looping over the choices of the parameters capital S and little j. Since little j is supposed to be drawn from the set capital S, it makes sense to first start by looping over all the subsets of the current.
00:09:07.740 - 00:09:10.438, Speaker B: Size size little S. Then for a.
00:09:10.444 - 00:09:14.486, Speaker A: Given subset capital S, we know all the choices of j, it's every vertex.
00:09:14.518 - 00:09:16.940, Speaker B: And capital S except for the vertex one.
00:09:19.970 - 00:09:38.178, Speaker A: So now in this inner loop iteration, it really corresponds to a specific sub problem sub problem capital of computing capital C, subscript capital S comma j, and we know how to do that. We just use the recurrence. So it's really just an exhaustive search over all choices. Little K for the penultimate vertex of an optimal path, visiting the vertices in.
00:09:38.184 - 00:09:39.220, Speaker B: Capital S.
00:09:48.390 - 00:09:59.420, Speaker A: Once all three of these for loops complete, we've solved all of the subproblems and then, as we know, the final solution can just be computed from the biggest of those subproblems with capital S equal to capital V.
00:10:06.800 - 00:10:07.164, Speaker B: One.
00:10:07.202 - 00:10:27.056, Speaker A: Sanity check that you always want to do when you write down your pseudocode for a dynamic programming algorithm is when you're computing one entry of your sub problem solution array you want to make on the left hand side. You want to make sure that all of the entries that you need from the array on the right hand side are already computed and therefore available for constant time. Lookup, and we see that that is.
00:10:27.078 - 00:10:29.536, Speaker B: Indeed the case here on the right.
00:10:29.558 - 00:11:05.372, Speaker A: Hand side of the recurrence. We're always looking up the value for sets that have one less vertex than capital s, so smaller subsets all of those will have been computed in the previous iteration of the outermost for loop. So that's the bellman held carp dynamic programming algorithm for the traveling salesman problem. As always, when we introduce an algorithm, we should think about what are its properties in terms of correctness and in terms of its running time. Correctness is not so interesting. It's kind of just the standard argument for dynamic programming algorithms, which you've seen many times. You proceed by induction on the sub problem side.
00:11:05.372 - 00:11:34.592, Speaker A: So you just argue that each sub problem gets solved correctly in the inductive step. Why is it true? Well, the correctness comes from the correctness of the recurrence. We're filling in the sub problem answers correctly. Why is the recurrence correct? Well, that just goes back to the optimal substructure that we started with. We observed that optimal solutions to a given subproblem can only have one of a small number of possibilities and the recurrence explicitly does exhaustive search over that small number of possibilities. So it necessarily computes the optimal solution.
00:11:34.736 - 00:11:35.568, Speaker B: For the running time.
00:11:35.594 - 00:12:03.836, Speaker A: We can just go back to our generic analysis of dynamic programming algorithms where we just count up the number of subproblems, multiply it by the time per sub problem, and throw in the post processing work. So here, first of all, how many subproblems are there? That's what we were calling F of N before. Well, there's two to the N choices for the set capital S, a little bit less than that, but roughly two to the N. And there's always at most N minus one choices for the second parameter, little j. So that means there's at most two to the N times n different subproblems.
00:12:03.868 - 00:12:05.136, Speaker B: We have to deal with.
00:12:05.318 - 00:12:40.364, Speaker A: How about the time required to solve each subproblem? Well, this is just exhaustive search over all of the possible choices of the penultimate vertex, little k. There's certainly a most N possible choices for little k at all times. So this is going to be a linear amount of work to fill in each array entry. Then finally there's the post processing step, what we were called h of N before. And so here it's not a constant time lookup, but it's that final line of the pseudocode where you do this exhaustive search over n minus one possibilities. Each case can be evaluated in constant time. So the post processing and that final line is also going to be big.
00:12:40.402 - 00:12:42.936, Speaker B: O of N. So in this analysis.
00:12:42.968 - 00:13:16.536, Speaker A: Of the post processing, I'm assuming that you're content to compute just the total cost of an optimal traveling salesman tour as opposed to the tour itself. But as always with dynamic programming, as you've hopefully seen many times, it's always possible to reconstruct the optimal solution itself by tracing backward through the filled in sub problem array. And if you implement this algorithm in the right way, caching the appropriate things on the forward pass, you can actually reconstruct the optimal tour itself in linear time O of N time. And I encourage you to think about.
00:13:16.558 - 00:13:18.330, Speaker B: That in the privacy of your own home.
00:13:18.700 - 00:13:49.024, Speaker A: So remember the formula for the running time bound of a dynamic programming algorithm. It's just F times g plus h, which in this case evaluates to n squared times two to the N. So one piece of fine print. In this running time analysis, I am assuming that you can generate the number of subsets capital S with a given size little s in time proportional to the number of such subsets. If you think about it, the number of such subsets is exactly n minus one. Choose s minus one because you know that the vertex one has to be in there. So this can be done.
00:13:49.024 - 00:13:58.196, Speaker A: I encourage you to think about how you might do it in a concrete implementation. You can use recursive enumeration, or if you really want to sort of venture out into the weeds, you can look.
00:13:58.218 - 00:14:00.976, Speaker B: Up something known as Gosper's hack.
00:14:01.168 - 00:14:26.632, Speaker A: So how should we feel about this running time? Well, sort of mixed feelings, I think. I mean, on the one hand, this is a lot better than exhaustive search. It's very satisfying to beat the pants off of exhaustive search, which would have been n factorial. Remember, by sterling's approximation, n factorial, that's roughly n divided by e n divided by 2.7 118 raised to the nth power. So that's exponentially bigger than two to the n. Whereas here we're just getting two to the n times a polynomial times n squared.
00:14:26.776 - 00:14:28.092, Speaker B: So that, on the one hand, is.
00:14:28.146 - 00:15:24.556, Speaker A: Deeply satisfying to sort of see yet another kind of killer application of the dynamic programming paradigm that you've spent so much time mastering to beating exhaustive search for a super fundamental problem. The bad news is this running time is still not that great. So with the exhaustive search algorithm, you might be able to handle sizes, maybe if you're lucky, up to like 15 or something. If we had an algorithm with running time two to the n, you could go up to around 40. This is n squared times two to the n, so you'll be able to handle more like inputs of size n equals 30. So we've doubled the problem size that can be handled compared to exhaustive search, which is nice, but if you have a traveling salesman problem that's bigger than that, let's say, has hundreds or thousands of vertices, you're not going to be able to use this dynamic programming algorithm there. You're going to have to resort either to heuristic algorithms, as discussed in the last chapter, or you could try your luck with a state of the art mixed integer programming solver, which we're going to talk about later this chapter.
00:15:24.556 - 00:15:47.220, Speaker A: So what we're going to move on to next is yet another application of dynamic programming to a problem of finding long paths in networks. It will again allow us to roughly double the problem size that you can handle. But actually, in biological applications, that doubling of the problem size that you can handle is totally crucial to getting meaningful results. So we'll talk about that in the next video. See you then. Bye.
