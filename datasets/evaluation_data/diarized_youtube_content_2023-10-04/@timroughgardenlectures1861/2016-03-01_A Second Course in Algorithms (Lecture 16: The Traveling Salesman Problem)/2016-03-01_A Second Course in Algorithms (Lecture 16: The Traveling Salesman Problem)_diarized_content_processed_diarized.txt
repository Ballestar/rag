00:00:00.330 - 00:00:52.766, Speaker A: Okay, so today's lecture is all about the traveling salesman problem, or Tsp as it's usually called. So the Tsp, it's a very famous problem, and really for about 70 years, it sort of served as like the main challenge problem, motivating people to figure out different ways to cope with NP complete problems. So, for example, George Danzig, who we talked about back when we were discussing linear programming, he spent a fair amount of time in the 50s thinking about how could you use linear programs to at least get some traction on integer programs, including the traveling salesman problem. And so MP completeness only came around in 1971, but well, before then, there is this understanding that this is a hard problem, even though it wasn't clear how to formalize that idea at the time. So if you remember one problem from approximation algorithms, it should probably be this.
00:00:52.788 - 00:00:53.360, Speaker B: One.
00:00:55.890 - 00:01:45.994, Speaker A: At least in terms of the name recognition. So what is the problem? Well, I'm going to give you a complete graph undirected with edge costs. And the goal is to compute traveling salesman person tour of minimum cost. So what's a traveling salesman tour? Well, that's something it's not to be confused with an Euler tour. An Euler tour visits every edge exactly once. A Tsp tour should visit every vertex exactly once. Compute the min cost Tsp tour.
00:01:45.994 - 00:03:18.874, Speaker A: Okay, so this is a simple cycle, each vertex visited once. So for example, it consider the following graph, supposed to be a complete graph. What is opt in this example? Instances MP complete problems are a little confusing, right? So what do you think? How about at least give me an upper bound on opt? You can certainly do at least as well as what how do you get 81256? You have to visit everybody once, so it actually doesn't matter where you start. It's going to be a simple cycle, visiting every vertex so you could go around the boundary. That's certainly a feasible solution. That's going to have cost 14, it looks like. So can you be 1413? 1345-1345? Yeah.
00:03:18.874 - 00:03:51.650, Speaker A: So 13. Okay, tell you an example. Okay, so, okay, why is it called what it's called? Well, like a lot of problems, there's sort of a silly story which masks how actually practically relevant it is. So the story is that there's some traveling salesman who used to visit a bunch of places, blah, blah, blah. But why would this come up, like, for reals? So imagine there's a bunch of tasks you have to do and you have to decide upon the order of operations.
00:03:51.810 - 00:03:52.422, Speaker B: Okay?
00:03:52.556 - 00:04:18.202, Speaker A: So you have n things. You need to pick a sequence, one of the N factorial ways of ordering them. And imagine that between each two tasks there's some kind of setup cost. Either you need to change equipment, you need to sort of change the people who are working, whatever. So between task I and task J, there's some setup cost like time or money or whatever, then the Tsp problem is really just the problem of figuring out the best order of operations to minimize some of the setup costs.
00:04:18.266 - 00:04:18.494, Speaker B: Okay?
00:04:18.532 - 00:04:26.974, Speaker A: So really, fundamentally, what the problem is about is about finding an ordering. People just talk about it as like, finding an ordering in a graph, an ordering of the vertices.
00:04:27.102 - 00:04:27.826, Speaker B: Okay?
00:04:28.008 - 00:05:20.210, Speaker A: So even you can imagine, even just like with a disk drive, suppose there's a bunch of outstanding requests for reads from a disk, and you want to know how to move things to complete them all as quickly as possible. It's basically a Tsp problem. All right, so let me start with some bad news. So I said this was a hard problem, and it really is, even to approximate. So assume P not equal to NP. Of course, if P equals NP, you can solve this in polynomial time, but if they're different, then actually there's no reasonable approximation algorithm for any alpha. So remember, what's an alpha approximation algorithm? It runs in polynomial time, and it always outputs a feasible solution with cost at most alpha times the smallest possible.
00:05:20.210 - 00:06:08.258, Speaker A: Okay, so there's no two approximation, there's no ten approximation, there's no log n approximation, there's not even any two to the n approximation for this problem. Okay, so it's kind of a disaster. So why do I still have a lecture to give? Well, most of the lecture is going to be about special cases of Tsp, which you can do something interesting for, but let me just prove this to you. So probably be the only NP hardness proof that I plan to do in lecture. Again, my inductive hypothesis is that you sort of know how to do these from a previous class, like 154, but here it'll serve the purpose to also rule out approximations plus, it's simple, so let's just do it. All right, so how do you prove something's MP hard? Generally, you take a known MP hard problem and you reduce it to the problem at hand. If you don't think there's any algorithm for that, then you certainly aren't going to be doing it by solving it.
00:06:08.258 - 00:06:39.850, Speaker A: The problem at hand either. So reduction from the Hamiltonian cycle problem. So what's the Hamiltonian cycle problem? I give you an undirected graph like this one. And I want to know, is there or is there not a simple cycle visiting every vertex exactly once? So, like in this graph, is there Hamiltonian cycle?
00:06:47.250 - 00:06:48.000, Speaker B: No.
00:06:48.930 - 00:07:09.054, Speaker A: Excellent. I heard a yes and a no simultaneously. Mpqb problems are pretty tricky. Yeah, so if there were one, it would be easy to convince you. I'd just show you the cycle. Normally it's hard to convince someone there's no Hamiltonian cycle, but in this one, there is sort of a sneaky argument. Which is I'll just think about the X's and O's.
00:07:09.054 - 00:07:38.160, Speaker A: Notice every edge goes between an X and between an O. So in the Hamiltonian cycle, you'd have to have an equal number of x's and O's, but there's five x's and four O's, so no Hamiltonian cycle. Okay, in general, it's an MP complete problem to check whether an underritograph has a Hamiltonian cycle or not. You probably saw this in some previous course. It's kind of one of the standard MP completeness reductions you do from threesat usually when you're learning about MP completeness. Okay, so we're going to take it on faith. This really is MP heart.
00:07:38.160 - 00:08:22.910, Speaker A: Now, we want to reduce this problem to approximating Tsp. So we want to show how, given a good approximation algorithm for Tsp, that would allow us to solve this problem efficiently. So we need to take an instance of Hamiltonian cycle and we need to reduce it. We need to exhibit a Tsp instance. So we're showing that Tsp instances can encode Hamiltonian cycle instances. So somebody gives you a Hamiltonian cycle instance, an undirected graph, and the responsibility of our reduction is to output a Tsp instance on some graph. G prime with some vertices, v prime, edges, e prime, and costs cost.
00:08:22.910 - 00:09:23.826, Speaker A: C, there's no cost in the original instance. Let me just tell you what in fact is the Tsp instance going to be new vertex set, same as the old vertex set? Tsp is always about complete graphs, so E prime presumably is all edges. And then what's interesting is where do we get edge costs from? There's no edge cost over here, we need to make them up. Well, whenever there is an edge here, we're going to have an edge with low cost here. Whenever there's not an edge here, we're going to have an edge with high cost here. That's the whole idea. Okay, so for e and E set Ce to be equal to one, and for edges not in the original edge set, again, remember, this is a complete graph, so every single edge is there for each edge over here, it may or may not have been on the left.
00:09:23.826 - 00:09:40.950, Speaker A: So if the edge was not there on the left, then we set the cost to be big. Okay, so bigger than alpha times n, say n. Here is the number of vertices. Alpha is just your favorite number, where we're ruling out an alpha approximation algorithm.
00:09:41.770 - 00:09:42.520, Speaker B: Okay.
00:09:44.250 - 00:10:43.012, Speaker A: So everyone clear on the reduction? So this is a polynomial time algorithm that translates a Hamiltonian cycle instance into a Tsp instance. We need to show that if we could solve this Tsp instance well, meaning an alpha approximation, we could actually figure out whether the original graph was Hamiltonian or not. Okay, so why is that true? Well, suppose the original graph g actually does have a Hamiltonian cycle. What is going to be the optimal, the minimum cost Tsp tour in the instance that we get? N. Right? Why? Well, what is a Hamiltonian cycle? It's a simple cycle that visits every vertex. What's a feasible solution to the Tsp problem? A simple cycle that visits every vertex. So if it's a Hamiltonian cycle, that means every edge was in there in the original graph.
00:10:43.012 - 00:11:16.000, Speaker A: So all of those edges have cost one in the new graph, G prime. So there's a Tsp tour with n edges, one cost per edge n overall. So opt in G prime equals n. Okay? It's a most n, but it's actually equal to n. On the other hand, so if G has no Hamiltonian cycle, what's a lower bound on the best possible tour in G prime?
00:11:19.480 - 00:11:20.900, Speaker B: Alpha n squared?
00:11:24.200 - 00:12:15.008, Speaker A: Yeah, I think more like alpha n plus one or something like that. Or alpha n plus some stuff I don't think you necessarily have to. So basically, if in your Tsp tour, every single edge was not in the original graph, then indeed you'd be getting the alpha n squared. But if only like, one edge is not in the original graph, you'd only be suffering alpha n once. But yeah, it's the right idea. Okay, so the point being, opt in G prime, all that we care about is bigger than alpha times n, okay? Because again, how could you have a Tsp torque which was less? The only way to do that would be to only use the edges which have cost one, but then that would map back to a Hamiltonian cycle in G. Okay? Therefore, if there were an alpha approximation algorithm for the Tsp problem, here's how you'd use it to solve Hamiltonian cycle in polynomial time, you'd just take the Hamiltonian cycle instance.
00:12:15.008 - 00:12:30.312, Speaker A: You'd run this reduction. You'd run the alleged alpha approximation algorithm. If you get back something which has cost alpha n or less, you'd say, yes, it's Hamiltonian. If you get back a tour which costs more than alpha n, you'd say, no, it's not Hamiltonian. And that's the correctness analysis right there on the left board.
00:12:30.446 - 00:12:31.224, Speaker B: Okay?
00:12:31.422 - 00:13:19.640, Speaker A: So again, if there's no polynomial time algorithm for the Hamiltonian cycle, there's no polynomial time approximation algorithm for the Tsp problem. Any questions about that? Okay, so this of course, strongly motivates, putting some restrictions on the input. So remember we were talking about possible compromises for MP complete problems. Last lecture, one of them was to restrict attention to a special case. Now, sometimes a special case will actually render it polynomial time solvable vertex cover on bipartite graphs being one example that you've seen on your homework. In other cases, it'll still be MP complete, but you can at least get a decent approximation, and that's going to be the case for Tsp. So there's a lot of restrictions you can make.
00:13:19.640 - 00:13:41.820, Speaker A: I'm going to make a sort of well motivated restriction, which you already are familiar with from the Steinertree problem. So I want to talk next about metric Tsp. So again, the input's the same. You have a graph, you have an edge cost per edge, but the edges are going to satisfy the triangle inequality. So the shortest path between two points is always the one hot path.
00:13:47.160 - 00:13:55.510, Speaker B: It okay.
00:13:59.080 - 00:14:40.972, Speaker A: So you might remember why is it useful to have the triangle inequality in an algorithm or an analysis? Well, it's because you can do shortcutting. If you produce some solution and you kind of want to massage it a little bit, you can always replace paths with direct edges. You'll get something that's only cheaper. So we're going to use that again here. I claim that this is still NP hard, actually. And I even claim that basically this reduction proves that. Now the reduction, as I did it with these big costs, alpha times N, that doesn't prove that this problem is empty hard because these edge costs do not satisfy the triangle inequality.
00:14:40.972 - 00:15:26.256, Speaker A: It's totally possible you have a unit cost edge, another unit cost edge, and the direct edge in between them is alpha times N, which is a violation of the triangle inequality. But suppose to recover the triangle inequality, instead of making this alpha times N, I just make it two. Okay, so still bigger for the edges, which are not in the original cost in the original graph. So if these edges have cost one and these edges have cost two, when there is a Hamiltonian cycle, once again the best TSB Tour will have value n. When G has no Hamiltonian cycle, it means any Tsp Tour uses at least one edge with cost two. So it's going to have cost at least N plus one.
00:15:26.438 - 00:15:27.120, Speaker B: Okay?
00:15:27.270 - 00:16:25.152, Speaker A: So that doesn't rule out any approximation, but it definitely proves it's empty complete, right? So checking whether the optimal solution has value N or N plus one tells you whether the graph had a Hamiltonian cycle or not. And notice, think about it for a minute, if all of the edge costs are one or two, then the triangle equality definitely holds. If you think about it, there's no way to violate it if you're only using ones and twos. So any questions so far? Okay, so let me highlight one. There's going to be a lot of similarities with our Steinertree lectures in the next 30 minutes or so. Let me point out one interesting difference. So Steinertree, which we talked about first for the online algorithm, where we did the greedy algorithm, then we did the MST.
00:16:25.152 - 00:16:56.092, Speaker A: Heuristic last lecture. So there we also thought about the special case where the edges cost satisfy triangle inequality. But remember, that was without loss of generality. So for the Steiner tree problem, the general case of an arbitrary graph and arbitrary edge costs reduces to the case of a complete graph and the triangle inequality. So you can translate algorithms from one to the other. That's on exercise set seven, I think Tsp turns out to be different. Okay, so it really turns out that it's not without loss of generality restricting to the metric case.
00:16:56.092 - 00:17:33.732, Speaker A: And remember, we have our fingers crossed about that because the general case is hopeless, right? So we're really hoping we get a strictly easier problem and we do. Okay, so it's different. This part is different. So it's really a restriction to impose the triangle inequality. So what I want to show you next are approximation algorithms for metric tsp. Now remember, often the main challenge in approximation algorithm design and analysis is to figure out what's your strategy for knowing that your algorithm is near optimal. How do you prove that your solution? Maybe it's not optimal, but it's close to optimal.
00:17:33.732 - 00:18:13.880, Speaker A: And the way you generally do this is you have some kind of bound on the optimal solution. So for a minimization problem like this, you'd want a lower bound. So you want to come up with a number which is only better than opt, and then you want to prove that your algorithm has value not too much larger than this thing, which is only better than opt and therefore not too much larger than opt itself. So we need to come up with a lower bound on how small the traveling salesman problem would be. Now, in the first half of 261, duality really kind of did that for us. And we will use duality again next week quite a bit. But for NP complete problems, you don't expect to exactly characterize the optimal solution using duality because that would prove something like p equal NP.
00:18:13.880 - 00:18:54.790, Speaker A: All right, so where do these lower bounds come from? Well, again today, like last lecture, we'll be able to just have some pretty elementary lower bounds. So for tsp, so consider any tsp instance. This part doesn't even use the triangle equality. Actually, consider any tsp instance. I claim that the minimum cost tor, it has to cost at least as much as the minimum spanning tree of g. Okay, so the cost of a minimum spanning tree is a lower bound on the cost of the optimal tsp tour. Triangle inequality or not.
00:18:54.790 - 00:19:03.204, Speaker A: Everyone see why that's true. So what is a tsp tour?
00:19:03.252 - 00:19:03.800, Speaker B: Right?
00:19:03.950 - 00:19:32.028, Speaker A: It's a cycle that visits every vertex exactly once. What's a spanning tree? Well, it's a subgraph. That's a tree. It's acyclic and hits every vertex. So just take any tour, take the optimal tour, remove your favorite edge, you get a very special type of spanning tree, right? A path of length, n minus one. Okay, so that's only better than opt. You only deleted an edge, and the MST, of course, is only going to be better than this arbitrary spanning tree.
00:19:32.124 - 00:19:32.770, Speaker B: Okay.
00:19:36.900 - 00:19:49.910, Speaker A: So proof, Optor minus any edge equals a spanning tree of g. So MST only better.
00:19:53.800 - 00:19:54.550, Speaker B: Okay.
00:19:57.740 - 00:20:04.264, Speaker A: And this is already enough of an observation to get a non trivial approximation algorithm for the metric tsp problem.
00:20:04.382 - 00:20:05.050, Speaker B: Okay?
00:20:08.880 - 00:20:43.408, Speaker A: So this is going to motivate what's known as the MST heuristic for metric tsp. And this is going to reuse the ideas we developed for the Steinertree problem. We're going to see doubling, we're going to see shortcutting. Here's the main difference to look out for. If you remember both our online and offline steinertree algorithms. The algorithms themselves were dirt simple, right? The online algorithm was just connect the new terminal to the old terminal, which is closest to it. What was our offline algorithm? It was just compute the MST on the terminals.
00:20:43.408 - 00:21:03.436, Speaker A: That was it. So we did all this doubling stuff, we did all this shortcutting stuff. But none of that was in the algorithm for Steinertree. It was all in the analysis. It was just our way of connecting the output of our algorithm to an optimal solution. Here in this MST. Heuristic, we're actually going to use all those ideas, but in the algorithm itself.
00:21:03.618 - 00:21:04.350, Speaker B: Okay?
00:21:12.260 - 00:21:42.280, Speaker A: So the first step is we just compute a minimum spanning tree of the graph G. Okay, so we know this is sort of only better than opt. That's what Lima one tells us. But it's not a feasible solution in general, right? This is not a tour. Okay, so now we really need to somehow massage the spanning tree into a tour. But actually, if you think about it, that's sort of exactly what we were doing in the proofs of our Steiner tree algorithms, especially the offline algorithm, we really took the optimal Steiner tree and then sort of massaged it into a tour on those same set of vertices. So we're going to do that now just in the algorithm.
00:21:42.280 - 00:22:12.630, Speaker A: All right? And so remember, just sort of reverse engineering where this comes from. We need to make use of triangle inequalities. Why are they useful? They allow you to shortcut. So what is it you might want to shortcut? You might want to shortcut an Olarian tour. Okay, but for an Eulerian tour, you need an Olarian graph. How do you get that? Well, you just double every edge, okay, that gives you an Olarian graph and you're off to the races. So in the algorithm, construct H.
00:22:14.600 - 00:22:14.916, Speaker B: Which.
00:22:14.938 - 00:22:53.344, Speaker A: Is T with all edges doubled, this of course is an Eulerian graph, okay. It's connected because T is connected and all the vertex degrees are even. So that means it has an Eulerian tor. And again, we're going to actually compute it in our algorithm, but that can be done in linear time, no problem. So compute an euler tour. Now remember, we want a Tsp tour so that visits every vertex once. This is an Euler tour, visits every edge once, so every vertex at least once.
00:22:53.344 - 00:23:23.372, Speaker A: So we just shortcut out repeated occurrences of each vertex. So you just look for the first occurrence of each vertex on the tour, on the Olarian tour, and that's your Tsp tour shortcut to get Tsp tour. And I'm sure you'll find it very easy to believe that all of this can be implemented in polynomial time. In fact, pretty much very close to linear time.
00:23:23.506 - 00:23:23.804, Speaker B: Okay?
00:23:23.842 - 00:24:30.000, Speaker A: So you can implement this very quickly. So that's the algorithm, it any questions on the algorithm before we move to the analysis? Which is quite short. Okay, so I'll say more about the final exam later, but let me just sort of remind you that we have one, and it's at the normal time listed by the registrar, which happens to be Monday, first day of finals in the afternoon, 330 to 630. They have not confirmed the room for me yet, so I'll keep you posted, and I'll say more about the exam a little later, probably about a week from now. I'll talk a little bit about it. Okay, so, analysis of the MST heuristic. So RTSB tour.
00:24:30.000 - 00:25:24.044, Speaker A: So, the final output of the algorithm, it's certainly no more expensive than the cost of the Euler Tour. This is where we're using the triangle inequality, right? So we shortcut from C down to our final Tsp Tour. It only gets cheaper by the triangle inequality. And so what is the cost of C? Well, C uses the Euler Tour, uses every edge of the Eulerian graph, h, exactly once. So the cost of that tour is just the sum of the edge costs in H. What is H? H is just the MST doubled with two copies of every itch. What do we know about the minimum spanning tree? Well, we've got Lemma one, which says that the minimum spanning tree is only better than the optimal solution.
00:25:24.044 - 00:25:35.152, Speaker A: It's a lower bound. Okay, so we're very happy to see that we can upper bound our algorithm solution by a constant two times something which is only better than ought.
00:25:35.296 - 00:25:35.990, Speaker B: Okay?
00:25:37.000 - 00:25:39.670, Speaker A: That was the plan all along.
00:25:41.560 - 00:25:42.310, Speaker B: Okay.
00:25:44.300 - 00:26:09.804, Speaker A: So summarizing this proves that for the metric case of Tsp, the MST heuristic is a two approximation algorithm, runs upon number of time, always outputs a Tsp tor with cost almost twice the minimum possible. Turns out, and I'll put this on exercise set eight, you can't prove a bound better than two for this algorithm. Okay, so the analysis is tight. There are examples showing that you might be off by a factor.
00:26:09.852 - 00:26:10.450, Speaker B: Two.
00:26:16.660 - 00:26:59.824, Speaker A: Questions about that. Next order of business is I want to add one more idea, which gets us a better approximation ratio. Actually, it brings us to the state of the art. State of the art in this case happens to be from 1976, but it is the state of the art. So any questions before that? Clear so far? Okay, good. So what's the idea? So how can we do better? Again, we can't do better just in the analysis. We need a different algorithm if we want to do better.
00:26:59.824 - 00:27:16.820, Speaker A: Well, we want to make use of the triangle inequality. So we want to shortcut. Seems like the right thing to shortcut is an Alarian graph. Now, if you start with a spanning tree, it's not Alarian. You got to make it a layer in. How do we do that? We doubled all the edges. Maybe that's overkill.
00:27:16.820 - 00:27:48.124, Speaker A: Maybe there's some way we could add stuff to the minimum spanning tree to make it Olarian without actually this sort of very draconian doubling every edge that's sort of the idea. I wonder if there's some cheaper way we can add edges to the spanning tree to make it Olarian. If so, we're off to the races. We just copy the rest of the MST. Heuristic okay, and so there is there's a way to do it. So, limit two. So this is going to be a second lower bound on the optimal solution.
00:27:48.124 - 00:28:08.820, Speaker A: In effect, this one actually does use, make use of the triangle inequality. So let M be a min cost perfect matching of some subset of the vertices.
00:28:11.000 - 00:28:11.750, Speaker B: Okay?
00:28:12.680 - 00:28:18.644, Speaker A: Now, for this to make sense, of course S is going to have an even number of vertices, otherwise you can't have a perfect matching.
00:28:18.692 - 00:28:19.096, Speaker B: Okay?
00:28:19.198 - 00:28:53.356, Speaker A: But suppose S has even cardinality, and suppose we think about the min cost matching on that set. S, the edge costs here are exactly the same as in our Tsp instance, okay? So we have this Tsp instance with a bunch of vertices. We say here's 1000 vertices. If I just match them perfectly, match them up 500 to 500, what's the cheapest way to do it? Okay, so call that M, then the claim is that the cost of this perfect matching is almost half the cost of an optimal Tsp tour.
00:28:53.548 - 00:28:54.348, Speaker B: Okay?
00:28:54.534 - 00:29:18.410, Speaker A: Or if you prefer, we have a second lower bound on the optimal solution. We already have the Tsp. Here's another one, which is if you double the cost of a min cost perfect matching, that's also a lower bound on the optimal solution. And this is quite slick. This is really quite pretty, very shoreproof it.
