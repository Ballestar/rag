00:00:00.810 - 00:00:17.502, Speaker A: So let's do the proof limit two, then limit three. So just to remind you, so what's the notation? So we've run the shrinking mechanism to completion. So these bidders have these sets of items that they're going after those are shrinking over time.
00:00:17.502 - 00:00:31.414, Speaker A: And at the end of the algorithm, s hat I is the smallest it ever got at the end. And we're defining opt hat as if I maximize welfare. But I insist that you only give bidder I items from S hat i.
00:00:31.414 - 00:00:50.042, Speaker A: How high can you make that welfare? And then some extra notation w hat are going to be the bidders that actually get a bundle? They want an opt hat with a winner I getting a bundle. They want T hat i, which again, by definition has to be a subset of S hat i, the only ones they're allowed to get. And the proof factors, two things.
00:00:50.042 - 00:01:18.050, Speaker A: First of all, it just says, well, at least if the original problem was we could only give people items from S hat i, how well are we doing with respect to that benchmark? And then lemma three says, actually that benchmark isn't too far away from the one we really care about, the actual maximum welfare. Okay, so proof of lemma two. So here's where intuitively we're going to use the fact that we're simulating the greedy algorithm to argue that our allocation is reasonable.
00:01:18.050 - 00:01:30.010, Speaker A: So let's just fix an iteration R. So I should say each of the proofs, the proof of limit two and the proof of limit three, they each have sort of one, neither one is long, but each has one kind of sneaky point.
00:01:30.160 - 00:01:30.860, Speaker B: Okay?
00:01:34.030 - 00:01:47.454, Speaker A: Right. So fixed and iteration R, let L sub R denote the dropouts this iteration. Okay, so these are the people we're not getting welfare from.
00:01:47.454 - 00:02:03.970, Speaker A: So we have to argue that the sum of the values in these loser sets is not too big. And we want to do that using the fact that the greedy algorithm is reasonable. So here's where we use the behavioral assumption.
00:02:03.970 - 00:02:22.054, Speaker A: Well, we use a few of them. But remember, we had this third behavioral assumption which said if you get to the point of you can either shrink or pass, and not shrinking is going to result in you getting kicked out of the auction and shrinking would potentially would let you get allocated and greedy. You're going to shrink.
00:02:22.054 - 00:02:44.142, Speaker A: Why not? Why would you just not shrink when it's going to get you kicked out? So consider someone who drops out this iteration, okay, what has to be true so what has to be true is that you didn't have the capability of shrinking to become a winner.
00:02:44.286 - 00:02:44.642, Speaker B: Okay?
00:02:44.696 - 00:02:48.030, Speaker A: So you dropped out. That means you lost. You didn't double your bid.
00:02:48.030 - 00:03:09.740, Speaker A: So in particular, there's no way of shrinking to enforce you winning, which means in particular okay, so what am I saying? So when greedy, when greedy reaches some I in L sub r.
00:03:12.590 - 00:03:13.340, Speaker B: All.
00:03:15.070 - 00:03:31.134, Speaker A: Subsets of S sub I that I might want. Remember, I has its preferred bundles, and in general, S sub I. So again, maybe S sub I is like ten different items, and there's these various bundles of size two to four in there.
00:03:31.134 - 00:03:56.854, Speaker A: That bitter I would be happy to have. It has to be the case that every single one of those bundles conflicts with something that greedy has already committed to, okay? Because if there was some bundle that I wanted, which doesn't conflict with what greedy already committed to, then by the behavioral assumption, it would shrink. It would shrink to one of those and not get kicked out all subsets of si that I wants, including t hat I.
00:03:56.854 - 00:04:30.420, Speaker A: So that's, again, the items that so in the event that I is a winner in this optimal solution and it gets the items t had I, that one in particular is also blocked by things that greedy have already chosen. And so this implies that the greedily computed set, which remembers new, is what I'm calling that. So as you do the greedy computation, you're adding bidders to new.
00:04:30.420 - 00:05:18.498, Speaker A: So the greedily computed new is equivalent to the output of the greedy algorithm. This is the sneaky part with the following sets. When I run the greedy algorithm, what am I really running the greedy algorithm with? I'm running it with these S sub eyes, okay? And I'm going to do this sneaky thing which basically says, well, for the bidders that get rejected by greedy, I rejected them because I thought their si was too big, okay? But then I gave the bidder the option to shrink if it wanted.
00:05:18.498 - 00:05:23.854, Speaker A: And the only reason the bidder wouldn't shrink is because if everything it could shrink to would also be rejected by greedy.
00:05:23.982 - 00:05:24.370, Speaker B: Okay?
00:05:24.440 - 00:05:29.486, Speaker A: So I'm running the greedy algorithm. I see the set of ten items. They conflict with stuff that I've already allocated.
00:05:29.486 - 00:05:45.690, Speaker A: So I'm the greedy algorithm. I reject this bidder. But actually something stronger is true given the behavioral assumption, okay? Any of the subsets of items that this bidder might have wanted, okay? So the bidder might have shrunk to just four items, but even there, that would conflict with stuff greedy's already allocated to.
00:05:45.690 - 00:05:47.142, Speaker A: If not, it would shrink.
00:05:47.286 - 00:05:47.980, Speaker B: Okay?
00:05:50.110 - 00:06:08.670, Speaker A: So what I'm trying to say is, imagine, so we ran the greedy algorithm with a certain collection of sets, the SIS. And so what I'm saying is, I'm going to, as a thought experiment, imagine running it again with a different, smaller collection of sets. And my claim is that the output is going to be exactly the same, and that's going to be useful for me, arguing that greedy is a good approximation.
00:06:08.670 - 00:06:31.658, Speaker A: So in the real world, in the actual auction, I run greedy. I go through the bidders in this order from high bids to low bids. I consider their S sub I's, and I reject the S sub I's that don't fit okay, but consider this alternative universe where for one of these bidders that got rejected, its actual set wasn't actually S sub I, it was something smaller, something smaller that it wanted.
00:06:31.824 - 00:06:32.442, Speaker B: Okay?
00:06:32.576 - 00:06:49.486, Speaker A: The point is, this greedy algorithm still would have rejected that bidder because even the smaller set would conflict, conflicts with items I've already given away. How do I know that it conflicts with items I've already given away? Well, if it didn't, then by the behavioral assumption, this bidder would have shrunk to the thing, okay? And I would have accepted it.
00:06:49.588 - 00:06:50.158, Speaker B: All right?
00:06:50.244 - 00:06:58.258, Speaker A: So again, for the purposes of analysis, I can imagine as if the bidders that got rejected actually declared these small bundles that they want.
00:06:58.344 - 00:06:58.642, Speaker B: Okay?
00:06:58.696 - 00:07:20.458, Speaker A: So that's what I'm doing here. So I'm saying the actual output of the greedy algorithm with the real sets, the S subis, is exactly the same as the output of the greedy algorithm, where I leave everybody the same except for the bidders that I'm rejecting that the optimal gets credit for. Which bidders are those? Well, the bidders I reject are L sub R.
00:07:20.458 - 00:07:41.774, Speaker A: The bidders the opt gets credit for is exactly W hat. So for any bidder in L sub R and W hat, I'm going to pretend as if its bundle was not S sub I, but was actually the bundle of items it gets in the optimal solution, okay? Greedy would run exactly the same. It would still reject this person.
00:07:41.774 - 00:07:54.420, Speaker A: The output would be exactly the same. But my approximation guarantee is going to look better if I think about the algorithm on the right hand side. Okay, so that's the sneaky part in this in the proof of Lemma two.
00:07:54.420 - 00:08:14.860, Speaker A: So we know greedy outputs something that's as good as any other feasible solution or within a D factor. And if I look at these bidders that are winners in the optimal solution and I look at their bundles, their bundles have to be disjoint because they're simultaneous winners in the optimal solution. So t hat I.
00:08:14.860 - 00:08:30.510, Speaker A: So in fact, all of the winners in W hat are disjoint, but in particular, the ones that dropped out in iteration R of our algorithm are disjoint. So this is a feasible solution.
00:08:31.970 - 00:08:32.720, Speaker B: Okay.
00:08:35.590 - 00:08:56.470, Speaker A: So now by greedy, so now we finally use the fact that we're running the greedy algorithm. So the output of our greedy algorithm is just all the bidders that won in our auction and their bids. And by the guarantee we began the lecture with, with a factor D loss.
00:08:56.470 - 00:09:32.100, Speaker A: This is at least as large as the sum for any other feasible solution. This is the feasible solution we're going to use now. All right? So because these bidders drop out in this iteration, what do we know? Right? Well, one of our behavioral assumptions is that you don't drop out until you're asked to bid above your value.
00:09:32.100 - 00:09:43.190, Speaker A: So if these guys are about to drop out, it means their bid is almost as high as their value. It's at least half their value if they drop out in this iteration. So this is, again, by one of our behavioral assumptions.
00:09:43.190 - 00:10:13.562, Speaker A: Okay, it so this is the first thing. Any questions about that? So this is just we envision running greedy, not in the way we actually run greedy, but with these smaller sets. That doesn't actually change the output of the greedy algorithm.
00:10:13.562 - 00:10:21.794, Speaker A: And then with respect to a collection of these that are a feasible solution, we can lower bound how well we do with respect to how well that feasible solution would have done.
00:10:21.912 - 00:10:22.580, Speaker B: Okay.
00:10:23.990 - 00:10:35.640, Speaker A: All right, good. All right. So let's see.
00:10:35.640 - 00:11:03.118, Speaker A: So this is the sum of the bids of the output of our greedy algorithm in this one iteration r by construction. Something I've enforced in the shrinking mechanism is that, first of all, in a given iteration, whatever you compute with the greedy algorithm, if it's not better than the last iteration, then you throw out the new greedy computation and inherit the previous one. And if you think about it, that means every single iteration of this auction has a better allocation than the previous one.
00:11:03.118 - 00:11:26.760, Speaker A: Okay, so the sum of the bids of your winners is only increasing over the course of this auction. So if this is the sum of our bids right now, at the conclusion of our greedy algorithm, the sum of the bids of our final allocation is at least as large at the end of the shrinking auction. So at end of the shrinking auction, the sum of the bids is at least this, and hence at least this.
00:11:26.760 - 00:11:44.720, Speaker A: And by our behavioral assumption, bids are always a lower bound on the values. You're never going to stay in after the bid exceeds your value. So the sum of the welfare sorry, the welfare at the end of the auction is at least this amount.
00:11:44.720 - 00:12:12.450, Speaker A: And this is true for every iteration R. Okay, everyone with me? Okay, so all I did is I put a few things on the left here that were even bigger. So the sum of the bids of the auction keeps getting bigger and bigger and bigger, and the welfare is at least the sum of the bids.
00:12:12.450 - 00:12:32.758, Speaker A: And that's for every iteration R. So now let's sum this inequality over all capital R iterations. So if I sum this over all capital R iterations, I get everybody who ever dropped out.
00:12:32.844 - 00:12:33.142, Speaker B: Okay.
00:12:33.196 - 00:12:39.738, Speaker A: And who also won. Okay, so sorry. Of all the winners of w hat, I get everybody who ever dropped out.
00:12:39.738 - 00:12:56.266, Speaker A: If I sum this over all the iterations R and right, and so then the people who didn't drop out are winners in our auction. So I can just add that in again on both sides. Again, there's capital R inequalities of this type.
00:12:56.266 - 00:13:11.910, Speaker A: I'm just adding them up. And then I'm throwing in all of the people who won in both auctions to get this. There's at least one over 2D sum over all of the winners.
00:13:11.910 - 00:13:39.226, Speaker A: Okay, and this is just opt hat. So that's number two. Any questions about that? The thing worth pointing out is how the third behavioral assumption that people shrink when they have no other options is what allowed us to apply the D approximation for the greedy algorithm in this.
00:13:39.248 - 00:13:39.882, Speaker B: Part of the proof.
00:13:39.946 - 00:13:55.700, Speaker A: That's kind of the only part of this proof that's important. The first two behavioral assumptions just basically, let us say we can exchange bids and values basically just says people are being truthful up to a factor of two on their values. Okay, so the third one is used here.
00:13:57.430 - 00:13:58.180, Speaker B: Okay.
00:14:00.070 - 00:14:33.134, Speaker A: Then let's just do limit three. Let me remind you what lemma three is. So in lemma three, we say we don't throw out too much welfare by restricting people to just the final sets s hat i.
00:14:33.134 - 00:14:52.946, Speaker A: So the miscoordination is bounded by how much the damage is. So what we're trying to prove here is that opt hat it is going to be smaller than opt. It's not going to be too much smaller than opt.
00:14:52.946 - 00:15:01.800, Speaker A: Again, capital R is the number of iterations. This is the real maximum welfare. This is maximum welfare if people are only allowed to get items from their final sets as hat i.
00:15:01.800 - 00:15:35.330, Speaker A: Okay, so why is this true? So let F sub R be the first time shrinkers in iteration R. So we recall that from a bidder's perspective, there's this phase in the auction where you've never shrunk and your set is all goods, okay? None are ruled out. And then at some point, for the first time, you commit to a set of at most D goods.
00:15:35.330 - 00:15:49.062, Speaker A: So I'm saying let R consider in R the bidders that shrink for the first time in this iteration, r. Now, there'll be some bidders that maybe never shrink. They just never actually shrink, and they just lose in the auction at the end of the day.
00:15:49.062 - 00:15:55.130, Speaker A: Okay, so F zero is the non shrinkers.
00:15:56.990 - 00:15:57.740, Speaker B: Okay.
00:16:00.270 - 00:16:16.640, Speaker A: So here's the really sneaky point in the proof of lemma three. This is nice. So, observation, consider two different bitters that shrink for the first time in the same iteration R.
00:16:16.640 - 00:16:27.890, Speaker A: Okay? The claim is that the bundles that they shrink to in this iteration have to be disjoint.
00:16:41.230 - 00:16:41.980, Speaker B: It.
00:16:43.310 - 00:16:59.978, Speaker A: So here I'm only claiming that their bundles at the end of the auction are disjoint, but something stronger is true, actually. So if you look at it when you shrink the first time, let's look at what has to happen. Right, so initially, your S sub I is just all goods.
00:16:59.978 - 00:17:37.638, Speaker A: To be eligible to shrink, the subset ti you provide me has to satisfy several criteria, has to have size at most D, and it has to be disjoint from the bundles I've already allocated in this iteration, okay? So if you don't hand me a good of that type, it doesn't count as shrinking. So to shrink, you have to be able to avoid the bidders that have already been allocated by the greedy algorithm in this iteration. So if two different bidders successfully shrink for the first time in exactly the same iteration, whichever one was second had to shrink to a bundle disjoint than the first one because the first one's already been allocated.
00:17:37.638 - 00:17:48.286, Speaker A: Okay, so in iteration R, their bundles have to be disjoint if they both shrunk. And then, of course, bundles only decrease, so they're disjoint at the end of the algorithm. So that's really cool.
00:17:48.286 - 00:18:04.020, Speaker A: So each iteration goes by there's some set of bidders that shrinks for the first time in this iteration, and they're a legitimate partition of the items. No overlaps between first time shrinkers in a common iteration. Okay, good.
00:18:04.020 - 00:18:27.290, Speaker A: So now we're pretty much done. Let W be the winners and the real optimal solution. So these guys have s hat I equal um, so claim.
00:18:27.290 - 00:19:02.102, Speaker A: Okay, actually the simpler one is this, right? So remember, opt hat is sort of the max revenue you can get if you only give people items from their final sets. S hat i. Well, what did you say? In a given iteration R, all of the first time shrinkers form a partition or no goods are in common, so you can actually simultaneously allocate to the entire collection f sub R of first time shrinkers of an iteration R.
00:19:02.102 - 00:19:18.780, Speaker A: Okay, so that means one feasible solution determining opt hat is just to have all the first time shrinkers from fr being winners. Of course, you could possibly, probably do better, but this is one feasible solution. So this is for all R.
00:19:18.780 - 00:19:44.430, Speaker A: It's also the case if you look at the people who never shrank and you look at the ones that are winners, so these are somehow the easy guys. F sub zero, right? So they never shrank, so you're literally allowed to give them any item at all. So in this case, you're restricted optimization.
00:19:44.430 - 00:19:50.420, Speaker A: You're not restricted. You can just do whatever. And so for these bidders, you can basically just copy opt.
00:19:50.420 - 00:20:12.940, Speaker A: So if you look at the non shrinkers who win in the optimal solution, because they're in the optimal solution, they also have to be disjoint. So they're also a legitimate feasible solution for the restricted problem. And so then if you just add these up on the left hand side, you get R plus one times the restricted opt.
00:20:12.940 - 00:20:24.122, Speaker A: And then over here, you get something which is a superset of the winners in the real optimal solution. Right? W are the winners in the real optimal solution. Here we pick up the non shrinkers.
00:20:24.122 - 00:20:36.820, Speaker A: Here we pick up all of the shrinkers, whether they're in the optimal or not. So this is at least OD. So that's lemma three.
00:20:36.820 - 00:20:52.630, Speaker A: So, again, remember, capital R is the iteration count, and that's log v max. So we lose one log v max in lemma three, and then we lose the D times log v max in Lemma two. So putting them together, we get the loss of D times log squared V max.
00:20:53.290 - 00:20:54.040, Speaker B: Okay?
00:20:57.050 - 00:21:04.734, Speaker A: So we don't know what bitters are going to do. We don't know when they're going to double, we don't know when they're going to shrink. But under minimal conditions, you get an approximation for this problem.
00:21:04.734 - 00:21:31.060, Speaker A: We don't know how to get with the Dsik mechanism. Any questions about that? All right, so like I said, it'd be cool to have more mechanisms of this flavor. Right now we don't.
00:21:31.060 - 00:22:49.510, Speaker A: All right, so what we're going to talk about next is a much more sort of standard notion of relaxed incentive compatibility, bayesian incentive compatibility. And today we're mostly going to spend time just kind of laying the groundwork. So stuff I could have covered in 364 A but decided not to, decided to postpone till nowhere.
00:22:49.510 - 00:23:36.070, Speaker A: The idea. Okay, so if we want to move beyond dominant strategies and sort of slight relaxations, like I just showed you, there's this really key conceptual problem which comes up, which is when there isn't an obvious thing to do and when your best response depends on what other players are doing, how do you compare the different options? So how do you reason about how to respond to other players actions when you don't know what people are doing? So the solution to Bayesian incentive compatibility is we're going to assume prior. So as a bidder, the assumption will be that you don't know exactly what people want, like, you don't know their valuations, but you at least know a distribution over what people want.
00:23:36.070 - 00:23:58.030, Speaker A: For example, their valuations. If you assume you know both a distribution over valuations and you also assume, you know, strategies mappings from valuations to actions, then you can think about best responding to the distribution overactions that you face, and then you can have an equilibrium concept and a corresponding notion of incentive compatibility. So let me elaborate.
00:23:58.030 - 00:24:23.862, Speaker A: So the general setup is pretty much so the first few things I'm going to say, we already discussed formally when I talked about X post incentive compatibility, x post Nash Equilibria way back in lecture one. So each player has what's called the type space. Just think of types as being valuations for this class.
00:24:23.862 - 00:24:39.180, Speaker A: Same thing. So this is the private stuff where you don't know. Okay, so capital T sub I, you know, these are all the possible preferences a bidder might have, but you don't know which one they are.
00:24:39.180 - 00:24:56.010, Speaker A: And then there's an action space. For example, bids.
00:24:56.170 - 00:24:56.880, Speaker B: Okay.
00:24:59.590 - 00:25:23.754, Speaker A: Now we've been talking so much about direct revelation mechanisms, one sometimes forgets that these two things could in general be different, right? So in something like just a second price auction, both of these are the same valuations and bids. Both are drawn from exactly the same set. When we were talking about indirect mechanisms, ascending auctions, that was a case where the action space was way bigger than the type space.
00:25:23.754 - 00:25:44.362, Speaker A: So we talked a little bit about how the action space is very rich in indirect auctions. And that's why you sometimes have to settle for X post incentive compatibility, not dominant strategy incentive compatibility, remember? There it was iterative so what you do next could depend on the entire history of actions. And so in particular, we saw that there are ways that bidders could behave that were inconsistent with any valuation.
00:25:44.362 - 00:25:54.846, Speaker A: So actions that didn't really correspond to a private valuation. So that was a case where there are way more actions than there were types. The next part of the course is going to be the opposite.
00:25:54.846 - 00:26:13.610, Speaker A: We're going to be looking at auctions where the action space is very small and you don't have the means of communicating your full valuation. So maybe you have two to the M private sets, but it'll ask you for only M numbers. So when we talk about the price of anarchy of simple auctions, AI will be much smaller than Ti.
00:26:13.610 - 00:26:50.200, Speaker A: So for direct revelation mechanisms, these are the same set and a strategy just specifies as a function of what you want, what are you going to do? So it's just a mapping from types to actions. And these can also be randomized, these can be a mixed strategy. So I really should write distributions over actions here.
00:26:52.650 - 00:26:53.400, Speaker B: Okay.
00:26:55.370 - 00:27:21.950, Speaker A: So what are some examples? So when we're talking about X post incentive compatibility, we are talking about sincere bidding being an example strategy, which would just say, every time you're given a query, don't ignore the history and just tell the truth with respect to your valuation. Direct revelation, that would be an example of a strategy. So direct revelation just corresponds to setting sigma I of Ti equal to ti.
00:27:21.950 - 00:27:25.940, Speaker A: Okay, so just truthfully reporting your private information.
00:27:27.510 - 00:27:28.260, Speaker B: Okay.
00:27:31.030 - 00:27:56.838, Speaker A: So the relaxed notion of incentive compatibility rests on a relaxed solution concept, equilibrium concept, which for the first time in this course involves a prior. So in 364 A, we did talk about priors and revenue maximization, but that was a little different because that we were really only using the prior to measure the performance of an auction. So something like Myerson's second price auction with a reserve.
00:27:56.838 - 00:28:06.378, Speaker A: We talked about the expected revenue of the auction, but we're always talking about DSIC mechanisms. Second price auction with a reserve. It's still a dominant strategy to report your true valuation.
00:28:06.378 - 00:28:16.180, Speaker A: You're not reasoning about what other people are doing. So that's the difference here. The prior will actually be used by bidders in deciding whether to take action A or action B.
00:28:16.180 - 00:28:43.210, Speaker A: So common prior. So there's some common knowledge, publicly known distribution capital F on the typespace or valuation space. So we think of bidders types or bidders valuations as being drawn by nature, say from a distribution.
00:28:43.210 - 00:29:08.660, Speaker A: We're mostly only going to talk about the special case where F is a product distribution. So that is where if I tell you something about one person's type it tells you nothing about anyone else's type. It doesn't change your conditional distribution for other people.
00:29:08.660 - 00:29:26.680, Speaker A: The general correlated case is interesting and relevant for lots of applications but there's a lot of pathologies that come up when you allow correlated distributions. So for a lot of the analysis you just wind up learning a lot more by focusing on the product case, the independent case.
00:29:28.110 - 00:29:28.860, Speaker B: Okay.
00:29:32.510 - 00:30:06.718, Speaker A: So what's a bayes? Nash equilibrium. So basically the Bayes Nash equilibrium just says every bidder based on what it knows, based on its information at the time is behaving optimally. So then the question is okay so what does a bidder know at a moment it's taking an action and so the assumption, I mean I'll write it down formally but okay, so what do you know as a bidder? Okay you know the distribution over types, you know your own type, you know your own valuation.
00:30:06.718 - 00:30:28.010, Speaker A: So at the time you're choosing what to bid, you know what you're willing to pay and then the assumption is also going to be here that you know players strategies. This is the same thing as in the X postnation equilibrium. So I'm not going to assume that I know someone else's valuation but I'm going to assume that I know they're mapping from their valuation to their action.
00:30:28.010 - 00:30:56.760, Speaker A: So if I knew what they wanted then I would know what they're doing. Okay so a strategy profile sigma one of the sigma N is a Bayes Nash equilibrium if for all I and valuations VI. So this is the moment in which I is reasoning about what its sort of best decisions are.
00:30:56.760 - 00:31:37.358, Speaker A: It should be that the action that its strategy tells it to take maximizes eyes, expected utility. Okay so what is this expectation over? So it's reasoning as if other players types were drawn from F conditioned on VI. So if types are independent then conditioning does nothing, right? So I learn my value, I learn I'm willing to pay $10.
00:31:37.358 - 00:31:45.666, Speaker A: Okay so if the model is that the types are independent then it doesn't matter. It doesn't tell me anything about other people's distribution. If it's correlated then I'm going to update on that information.
00:31:45.666 - 00:32:01.478, Speaker A: Okay I'll look at the conditional distribution of others types given my own. So in my reasoning I'll think of v minus I as being drawn from this distribution and then I will assume that others play according to the strategies in the strategy profile.
00:32:01.654 - 00:32:02.380, Speaker B: Okay.
00:32:04.670 - 00:32:41.480, Speaker A: So assuming others play sigma minus i, v minus i, okay so the fact that I know a distribution over others valuations and I know their strategies, I'm then faced with a corresponding distribution over actions and I can reason about my best responses with respect to that action distribution. All right so I'll do an example in a second. Let me just sort of compare and contrast this to the other equilibrium notions we saw for games of incomplete information.
00:32:41.480 - 00:32:51.802, Speaker A: So x post nash equilibrium was stronger, more stringent. In particular, we didn't have a prior. When we defined x post Nash equilibrium, we didn't need one.
00:32:51.802 - 00:33:22.174, Speaker A: So x post Nash equilibrium, we were saying that the action that I is about to take should maximize its just utility. Assuming nothing about v minus i, v minus I was arbitrary, assuming only that other players play according to the strategies sigma minus i, for example, sincere bidding is the best response, assuming only that other players bid sincerely whatever their valuations might be. So expos national equilibrium said valuation profile by valuation profile.
00:33:22.174 - 00:33:37.270, Speaker A: This is the best thing to do. Given sigma here, we're saying on average over the v minus I's, this action is the best thing to do. Of course, that only makes sense with a prior even stronger than x post Nash equilibrium is dominant strategy equilibrium.
00:33:37.270 - 00:33:55.930, Speaker A: And that says what your strategy is telling you to do is a best response. No matter what other actions people take, whether or not they are of the form sigma I v minus i. Okay, so for expose natural equilibrium, you at least have a notion that there are these strategies that people are using.
00:33:55.930 - 00:34:03.006, Speaker A: You don't know the input, but at least you know the mapping. Dominant strategy equilibrium all you, you don't even know that. You just say whatever the possible action might be.
00:34:03.006 - 00:34:14.718, Speaker A: This is what I want to be doing. So for direct revelation mechanisms, as we discussed, those coincide. But when you have these richer action spaces, sometimes you can be x post, you can have x post Nash equilibrium, which are not dominant strategy equilibrium.
00:34:14.718 - 00:34:27.190, Speaker A: Okay, so that's the key idea here. Best responding with respect to the distribution of reactions that you face given all of your information. Okay, so let's get much more concrete.
00:34:27.190 - 00:34:54.610, Speaker A: So for example, let's just look at a first price auction. Two bidders ID distributions, uniform one. Okay, I may have made you do this as homework last quarter, I forget, but let's just review anyways.
00:34:54.610 - 00:35:09.106, Speaker A: Okay, so this is a product prior, first price auction, no dominant strategies. So the claim is that bidding half year value is a Bayes Nash equilibrium in this setup.
00:35:09.218 - 00:35:09.880, Speaker B: Okay.
00:35:21.690 - 00:35:28.662, Speaker A: So good. And so you know this. So the Bayes Nash equilibrium depends on the setup.
00:35:28.662 - 00:35:44.898, Speaker A: Okay, so if instead of two bidders, there were n bidders, then you would multiply your value by n minus one over n. So as the number of bidders increases, the amount by which you would shade your bid is getting less and less and less. As it gets more competitive, your bid will drift closer to your value.
00:35:44.898 - 00:35:57.510, Speaker A: If even with two bidders I change the distribution, that would change the Bayes Nash equilibrium. It would look different, be some other function. Okay, all right, so proof.
00:35:57.510 - 00:36:10.986, Speaker A: Consider e g player one. It's obviously symmetric. So we have to show that no matter what player one's valuation is, bidding half its value is the best thing.
00:36:10.986 - 00:36:11.530, Speaker A: For it.
00:36:11.600 - 00:36:12.220, Speaker B: Okay.
00:36:13.950 - 00:36:31.882, Speaker A: So fix v one. So we said that from a bidder's perspective, when you're reasoning about what to do, given a distribution over others values and knowledge of their strategy, it gives you a distribution over actions. So in this context, even though bidder two, in its mind it's playing deterministically.
00:36:31.882 - 00:36:47.366, Speaker A: If I'm bidder one, it's like it's acting randomly, right? So nature draws this value, v two, uniformly from zero one. And whatever that is, I know it's bidding half its value. So to me, I just see this bid, b two, which is uniform in zero one half.
00:36:47.548 - 00:36:47.894, Speaker B: Okay?
00:36:47.932 - 00:37:05.530, Speaker A: And that's what I reason about with how to bid back. So fixed v one, v two is distributed like zero one half. And this is, again, because we're thinking of the strategy profile as known when we do these computations.
00:37:05.530 - 00:37:34.590, Speaker A: Okay, so expected utility of a bid, b one. Well, it's just how much money? What would be your utility if you won times the probability that you win? So this obviously, as you bid higher, remember, it's a first price auction. So that's why this is your value and you pay what you bid when you win first price auction.
00:37:34.590 - 00:37:55.850, Speaker A: So this is obviously getting smaller and smaller the higher you bid, and this is obviously getting bigger and bigger the more you bid. Okay, so we're sort of looking for the optimal trade off between those two. So what's the probability that we win? Well, so this is just the probability that our bid is bigger and thinking of b one is fixed and b two is random.
00:37:58.910 - 00:37:59.226, Speaker B: Right?
00:37:59.248 - 00:38:22.020, Speaker A: So b two is half of v two's valuation. So this is just the probability that v two is less than double what we bid, which in turn because v two is uniform zero one, this is just two b one or can't be bigger than one.
00:38:22.630 - 00:38:23.380, Speaker B: Okay.
00:38:25.430 - 00:38:39.640, Speaker A: If you think about it's sort of obvious, like, why would you ever bid more than a half given that the other person's maximum bid is a half? So let's just think of b one as being at most a half. Again, we're thinking. So b one is the variable we're solving for.
00:38:39.640 - 00:38:58.320, Speaker A: So the expected utility, b one minus b one times two b one is uniquely maximized at b one equals v one over two.
00:38:58.770 - 00:38:59.278, Speaker B: Okay?
00:38:59.364 - 00:39:01.580, Speaker A: Which is what we wanted and.
