00:00:00.570 - 00:00:18.122, Speaker A: Hi everyone, and welcome to this video that accompanies Section 23.3 of the book Algorithms Illuminated, part Four a section about the complexity class NP corresponding to problems with easily recognized solutions. So with this video, we really arrive at the hearts of the discussion.
00:00:18.122 - 00:01:00.170, Speaker A: How are we going to define exhaustive search solvable problems? In other words, the problems that might plausibly reduce traveling salesman problem? Or, to think about it another way, what are the minimal ingredients necessary to solve a problem using naive exhaustive search? The big idea behind the complexity class NP is the efficient recognition of purported solutions. That is, if someone handed you a feasible solution on a silver platter, you could quickly check whether or not it was indeed a feasible solution. For example, if someone hands you a filled out Sudoku or KenKen puzzle, it's quite easy to check whether or not they followed all of the rules.
00:01:00.170 - 00:01:32.598, Speaker A: Similarly, if someone hands you a sequence of vertices in a graph, it's easy to check whether or not it constitutes a traveling salesman tour, and if it does, whether its total cost is at most some target total cost capital t formalizing that idea leads us to the complexity class NP. As we mentioned in the previous video, the class NP is going to consist solely of search problems. So remember, a search problem that means there's a notion of a feasible solution and the algorithm is responsible either for returning a feasible solution or correctly reporting that none exist.
00:01:32.598 - 00:02:07.210, Speaker A: So for example, given an instance of Tsp and a target Tor cost like 1000, either return a Tor with cost at most 1000 or correctly report that none exist, or given an instance of threesat, either exhibit a satisfying truth assignment or correctly report that the instance is unsatisfiable. Under what conditions does a search problem belong to the complexity class NP? Well, there are two conditions. So first of all, we talked about handing you an alleged feasible solution on a silver platter.
00:02:07.210 - 00:02:21.278, Speaker A: Condition one is that silver platter shouldn't be astronomically large. So to write down a feasible solution, that should be possible using a polynomial number of bits. The second condition asserts efficient recognition.
00:02:21.278 - 00:02:38.614, Speaker A: So, given an alleged feasible solution and by the first condition, we know its description length is at most polynomial. Given an alleged feasible solution, you should be able to, with only a polynomial amount of work, confirm or deny that candidate solution's feasibility. And that is it.
00:02:38.614 - 00:03:00.746, Speaker A: That is one of the most important definitions in all of computer science, the definition of the complexity class NP. I should warn you, especially if you go and look at some complexity theory textbooks, you can equivalently define the complexity class NP as these search problems that are efficiently solvable by what's called a nondeterministic Turing machine. Nondeterministic turing machine.
00:03:00.746 - 00:03:13.006, Speaker A: That's a fictitious and frankly, at this point, somewhat anachronistic computational model. And indeed the acronym NP, it stands for nondeterministic polynomial time. Don't forget our rookie mistake from the beginning.
00:03:13.006 - 00:03:19.750, Speaker A: Never mistake. Capital NP is standing for not polynomial. It's not what it stands for stands for non deterministic polynomial time.
00:03:19.750 - 00:03:40.110, Speaker A: And it's because of this alternative definition of the complexity class capital NP. When you're thinking about algorithms, as we are in this book series and in these videos, you should really always think about NP problems as those with efficiently recognizable solutions in the study of algorithms. There's really no reason to ever worry about these nondeterministic Turing machines.
00:03:40.110 - 00:03:55.102, Speaker A: We have throughout this video playlist, seen many examples of problems that satisfy both of these two conditions. So let's just go over a few so that this definition starts to gel for you. Let's start with the traveling salesman problem.
00:03:55.102 - 00:04:16.630, Speaker A: So that's an optimization problem, so it's actually not eligible for membership in the complexity class NP. But we can think about the search version of the Tsp, where in addition to the n vertices and the edge costs, you're also given a target objective function value capital T. And the search version says either produce a tour with total cost at most the target capital t, or correctly report that no such tour exists.
00:04:16.630 - 00:04:27.722, Speaker A: The search version of the traveling salesman problem is certainly a member of the complexity class NP. We have to check two things. So first we have to check the candidate solutions have length polynomial in the input size.
00:04:27.722 - 00:04:45.358, Speaker A: Well, what's a candidate solution? That's just going to be a sequence of n vertices. Now, if the vertices are named sort of one, two, three up to n, then it's only going to take a log base two of n bits to reference a vertex. So you're going to need O of n log n bits to describe a candidate tour.
00:04:45.358 - 00:05:06.370, Speaker A: Second condition asserts efficient recognition. And if someone handed you a sequence of n vertices straightforward to check that it constitutes a tour, that is, each vertex appears exactly once, and it's easy to compute the total cost of that tour and to check is it at most the target capital t. So certainly Tsp, the search version passes both tests with flying colors.
00:05:06.370 - 00:05:16.566, Speaker A: That's going to be an example of an NP problem. It's probably even easier to see that the three sat problem belongs to the complexity class NP. So here candidate solutions.
00:05:16.566 - 00:05:26.842, Speaker A: Those are just going to be truth assignments. So you can specify a truth assignment to n boolean variables using n bits just n, zeros, and ones for false and true. And that's the first condition.
00:05:26.842 - 00:05:50.150, Speaker A: Second condition asserts efficient recognition. And if someone hands you a candidate solution, a truth assignment, it's easy enough to check whether it's a satisfying truth assignment or not, right? You just walk through the constraints one by one, and then you just check that every single constraint is satisfied. And a constraint is going to be satisfied if and only if one of the variable assignment requests that it makes is in fact fulfilled by the suggested truth assignment.
00:05:50.150 - 00:06:11.338, Speaker A: Similarly, the Hamiltonian path problem, undirected directed, doesn't matter. Why is that an NP? Well, again, a candidate solution that's just going to be a list of vertices, polynomial description, length, and it's easy to check whether or not in fact it only uses legitimate edges in the graph and whether every vertex is visited exactly once. We could think about say, Makespan minimization and that's an optimization problem.
00:06:11.338 - 00:06:40.546, Speaker A: So again, to talk about membership at NP, we have to turn to its search version. So given an instance of Makespan minimization plus a target makespan like 100, the question is, is there a schedule with Makespan at most 100 or not or correctly report that every schedule has makespan bigger than 100 that again belongs to NP, right? So here alleged solution that's just going to be a schedule of the jobs on the machines, easy enough to describe. And of course, given a schedule, it's straightforward to compute the Makesband and check it against the target makes band.
00:06:40.546 - 00:06:55.642, Speaker A: So no worries there. Similarly, if we want to think about the independent set problem, the search version, again, it's an optimization problem. So we think about the version where you're also given a positive integer k and the question is whether or not there's an independent set with size at least k.
00:06:55.642 - 00:07:16.006, Speaker A: And if there is, you're responsible for returning one that again belongs to NP. Again, candidate solutions, just subsets of vertices, straightforward to describe and as we know, it's straightforward to check whether or not a subset of vertices is in fact an independent set, whether or not all of them are mutually non adjacent. And there's many, many, many more examples.
00:07:16.006 - 00:07:39.782, Speaker A: Almost everything that we've seen in this book series, in these video playlists at least, the search version of the problem is going to be a member of the complexity class NP. And maybe the big takeaway is just that we've deliberately defined the requirements for membership and NP to be very easy to pass, very weak. And so that means is kind of almost all the problems that we can think of easily pass the two criteria and boom are members of MP.
00:07:39.782 - 00:08:04.442, Speaker A: So MP is a big class because the entrance requirements are so easy to pass. In the first video corresponding to this chapter, we discussed our plan for amassing evidence of intractability of the traveling salesman problem by reducing tons and tons of other problems to it. So we wanted to settle on this set script C so that all of the problems in script C reduce to the Tsp.
00:08:04.442 - 00:08:40.654, Speaker A: And we want that set to be as big as possible because the more stuff reduces to the Tsp, the stronger the evidence that the Tsp is in fact an intractable problem. The question then was, okay, so how should we choose this set script C? We can't set it to be everything because there's definitely problems out there like the halting problem, which are unsolvable and therefore could never be reduced to a problem as easy as the Tsp, which if nothing else, can be solved in exponential time using naive exhaustive search. So it seemed like the most ambitious thing we could try to do is try to reduce every other problem that is equally well solvable by naive exhaustive search.
00:08:40.654 - 00:08:57.790, Speaker A: Reduce all of those problems to the Tsp. That would mean the Tsp is sort of the hardest among all problems solvable by naive exhaustive search. Now, I just gave you the formal definition of the complexity class NP and it was in terms of the efficient recognition of alleged solutions.
00:08:57.790 - 00:09:25.068, Speaker A: So let's now connect those two ideas efficient recognition with solvability by naive exhaustive search. On this slide, I just want to point out that any NP problem is indeed solvable by the same naive exhaustive search that you could use, say for the traveling salesman problem. When we solved the Tsp by exhaustive search, we enumerated through all sequences of vertices, all possible orderings.
00:09:25.068 - 00:09:42.560, Speaker A: And so here, similarly, we're just going to enumerate all possible candidate solutions one by one. The first requirement for membership in NP says that candidate solutions have to have length bounded by a polynomial in the input size. So this is an NP problem by assumption.
00:09:42.560 - 00:10:00.088, Speaker A: So that means we only have to worry about candidate solutions that have length at most big O of n to the d sub one, where d sub one is some constant, maybe it's 100, maybe it's 1000. But anyways, independent of n. What that means is that there's only an exponential number of different candidate solutions that we need to try.
00:10:00.088 - 00:10:34.322, Speaker A: So for example, if we know that all candidate solutions can be described in at most end of the 10th bits, then the number of candidate solutions we need to look at is at most two raised to the end of the 10th. Now, we use the second requirement of an NP problem, which is that a candidate solution can be efficiently recognized. So for each of these possibly exponentially many objects that we're enumerating for each one separately we check whether it is in fact a Fusible solution and if we ever find one, then we return it.
00:10:34.322 - 00:10:53.378, Speaker A: That's our answer. If we exhaust all of the possible candidate solutions and we discover that none of them are feasible solutions, we can correctly report that fact. That second requirement for membership in NP then tells us that for each step in the enumeration this feasibility checking is going to require only polynomial number of steps.
00:10:53.378 - 00:11:11.866, Speaker A: So at most C two times n raised to the d two where C two and d two are constants independent of n. Correctness of this generic algorithm should be obvious. It literally checks every conceivable feasible solution, checks them one by one, returns a feasible solution if it finds one otherwise correctly reports no solution.
00:11:11.866 - 00:11:36.610, Speaker A: And because it has at most an exponential number of steps in the enumeration with polynomial time for each step. The overall running time of this generic exhaustive search algorithm is only exponential in the input size n as we've mentioned, the requirements for the membership and the complexity class NP are very weak. And so almost any search problem you ever come across is going to wind up being a member of NP.
00:11:36.610 - 00:12:15.756, Speaker A: So what that means is that if some problem like the traveling salesman problem, if in fact every problem in NP reduces to it, that's extremely strong evidence that the traveling salesman problem is intractable. Because if there were a polynomial time algorithm for the Tsp, that would automatically give you a polynomial time algorithm for every single problem in NP, every single problem with efficiently recognizable solutions, which in turn is almost every problem you're ever likely to encounter. This strong evidence of intractability is exactly the formal definition of an NP hard problem.
00:12:15.756 - 00:12:41.872, Speaker A: A problem is NP hard if every single problem in the complexity class NP reduces to that problem. Or in other words, a polynomial time algorithm for that NP hard problem would automatically give a polynomial time algorithm for every single problem in NP every problem with efficiently recognizable solutions. Now, when I defined formally the complexity class NP, I sort of made a big deal of the fact that it was a class only of search problems to avoid type checking errors.
00:12:41.872 - 00:13:20.946, Speaker A: So an optimization problem like the Tsp is ineligible for membership to belong to the class NP, but the Tsp is not ineligible for being NP hard. Indeed, the optimization version of the Tsp is indeed an NP hard problem, as are all of the other optimization problems that we've discussed in this video playlist. I again need to warn you, just so you don't get confused, if you look up the definition of NP hard from other sources, like in books and complexity theory or even many books and algorithms, it's common to define NP hardness in a less liberal manner than I've done.
00:13:20.946 - 00:13:31.178, Speaker A: So here. So here, when I say a problem is NP hard, if every problem in the class NP reduces to it, I'm speaking, as usual, about these Cook reductions. So these are reductions where I give you the magenta box.
00:13:31.178 - 00:13:39.034, Speaker A: You're allowed to invoke it a polynomial number of times. You're also allowed to do a polynomial amount of additional work. And then if you can solve the problem, that counts as a reduction.
00:13:39.034 - 00:13:56.114, Speaker A: That's what a Cook reduction is. Many books actually use only a restricted form of reduction called eleven reduction, which we'll discuss in the last video. In this sequence, when we talk about NP completeness, and in particular, if you're using only eleven reductions, you're stuck with only search problems being NP heart.
00:13:56.114 - 00:14:04.998, Speaker A: So if you use that definition, you can't actually say the Tsp is NP heart. That would be incorrect. You'd have to say the search version of the Tsp is NP heart.
00:14:04.998 - 00:14:30.180, Speaker A: Whereas with this definition, we can just flat out say the Tsp is NP heart. That's just a true statement with this definition. Why am I using this more liberal definition in this video playlist? Because it's maybe less convenient for the development of complexity theory, but it accords much better with the algorithmic viewpoint that we're taking in this book series.
00:14:30.180 - 00:14:41.624, Speaker A: Let's revisit the fundamental Cook Levin theorem. So we talked about the Cook Levin theorem in the last chapter, chapter 22. That's when we were learning how to prove that problems are NP hard.
00:14:41.624 - 00:14:53.950, Speaker A: What we did is we took the Cook Levin theorem on faith. The Cook Levin theorem tells us that the three Sat problem is NP hard. And then using our two step recipe from that one NP hard problem, we generated 18 more.
00:14:53.950 - 00:15:29.960, Speaker A: Now that we have a mathematical definition of what it really means for a problem to be NP hard, we correspondingly have a very precise understanding of exactly what the Cook Levin theorem shows, namely, in asserting that the three Sat problem is NP hard. What Cook and Levin are saying is that every single NP problem, every problem with efficiently recognizable solutions can in fact be reduced to the three Sat problem. And you might well be wondering, how could this possibly be true? The three Sat problem seems so simple, the complexity class NP so vast.
00:15:29.960 - 00:15:45.420, Speaker A: Well, the details of the proof get kind of messy, but let me just give you the gist. What do we have to show? We have to show that every problem in NP reduces to three SATS. So let's fix one arbitrary problem from the class NP.
00:15:45.420 - 00:15:56.848, Speaker A: We know almost nothing about this problem A. We have no idea what it looks like. The only thing we know is that it happens to be a member of the complexity class NP, so we know it's a search problem.
00:15:56.848 - 00:16:13.312, Speaker A: And then there are those two defining characteristics of NP problems. So, first of all, we know that candidate solutions must have length polynomial in the input size. So let's say describable at a most c one times n raised to the d one bits, where n is the size of the input.
00:16:13.312 - 00:16:29.284, Speaker A: And then secondly, we should be able to efficiently recognize purported solutions. So, given one of these candidate solutions, which again has a most polynomial length, we should be able to check whether or not it constitutes a feasible solution in polynomial time, say, time at most. C two times n raised.
00:16:29.284 - 00:16:41.836, Speaker A: To the D. Two here C, one C are all constants independent of N. Now we have to somehow reduce this abstract NP problem A to the seemingly very simple three set problem.
00:16:41.836 - 00:16:54.450, Speaker A: So it's going to be a reduction. So we're going to have to draw a cartoon which will give you fond, or maybe not so fond memories of the proofs that we did in the previous chapter. We're going to need a reduction from this problem A to three set.
00:16:54.450 - 00:17:29.624, Speaker A: This reduction will be from the light blue problem, the arbitrary NP problem A to the magenta problem three Sat. So we need to show that given a subroutine solving threesat given a magenta box, we need to see how to build that light blue box for solving this arbitrary MP problem that we started with. The plan then is to translate any instance of the arbitrary MP problem A to somehow encode it as an instance of Satisfiability so that the status of the problem we started with, whether it's feasible or not is reflected in the status of the Satisfiability instance that the reduction produces.
00:17:29.624 - 00:17:58.032, Speaker A: And furthermore, satisfying truth assignments to the three side instance that we construct, we should be able to extract from that a feasible solution to the problem that we started with. So what are the basic ingredients in this reduction? How are we going to do this encoding? Well, at the end of the day, we're going to be responsible for producing a feasible solution to the instance that we started with, if one exists. And the only thing we know about those feasible solutions is that they can be described in at most c one times n to the D one bits.
00:17:58.032 - 00:18:28.846, Speaker A: So the sensible thing it would seem to do would be to just have a bunch of Boolean variables c one n to the D one Boolean variables whose true false assignments record whether those bits are zero or one. So the reduction has now already taken advantage of that first property satisfied by NP problems that the description length of candidate solutions is not too big. The reduction therefore, can just explicitly have Boolean variables encoding the bits of an alleged feasible solution.
00:18:28.846 - 00:18:53.654, Speaker A: We still have to take advantage of the second property possessed by NP problems efficient recognition. So we need to use the fact that there is this polynomial time verification algorithm that given a candidate solution, confirms or denies its feasibility, and at most c two times n to the d two steps. What we're going to do is have a whole nother collection of Boolean variables in our three side instance, which we're going to call state variables.
00:18:53.654 - 00:19:18.850, Speaker A: And the point of these state variables is to encode the execution of the assumed verification procedure on a candidate solution. Now, by assumption, by virtue of problem A belonging to NP, the verification algorithm runs and it most c two times n to the D two time. Moreover, let's assume we use a computational model where referencing one bit of memory costs you one time step.
00:19:18.850 - 00:19:43.230, Speaker A: That means if you perform at most c two times n to the D two timesteps, you only have time to reference at most c two times end of the D two bits of memory. Therefore, we can basically summarize the entire computation of this verification procedure with a table with c two times n to the d two rows and the same number of columns. So the rows of this table correspond to time steps.
00:19:43.230 - 00:20:04.266, Speaker A: It's a snapshot of what things look like at each time step that the algorithm runs. And then within a row, the columns are encoding the bits of memory at that particular time step of the execution. I sympathize if this description of the state variables encoding the verification algorithm's execution on a candidate solution.
00:20:04.266 - 00:20:28.406, Speaker A: If this description of the state variables seems a little vague, I've done that deliberately. Because the additional details of exactly what the state variables are and exactly how you faithfully encode the entire execution of the algorithm. That depends on the exact details of your computational model, on your exact definition of what an algorithm is allowed to do from one time step to the next.
00:20:28.406 - 00:20:52.990, Speaker A: Now, the simplest choice, and the one you'll usually find in books, is to use a Turing machine as the computational model. And if that were the case, you could get away with this table of state variables that I've listed on this slide, plus a few extra variables at each time step to encode the machine's current state. And with some additional grunt work, that same proof can be adapted to computational models that more closely resemble modern computers and programming languages.
00:20:52.990 - 00:21:12.498, Speaker A: Given our goals here, it would take us too far afield to really nail down sort of the details of the computational model to get into the sort of guts of Turing machines and so on. So just to have the gist of the proof, we'll be content to leave the description of the state variables like this. It's a table where rows correspond to timesteps, columns correspond to bits of memory.
00:21:12.498 - 00:21:32.762, Speaker A: And hopefully it's plausible that, okay, maybe with a little bit of extra bookkeeping to keep track of a few things, you really can, using this table, encode what the verification algorithm's entire trajectory would look like given a particular candidate solution. Let's now complete the construction of our three set instance. We've already said what the variables are.
00:21:32.762 - 00:22:03.750, Speaker A: There are these solution variables encoding a candidate solution to the instance of the problem A that we were given in the first place. And then we also have these state variables which encode the execution of the verification algorithm on some candidate solution. So what about the constraints? Well, the point of the constraints is to enforce our intended semantics for the state variables, to enforce that any truth assignment given to those variables in that table, that it really should encode a legitimate computation of the verification algorithm.
00:22:03.750 - 00:22:30.174, Speaker A: For example, you're going to have one constraint, or actually a bunch of constraints for each entry in that two dimensional table. So for each choice of a timestep t, each row, each choice of a column, like the ith bit of memory, there's going to be some constraints which enforce that the contents of the ith bit of memory at time t are what it should have been. That is, whether or not that bit is a zero or one, that depends on the relevant bits of memory from the previous time step.
00:22:30.174 - 00:22:44.050, Speaker A: It depends on what instance of problem A we were given in the first place. It depends on what candidate's solution to that instance we're looking at. And then finally, it depends on description of exactly how the verification algorithm operates.
00:22:44.050 - 00:23:08.758, Speaker A: That probably seems like quite a mouthful. But here's the good news. The good news is that in a computational model like, say, a Turing machine, what can happen from one time step to the next is really, really limited.
00:23:08.758 - 00:23:28.660, Speaker A: The machine basically has some internal state. It's going to be reading some character from some cell and possibly replacing that with a different character on that same cell. So because what happens from one time step to the next is so simple, it turns out you can enforce these intended semantics with a reasonably small number of disjunctions of, at most, three literals each.
00:23:28.660 - 00:24:03.930, Speaker A: So let's take stock of where we are. So, given these constraints, how can we interpret a truth assignment to the three side instance that we've constructed? Well, there's going to be these true false values assigned to the solution variables, so we can interpret that as a candidate solution, and then all of these state variables tell us what the verification algorithm would do with that particular candidate solution. Oh, so we are missing actually one constraint, which is that in addition, at the end of the verification algorithm's execution, it should accept the candidate solution described by the solution variables.
00:24:03.930 - 00:24:19.134, Speaker A: So if I happen to give you a magic box that solved threesat, what could you do with it? Well, you could solve any NP problem that you wanted. How would you do it? Take your NP problem, take an instance of it. Run it through this reduction construct, a three sat instance, as we just discussed.
00:24:19.134 - 00:24:41.606, Speaker A: Run this magic box that will either give you a satisfying truth assignment or tell you that none exist. If it gives you a satisfying truth assignment, you can read off a feasible solution to the instance of problem A you started with just by reading off the bits encoded by the solution variables. And if the three set instance winds up being unsatisfiable, you can correctly conclude that there were no feasible solutions to the instance of the problem A that you started with.
00:24:41.606 - 00:24:55.882, Speaker A: So that's the Cook Levin theorem. The three set problems NP hard, literally every single problem with efficiently recognizable solutions actually reduces to the seemingly quite simple whole three side problem. Coming up next, the P versus NP conjecture.
00:24:55.882 - 00:24:56.540, Speaker A: I'll see you there.
