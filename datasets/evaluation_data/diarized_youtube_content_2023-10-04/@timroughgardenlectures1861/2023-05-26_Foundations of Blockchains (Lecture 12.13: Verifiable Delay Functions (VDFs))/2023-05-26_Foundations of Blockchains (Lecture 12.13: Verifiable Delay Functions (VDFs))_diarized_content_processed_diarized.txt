00:00:00.570 - 00:00:46.986, Speaker A: So we wrapped up the last video knowing basically what we wanted for this sort of third approach we were taking to trying to crowdsource randomness from the nodes, running a proof of stake blockchain protocol. Namely, what we really were after was a function, little F, whose evaluation time is largely invariant to the amount of computational resources that go into its evaluation. So specifically, we'd like honest nodes to be able to evaluate it in a reasonable amount of time using a reasonable amount of computation power. Our phase two duration has to be at least as long as what honest nodes need to be able to evaluate it. On the other hand, we would like even a well funded attacker to not be able to evaluate it that much more quickly. And in particular, we need to take the duration of phase one. That's the phase in which everybody's reporting their R sub eyes.
00:00:46.986 - 00:01:42.300, Speaker A: We need to take the duration of phase one to be less than the amount of time it's going to take a well funded attacker to evaluate little F. And we concluded the last video wondering whether this was just too good to be true, whether such functions, little f, could could even exist. So let's start thinking about what might they look like. And the first thing I want to point out is that if such a function little f exists, it seems like it has to be a quote unquote, inherently sequential function, a function for which parallelism doesn't help you. And that's because an honest node, we don't really want to assume that it has more than one machine, right? Maybe it has a reasonably modern computer, but we don't really want to make more assumptions about that, whereas we want to allow attackers that could buy 1000 or 10,000 machines. So if it was a function that allowed speed up through parallelism, then clearly you couldn't be a verifiable delay function. Then a well funded attacker really could get a large speed up over what an honest node with a single machine could do.
00:01:42.300 - 00:02:46.800, Speaker A: All right, but then you ask are any functions really inherently sequential? Or maybe every function can just be sped up with lots of machines to some extent. We don't really know the answer to that question, just with the state of modern day complexity theory, but we definitely have proposals for functions that would appear to be inherently sequential. Maybe the one that's sort of closest to things we've been discussing a lot anyways would be to take a cryptographic hash function like, say, shot 256 and just iterate it sequentially a whole bunch of times. So this is a good starting point. Certainly the obvious way to compute this function little f would be sequential, right? You just take X, you apply the hash function like shot 256, you apply it again, you apply it again, you apply it again, et cetera, k times. Intuitively, it's hard to imagine for a cryptographic hash function, what shortcut you could possibly come up with to do better because you don't really know by the random oracle assumption at least you don't know h of x until you evaluate x. So you have no idea what to evaluate that second hash function on until you know the result of the first hash function.
00:02:46.800 - 00:03:44.014, Speaker A: So intuitively then it would seem there's nothing you can do to skip just k evaluations of the hash function. And of course you can tune the parameter k to get the desired evaluation time that you're after. So this idea definitely gets some of the properties that we're after, but it's also missing a key one which is efficient verifiability to see what I mean. Like remember how permissionless consensus protocols work with nodes coming and going all the time. You can pause this video right now and go spin up a bitcoin node or an ethereum node if you want it. If you did that, the first thing your node would have to do would be to sort of get up to speed need with the many years of transactions that those blockchain protocols have executed thus far. But if in fact that involved these verifiable delay functions, like if every block required a lengthy computation of this sort of for example, iterating shot 256 over and over and over again, how would you ever catch up? Right? It's going to take you basically as long to sort of regenerate the last eight years of transactions.
00:03:44.014 - 00:05:05.142, Speaker A: It's going to take you almost as long as it took to sort of process those transactions in the first place. So after the work has been done, after someone has gone through the trouble of doing this inherently sequential computation, we would really like it to be true that others could check the correctness of their computation without redoing that whole computation from scratch. So for example, maybe each round of a blockchain protocol involves sort of one VDF evaluation for example to generate the pseudorandous RCT used in that round. But then any node that sort of shows up eight years later, they can just go back and check the correctness of all the VDF computations from the last eight years in much much less than eight years. So the rough summary then of what we're looking for is first of all, if someone's willing to put in the work, like of actually evaluating Little F from scratch, then they can figure out the output of Little F. If someone's not willing to put in the work and also doesn't know the answer, then they are unable to quickly compute what the output of F would be. And third, that if someone else has already done the work and evaluated it, then anyone else can sort of quickly check the correctness and said that way you should be seeing some strong parallels to verifiable random functions VRFs that we talked about a few videos earlier.
00:05:05.142 - 00:06:01.630, Speaker A: A VRF, right? What's true about it? If you know the right private key, you can quickly evaluate it. If you don't know the right private key and nobody tells you the output, you can't guess what the output of the VRF is going to be. On the other hand, given knowledge of the public key and the VRF output, you can quickly check that the computation was done correctly. So in effect, so in a VDF putting in the work, so spending some T units of time, that's roughly playing the role of the knowledge of the private key in the VRF case. So those are the three properties that we want and those are also the three divining properties of a verifiable delay function. So this is a relatively recent definition only about five years old as of the time of this recording, originally proposed by Bonet, Bono, Boons and Fish. And so let's look at the three properties which correspond to what we said though with a little bit of a twist.
00:06:01.630 - 00:06:57.730, Speaker A: The first property says that anyone willing to put in the work should be able to evaluate the VDF little F. How much work exactly? That's controlled by a parameter capital T, which when you choose your VDF, you're also sort of choosing what you want your parameter capital T to be. Again, that's going to correspond sort of roughly to the duration of phase two in a randomness generation protocol. If you're thinking about the example of kind of an iterated cryptographic hash function like shot 256 in mind, then obviously capital T is going to be scaling with the number of times you ask the hash function to be evaluated. Now the twist in this property you may not have been expecting was this certificate pi. So it needs to be true that not only can you evaluate little F on an input little x and capital T timesteps, but along the way you need to be able to show your work in a certain sense. So pi is something you generate kind of on the fly as you're evaluating little F for as we'll see the purposes of efficient verification.
00:06:57.730 - 00:07:50.290, Speaker A: The second property is a sort of converse. It says if you're not willing to put in the work, meaning you're going to do much less than capital T timesteps worth of work and moreover, nobody actually just tells you the answer, then there's no way you can magically guess what the output of little F is going to be. Now, when we write down these two properties, we have in mind honest nodes and an attacker respectively, right, when we say anyone can compute it in capital T timesteps, we mean any of the honest nodes with whatever computational resources are reasonable to assume for them. Similarly, in the second property where we say no one, we really mean a well funded attacker cannot get significant speed up over what the honest nodes can achieve. And again, thinking about a well funded attacker, nothing's stopping them from buying thousands of machines and running them in parallel. So again, this function, little F, is going to be in some sense, an inherently sequential function. The third property is the efficiency verification property.
00:07:50.290 - 00:08:58.102, Speaker A: So if somebody does go through the trouble of evaluating F and generating the corresponding certificate pi that we talked about in property one, well then, given the alleged output and the supporting certificate pi, anybody, including honest nodes, can verify correctness really quickly, meaning much, much faster than capital T timesteps. Now, in some cases, we would like the output of the VDF to in addition, be indistinguishable from uniformly at random for all practical purposes. But that's not a big deal to add, right? If you have a VDF that satisfies one through three, you can get a VDF that satisfies one through three plus, for all practical purposes, uniform by composing it with a cryptographic hash function for which you're comfortable making the random oracle assumption. And actually, it's a good exercise for you to think through exactly that. So I give you a VDF satisfying one through three. How would you come up with one that, in addition, is basically as random as your cryptographic hash function? Now, we've completely, to this point, failed to answer the cliffhanger question from the last video about whether or not VDFS even exist. Remember last video? We sort of concluded by saying, oh, we want a function whose evaluation time is largely invariant to the amount of computational resources you throw at it.
00:08:58.102 - 00:09:34.500, Speaker A: And that already seemed like a really strong thing to want. And so now we've sort of clarified our understanding of what these functions might look like. We understand that, okay, in particular, they must be, in some sense, inherently sequential. We talked about iterating a cryptographic hash function like shot 256 as one possible example. But then we realized we also wanted this efficient verification property for correctly computed VDF outputs. And so then that gave us these three parts of this definition. We still have not said anything about whether we think this definition is non vacuous, whether we think there might be a function that satisfies all three properties, and if so, what that function might be.
00:09:34.500 - 00:10:48.966, Speaker A: So for the one candidate we discussed, iterating a cryptographic hash function. Over and over again, we talked through why that clearly has the first property here, why intuitively it would seem to have the second property as well, why it's sort of inherently sequential. But what about that third property, right? So like, if I claim that I evaluated shot 256 a thousand times on some input, how would I ever convince you that I did it correctly without you actually just redoing those thousand invocations sequential invocations of shot 256? So, believe it or not, it is possible, at least in principle, to generate certificates that allow someone to quickly check that you actually iterated a cryptographic hash function, a prescribed number of times that follows from a fairly general sort of tool known as a snark, which perhaps we'll have something to say about in future lectures in this series. But I want to emphasize those, at least at the moment, in early 2023, those are not regarded as practical. So the proposed practical constructions of EDFs do not rely on the iterated invocation of a cryptographic hash function. Rather, they rely on a different problem, namely repeated squaring in groups. So more formally, we're thinking about problems where there's two inputs.
00:10:48.966 - 00:11:20.070, Speaker A: Input number one is a group element, little G drawn from capital G. Input number two is a parameter, capital T. And the output your responsibility is to compute. What would be the results if you invoked the group operation on G with itself two raised to the capital T times. Many of the most promising proposals for VDF constructions are based on this idea. Let's talk through the three defining properties of VDFS. First of all, the first property, no matter what the group is, capital G is, property one is going to hold.
00:11:20.070 - 00:11:56.606, Speaker A: There is a straightforward way of computing G raised to two raised to the T, which is just repeated squaring. Take the group operation on G with itself to get G squared. Take the group operation with G squared with G squared to get G to the fourth, and so on. So just generate powers of two in the exponent until you get up to G raised to the two to the T. So that first property is going to hold literally no matter what the group is. The second property on their hand is definitely going to fail for some sufficiently sort of simply structured groups. There will be groups where there will be shortcuts to computing G to the two to the T which are qualitatively faster than repeated squaring.
00:11:56.606 - 00:12:39.898, Speaker A: But all we need is sort of one group to get a VDF. So the hope would be that there exists groups for which there's no significant shortcut over repeated squaring, and that is conjectured, in fact to be the case. So that conjecture is a necessary condition for there to exist verifiable delay functions. Based on this repeated squaring idea, the only way you're going to get that second property of the VDF definition is if in fact there's no significant shortcut over repeated squaring. I want to emphasize this is definitely not a theorem. We definitely don't know some group for which we can prove unconditionally that you can't beat repeated squaring. Just like we don't know for a fact that there's no efficient algorithm for, say, factoring or computing discrete logarithms.
00:12:39.898 - 00:13:25.070, Speaker A: It's just a conjecture. It just says that as long as this is true, as long as you can't have a significant shortcut, then hopefully we'll get a VDF with the desired properties. Now, there's a big difference, of course, between these examples, which is that factoring and discrete logarithm have been around for a very long time. I mean, arguably they've been around for millennia, but computer scientists have been trying to come up with efficient algorithms for both of those problems for almost a half century now. Meanwhile, this sort of EDF definition is only five years old. And so the computational hardness assumptions we're working with here are much, much less battle tested. So keep that in mind, but nonetheless right so the experts in the area at the moment believe there's a solid chance that you can't do much better than repeated squaring if you choose your group capital G appropriately.
00:13:25.070 - 00:14:27.886, Speaker A: So for the rest of this video, let's just assume that the conjecture is true. So then we have properties, both one by construction and two by assumption. What about property three? So how can you convince somebody that you did this repeated squaring operation without forcing them to redo it themselves? So there's a lot of proposed VDF constructions out there, including multiple that are based on this repeated squaring idea. But let me just highlight the two that, according to the experts that I speak to, are the most promising for practical deployments. So these are two different constructions, both based on the repeated squaring idea, but using different approaches to computing the certificate pi done independently, both coming out in that same year, 2018, about five years ago, one by Pirchazak and the other by Wesalowski. So both of these works identify groups, capital G, where we believe property two holds, where we believe that there's no significant shortcut to repeated squaring if you're forced to do it from scratch. But there are also groups.
00:14:27.886 - 00:15:04.350, Speaker A: And now this next part is not conjecture. This next part really was proved by these researchers groups, where property three does in fact hold. So there is, in fact a way to generate a certificate. And again, the two different works use different approaches. But in both cases, there's a way to show your work generate a certificate pi so that if you hand pi along with the alleged output to someone else, they can very quickly verify that you computed the VDF correctly. So if this sounds really cool, and honestly, it is really cool, so if it sounds that way, I really encourage you to check out these two papers. Or alternatively, there's a survey of those two papers by Bonet Boons and Fish.
00:15:04.350 - 00:16:02.320, Speaker A: As you maybe could have guessed, the approaches to computing pi, they're based on sort of very clever algebra. It's somewhat reminiscent of sort of backdoors that you see in other contexts that turn a hard problem into an easy one. So that now brings us up not merely to the state of the art in proof of stake random sampling, but even maybe a little bit beyond to ideas that I expect we'll see much more of as 2023 rolls on and then 2024 and 2025, et cetera. There is actually one significant project in deployment that uses verifiable delay functions, which is the chia blockchain, which, despite not being a proof of stake blockchain, nonetheless uses VDFS for reasons similar to the reasons why you use them in proof of stake chains. VDFS are also on the roadmap for proof of stake ethereum. They are not in the version of proof of stake ethereum that's running as of this recording in early 2023. But I think there's a good chance we will see them as part of ethereum in the future as well.
00:16:02.320 - 00:16:56.730, Speaker A: So earlier in lecture twelve, we talked about one super cutting edge topic single secret leader election. And here is our second one verifiable delay functions. So we're now done with part two of lecture twelve, right? So in this part, we really didn't worry about consensus at all. We just really isolated the problem of, look, you have these public keys in the Staking contract. You want to sample one with probability proportional stake. How do you do that? And that problem already obviously kept us busy for a while. Looking ahead to part three, we're going to say, okay, now that we've got a handle on how we actually sample from this Staking distribution that's in the Staking contract, how do we actually extend that into a proper permissionless consensus protocol? So how do we stitch together these part two ideas with the consensus protocols that we know and love from lectures one through nine? Both BFT type consensus protocols and longest chain consensus protocols.
00:16:56.730 - 00:17:01.690, Speaker A: So that will be what we do a deep dive on in part three. I'll see you there. Bye.
