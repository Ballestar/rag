00:00:00.650 - 00:00:16.254, Speaker A: Hi everyone, and welcome to this video that accompanies Section 19.3 of the book Algorithms Illuminated part Four. It's a section about an initial informal understanding of what it means for a problem to be computationally easy or computationally hard.
00:00:16.254 - 00:00:40.280, Speaker A: So some computational problems are easier than others. And the point of the theory of NP hardness is to classify in a precise sense which problems are easy, like the minimum spanning tree being one example, and which problems are hard, the traveling salesman problem being an example. So let me just sort of give you an oversimplified dichotomy of what theory of NP hardness says.
00:00:40.280 - 00:01:03.214, Speaker A: So in theory of NP hardness, what we're going to mean by an easy problem is a problem that can be solved by an algorithm whose running time scales as a polynomial function of the input size. Ideally, you've got a blazingly fast implementation, like a nearlinear time algorithm like we have for the minimum spanning tree problem. But for the purposes of the theory of NP hardness, a quadratic time algorithm would be fine, a cubic time algorithm would be fine.
00:01:03.214 - 00:01:35.590, Speaker A: Even a one to the 100 algorithm where n is the input size would qualify a problem as being easy for the purposes of this dichotomy, whereas a hard problem and Tsp is conjectured to be hard in this sense, a hard problem is one that's going to require an exponential amount of time in the worst case. So any correct algorithm, there will be inputs with a running time of that algorithm scales like an exponential function of the input size. Now, mind you, this dichotomy is not 100% accurate.
00:01:35.590 - 00:01:54.670, Speaker A: I'm giving you an oversimplified version just to get us started. It overlooks a number of subtleties that we'll discuss in detail later. But ten years from now, if you remember only kind of a few phrases about the theory of empty hardness and what it says, this is a pretty good oversimplified dichotomy to just keep in mind.
00:01:54.670 - 00:02:03.170, Speaker A: So let's now go into the details a little bit more. Let's start with easy problems, problems that are solvable in polynomial time. We've seen many such problems.
00:02:03.170 - 00:02:19.682, Speaker A: That's actually been the whole point of the first three parts of the book series and the previous video playlist. So let's just recall some of the examples that we've seen so far of polynomial time algorithms that exactly solve interesting computational problems. So, for example, very early days we saw the Merge sort algorithm.
00:02:19.682 - 00:02:38.074, Speaker A: So we saw that sorting is an easy problem because there's an algorithm Merge short whose running time runs in almost linear time. So n log n, where n is the array length. In the context of graph search, we saw Kazaraju's linear time algorithm for computing the strongly connected components of a directed graph.
00:02:38.074 - 00:02:51.810, Speaker A: You might remember this was the algorithm where you do two passes of depth for search to compute the strongly connected components. That was linear time o of m plus n. But here I'm using m to denote the number of edges in the graph and n to denote the number of vertices.
00:02:51.810 - 00:03:13.514, Speaker A: One of the greatest hits of what we've discussed so far, of course, is Dijkstra's shortest path algorithm. So that takes as input a directed graph with non negative edge lengths along with a starting vertex, and it tells you the distance of the length of a shortest path from the starting vertex to every other vertex in the graph. Just like prim's MST algorithm that runs for basically the same reasons in near linear time.
00:03:13.514 - 00:03:43.294, Speaker A: So blazingly fast implementation, solving the shortest path problem when all the edge lengths are non negative. And as we mentioned, it's not just prim's MST algorithm, but Kruskal's MST algorithm, also implemented with suitable data structures, gets you a blazingly fast near linear running time. So back when we were discussing dynamic programming algorithms, we talked about the sequence alignment problem where you're given two strings and you want to find a nice alignment between them, where you pay a penalty for inserting gaps and you pay a penalty for mismatching pairs of distinct characters.
00:03:43.294 - 00:04:03.206, Speaker A: So there we never saw a linear time algorithm for that problem. And actually we'll see much later in this playlist evidence of why there we haven't seen a linear time algorithm for it, but we did see a polynomial time algorithm for it. Our dynamic programming algorithm ran an O of m times n time where m and n denote the lengths of the two input strings.
00:04:03.206 - 00:04:22.042, Speaker A: So that's bounded above by a polynomial function of the input size, the input size being m plus n. And one of the slowest algorithms that we've seen also from the dynamic programming discussion is the Floyd Warshall algorithm for all pairs shortest path. So here I give you a graph with real valued edge lengths but no negative cycle.
00:04:22.042 - 00:04:41.126, Speaker A: And the Floyd Warshaw algorithm is going to run in cubic time n to the third, where n is the number of vertices, and it's going to tell you the shortest path distance between each of the n. Choose two pairs of vertices in the graph. And the point of doing this enumeration is just to observe that we've talked about lots of efficient algorithms for lots of different computational problems.
00:04:41.126 - 00:04:55.290, Speaker A: It was great when we got linear and near linear time, but that didn't always happen. Some of the harder problems, we had algorithms that ran in quadratic or even cubic time. But the point is, all of these algorithms have running time bounded by some polynomial function of the input size.
00:04:55.290 - 00:05:03.198, Speaker A: All of these are polynomial time algorithms. Precisely. What is a polynomial time algorithm? Well, just generalizing these.
00:05:03.198 - 00:05:20.946, Speaker A: It's an algorithm that runs in time big O of n to the d, where n is the length of an input and d is some constant independent of n. So if d equals one, you'd be talking about a linear time algorithm. If d equals two, a quadratic time algorithm, d equal three, a cubic time algorithm, and so on.
00:05:20.946 - 00:05:35.080, Speaker A: So in the definition of a polynomial time algorithm, it's crucial that the d, the exponent d is just a number, like three or four. It should not depend on n. So n to the n, that's not a polynomial time algorithm because d depends on n.
00:05:35.080 - 00:05:57.322, Speaker A: Now, this might seem like a very permissive definition, right, because, yeah, hopefully an algorithm has near linear time. But in principle, if d is equal to 77, that would still count as a polynomial time algorithm according to this definition. That said, we have seen algorithms which are not even polynomial time that even meet this very permissive definition.
00:05:57.322 - 00:06:12.814, Speaker A: Remember, any exponential function grows faster than any polynomial function. So, for example, two to the n will eventually grow much, much faster than n to the 77th. Now, for example, in the MST problem, we saw that there was an exponential number of spanning trees.
00:06:12.814 - 00:06:23.986, Speaker A: And so exhaustive search for the MST problem would be exponential time. That would not be a polynomial time algorithm. There are other algorithms for the MST that are polynomial time, like Prim and kruskal.
00:06:23.986 - 00:06:42.766, Speaker A: But exhaustive search is not a polynomial time algorithm for the MST problem. So there's really something special and clever about the polynomial time algorithms that we've seen so far. So here's just a quick plot to drive home this point that every exponential function eventually grows much, much faster than any polynomial function.
00:06:42.766 - 00:07:06.518, Speaker A: This graph shows in the solid line, the graph of 100 times n squared, and in the dotted line shows the exponential function two to the n. And so initially, for small values of n, right, the dashed line is lower because there's a high constant factor in front of the n squared, there's a factor of 100 there. But this picture is totally characteristic of what you see with polynomial versus exponential running times.
00:07:06.518 - 00:07:24.540, Speaker A: At a quite reasonably modest input size input, something like 13 or 14, the two curves cross, and after they cross, the exponential function grows way faster than the polynomial function. So the gulf between the two functions is huge. And the bigger n is, the bigger the Gulf gets.
00:07:24.540 - 00:07:49.806, Speaker A: One interesting implication of that is whenever anyone talks about sort of Moore's Law and computers getting faster and just being able to over time solve kind of anything, we could imagine, what's important to realize is that actually Moore's Law doesn't make sort of polynomial versus exponential time distinctions less relevant. It actually makes them more relevant. Because don't forget, as our computational power grows, so do our computational ambitions.
00:07:49.806 - 00:08:04.150, Speaker A: We look at bigger and bigger problem sizes once we have bigger disks, bigger memory, faster CPUs, et cetera. And the bigger the input size, the more dramatic the difference between a polynomial time algorithm and an exponential time algorithm. So this is not a distinction that's going away.
00:08:04.150 - 00:08:36.954, Speaker A: This is a distinction that's becoming ever more important as technology advances so for a different way to think about the difference between polynomial and exponential running times, you might want to think about the sort of very realistic scenario where you have some fixed time budget. Like you're willing to run a program for an hour, but that's it. Are you willing to run it for a day, but that's it? And the question then is how big an input, how big a problem size can you handle in this fixed time budget, like in an hour? And what's so great about polynomial time algorithms is that as you speed up your computer, the input sizes you can accommodate also speed up multiplicatively.
00:08:36.954 - 00:09:08.170, Speaker A: So, for example, if you had a linear time algorithm and you doubled the amount of your computational budget, you would double the size of the inputs that you could handle. If you had a quadratic time algorithm, it wouldn't increase by as much, but it would still increase by a multiplicative factor of square root of two. Whereas if you had an exponential time algorithm, like, say, something with running time two to the n, then all of a sudden you double your computing power and it increases the size of the problems you can solve by plus one.
00:09:08.170 - 00:09:23.102, Speaker A: So if you could handle inputs of length a million before, you've just got a twice as fast computer, and now you can handle inputs of size a million and one. Wow. So that's a pretty big difference, right? So really, polynomial time algorithms are the one that see benefits of increasing technology.
00:09:23.102 - 00:09:57.818, Speaker A: Exponential time algorithms, they're going to be slow till the end of time. So in theory of NP hardness, then, one defines an easy problem as a problem solvable by a polynomial time algorithm, or equivalently solvable by an algorithm whose input size it can accommodate in a fixed period of time scales multiplicatively with the amount of computing power. So, for example, all of the problems we just discussed, like sorting shortest paths, all pair, shortest path, sequence alignment, minimum spanning tree, those are all polynomial time solvable problems because we've seen polynomial time algorithms that solve them.
00:09:57.818 - 00:10:18.366, Speaker A: Now, in all of our examples, the exponent in the polynomial time algorithm was pretty reasonable. Ideally it was one, but sometimes it was maybe two or even three in the Floyd Warshaw, but still, it wasn't that big a number. Now, in principle, if a problem is solved by an algorithm running in time n to the 100, where n is the input length, that counts as a polynomial time algorithm.
00:10:18.366 - 00:10:41.590, Speaker A: And so that would qualify for being a polynomial time solvable problem. But actually, what's really then interesting is if you look at the sort of if you turn this statement on its head so what this means is that if I told you that a problem was not solved by any polynomial time algorithm, I'd be saying there is not even an end of the 100 time algorithm that solves it. There is not even an o of end of the 10,000 time algorithm that solves it.
00:10:41.590 - 00:10:59.850, Speaker A: So that's a pretty crazy statement. So if you can say that a problem is not polynomial time solvable, that's really saying in a strong way, you cannot have any algorithm which is always guaranteed to be fast and always guaranteed to be correct for that problem. So that's the definition of an easy problem, a problem that's solved by some polynomial time algorithm.
00:10:59.850 - 00:11:11.566, Speaker A: Let's get to the trickier point of how we want to define a hard problem. So consider a problem like the traveling salesman problem. Suppose we sort of thought that the problem was not easy in the sense of the previous slide.
00:11:11.566 - 00:11:37.750, Speaker A: That is, we thought we think there's no polynomial time algorithm for it. How would we amass evidence to support that belief? Well of know the strongest evidence would be an actual mathematical proof that in fact there is no polynomial time algorithm for the Tsp. Unfortunately, to this day, the status of the Tsp is in limbo and nobody knows whether there is a fast algorithm for it and we haven't found it yet, or whether in fact no such algorithm exists.
00:11:37.750 - 00:12:11.670, Speaker A: So we don't have airtight mathematical evidence that the Tsp is not polynomial time solvable. But can we at least amass some kind of circumstantial evidence? So the first piece of circumstantial evidence, which know somewhat weak but still kind of compelling, is that this is a super famous problem that a lot of super smart people have worked on for a very long time. We're talking about probably thousands of people, extremely well trained, brilliant minds over really 70 years have failed to find a polynomial time algorithm solving the traveling salesman problem.
00:12:11.670 - 00:12:28.422, Speaker A: That does not mean it's impossible, but it makes you wonder, makes you wonder if actually there's nothing to be found, given that all of these people have failed. And certainly if there is such an algorithm, it's not going to be something you write down on a cocktail napkin. It's going to be something presumably quite ingenious and possibly very complicated.
00:12:28.422 - 00:12:55.540, Speaker A: So that's the first thing. It's like, okay, famous 70 year old problem probably if there was an algorithm, one of these people would have found it during this time. But the magic and power of NP hardness is that it gives much stronger evidence that there's no polynomial time algorithm for the Tsp by showing that if there were a polynomial time algorithm for the Tsp, that would automatically give you polynomial time algorithms for thousands of other currently unsolved problems.
00:12:55.540 - 00:13:29.706, Speaker A: So in effect, the theory of NP hardness shows that thousands of computational problems, including the traveling salesman problem, are all variations of the same problem in disguise, all destined to suffer identical computational fates. If you're trying to devise a polynomial time algorithm for an NP hard problem like the Tsp, you're inadvertently attempting to come up with such algorithms also for these thousands of related problems. Now playing Deadpool's advocate, you might well but you know what? Maybe a lot of people have tried to do a polynomial time algorithm for the Tsp, but haven't.
00:13:29.706 - 00:13:40.318, Speaker A: A lot of people tried to prove that there is no polynomial time algorithm for the Tsp. And it's true. Hundreds, if not thousands of brilliant minds have failed to prove the other direction that the Tsp is not polynomial time solvable.
00:13:40.318 - 00:14:09.542, Speaker A: So you might argue, isn't that sort of equally strong evidence that that might be? You know, the difference is that human beings, we seem so far at least, much better at proving computational tractability that is coming up with clever algorithms when they exist. Look at all of the examples that we've seen in this book series and throughout these video playlists, and we seem much worse at proving unsolvability. There are not that many successful examples of us showing what can't be done computationally.
00:14:09.542 - 00:14:24.254, Speaker A: Thus, if the Tsp were polynomial time solvable, it would really be quite surprising that no one had found that algorithm yet. Whereas if it's not polynomial, given the mathematical sort of state of the art, it's maybe not that surprising. We haven't yet figured out how to prove it.
00:14:24.254 - 00:14:51.802, Speaker A: So that's sort of where the belief comes from that it seems much more likely to most experts that there is no polynomial time algorithm for the Tsp, as opposed to the minority who believes that actually there is one and we just haven't found it yet. All right, so let's now finally talk about what we mean by an NP hard problem. Basically, what we mean is that there's strong evidence of intractability, like we saw in the previous slide, that a polynomial time algorithm would automatically give a polynomial time algorithm for thousands of related problems.
00:14:51.802 - 00:15:18.670, Speaker A: That said, I'm not going to give you a formal mathematical definition of NP hard until we get to those optional videos deep in the playlist. But let me tell you what's going to be our provisional definition for the next large number of videos. So the provisional definition is that a problem is going to be deemed NP hard if a polynomial time algorithm solving it would refute a famous mathematical conjecture known as the P not equal to NP conjecture.
00:15:18.670 - 00:15:46.150, Speaker A: So turning this statement around, what this says is that if the P not equal to NP conjecture is true, which most people believe that it is, if that conjecture is true, then no NP heart problem, including the Tsp, is polynomial time solvable. Meaning there is not even an end of the 100 time algorithm solving it, or an end of the 10,000 times algorithm that solves it. So you must be wondering, what is this P not equal to NP conjecture? Well, it is a little technical to define formally.
00:15:46.150 - 00:16:03.318, Speaker A: We will do it, but we're not going to do it until those optional videos late in the playlist. So let's just settle for now for an informal understanding of the P not equal to NP conjecture. And it's a statement that should resonate with anybody who's had experience both doing homeworks and grading other people's homeworks.
00:16:03.318 - 00:16:20.546, Speaker A: So the informal version of the conjecture states something very intuitive. It states that checking someone else's alleged solution to some computational problem should be a fundamentally easier task than coming up with that solution yourself. Think, for example, about like a Sudoku or KenKen puzzle.
00:16:20.546 - 00:16:47.374, Speaker A: If someone handed you their solution, it would be quite straightforward to just quickly check that indeed they obeyed all the rules of the puzzle, whereas for the more difficult puzzles, they seem to take a long time to come up with a solution from scratch. At least they take me a long time to come up with the solution from scratch. Or in the context of the traveling salesman problem, it would be very easy to check that someone had found a good tour, say a tour with total cost at most 1000.
00:16:47.374 - 00:17:02.420, Speaker A: It feels like it's probably a lot easier than actually having to come up with a Tour yourself that has cost at most 1000. You'd be happy to have someone else do that work for you. So that's the P zero equal to NP conjecture, the checking solutions, is fundamentally easier than finding them yourself.
00:17:02.420 - 00:17:08.870, Speaker A: Maybe it seems obvious. So it turns out that we don't actually know if this is true or not. This is an open question.
00:17:08.870 - 00:17:20.046, Speaker A: Most people believe it's true, but we haven't proved it. Why isn't it obvious? Well, it's not obvious because polynomial time algorithms can be like completely crazy and ingenious. We've seen a lot of examples.
00:17:20.046 - 00:17:47.550, Speaker A: Maybe I think the most vivid example is if you saw Strassen's subcubic algorithm for matrix multiplication. Just kind of mind blowing how he beats cubic time from matrix multiplication. So once you see a bunch of examples like that of these sort of crazy, ingenious polynomial time algorithms, it starts feeling pretty intimidating to argue that there's something you can't do with polynomial time algorithms, right? So who's to say you can't solve the Tsp given that polynomial time algorithms can be the space is so rich.
00:17:47.550 - 00:18:12.826, Speaker A: That said, kind of, most experts, not 100%, but most experts believe that the PNET equal to NP conjecture is in fact true. And remember that if it is true, that implies that all NP hard problems, including the traveling salesman problem, cannot be solved by any algorithm exactly that runs in end of the 100 time or even end of the 10,000 time. So now it's time to revisit the oversimplified explanation of MP hardness that I gave you at the beginning of this video.
00:18:12.826 - 00:18:34.926, Speaker A: So back at the beginning, I said that if you only remember a few words about theory of MP hardness sort of a few years from now, remember that it identifies easy problems as those that are polynomial time solvable, and it identifies hard problems as those that require exponential time to solve. In the worst case, so I said that at the beginning of the video. But now I've actually told you what I really mean by a hard problem is NP hard.
00:18:34.926 - 00:18:56.466, Speaker A: And an NP hard problem is one that if you solved it with a polynomial time algorithm, then it would refute the P not equal to NP conjecture. That's not the same thing that we said at the beginning, that it requires exponential time in the worst case. So let's just actually kind of clarify everything and talk about what are the differences between the real definition of NP hardness and that oversimplified definition at the beginning.
00:18:56.466 - 00:19:13.590, Speaker A: So the first discrepancy between what I said at the beginning of the video and our actual definition of a hard problem is that MP hardness, it's relevant primarily if the P not equal to NP conjecture is true. And again, this is not a proven conjecture. Most people believe it, but it has not been proved.
00:19:13.590 - 00:19:35.182, Speaker A: So if the P not equal to NP conjecture is false, then all bets are off. And in fact, if it's false, then we know that many, many problems, MP hard problems, including all the problems we'll see in this playlist, in fact, can be solved in polynomial time. So in other words, the computational intractability is conditional on this unproven mathematical conjecture, the P not equal to NP conjecture.
00:19:35.182 - 00:19:50.210, Speaker A: That's the first discrepancy. So the second discrepancy is that even in the likely events that the P not equal to NP conjecture is true, it only rules out polynomial time algorithms for NP hard problems. It does not imply that the algorithms need to run an exponential time.
00:19:50.210 - 00:20:09.450, Speaker A: There are running time bounds that are in between that grow faster than polynomials, but slower than exponentials. So, for example, n raised to the log n, that's an example of something that's in between or two raised to the square root of n. That's another function which grows more slowly than any exponential, but more quickly than any polynomial.
00:20:09.450 - 00:20:43.914, Speaker A: And so these would remain candidates for running times of algorithms solving NP hard problems. That said, for all of the problems that we're going to discuss in this video playlist, experts believe that not only is there no polynomial time algorithm, but there's in fact, no sub exponential time algorithm either, that you really do require exponential time to solve the NP hard problems we're going to be discussing. So that's formalized by something known as the exponential time hypothesis, which is a strengthening of the P not equal to NP conjecture, which we'll discuss briefly in those optional videos about the P versus NP question.
00:20:43.914 - 00:21:08.846, Speaker A: So I should also mention that while the problems we discuss in this playlist can be solved in exponential time using exhaustive search, in general, there are problems that are even harder than that. There are problems out there in the world that can't even be solved in exponential time. In fact, there are famous problems, like maybe you've heard of the halting problem, which can't be solved at all in any finite period of time by computers.
00:21:08.846 - 00:21:25.282, Speaker A: So we'll say a little bit more about the Halting problem in the complexity theory lectures. But for the most part, we're going to be discussing problems that, if nothing else, could be solved in exponential time. So the strongest possible computational intractability statement you could make is that exponential time is required.
00:21:25.282 - 00:21:41.642, Speaker A: And that's exactly what the exponential time hypothesis asserts for the M heart problems we'll be discussing in this. Finally, you know, I sort of painted a picture at the beginning of this video of a rigid dichotomy. Like every problem has to be either easy, or if it's not easy, then it has to be hard.
00:21:41.642 - 00:22:00.286, Speaker A: And that's true for 99% of the problems you're going to come across. But just so you know, there are a couple of natural problems that seem to reside in between, problems that seem too hard to be polynomial time solvable, but too easy to be NP hard. There's at least two famous examples, the two most famous ones being factoring.
00:22:00.286 - 00:22:11.874, Speaker A: So I give you an integer. I want you to find me a non trivial factor of that integer or correctly declare that none exist. That's an important problem, for example, for the security of the RSA crypto system or graph isomorphism.
00:22:11.874 - 00:22:36.702, Speaker A: So given two graphs, is one really just sort of a relabeling of the other? So those are pretty natural intermediate problems that people believe are neither polytime solvable nor MP hard. But again, still, this sort of approximate dichotomy is going to cover almost everything that you're going to come across. And more generally, that oversimplified version where MP hardness, what it generally means is it generally means that exponential time is required in the worst case.
00:22:36.702 - 00:22:53.980, Speaker A: Again, there are these subtleties, but I think that's a pretty good one sentence summary to keep in mind as you go forward. So coming up next, let's do an overview of what does the algorithmic toolbox look like for tackling NP hard problems? If one comes up in your own application, what should you do? I'll see you then.
