00:00:00.570 - 00:01:12.050, Speaker A: As we've discussed, to the prisoner's dilemma, it's a real thing. You really do see it in the world where the very strong forces for incentives to take a particular action leads us to an outcome which is worse off for everybody. On the other hand, it's also not hard to look around and see scenarios which seem to have a prisoner's dilemma type structure, but where you don't have this kind of all defection kind of outcome, where people do tend to cooperate. You can even think of something as simple as a transaction between a buyer and a seller, because each party in that transaction kind of has an incentive to rip off the other one, right? So if you're the seller, maybe you'd love to be able to get away with not actually supplying the product, or at least supplying a cheaper, inferior version of the product so that you can make more profit off of it. Meanwhile, on the buyer's side, it's possible you'd like to try to get that item without paying what you said you were going to pay, for example, by writing a check for which your bank does not have sufficient funds to cover it. And while fraud certainly does exist in the world, the vast majority of the transactions, both parties play along. The sellers deliver what they promised they would deliver, and the buyers pay what they promised they would pay.
00:01:12.050 - 00:02:13.990, Speaker A: So how do we explain this? How do we explain the fact that we do see prolonged cooperation in settings with a prisoner's dilemma type incentive structure? I think a natural intuition for why this might be true, why you might see long term cooperation, is because in many of these settings, the short term gains from a defection are outweighed by the long term costs. So, for example, if you're a seller and you rip off a buyer, maybe you accrue a reputation for being a dishonest seller and then you just lose sort of all your business in the long run. So that's a reasonable intuition. But there's still the question of how do we actually model that? And that's a bit of a subtle issue, but that's what we're going to talk about next about the repeated prisoner's Dilemma. So our first attempt at modeling repeated interactions between the same two players in a prisoner's dilemma type setup is we're going to fix some number of stages. So some number N of stages maybe think of N as, say, 100. For concreteness, Alice and Bob are going to play the prisoner's dilemma over and over again, a total of N times.
00:02:13.990 - 00:03:12.760, Speaker A: And the payoffs to Alice and Bob will just add over these 100 times that they play the prisoner's dilemma. And because these are played in sequence, alice and Bob can use the results of the previous interactions if they want. So if you're in stage 17, alice can look at what happened in the first 16 stages and use that information to decide whether she wants to cooperate or defect in stage number 17. Now, one might hope that with this repeated model, because Alice and Bob have to deal with each other for a long period of time, you might hope that now you will see cooperation. There'll be incentives to cooperate at least if the number of stages, if N is a big enough number. On the other hand, if we just apply cold clinical logic to this model, we will see that that is not in fact the case. So to see that, let's think about the very last stage, stage N, stage number 100.
00:03:12.760 - 00:03:46.258, Speaker A: Now, in the last stage, Alice and Bob both know it's the last stage and they know there's no future. So there can't possibly be any future consequences of their actions right now. So we're back in sort of the usual prisoner's dilemma. At stage N, at stage number 100, it's a dominant strategy for each of them to defect. So that's what they're going to do. Now, let's rewind to the previous stage, stage N minus one or stage 99. So here in principle, a player has the opportunity to retaliate against the other player with what happens in stage 99, retaliate in stage 100.
00:03:46.258 - 00:04:28.478, Speaker A: But what did we just argue? We just argue that we know what's going to happen in stage 100, no matter what happens in stage 99 or in the first 98 stages, whatever happens in the first 99 stages, alice and Bob are going to defect in stage 100 because that is a dominant strategy. Well, once you realize that, so now you're at stage 99 and you're like, well, in principle, my action today could affect the other player's action tomorrow. I now know that's not going to be true. The other player is just going to defect tomorrow no matter what I do. Well, if that's the case, I may as well just do what's best for me in stage 99. I can't affect stage 100. And so it's again a dominant strategy for me to defect if I just want to do as well as I can in stage 99.
00:04:28.478 - 00:05:13.290, Speaker A: So given that both players defect in 100, they're also going to defect in 99. You continue this argument and you go all the way back and you find that the only logical strategy for both players to use is to defect in all 100 stages. So that's kind of a bummer kind of pessimistic. Another quick piece of game theory jargon. So this idea where you sort of fast forward to the end of the time horizon and work backward, that's sometimes referred to as backward induction. Now, the takeaway of this exercise is not that cooperation in real world settings is somehow irrational. Rather, the exercise points out that we need a mathematical model different from the most obvious one of a fixed number of N stages.
00:05:13.290 - 00:06:03.230, Speaker A: So, in effect, what we're doing here is we're taking as ground truth the phenomenon that cooperation can be sensible in repeated prisoners dilemma type settings. Because, after all, we see that repeatedly in the real world and we're really trying to reverse engineer a mathematical model consistent with that belief that cooperation can be a reasonable thing to do. So our second idea is going to have a repeated prisoner's dilemma with a random number of stages where Alice and Bob do not know exactly which stage is going to be the last one. Specifically, here's how it's going to work. So stage one, Alice and Bob show up, they play a normal prisoner's dilemma. Whatever happens, happens after stage one. Think of a nature or some third party as flipping a biased coin that comes up heads with 5% probability, one out of 20 times.
00:06:03.230 - 00:06:41.062, Speaker A: If the coin comes up heads, the game stops. Alice and Bob never play again. Stage one was the last stage, 95% chance the coin comes up tails and then Alice and Bob will play again in stage two, whatever happens happens. After stage two, we flip this exact same coin again, 5% probability. Stage two is the last 195 percent probability, it continues on to a third stage and so on. So in principle, this game could go on forever if you keep flipping tails over and over again. But really you're going to stop after some random number of stages, right? So with the example of a 5% stopping probability, you'd expect around roughly 20 stages.
00:06:41.062 - 00:07:24.200, Speaker A: But sometimes it'll be ten stages, sometimes it'll be 30 stages. It'll be some random number of stages. As in the first model, the payoffs to the players just add over the various stages and we assume that each player wants to maximize the sum of their payoffs. So here we actually now have randomness, there's this random chance of the game ending. So really we want players to maximize their expected payoff, I mean their average payoff averaging over the possible outcomes of these coin flips. So this is a very easy to swallow Tweak to the model, right? If you think about real world settings where people interact over and over again over time, you know, it is sort of true that they don't know exactly which interaction is going to be the last one. That is kind of a random thing if you just think about our real life experience.
00:07:24.200 - 00:08:07.240, Speaker A: So it's pretty easy to swallow Tweak to the model. And the good news is that this Tweak actually fixes the problem. So what we're going to see on the next slide is that actually cooperation can be a sensible strategy in the repeated prisoner's dilemma when you have a random number of stages. The way we're going to argue this is we're going to look at a specific, very famous and also very sort of appealing strategy known as the tit for tat strategy. And we'll see that the tit for tat strategy encourages cooperation in the repeated prisoner's dilemma. And then on the final slide, we'll see how the Tit for Tat strategy has actually influenced development of the BitTorrent peer to peer file transfer system.
