00:00:00.170 - 00:00:12.142, Speaker A: Pick up where we left off on Wednesday. That was a long time ago. What were we doing? We were trying to prove senses in which clustering grouping points meaningfully is hard only when it only when it doesn't matter.
00:00:12.142 - 00:00:20.718, Speaker A: Meaning when it matters, it's easy. Right? That's the contrapositive. What does that mean? Well, by it mattering, it means there's sort of some clustering, some meaningful groups that is there to be found.
00:00:20.718 - 00:00:34.694, Speaker A: And maybe if we really articulate that assumption precisely, it will imply sure about the input, which we can then actually provably make use of in an algorithm. Okay, so we executed that on Wednesday, and we're going to do it again today. Okay.
00:00:34.694 - 00:00:46.342, Speaker A: So it's going to be a different model, it's going to be a different algorithm, and I'll explain in a second why we might want to do this exercise twice. There's a lot of different reasons. We're going to study the exact same clustering problem even as Wednesday.
00:00:46.342 - 00:01:00.762, Speaker A: Although let me say now, almost everything we say today carries over to other popular clustering objectives as well, like k means. But I just want to keep things focused. We're going to keep talking about the K median problem to remind you the input is a metric space finite.
00:01:00.762 - 00:01:08.078, Speaker A: So N is the number of points x. What does it mean? It's a metric. Well, the biggest thing is that D, this distance function, satisfies the triangle inequality.
00:01:08.078 - 00:01:14.110, Speaker A: So we use that over and over again on Wednesday. We're going to use it over and over again today. Okay, so D satisfies the triangle inequality.
00:01:14.110 - 00:01:22.418, Speaker A: Shortest distance between two points is a straight line. Intermediate destinations only makes it longer. What's the objective function? Your responsibility is to choose k centers.
00:01:22.418 - 00:01:41.898, Speaker A: So K out of the end points, and then what you think of is you think of every point in the metric space being assigned to its closest center, and you want to minimize the sum of everybody's nearest neighbor distances. Okay, so sum over the end points for each point x, you look at which of the K centers is closest to it, you look at that shortest distance, and you sum up those N numbers. Okay, so that's what we want to do.
00:01:41.898 - 00:02:06.478, Speaker A: I want to choose the K points to make that as small as possible. So I was doing this Wednesday, but let me just sort of remind you, I'm going to be a little sloppy about talking about clusters or clusterings and talking about a choice of centers. Okay, so in the K median problem, if we choose centers, that obviously induces clusters according to who gets assigned to which center when we assign them to their shortest centers.
00:02:06.478 - 00:02:20.054, Speaker A: Okay, so a cluster is just around center I is just the points for whom center I is the closest. Similarly, if I give you clusters, you can easily just pick which the best center separately for each cluster. Okay, so you can go from clusters to centers and back.
00:02:20.054 - 00:02:35.360, Speaker A: So I'm just going to be sloppy about that. Okay, so let me tell you about the restriction on input, the stability notion that we're going to be doing today. And it shares some of the spirit of Wednesdays, but it comes from a slightly different motivation and it is technically different.
00:02:35.360 - 00:02:51.780, Speaker A: So here's what it is. So we're just going to assume, first of all, that our target clustering, what we want is the numerically optimal solution to this objective function. Okay? That's the first thing.
00:02:51.780 - 00:03:04.646, Speaker A: So last time I made a distinction between a target clustering and the one which happens to optimize the objective function. Here, we're definitely going to assume they're the same. And actually we're going to assume something much stronger than that.
00:03:04.646 - 00:03:32.694, Speaker A: We're going to assume that the target clustering and call its clusters c star one up to c star k remain optimal under many different perturbations of the input. Remember, the input is really this distance function d. So not only is it optimal for the distance function d, but also for many other nearby distance functions.
00:03:32.694 - 00:03:56.390, Speaker A: So precisely under any what I'm going to call a gamma perturbation. And in general, I'm going to call this notion gamma stability. And by gamma perturbation, I just mean for each real distance DXY.
00:03:56.390 - 00:04:27.138, Speaker A: For the purposes of thinking about whether this clustering remains optimal or not, you're allowed to perturb the distances where here sigma is some number between one and gamma. Okay? So if gamma is three, for example, when the gamma perturbation for each of the n choose two distances, you're allowed to independently make a choice of how much to blow the distance up. I'm not going to allow you to shrink it, but you can blow it up.
00:04:27.138 - 00:04:32.958, Speaker A: You can blow it up. By most a factor of gamma, think of gamma equal three. Okay, one comment.
00:04:32.958 - 00:04:43.010, Speaker A: So for the original input, we're assuming it's a metric space. So d satisfies the triangle inequality. After you do some perturbations, it might not satisfy the triangle inequality.
00:04:43.010 - 00:04:49.494, Speaker A: Indeed, just think about a triangle one, one, one. If you make one of those a three, all of a sudden it's not satisfied anymore. That's okay.
00:04:49.494 - 00:05:02.986, Speaker A: Okay, we're going to allow that. Okay, so we insist that opt remains optimal even under gamma perturbations that do not result in a metric distance function here. So that's an assumption.
00:05:02.986 - 00:05:22.254, Speaker A: So an instance, a k median instance which satisfies this property is called gamma stable. So as we take gamma bigger and bigger, does this constraint get more or less stringent, more or less demanding, more demanding? More demanding, right. So we're asking more from the instance.
00:05:22.254 - 00:05:40.422, Speaker A: So in other words, fewer instances will satisfy this property as we take gamma higher and higher. So the computational problem is easier and easier for bigger gamma. Okay? So what we're wondering is if there's some magic value of gamma that's sufficiently large that we can actually solve these problems in polynomial time and that's what we're going to quantify today.
00:05:40.422 - 00:05:53.870, Speaker A: Okay, look at the gamma, which is sufficiently large so that k median goes from NP hard, which we know it is in general, to polytime solvable. So that's the technical problem. Let's talk a little bit about motivation.
00:05:53.870 - 00:06:24.658, Speaker A: So I mean, just like Wednesday, the authors were intending to articulate assumption, which is generally implicit when you run clustering algorithms and then make it precise and ask if taking that assumption literally results in easier instances. But they're doing it for a slightly different implicit assumption than on Wednesday, so we're getting a slightly different mathematical definition. So on Wednesday we had this c epsilon notion of stability.
00:06:24.658 - 00:06:45.082, Speaker A: And there the implicit assumption was that if you ran a good approximation algorithm, a c approximation algorithm, you'd be happy with the result in the sense that it was almost the target clustering. So that resulted in this formal definition saying numerical approximation of the objective function better imply structural approximation of the target clustering. Okay? So that was the implicit assumption we were formalizing on Wednesday.
00:06:45.082 - 00:07:06.230, Speaker A: Here, it's a different one here really, the motivation is, well, think about these k median problems. They're specified by this distance function, d. Where does D come from? Okay, sometimes maybe you're literally thinking about points in three space or n space, and Euclidean distance is exactly meaningful.
00:07:06.230 - 00:07:31.546, Speaker A: Often in say, unsupervised learning applications, that's not what D is, okay? X and y are not points in space, x and y are images or documents or proteins. And there isn't some canonical, obviously correct way to assign distance or dissimilarity between two objects. You probably came up with a formula or an algorithm, which when you run it on two images or two proteins or two documents, gives you a number.
00:07:31.546 - 00:07:55.154, Speaker A: But there's aspects of that algorithm that are going to be ad hoc, that you're not fully confident about. You can imagine a slightly different algorithm being just as meaningful. Okay, so if you have one of these heuristically defined distance functions and then you're solving the induced clustering problem task, it in your mind for this to be a worthwhile exercise is the idea that the answer to this clustering problem shouldn't be highly dependent on exactly what the distance function is.
00:07:55.154 - 00:08:11.110, Speaker A: So if you tweak, if you hard code one parameter and your distance heuristic function slightly differently and you start getting one point ones instead of 1.5s, hopefully you get basically the same clustering back. So because D is heuristic in these applications, you want the answer to be robust, to perturbations in D.
00:08:11.110 - 00:08:18.586, Speaker A: Right? So that's where this comes from. Okay? In clustering problems you really want to solve that's. Often what you're sort of hoping is true.
00:08:18.586 - 00:08:43.170, Speaker A: And again, in the spirit of Wednesday, they're saying let's really write that down as a mathematical definition, look at the instances that conform to it and study the complexity of that special class of instances. We assume that the exactly the same clustering is still optimized? Good question. So I'm going to spend the lecture on the simplest version of this definition, where unlike Wednesday, where we allowed a few points to be classified incorrectly, today I'm going to ask you get every single point correctly.
00:08:43.170 - 00:09:07.446, Speaker A: There is a recent work which relaxes this in the way you'd expect given what you saw Wednesday and says, well, you move the DS by gamma and it only changes on an epsilon fraction of the points. Similar statements are true via similar algorithms, similar arguments, all of it's more complicated than what I'm going to talk about today. But the algorithms are in a similar spirit, the proofs are in a similar spirit, the constants are worse.
00:09:07.446 - 00:09:21.706, Speaker A: And you need a big clusters assumption like Wednesday, which we won't need today. This assumption is extremely stringent. It is stringent, it is stringent, but as we'll see, it doesn't trivialize the problem, okay? Especially as you think about varying gamma.
00:09:21.706 - 00:09:39.954, Speaker A: There's actually going to be interesting regimes under which the problem is sort of trivial, solvable, but not trivial, and then unsolvable, meaning NP hard. So gamma gives us a nice sort of parameter to play with. How much stability you need to imply tractability, but absolutely, I mean, epsilon equals zero, if you like.
00:09:39.954 - 00:09:59.500, Speaker A: Assumption is a strong one, but that's going to make it so I can make all of the points I want to make in this lecture as cleanly as possible. And there is work without that assumption. Okay, so motivation, distance, function is often heuristic and so you want robustness with respect to it.
00:09:59.500 - 00:10:13.780, Speaker A: Okay. So you're thinking about that being sort of that's a property you're really hoping your clustering problems possess. Okay, bless you.
00:10:13.780 - 00:10:32.438, Speaker A: Good. All right, so why are we doing this twice? Why do we have a second lecture doing kind of the same thing as Wednesday? Well, there's a few reasons. So one reason is just that this is sort of in every respect a bit different.
00:10:32.438 - 00:10:46.010, Speaker A: Okay. I really think this is a distinct, implicit assumption people make when they do clustering. And so we get a different formal definition, we're going to talk about different algorithms and we'll see some similar things in the analysis, but the proofs aren't the same either, so we'll see a host of new ideas.
00:10:46.010 - 00:10:59.386, Speaker A: The second reason is that just out of academic honesty, I want you to be aware that Wednesday's notion is not the only notion of stable clustering. Actually, this has been a super hot area for about five years. There's at least five different candidates.
00:10:59.386 - 00:11:10.370, Speaker A: They all have roughly the same spirit. And you get roughly these same conclusions about strong enough stability implies tractability, and I wanted to show you that at least there isn't just one. Okay? There's a couple.
00:11:10.370 - 00:11:16.870, Speaker A: All right. And on the homework you'll explore the relationships between some of these. So that's another reason, just so you have some awareness of that diversity.
00:11:16.870 - 00:11:44.266, Speaker A: Another thing which is kind of cool is that even though this mathematical definition is different than Wednesday, we'll see that it winds up imposing similar types of structural rigidity on the optimal solution. It's again going to basically imply, and you'll see this in the proofs, it'll imply that in stable instances for big enough gamma opt basically has to be a bunch of tightly knit, well separated clusters, maybe modulo a few outliers. That's basically what we proved in the analysis.
00:11:44.266 - 00:12:05.250, Speaker A: On Wednesday, we'll see all those themes reoccur. So it's kind of cool seeing this different definition lead us to kind of the same point. And so relatedly, another reason why I want to do this twice is because both of the definitions I'm showing you, plus the other three or four definitions out there in the literature, you look at any one of those definitions and you're probably not going to literally believe any one of them.
00:12:05.250 - 00:12:21.842, Speaker A: Exactly. Okay, you assume real data might sort of satisfy it, but strictly speaking will definitely violate some of these definitions, at least for the parameters that we're talking about. So if you're trying to reason mathematically about some phenomena like say, clustering, seems easy in practice.
00:12:21.842 - 00:12:42.526, Speaker A: Obviously the Holy Grail would be you just write down this mathematical model that's uncontroversial, everybody's like, oh yeah, you totally nailed what this is really about. And then you prove some awesome theorem that just like exactly matches what people observe, okay, that almost never happens once in a while, once in a blue moon, but that's just not how it works in mathematical modeling. So you have to make some compromises, some overly strong assumptions to prove things.
00:12:42.526 - 00:13:07.758, Speaker A: And so then you get nervous. You're like, well, is our conclusions an artifact of our assumptions? Are they actually sort of uncovering something real? So in lieu of one perfect model, a nice alternative is to have many models that are imperfect in different ways, all of which seem to be leading you toward the exact same conclusion. Okay? Then you start having even though each model is highly questionable, the overall message from the suite of models starts looking believable.
00:13:07.758 - 00:13:20.874, Speaker A: You start amassing some confidence in what these models are saying. Like clustering is easy in many instances of interest. Okay? And so that's sort of another takeaway of these couple of lectures.
00:13:20.874 - 00:13:42.062, Speaker A: Okay, we'll get basically similar messages, takeaway messages from two different models that reinforces our confidence in both of the models and their predictions. One final reason, this is a fun one to do on top of the previous one. So last time showcased how making explicit an assumption in the data can lead you to exploited an algorithm.
00:13:42.062 - 00:13:56.706, Speaker A: So we came up with an algorithm on Wednesday we really never would have designed had we not written down that stability definition. Probably the high level ideas were pretty nice. Distance thresholding and looking for number of common neighbors, those are kind of high level ideas.
00:13:56.706 - 00:14:51.274, Speaker A: But the exact algorithm was definitely driven by the stability condition. Today, by contrast, we're going to show that a very nice novel twist on a very simple, very well known clustering algorithm actually does get polytime solvability results for stable cable median instances. Okay? So here we're going to use this definition more just in the analysis of a known algorithm as opposed to drive new algorithms, all right? So any questions before we begin the technical development for today's? Let me just segue from that into, you know, all right? So if we wanted to explain the performance of some simple, well known clustering algorithm, okay, what's a simple, well known clustering algorithm? So let me tell you about one.
00:14:51.274 - 00:15:14.510, Speaker A: You might have seen this. I usually teach this in 161. It's called single link clustering, which might well win popularity or at least kind of name recognition contest in clustering contest.
00:15:14.510 - 00:15:22.980, Speaker A: So how does it work? It's really nice, actually. Remember, we're doing like k median or something like that. So you give me a metric space and you tell me k.
00:15:22.980 - 00:15:36.946, Speaker A: So this metric space, I want to think of it as a graph, okay? So the nodes are just the endpoints, and the distances are just edge weights. So it's a complete graph, a clique weighted. The distances are the edge weights.
00:15:36.946 - 00:15:47.130, Speaker A: Then what I'm going to do is I'm just going to run Kruskal's minimum spanning tree algorithm. So hopefully you remember minimum spanning trees. Maybe you forget which one's cruscal, which one's prim.
00:15:47.130 - 00:15:59.690, Speaker A: Kruskal is the one where you sort first from smallest to biggest. You do a single pass through the edges. And if an edge is going to create a cycle with what you've already taken, of course you don't take it because you want a tree.
00:15:59.690 - 00:16:10.302, Speaker A: But if it's not going to create a cycle, then you include it and it fuses two connected components into one. Normally in crust goes algorithm, you want an MST. So you just run it until you have a tree, till you take N minus one edges.
00:16:10.302 - 00:16:18.360, Speaker A: Here we don't want a tree. We want k clusters. So we're just going to stop it once we have K connected components rather than just one.
00:16:18.360 - 00:16:30.090, Speaker A: So that's single and clustering. So run. Kruskal's MST algorithm.
00:16:30.090 - 00:17:01.122, Speaker A: I'm just going to write on the metric space when I really mean on the complete graph with distances as edge weights stop when K connected components. Okay? So remember, when you're including these edges one at a time, initially you have N connected components, just the isolated vertices. Each new edge replaces two connected components with one, so it drops the number of connected components by exactly one.
00:17:01.122 - 00:17:16.454, Speaker A: So each edge addition of Krusko's algorithm decrements the number of connected components at some unique point, we hit k. All right, questions about the algorithm. It's clear what it is.
00:17:16.454 - 00:17:35.774, Speaker A: Could I code this up in ten lines of Python right now or you have in the past? Probably. Okay, so if we wanted to prove that a simple clustering algorithm works well in stable instances. Seems like the obvious thing to try to prove would just be to say, well, as long as gamma is sufficiently big.
00:17:35.774 - 00:17:48.990, Speaker A: Okay, we don't really have a feel yet for what gamma needs to be, but it's easier if gamma is bigger. So somehow for some magical value of gamma sufficiently large. Just run single link clustering, right? That's going to give you K clusters.
00:17:48.990 - 00:18:03.110, Speaker A: As we discussed, given the clusters, you can just separately for each one, figure out which the best center is, and then you're just hoping that's going to boom be the optimal K median solution. Okay. That would be kind of like the simplest possible thing that could be true, basically about clustering stable instances.
00:18:03.110 - 00:18:13.350, Speaker A: All right. Well, it's not going to be quite so simple. Let me show you an example which shows we're not quite off the hook.
00:18:13.350 - 00:18:23.100, Speaker A: So suppose K equals three. So we want three clusters and M is going to be a parameter and it's going to be big going.
