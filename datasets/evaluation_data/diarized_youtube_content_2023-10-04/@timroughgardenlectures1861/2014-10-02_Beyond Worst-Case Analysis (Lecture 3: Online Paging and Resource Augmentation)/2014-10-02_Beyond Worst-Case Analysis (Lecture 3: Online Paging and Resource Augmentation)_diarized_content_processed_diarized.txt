00:00:00.170 - 00:00:18.986, Speaker A: As far as what's the plan for today, I'm going to just open with a few comments, making sure we're all clear about the big picture. And then three of the next four lectures are about the online Paging or Caching problem, which we talked about briefly in the very first lecture. So the problem with traditional worst case analysis for online Paging is twofold.
00:00:18.986 - 00:00:33.110, Speaker A: So first of all, it doesn't give very accurate performance predictions. Second of all, it doesn't meaningfully differentiate between different solutions. So today we're going to talk about two techniques, resource augmentation and loose competitiveness, that really focus on the former issue.
00:00:33.110 - 00:00:59.914, Speaker A: So these are really different ways of analyzing Paging algorithms that give you more meaningful worst case guarantees. And then the other two lectures about Paging will be more trying to model data more carefully so that we get sharper distinctions between different algorithms like LRU and FIFO. Okay, so starting with this lecture, in particular these two topics, I'm going to start throwing different ways of analyzing algorithms at you at a pretty rapid clip, and that's going to persist throughout the whole quarter.
00:00:59.914 - 00:01:28.040, Speaker A: So periodically, I just kind of want to stop and make sure we don't lose the forest for the trees and just help you think about how you might organize all of the different ideas you're going to be seeing in this class. So first, let me just make kind of a comparison, a sort of metaphor, with how many people, including myself, teach undergraduate algorithms. So undergraduate algorithms I think of as trying to equip you with a toolbox primarily for algorithm design.
00:01:28.040 - 00:01:45.806, Speaker A: Now, a class like CS 161, it's not solely about design. You learn some things about analysis, like the language of Asymptotic analysis. But really, I think most people, when they think about that, you think things like greedy algorithms, divide and conquer, dynamic programming, that kind of stuff.
00:01:45.806 - 00:01:58.082, Speaker A: So these sort of high level paradigms for algorithm design, and you learn several. And what you learn is that there's no one silver bullet. So no algorithmic technique is going to solve every computational problem that you come across.
00:01:58.082 - 00:02:13.750, Speaker A: You really need a toolbox, and frankly, it's a bit of an art to know exactly when, which one you should apply. But we give you practice throughout the lectures and through the exercises, and you come away with a class, hopefully with at least some initial expertise on that point. So that's the first thing.
00:02:13.750 - 00:02:38.986, Speaker A: The second thing is the way that we teach these fundamental techniques and algorithm design, we try to kill two birds with 1 st. So on the one hand, we give you lots of examples of these techniques in action, but we also pick examples that are famous and fundamental and useful in their own right. So you both learn about sort of the general paradigm, and you also learn about specific algorithms that fit in that paradigm, which any card carrying computer scientist would want to know right so we do divide and conquer.
00:02:38.986 - 00:02:50.402, Speaker A: We talk about sorting, merge, sort quick sorting, two instantiations greedy algorithms. We talk about minimum spanning treaties, prim and crustchool and so on, dynamic programming. We talk about bellman, floyd, floyd warsall and so on.
00:02:50.402 - 00:03:08.460, Speaker A: And there's really a very strong analogy to this class. C is 264. But rather than really focusing on how to design algorithms, we're really trying to understand how should you analyze algorithms so that you get meaningful information about them and meaningful comparisons between them.
00:03:08.460 - 00:03:16.970, Speaker A: So you might want to think about this class as a toolbox for algorithm analysis.
00:03:20.590 - 00:03:21.340, Speaker B: Okay?
00:03:22.430 - 00:03:32.234, Speaker A: And in undergraduate algorithms, again, you don't see very many perspectives on analysis of algorithms. You almost only see worst case analysis. Maybe you see a little average case analysis.
00:03:32.234 - 00:03:36.666, Speaker A: For example, when you'd studied hashing in this class, you're going to see lots.
00:03:36.778 - 00:03:37.294, Speaker B: Okay.
00:03:37.412 - 00:03:57.506, Speaker A: And again, it will not be the case that any one of these methods of analyzing algorithms is going to always be the right way for a problem that you encounter, but rather it's to equip you with many ways of making progress. On understanding algorithms. Again, I would say even among kind of people who do this for a living, I don't think there's a very good understanding of when you should use which analysis framework.
00:03:57.506 - 00:04:07.530, Speaker A: We're kind of still figuring that out, even in the research community. So it's a bit of an art to figure out which way to analyze which algorithm. But at least this way you'll understand that there are a lot of different ways of doing it.
00:04:07.530 - 00:04:26.340, Speaker A: And again, we'll study them through fundamental examples that are interesting in their own right. So things like the paging problem, linear programming, and the clustering problem that we all talked briefly about on Monday. So that's kind of big picture, how you might want to sort of slot this class into your overall curriculum and computer science education.
00:04:26.340 - 00:04:39.462, Speaker A: All right, so any questions about the preamble? Otherwise, I want to start our discussion about paging. I think I was calling it caching on Monday. I mean exactly the same thing.
00:04:39.462 - 00:04:46.830, Speaker A: Paging, caching. I'll use them synonymously for this class. Okay, good.
00:04:46.830 - 00:05:04.244, Speaker A: This is a real world problem, so it really would be nice to have accurate theory around this problem. But it's simple enough that allows us to showcase sort of many different attempts at how to meaningfully think about algorithms. That's why, in addition to being real, it's a great one.
00:05:04.244 - 00:05:16.548, Speaker A: Pedagogically. Now I should say, every time I teach, this word online is more and more of an anachronism. So there's this field called online algorithms, which some of you may have heard of.
00:05:16.548 - 00:05:29.988, Speaker A: It was invented in the mid 80s, so the internet did exist in the mid 80s, but it was only in a very embryonic form. So to the person on the street, the word online didn't really have much meaning. So now this is a strange phrase.
00:05:29.988 - 00:05:36.524, Speaker A: So online here means data arrives over time. You have to make a choice when data comes in irrevocably without knowing the future. Right.
00:05:36.524 - 00:05:45.744, Speaker A: Just like the Paging problem we talked about on Monday. So probably the field should have a different name now, but this is still what people call it. Yeah.
00:05:45.744 - 00:06:02.980, Speaker A: And so I'm going to start where the field of online algorithm started, namely a paper of Slater and Tarjan, like I said, from the mid eighty s. And there's a lot of cool results in this paper. I'm only going to cover the results, which are kind of important for the narrative I want to tell here.
00:06:02.980 - 00:06:11.530, Speaker A: And those aren't necessarily the strongest results in the paper. So apologies to Slater and Targent. I'm sort of going to focus on some of the least satisfying results in their paper.
00:06:11.530 - 00:06:25.388, Speaker A: So let me just remind you quickly, the paging or caching problem we talked about Monday, so you have a cache so that's the small fast memory, it can hold some number of pages. We're going to call it K. And there's something requesting pages over time.
00:06:25.388 - 00:06:34.832, Speaker A: If something's already in the cache, no problem, they can just access it directly. Whenever a requested page is not in the cache, you have to bring it into the cache and you have to evict something else. And that's the algorithmic problem.
00:06:34.832 - 00:06:50.592, Speaker A: So that's the problem of designing the cache policy, what to evict on a page fault or on a cache miss. And so the performance measure that we're interested in for the Paging problem is what you think it would be. So here Z is an input, that is a sequence of page requests.
00:06:50.592 - 00:07:12.056, Speaker A: A is some online algorithm, and we just count the number of page faults or cache misses that the algorithm incurs on Z. Okay, so the formalism is identical to on Wednesday, but the semantics have changed. Right? Wednesday we were talking about the running time of algorithms.
00:07:12.056 - 00:07:16.492, Speaker A: That is not what we are talking about now. We're talking about totally. The units are completely different.
00:07:16.546 - 00:07:16.716, Speaker B: Okay?
00:07:16.738 - 00:07:26.400, Speaker A: We're not measuring running time. We're just counting number of page faults that could be related to the running time of some application. But we're not modeling that, just counting page faults.
00:07:26.400 - 00:07:44.576, Speaker A: All right, so what I want to do is first introduce you to sort of the traditional, the dominant paradigm that you analyze online algorithms. We're going to see what happens. We're going to look at some respects in which it leaves us wanting, and then that will motivate two sort of follow ons alternative ideas to the traditional approach.
00:07:44.576 - 00:08:06.860, Speaker A: But first I need to tell you kind of the traditional approach. So a key definition here, which if you've ever studied online algorithms you've seen before, is the notion of the competitive ratio of an online algorithm. Okay? So this is an important definition for this lecture.
00:08:06.860 - 00:08:21.916, Speaker A: So we look at the cost. So if we given online algorithm A for an input Z, we'll get in this case, how many page faults this algorithm a incurs. So this is our protagonist.
00:08:21.916 - 00:08:39.524, Speaker A: If this is something like LRU, we're trying to prove this is small. Okay, small relative to what? Well, small relative the best you could have done in hindsight. Okay, so given that the input is z, we look at the optimal algorithm for that input.
00:08:39.524 - 00:08:53.196, Speaker A: Okay? So basically we look at what is the smallest possible cost that any algorithm could have achieved on Z. Now, this is the paging problem. So as we discussed Monday, sometimes we're not even going to know what this is.
00:08:53.196 - 00:09:09.772, Speaker A: Sometimes this is an unknowable object. But for the paging algorithm, we know what this is. So let me emphasize, we're going to compare ourselves to a very tough competitor, namely the best offline algorithm, okay? An algorithm which actually does know the entire future sequence of requests and can act accordingly.
00:09:09.772 - 00:09:30.488, Speaker A: In the paging problem, we know exactly what that algorithm is. That's that greedy furthest in the future algorithm, okay? So that's the denominator we look at how many page faults does say LRU have? How many page faults could we have hypothetically obtained with clairvoyant knowledge of the future running the furthest in the future algorithm? And we look at the ratio. The ratio, of course, is always at least one.
00:09:30.488 - 00:09:35.192, Speaker A: You can't do better than opt. You can only do worse. The closer this is to one, the better.
00:09:35.192 - 00:09:51.128, Speaker A: We're sort of simulating the optimal offline algorithm. Now, there's also this Z here, which is unspecified, and the traditional approach is worst case analysis. So you look at how big this ratio can ever get over all inputs.
00:09:51.128 - 00:10:05.152, Speaker A: Z use a different color for this. So that's the full definition of the competitive ratio.
00:10:05.296 - 00:10:06.036, Speaker B: Okay?
00:10:06.218 - 00:10:21.096, Speaker A: So it summarizes the performance of an algorithm with a single number. This is the number, okay? And the smaller the number, the better. Any questions about that? Yeah, what happens if you have an.
00:10:21.118 - 00:10:24.216, Speaker B: Optimal algorithm that makes no phase code?
00:10:24.398 - 00:10:44.088, Speaker A: Yeah, so the denominator is zero. Well, so let's define it as infinity then. So in particular for this, what the question is pointing out is that basically, if you have a finite competitive ratio, that implies in particular that whenever there exists a way of having no page faults, your algorithm also has no page faults.
00:10:44.088 - 00:11:02.928, Speaker A: Now, if you think about it, in the paging problem, this isn't a big issue. If you think about it for 30 seconds, you realize that the only way you're never going to have any page faults is if at most K pages, distinct pages, are ever requested for a cache size K. And so that means any paging algorithm, which doesn't just for no good reason evict stuff, is going to also have no page faults.
00:11:02.928 - 00:11:26.364, Speaker A: But you're right, it's almost like a sanity check of like, does this definition even make sense for a given problem? Can you at least get zero when it's possible to get zero? But in this problem, you can, so that's a fair point. Okay, good. Now, just to remind you, okay, so the pros and cons of sort of so grossly compressing the entire performance profile of an algorithm down to one number.
00:11:26.364 - 00:11:32.476, Speaker A: So the con, obviously, is that you just lose tremendous information about the performance of this algorithm across all the different inputs.
00:11:32.508 - 00:11:32.704, Speaker B: Okay?
00:11:32.742 - 00:11:50.640, Speaker A: You just summarize it with one number. The good news is, now you have maybe not uncontroversial, but certainly an unequivocal ordinal ranking of algorithms. And traditionally in this field, one declares an online algorithm to be better than another one if and only if it has a smaller competitive ratio.
00:11:50.640 - 00:11:59.728, Speaker A: There's an unequivocal notion of the optimal online algorithm that's just an online algorithm with the smallest possible competitive ratio of any online algorithm.
00:11:59.824 - 00:12:00.470, Speaker B: Okay?
00:12:01.000 - 00:12:11.690, Speaker A: So that's traditionally how research is done in algorithm analysis. So let's see what oh, so one other point. Let me just try to throughout the course, I'll try to make connections wherever they exist between all the.
