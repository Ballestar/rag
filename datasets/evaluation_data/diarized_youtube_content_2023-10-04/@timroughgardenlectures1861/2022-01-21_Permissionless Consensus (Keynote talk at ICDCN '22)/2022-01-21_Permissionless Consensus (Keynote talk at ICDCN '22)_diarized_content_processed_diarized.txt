00:00:00.730 - 00:00:30.370, Speaker A: Hi, everyone. My name is Tim Roughgarden. I'm a professor at Columbia University. Thanks for checking out this talk, which is about permissionless consensus, both possibility results and impossibility results. This is a talk I first wrote for and presented at the Icdcn conference in at the beginning of 2022. Now, I'm sure some of you out there know quite a bit about distributed computing. Sure, some of you out there know quite a bit about blockchains, which is also going to be the focus of this lecture.
00:00:30.370 - 00:01:21.330, Speaker A: However, for this talk, I want to assume very little background about either of those. So for the first, probably more than 50% of this talk, I'm going to be focusing on background, context, basic definitions, basic examples around both permissioned and permissionless consensus. Toward the end of the talk in the second half, I will be getting to a number of new results delineating what you can and cannot do on basic permissionless consensus problems. And when we get to that part, when we get to the novel results, all of that is going to be joint work with Andy Lewis Pye, who's a professor at the London School of Economics. So let's get started. So the title of the talk is Permissionless Consensus. What do I mean by each of those two words? Well, let's start by talking about consensus.
00:01:21.330 - 00:01:50.590, Speaker A: I'm still going to be pretty informal and high level on this slide. I'll get more specific about the exact consensus problem that I want to study on the next slide. So informally, the goal in consensus is you just want to keep a bunch of machines, right? So these are computers, maybe think of them as scattered all over the globe and connected by the Internet. You want to keep a bunch of scattered machines in sync. And you want to do this even in the face of two types of challenges. Number one, network failures and outages. And number two, malicious attacks.
00:01:50.590 - 00:02:30.220, Speaker A: So let's drill down on what the different phrases in this informal statement mean. So what do I mean keeping machines in sync? Well, the answer actually depends a little bit on which consensus problem we want to focus on. So there's single shot consensus problems where the machines running the protocol eventually conclude with an output. And then consensus means that all of the machines agree on the output when they all terminate. What's actually more relevant for a blockchain context is sort of multi shot consensus, where you're coming to consensus over and over again. So here all of the machines are basically maintaining a running log of events that have transpired. And you would like all of the nodes to stay in sync on that running log.
00:02:30.220 - 00:03:10.082, Speaker A: You'd like all of them to be maintaining the exact same ordered sequence of things that have happened. So, for example, you might have heard of, say, the Byzantine agreement problem or maybe Byzantine broadcast. Those would be examples of the first type single shot consensus where all of the nodes should terminate with a common input. And we're going to be much more focused on multi shock consensus because that's really what makes sense in a blockchain context. And there we're going to look at a problem known as State Machine replication or SMR. I'll define that formally on the next slide. So what do I mean by a network outage? Well, imagine that these machines are communicating to each other over the Internet.
00:03:10.082 - 00:03:57.698, Speaker A: Of course, if you're having a good day on the internet, messages will arrive in a reasonable amount of time. But as we all know, sometimes you have failures in the internet. Messages can be delayed considerably longer. Sometimes you even might suffer from something like a denial of service attack where a malicious actor actually prevents communication between different machines. So speaking of malicious actors and attacks in addition to something like a denial of service attack which is really an attack on the network, we also probably want to think about attacks on the protocol itself, especially if we're looking toward blockchain protocols, many of which secure literally billions of dollars. So maybe we have a protocol being run by say 1000 nodes. We want to be robust to the fact that some fraction of those nodes may be trying to mess up the protocol.
00:03:57.698 - 00:04:47.080, Speaker A: So maybe say 300 out of the 1000 nodes are actually running incorrect or buggy or malicious versions of the protocol with the goal of keeping the other 700 nodes out of sync. Obviously the more of the nodes are broken, the more of the nodes are malicious, the harder it's going to be to guarantee consensus. Now let's get a little more precise about the exact consensus problem that I want to look at. It's a classic one known as state Machine replication or SMR. And remember, this is the multi shot consensus problem where each of the nodes is maintaining a running sequence of events in a blockchain context. That would be transactions like say, cryptocurrency payments or smart contract function calls. So all of the nodes should be staying in sync on the ordered sequence of transactions that have been executed thus far.
00:04:47.080 - 00:05:21.810, Speaker A: So formally in the definition of this problem there's going to be a notion of nodes. These are machines that are actually running the protocol. You can think of those as like full nodes running say, Bitcoin or ethereum. And there's also going to be a notion of clients who don't really run the protocol. They merely submit transactions to the nodes who are running the protocol. So you should think of clients as being like random sort of ethereum users, for example. So you can imagine there being dozens or hundreds or thousands of nodes and potentially millions or even tens of millions of clients.
00:05:21.810 - 00:05:57.034, Speaker A: The clients really don't have any responsibilities. They just submit transactions that they want to see executed. The nodes running the protocol do have a responsibility. They're the ones who are supposed to stay in sync and each of those nodes is going to be maintaining locally and append only data structure. So you're just going to keep tacking on at the end the latest things that have happened. So that's the setup. Now what is it we would want of a protocol? What would it mean to solve the SMR problem? Well, the whole point is to keep machines in sync and each machine has its own local appendonly data structure.
00:05:57.034 - 00:07:06.850, Speaker A: So presumably what we want is consistency, meaning that the data structure should have exactly the same contents across all of the nodes. Okay, maybe that's like a little strong because there are going to be sort of delays in sort of message delivery, right? Maybe one of the nodes is sort of stuck off in Siberia and here's about everything 10 seconds later than everybody else. So it's okay if two machines have different data structures as long as one of them is a prefix of the other, just waiting to catch up with the latest events. Now, we would not have an interesting problem if all we required was consistency, that all the data structures match because there's always the protocol which does nothing, where no node ever appends anything to their data structure, then they trivially stay in sync forever. So for this to be nontrivial, we want to say that when there is work to be done, that work actually does get done. So this is the liveness criterion which says that if a client submits a transaction to the nodes running the protocol, eventually assuming it's sort of a valid transaction, it should be appended to everybody's local data structure. And so that then is the big question in the state machine replication problem.
00:07:06.850 - 00:08:30.334, Speaker A: Does there or does there not exist a protocol which guarantees both of these properties, guarantees consistency, guarantees liveness? Now, the answer to that is going to depend in an interesting way on different assumptions that we might make. So actually, really the question is under what sets of assumptions is it possible to solve consensus in this sense to have an SMR protocol guaranteeing both consistency and liveness? So that's the state machine replication or SMR problem, and it's a census problem that's really quite relevant to blockchains. For example, Bitcoin and Ethereum, just to name two examples you can view as being SMR protocols. Protocols that solve a state machine replication and keep a bunch of nodes in sync on an ordered sequence of transactions. That said, the SMR problem was defined way before blockchains come along, right? Bitcoin was launched in 2009, Ethereum 2015. The SMR problem was defined way back in the 1980s, well before blockchains were a gleam in anyone's eyes. So why on earth was anyone studying state machine replication sort of a quarter century plus before blockchains came along and were invented? Well, like most fundamental problems, SMR, which is a fundamental consensus problem, it comes up in many different guises, in many different types of technology.
00:08:30.334 - 00:09:11.782, Speaker A: So an example back in the 1980s, why would anybody be thinking about it? Imagine you wanted to replicate a database to boost the uptime, right? So you're some company like IBM or whatever. You have some database of valuable information. You want to give your customers access to that database, but you'd like very high uptime, 99.99% uptime. That's something you're not going to get if you just have a single copy of the database hosted on a single server. So a very natural idea is, okay, well, why not make sort of four copies of the database or seven copies of the database, put them on different machines, and that way if one of them goes down, it's no big deal. You can just sort of service requests to the database from one of the other copies.
00:09:11.782 - 00:09:55.046, Speaker A: But of course, as soon as you have multiple copies of the database, you want them to all stay in sync, right? If someone does a write to the database, that should be reflected in all of the machines, all of the different copies of the database, not just a subset of them. So therefore, that very basic problem. Replicate a database to make sure the uptime is really high. That forces you to come up with protocols for state machine replication, SMR. And there were other examples back in the 1980s as well. In fact, as some of you may know, there's actually a couple of Turing Awards that have been given out in the 21st century for fundamental work done on distributed computing, including state machine replication that was carried out back in the 1980s. Specifically, the 2008 Turing award was given to Barbara Liskoff.
00:09:55.046 - 00:11:14.050, Speaker A: Leslie Lamport was awarded the 2013 Turing Award, again in part for their fundamental contributions to the study of consensus protocols. Okay, so what I want to do next is review two kind of classic results from the 1980s concerning state machine replication, basically delineating under what assumptions there exist SMR protocols that satisfy both consistency and liveness. So after this slide, after reviewing these two famous results from the 1980s, we'll segue into start talking about blockchain protocols, which, as I mentioned, can be regarded as state machine replication protocols. And we'll ask the question about how does a blockchain protocol like, say, bitcoin or ethereum, how does it stack up? How does it measure with respect to these famous results from the 1980s? So the first famous result is going to apply under quite strong assumptions about the reliability of the underlying communication network. These are going to apply in what's known as the synchronous model of distributed computing, where you have an OPERATORI known I E, known to the protocol upper bound capital delta on the maximum message delay that you're ever going to see. So, for example, you could imagine defining time steps to be one millisecond long. You could take delta to be 10,000.
00:11:14.050 - 00:12:01.726, Speaker A: And that would be tantamount to assuming that any message ever sent by any node to any other node is guaranteed to arrive within 10 seconds or less. So that's the synchronous model. You basically assume the network always operates reasonably normally, no message is ever delayed more than capital delta time steps. So that's a super strong model. It doesn't allow for unbounded network delays and denial of service attacks and so on. The good news is, under this strong assumption on the communication network, you can achieve quite strong guarantees. Specifically, I want to highlight a protocol usually called the Doloph strong protocol, which shows that actually you can solve the SMR problem in the sense that there exists a protocol achieving both consistency and liveness.
00:12:01.726 - 00:13:04.882, Speaker A: No matter how many of the nodes deviate from the intended protocol, no matter how many nodes are, for example, controlled by a malicious actor, you could have 100 nodes, 98 of which are all under control of some malicious actor. Nevertheless, those 98 nodes will not be able to keep the remaining two correct nodes out of sync. And that's kind of amazing. There's not that many results in distributed computing that can tolerate literally an arbitrary number of nodes that are deviating from the protocol. But the Dole of strong protocol shows that you can actually solve state machine replication with no assumptions about the power of the adversary, as long as you're willing to make the strong assumption on the reliability of the communication network. For the experts out there, I'll mention that this results for the authenticated setting. So there's a trusted setup assumption, what's called PKI or public key infrastructure, namely that all of the nodes have public keys and that those public keys are all common knowledge at the start of the protocol.
00:13:04.882 - 00:13:45.598, Speaker A: I'm not going to make a big distinction between the authenticated and unauthenticated settings, the authenticated setting being reasonably natural in a blockchain context. Okay, so that was just a comment for the experts. But the high level point is that if you make strong assumptions about network reliability, you get very strong guarantees. You can basically solve state machine replication as broadly as you could imagine. The second famous result is going to concern what's called the partially synchronous model. And so here, unlike the synchronous model, you do want to allow for the possibility of denial of service attacks of network outages of unbounded duration. So intuitively.
00:13:45.598 - 00:14:42.370, Speaker A: What you're looking for in the partially synchronous model is a protocol that while you're under attack in the middle of a denial of service attack. Maybe you don't have all the properties that you'd like to have, but as soon as the network resumes normal operation, your protocol should very quickly behave as if you'd been in the synchronous model all along. Formally, the way this works is you again have the known bound capital delta on the maximum message delay. But that bound will be in effect only after some unknown point in the future GST or Global Stabilization Time. So intuitively, it's like the protocol kind of begins under attack where you have no idea what the message delays are going to be. At some point, message delays start being bounded and the protocol should just sort of automatically sort of notice that in effect and start operating as if it had been in the synchronous model all along. So that's the partially synchronous model, it was defined in a landmark paper of Dwark, lynch and Stockmeyer.
00:14:42.370 - 00:15:50.058, Speaker A: And in addition to the definition, that paper had many results about what you can and cannot do in this model, among which is a complete characterization of the fraction of Byzantine nodes that can be tolerated in state machine replication. Specifically, there's a magical threshold right at 33%. So if less than a third of the nodes running a protocol are Byzantine, so strictly more than two thirds of the nodes are running the protocol correctly, then it is possible to solve state machine replication with guaranteed consistency and liveness. Conversely, if at most two thirds of the nodes are running the protocol correctly, if at least a third are deviating in some way, then it doesn't matter how smart you are. No protocol you can write down guarantees, consistency and liveness. So you can solve SMR if and only if the fraction of nodes are less than a third. And I should say this 33% is the same 33% you see sort of in lots of different discussions around various blockchains, like salana, ethereum 2.0,
00:15:50.058 - 00:16:51.866, Speaker A: et cetera, whenever you hear about 33% malicious stake or something like that, in a blockchain context, this is exactly the result that they're referring to Dwark, lynch and Stockmeyer. So, just to clarify the statement a little bit, there's actually some asymmetry in the sense in which you get consistency and the sense in which you get liveness. So if you have less than a third Byzantine nodes, it's possible to get consistency always, whether or not you're under attack and liveness once the network resumes normal operation. So after this GST, or Global Stabilization time before GST, it's known that you have to give up one of the two. And so the usual approach is you keep the safety condition consistency always and then you have liveness eventually, meaning after Global Stabilization time. So that's the sense in which you solve the SMR protocol in the partially synchronous model. So the takeaway of this slide is that really since the 1980s we've known exactly when you can solve the state machine replication problem in these two fundamental models.
00:16:51.866 - 00:17:26.566, Speaker A: In the synchronous model you can solve it really whenever, doesn't matter how many of the nodes are Byzantine. In the more demanding partially synchronous model there's this magical threshold of 33%. So you can solve SMR if and only if less than a third of the nodes are Byzantine. Now, obviously these two results aren't the whole story. There's a vast and rich literature in distributed computing around consensus problems and consensus protocols. But one can say that for state machine replication. In these two communication models, we have understood exactly what fraction of Byzantine nodes can be tolerated.
00:17:26.566 - 00:18:21.050, Speaker A: We've understood that question for well over 30 years at this point. Now, something that might be bothering you is I said how blockchain protocols like, say, bitcoin and ethereum really can be thought of as protocols solving state machine replication. They really are keeping a bunch of nodes in sync on an ordered sequence of transactions. But on this slide, I'm telling you, we kind of, at least with respect to fault tolerance, kind of knew everything there was to know about state machine replication all the way back in the 1980s. So the question would then be like, why would we need blockchains? Why would we need some different type of SMR protocol newly in the 21st century when we knew all this stuff well over 30 years ago? So let me actually drill down on that question. So let's pick, let's say bitcoin as a blockchain protocol, treat it as a state machine replication protocol and probe its properties. And it's not going to be important on this slide to know how bitcoin works.
00:18:21.050 - 00:18:58.578, Speaker A: I imagine many of you do know how it works, at least at a high level. We'll review that in a few slides. But at the moment, I just want to state its properties as an SMR protocol and compare that to what we've known from the 1980s is in fact possible, starting with the synchronous models. Remember, this is where we make strong assumptions about the communication network. Specifically that there's an operator known bound capital delta on the maximum message delay that you'll ever observe. This was the setting where the dull of strong protocol gave us everything we wanted. It's an SMR protocol guaranteed consistency, guaranteed liveness.
00:18:58.578 - 00:19:35.506, Speaker A: Even when 99% of the nodes are Byzantine, even then, the remaining 1% of the nodes are going to stay in sync. So what about the bitcoin protocol viewed as an SMR protocol in the synchronous model? Well, so it is understood exactly when the bitcoin protocol satisfies consistency and liveness. And the magical threshold here is 50%. If less than half the nodes are Byzantine, then you get consistency and liveness. If more than half the nodes are Byzantine, you do not. So strictly speaking, it's not 50% of the nodes as we'll see. It's 50% of the computational power, but whatever.
00:19:35.506 - 00:20:44.490, Speaker A: For the purposes of this slide, there's a threshold at 50%. And so if we just are sort of comparing the Doloph Strong and Bitcoin protocols in terms of their fault tolerance, the Doloph Strong protocol clearly dominates Bitcoin, okay? Dolo Strong can tolerate 99%, bitcoin can only tolerate 49%. So at least looking at the synchronous model, it would seem that Bitcoin is simply a bad protocol for state machine replication, right? It's strictly dominated by this 40 year old duele of strong protocol in terms of its degree of fault tolerance. So maybe looking at the partially synchronous model will sort of rescue our opinion of Bitcoin. Remember, this is where we actually do allow unbounded network delays, but only not after some unknown Global Stabilization Time. So there will be some point in time in the future after which the network operates normally in the sense that the message delay bound capital delta will apply. And this is where we had the Dwark Lynch stockmire result saying that 33% is the magical threshold for the Byzantine nodes you can tolerate, subject to consistency and liveness.
00:20:44.490 - 00:22:06.546, Speaker A: So what about Bitcoin in the partially synchronous model? Well, actually, this is even more of a disaster. And so in Bitcoin, and this will become clear once we review how it works, as the longest chain protocol is not guaranteed to be consistent while under attack prior to Global Stabilization Time. And this is the case even without any nodes that are running the protocol incorrectly. So just if the network is having problems, if you have long delays between nodes trying to communicate with each other, that already breaks consistency of the Bitcoin protocol. So at this point, you should be feeling even more confused about why anyone would bother to invent a blockchain protocol in the 21st century when they would appear to be just inferior to the state machine replication protocols that we already had at our disposal back in the 1980s. And if you were feeling kind of cranky or cynical, maybe at this point you would conjecture that whoever it is who came up with these blockchain protocols just didn't have a proper computer science education, right? Maybe they just didn't know about this work or didn't understand this work back in the 1980s, and so they were just reinventing the wheel in some kind of inferior way. So who is it who came up with these blockchain protocols? Well, Bitcoin, as I'm sure almost all of you know, was proposed by Satoshi Nakamoto.
00:22:06.546 - 00:23:23.790, Speaker A: We still don't know who that is, but in any case, Nakamoto put out a white paper in late 2008 proposing the Bitcoin protocol. And if you were feeling suspicious about Nakamoto's background on classical consensus, your suspicion would deepen if you examined the bibliography of the Bitcoin white paper, which is very short, only eight citations, half of which are about timestamping digital timestamping, which is a natural precursor to an inspiration for blockchain. A couple citations on previous attempts to have a notion of digital cash, digital gold solving the double spend problem, and then a couple classic references for merkel trees and basic probability theory in particular. You'll see, there is no dolo strong, there is no Dwarque Lynch Stockmeyer, et cetera. So this might reinforce your conjecture that Nakamoto was just reinventing the wheel, but that's definitely not true. So here's an email from the cryptography mailing list. It was after the white paper was posted, but it was before the Genesis block was mined, and someone else on the mailing list sort of complained about the white paper know, it seems like you're trying to reinvent the wheel and kind of solve this known hard computer science problem.
00:23:23.790 - 00:24:11.306, Speaker A: To which Nakamoto replied, actually, I did in fact solve this known hard computer science problem, the Byzantine general's problem. And Nakamoto goes out to outline how the Bitcoin protocol can be used as a solution to classical consensus. So apparently Nakamoto very much was a student of computer science, really was familiar with the classic work from the 1980s. Classical consensus, was well aware of that literature, and yet still for some reason proposed what would seem to be just a strictly inferior state machine replication protocol. So why would Nakamoto do that? Well, that dovetails with a separate question. Remember, the title of this talk is Permissionless Consensus. I told you I wanted to tell you what each of those words mean.
00:24:11.306 - 00:25:17.594, Speaker A: We've talked a lot about consensus in terms of state machine replication. What about permissionless? What do I mean by that? And so the reason Nakamoto needed to invent Bitcoin and couldn't just use off the shelf technology from the 1980s is because Nakamoto wanted to solve permissionless consensus in contrast to the classical permissioned version of consensus. So what is this distinction I'm drawing between permissioned and permissionless consensus? Well, the permission case, classical consensus is where a protocol has OPERATORI knowledge of the set of nodes that are going to be running the protocol. In effect, the participating nodes are hard coded into the protocol itself. And if you think about it, if you think about those Motivating applications back in the 1980s, this is a super reasonable assumption. It would be kind of crazy not to make this assumption. You're talking about, what, like IBM replicating a database for higher uptime? IBM is going to go buy, whatever, seven machines, 22 machines, some set of machines.
00:25:17.594 - 00:25:47.754, Speaker A: It's going to own all of them. It's going to deploy a protocol on all of them itself. At the same time, you may as well use the fact that you know those 22 machines at the time that you deploy the protocol. Makes perfect sense. Furthermore, these classic consensus protocols make crucial use. They crucially rely on the permissioned assumption. Now, it's not important for us in this talk to know how these classic permissioned consensus protocols work.
00:25:47.754 - 00:26:30.650, Speaker A: But just to sort of illustrate the reliance on the permissioned assumption, let me mention that one recurring technique in those protocols is to proceed by voting. Namely, the nodes want to decide whether to take some action, like whether or not to add some new transaction to the end of all of their appendonly data structures. And they make a decision about whether or not to proceed by collecting votes. So each machine gets only one vote votes after the first one are discarded. And then if a majority or maybe a supermajority of the nodes agree on taking the action, then everybody takes the action. So proceeding by voting with one vote per participating node. So that's the permissioned model.
00:26:30.650 - 00:27:07.190, Speaker A: We have a fixed set of nodes running the protocol. Those nodes are known to the protocol upfront with voting being a common technique for achieving consensus. So what do I mean by permissionless consensus here? It's a much more ambitious goal of designing a consensus protocol where anybody can join and start contributing at any time. So if I want to be one of the nodes running this protocol, I don't need to be there at the start, I don't need to register with anybody, I don't need anyone's permission. Literally. I should be able to just download some software, fire up a node and start contributing to the protocol. That is the goal in permissionless consensus.
00:27:07.190 - 00:28:11.740, Speaker A: None of the classic consensus protocols from the 1980s solve permissionless consensus, which is no surprise, right? So no one in the 1980s was trying to solve permissionless consensus because they had no reason to do so, right? Nobody was thinking about blockchains, they were thinking about replicating databases and other problems that were most naturally modeled in the permissioned setting anyways. Now, it's not just that the classical protocols from the 1980s failed to achieve permissionless consensus simply off the shelf, out of the box. Consider, for example, the tried and true technique of proceeding by voting that's used so much in the permissioned protocols. So, for example, maybe you have a set of seven nodes running the protocol that are all known up front. You're willing to assume that at least five out of the seven are going to be running the protocol correctly. And then intuitively, you should be able to do something like collect votes. And as long as you have at least five votes for an action, then you go ahead and take that action and you should be robust to misbehavior by up to two misbehaving nodes out of the seven.
00:28:11.740 - 00:29:16.858, Speaker A: So how would this even work in the permissionless case? Well, remember, the whole point is that anybody, anywhere in the world at any moment in time can just download some software and start running the protocol. Like, how would you do voting in a permissionless setting? I mean, you could try collecting votes, but then how are you going to prevent people from voting more than once when you don't know who anybody is? You could ask people to sign votes or identify themselves by a public key, something like that. But then some malicious actor could just create sort of a billion sort of public private key pairs for themselves. That's actually quite cheap to do and hijack whatever vote it is you're trying to take. So this specter of civil attacks, the idea that a single malicious actor could create for themselves a billion different identifiers and that the protocol would be none the wiser. Not only did this sort of break all of the classical permissioned consensus protocols from the 1980s, it also makes you wonder whether or not any permissionless consensus protocol could possibly. Exist or whether that part of the design space is simply empty.
00:29:16.858 - 00:30:23.982, Speaker A: So that then is the reason why Nakamoto had to invent the Bitcoin protocol to show that that part of the design space is not empty. There's at least one permissionless consensus protocol with provable guarantees, namely the Bitcoin protocol. Now, we know that there's others as well, but the Bitcoin protocol was the first one to solve permissionless consensus in this sense. So now that we've established that the Bitcoin protocol is in fact a very interesting state machine and application protocol, what with being the first one ever to achieve consistency and liveness under reasonable conditions in the permissionless setting, let's go ahead and turn to reviewing how Bitcoin and other similar protocols actually work. So for our purposes, there will be two properties of Bitcoin that are important for us. So first of all, that it achieves civil resistance through something known as proof of work, and secondly, that it decides on which transactions have been confirmed through something known as the longest chain rule. Bitcoin is not the unique protocol with these properties.
00:30:23.982 - 00:30:58.730, Speaker A: For example, Ethereum at least pre merge, also has these same properties as well as a number of others. So really, this is just going to be about any proof of work longest chain protocol. Now, there's a million different details of Bitcoin that one could go into. For our purposes, we're just going to need a very sort of cartoony high level description. And so the protocol is going to add blocks one by one. And here a block is just a batch of transactions. So some set of transactions that has been ordered and the protocol has to get started somehow.
00:30:58.730 - 00:31:40.934, Speaker A: So as part of the protocol code will be a description of a genesis block doesn't have any nontrivial transactions in it's, just kind of a place to get started. That's what's denoted by b zero here on the slide. Blocks then get added by nodes one at a time. And so, in effect, there's sort of an infinite while loop. And in each iteration of the while loop, one of the participating nodes is chosen randomly to produce the next block randomly with probability proportional to the amount of computational power or hash rates that they bring in to the protocol. To be clear, this is not literally how the protocol works. The protocol doesn't literally randomly select a node.
00:31:40.934 - 00:32:37.930, Speaker A: Rather, all of the nodes are sort of simultaneously trying to partially invert some cryptographic hash function. Under the random oracle assumption, all of those nodes may as well just do random guessing. And so the probability that a given node is going to be the first one to get a lucky guess is proportional to the number of guesses that they're contributing compared to what everybody else is contributing. Anyways, point being is in effect, what happens in Bitcoin is that each block, a single node, is chosen randomly with probability proportional to their computational power. The chosen node then gets to pick what the block is and where it gets attached to the previous one. So it specifies the transactions in its block, the ordering of those transactions, and it specifies a single predecessor block. So at the bottom of the slide, when the second block, b one is created, there's only one previous block that could possibly be specified as a predecessor.
00:32:37.930 - 00:33:14.842, Speaker A: So b one points back to b zero. Now later on, the intended behavior of the bitcoin protocol is that nodes are supposed to extend, that is name as a predecessor, the end of the currently longest chain. So you can see here where the third block is produced, b two. It's pointing back to the more recent, to the end of the longest, to the end of the only chain, b one, as opposed to also pointing back to b zero. So that's what the bitcoin protocol requests that you do as a participating node. If you create b two, you should point to b one. You should not point to b zero.
00:33:14.842 - 00:34:00.970, Speaker A: That said, there is no mechanism by which nodes are prevented from pointing to a block other than the end of the current longest chain. If a node wants to, they can point their block wherever they want. But here in the bottom of the slide, here's another extension of the longest chain, b three, extending b two. Here's b four extending b three. While each block specifies a unique predecessor, a given block might be specified as the predecessor of more than one block. So here is b four prime which also extends the exact same block, b three, that the block b four did. Now why would this happen? Why would b four prime be extending b three instead of b four? Well, there's two plausible explanations.
00:34:00.970 - 00:35:00.550, Speaker A: One is that b four prime was created by a malicious node or some node that for whatever reason is deliberately deviating from the intended behavior in the bitcoin protocol. Total possibility that b prime four is maliciously created. Also possible that it was created by an honest node that just happened to not have heard about b four yet. Maybe the broadcasting of b four was delayed and so the node instead extended b three because that was the end of the longest chain as far as it knew, because it was not aware of b four at the time it produced b four prime. So this is called a fork where you have more than one block with a common predecessor, but one expects forks to be kind of broken, the tie to be broken because it's some future in the next iteration, some other node is going to produce some block, it has to choose some block to extend. Maybe it chooses b four, maybe it chooses b prime four. But if it's following the protocol correctly, it's going to choose one of those two blocks and now that will be the longest chain.
00:35:00.550 - 00:35:36.818, Speaker A: So b zero through B five will then be the longest chain. You would expect future nodes, as long as they're following the protocol, to be extending B five and ignoring B prime four. So that's the brief description of Bitcoin and other longest chain proof of work protocols. Each block producer is chosen with probability proportional to their computational power. If they're following the protocol correctly, they're going to add their block to the end of the longest chain that they're aware of. So there were a lot of innovations in the Bitcoin protocol. Let me just single out what, at least for our purposes, are the two big innovations.
00:35:36.818 - 00:36:09.550, Speaker A: So innovation number one is solving the civil problem through proof of work. So using proof of work to give civil resistant election of nodes to produce blocks. Proof of work was not invented for Bitcoin. It was invented for spam fighting, actually in the early 90s by Dwark and Noir. It was also used earlier in a digital currency context by Back in his hash cash system. But Nakamoto was the first one to use it to unlock permissionless consensus. So that was a really big, really big idea.
00:36:09.550 - 00:37:04.994, Speaker A: And so just to drill down on the civil resistance of proof of work. So remember what we were worried about in the classical permissioned protocols where they proceeded by sort of voting so where they just had sort of one vote per machine and you proceed if you have a majority or a supermajority. And it seemed like that wouldn't really extend to the permissionless case because you can just very cheaply create a ton of IDs and sort of hijacked any vote just by voting a billion times with your sort of billion different public keys that you created for yourself, but with proof of work, right? So the probability, your control over the protocol, meaning the probability that you will produce any given block, it's proportional to the total amount of computation you bring to the table. It doesn't matter if you have 100 asics under a single public key or if you have 50 asics each under two different public keys. Doesn't matter. You're bringing the same amount of hash rate to the system. Your voting power, if you like, your control over the protocol, will be the same either way.
00:37:04.994 - 00:37:58.050, Speaker A: So it does not matter how many IDs you have. It only matters sort of how much computational power you have. So your voting power is controlled by something that's economically scarce, that's expensive computational power, as opposed to something which is abundant and basically free, which would be public keys. So that's the first big innovation proof of work. That's obviously a crucial reason why the Bitcoin protocol was the first ever permissionless consensus protocol. But I want to emphasize that even if we ignore the permissionless aspect, even if we just sort of focus on the permissioned case and apples to apples, compare Bitcoin versus the classic protocols from the there's still a second really interesting innovation, which is its longest chain rule. And I want to emphasize the longest chain rule is very different in philosophy from those protocols in the for that reason also has very different properties.
00:37:58.050 - 00:38:32.094, Speaker A: So the classic protocols in effect are designed so that forks literally never happen. You will never have even temporary disagreement between, for example, whether the fourth block should be B four or B four prime in lockstep. There will never be disagreement. Bitcoin takes a sort of radically different philosophy. It says, you know what, forks are going to show up in the natural sort of course of business. It's fine, just embrace the forks. Let's just make sure we have an in protocol rule so that everybody knows which branch of the fork is the one to pay attention to.
00:38:32.094 - 00:39:27.870, Speaker A: And that's the longest chain rule. So the confirmed transactions are defined to be the ones in the blocks on the longest chain, perhaps ignoring the last sum number of blocks to allow temporary disagreements to sort of resolve themselves. So now that we have kind of hopefully newfound appreciation for Bitcoin, both in the sense that it proves that the permissionless consensus protocol design space is non empty, and in the sense that the specific way that it works has these two really fairly radical and interesting ideas in it. With this appreciation. Now, we can maybe have a very different attitude when we go and sort of look at Bitcoin's guarantees in terms of fault tolerance as an SMR protocol guarantees that we kind of made fun of a few slides ago. So, first of all, in the synchronous model, this is where we have an operi known bound on the maximum message delay. This is where the dull of strong protocol can tolerate even 99% of the nodes being Byzantine.
00:39:27.870 - 00:40:07.646, Speaker A: For the Bitcoin protocol, the fault tolerance is nontrivial, but not as strong. The Bitcoin protocol achieves consistency and liveness if and only if less than half of the hash rate is Byzantine. So more than half of the hash rate should be following the protocol correctly and trying to coordinate on the same longest chain. So we get a drop in fault tolerance from 99% from 49%. On the other hand, Bitcoin achieves permissionless consensus. Duel of strong only achieves permissioned consensus. Meanwhile, in the partially synchronous model, so this is where we don't even have a bound on the maximum message delay until after some unknown global stabilization time.
00:40:07.646 - 00:42:21.130, Speaker A: So here the bitcoin protocol doesn't even have consistency, even when there's 0% Byzantine hash rate, basically, because if you have different sort of nodes who kind of can't talk to each other because the messages are getting delayed, they might just grow independent parallel longest chains that have nothing to do with each other and that's going to be a violation of consistency. And so Summarizing, how should we feel about Bitcoin or generally proof of work longest chain protocols? Well, we feel good, quite good. That it achieves permissionless consensus and all these other ones only achieve permissioned consensus. On the other hand, it's kind of a bummer about these fault tolerance properties, right? So the fact that you don't get consistency and liveness if you have more than the hash rate being Byzantine, and the fact that you just basically don't have any non trivial consistency guarantees at all in the partially synchronous model. So now that we know, courtesy of Bitcoin, that the permissionless consensus protocol design space is not empty, it is of course our duty to ask what else is in that design space. Are there other perhaps very different looking permissionless consensus protocols, perhaps protocols that do not suffer from these same flaws? So, in other words, can we achieve permissionless consensus tolerating more than 50% Byzantine hash rate in the synchronous model? Can we have permissionless consensus with some kind of interesting consistency guarantees in the partially synchronous model? Or are these flaws suffered by Bitcoin an inevitable consequence of the fact that it works in the permissionless setting? Must permissionless consensus suffer the same flaw, be doomed to suffer the flaws of Bitcoin? Or can we do better? So the sub questions to that would be, is proof of work necessary for permissionless consensus? Bitcoin shows it's sufficient. Is it the only way to get permissionless consensus? Similarly, you could ask, is the other key innovation of Bitcoin essential to permissionless consensus? Do you have to use a longest chain rule? Or could you have permissionless consensus protocols that do something quite different? Formalizing and answering questions of this type is exactly the research agenda that I want to explore.
00:42:21.130 - 00:43:17.174, Speaker A: So by now, we understand that the answer to these questions are no, at least if we're willing to make some minor compromises. So we now understand that there are points in the permissionless consensus protocol design space that do not rely on proof of work for civil resistance and that do not rely on the longest chain rule for figuring out which blocks qualify as being confirmed. So let me tell you about some of those points in the design space. Next, then, I'll segue into the model and results that Andy Lewis, Pi and I have been working on, reasoning about the different points in this design space. So there's more than one alternative out there to proof of work civil resistance. But by far the dominant one is proof of stake civil resistance. So proof of stake is going to make sense primarily in blockchain protocols with a couple properties.
00:43:17.174 - 00:44:02.406, Speaker A: So first of all, it should be a blockchain with its own native currency, as of course most permissionless blockchain protocols do. Now, most proof of work blockchains, like Bitcoin, also have a native currency. But if you think about it, the native currency and say Bitcoin, it's not fundamentally essential to implementing proof of work. I mean, you definitely want the native currency to incentivize block producers to produce blocks. You want the native currency to charge for usage. But proof of work, it's still well defined even in a blockchain that has no native cryptocurrency. Whereas proof of stake civil resistance basically doesn't make sense unless you have some native currency in which block producers can stake for the privilege of producing blocks.
00:44:02.406 - 00:44:48.530, Speaker A: So that lends me into the second property we're going to want the blockchain protocol to have, which is it should have general smart contract functionality or at least enough functionality that you can enable users of the blockchain to lock up a stake in the native currency on chain in a smart contract. In principle, you can have proof of stake civil resistance without these lockups. And indeed, a couple of blockchains have at least at points of time, experimented with that. Generally speaking, in proof of stake protocols, you should think about block producers locking up stake for some period of time. So there's a lot of proof of stake blockchain protocols out there. For the most part, they fall into one of two camps. There are exceptions that fall into neither, but many of them are of one of two types.
00:44:48.530 - 00:45:35.430, Speaker A: So first of all, longest chain proof of stake protocols. So really, protocols that just mimic Bitcoin continue to use the longest chain rule for block confirmation and rather just sort of substitute out the proof of work random selection of block producers with a proof of stake based random selection of block producers. So Cardano and Tezos are two projects of that type that perhaps you've heard of. The second genre is actually very strongly motivated by the classic work in the 1980s that we discussed earlier in the Talk. Now wait a minute, you say that work from the 1980s is about the permissioned case. The whole point here is that we want to solve permissionless consensus. So what's the deal? But there are protocols which in effect kind of reduce the permissionless case to the permissioned case.
00:45:35.430 - 00:46:40.810, Speaker A: You can think of it as sort of like there's this outer subroutine, this outer wrapper which selects from all of the unbounded number of participating nodes, selects a sort of committee whose responsibility is then going to be to run one of those 1980s style protocols which I'm going to be calling a BFT type protocol. So there's also a number of blockchain protocols that are in this second camp, tendermint being one of the original ones that you may have heard of the Basis of Cosmos and Terra. I'll mention Algorand also on the next slide. But just to be clear, I mean the takeaway from this slide is that there really are points in the permissionless consensus protocol design space that look quite a bit different from Bitcoin. So proof of work is not essential just to some form of permissionless consensus, nor is the longest chain rule. So what I want to do next is dive down into the protocol that looks least like Bitcoin, namely proof of stake BFT type consensus protocols. And we'll see that they actually offer a quite different set of guarantees as an SMR protocol compared to Bitcoin.
00:46:40.810 - 00:47:48.050, Speaker A: Consider, for example, the case of the Algorand blockchain protocol. Not the unique protocol I could use to make my points on this slide, but sort of convenient for my purposes. So Algorand is definitely a proof of stake chain, so there's no proof of work in it at all. And your control over the protocol is, in effect, proportional to the fraction of the overall stake that belongs to you. Blocks are produced one by one, each block is produced by a subroutine which really sort of resembles classic Byzantine agreement protocols from the 1980s and the 1990s. And basically there's a reduction from the permissionless case to the permissioned case, where at every step of this Byzantine agreement protocol, you're going to select a random subset of stakers with probability proportional to their stake and they will then carry out a step of what's effectively a permissioned Byzantine agreement subroutine. Now, as a reduction from permissionless consensus to permissioned consensus protocols like Algorand inherit the guarantees for those classic permissioned protocols from the 1980s and 90s.
00:47:48.050 - 00:48:54.242, Speaker A: So, for example, if we're in the partially synchronous model, this is the one where you have periods of asynchrony, so there's going to be some unknown global stabilization time and only after that GST is there a bound delta on the maximum message delay. So in the partially synchronous model, remember those classical protocols achieve consistency and liveness as long as the fraction of Byzantine power is less than a third. So back then the third was in the number of nodes here, power corresponds to stake. So the assumption here is going to be that less than a third of the stake should be owned by malicious actors. Now, to be clear when I say that a protocol like Algorand inherits the guarantees of classical permissioned consensus protocols in terms of consistency and liveness, there is sort of a fair amount of fine print in that statement. So, for example, the size of the random committees you're going to need to run each step of your Byzantine agreement protocol while you're producing a block. The size of that committee is going to depend on what fraction of the stake is Byzantine.
00:48:54.242 - 00:49:54.810, Speaker A: And the closer that fraction is to the magic number of 33%, the bigger the committees. So if you're only sure that at most 32% of your stake is Byzantine, you need very big committees. If you're confident that at most, say, 20% of the stake is Byzantine, then you can get away with much smaller committees. The reason for that is just sort of the noise inherent in random sampling, right? So if you have like, say, 25% Byzantine stake and then you do a randomly selected committee, you expect a quarter of the members to be Byzantine. But it might be a little more, it might be a little less, just depending on the randomness in the sampling. So in order to make sure that the randomly sampled committee is, with very high probability, less than one third nodes Byzantine, you need there to be some kind of buffer between the magical 33% threshold and the actual assumption about the Byzantine stake. And the smaller that gap is, the bigger the committees you're going to need to, with high probability, be confident that the randomly sample committees themselves will be less than a third Byzantine.
00:49:54.810 - 00:51:12.930, Speaker A: Another important part of the fine print is that for these guarantees to hold, you need to make assumptions about sort of keys from the past not being compromised or being sort of resold on some kind of secondary market. Similarly, if you wanted to split hairs, you could argue that any proof of stake blockchain, right, algorand or otherwise, couldn't possibly be as permissionless as a proof of work blockchain like Bitcoin, basically because in proof of stake, really, by definition, you have to make your public key known before you have any opportunity to produce blocks, right? If you don't have any stake on the chain, you are by definition powerless to influence block production in proof of stake. Whereas in a proof of work blockchain like Bitcoin, you only have to report your public key after you've created a block, produced a block, you're going to put your public key in the coinbase transaction. But before you've done that, literally, it's possible nobody's ever heard of you. So in that sense, there's a little bit more advanced registration on a proof of stake blockchain than there is in proof of work. But again, this is a distinction that's just not going to be very important for us in this talk. So we're going to sort of gloss over that difference and think of both proof of work and proof of stake as permissionless protocols.
00:51:12.930 - 00:53:09.250, Speaker A: So with all these caveats in mind, we can now say that proof of stake BFT type protocols like, say, Algorand are additional points in the permissionless consensus, protocol design space and data points that look quite different from Bitcoin. They look different from bitcoin both operationally and how they work using a different civil resistance mechanism and using a different method for confirming blocks. But also they have quite different mathematical properties. In particular, these proof of stake BFT type protocols offer guarantees even in the partially synchronous model, as long as less than a third of the stake is Byzantine. And so the research questions that we really want to now drill down on is what is it about the different implementation that is driving the different properties, the different guarantees as a state machine replication protocol? So is proof of stake the key? Is it the case that you could only possibly get these kinds of guarantees in the partially synchronous model using a proof of stake protocol? Or could you do it also with, say, proof of work civil resistance? Similarly, is it essential that you use a BFT type protocol to achieve consistency and liveness in the partially synchronous model? Or could you do it alternatively with a longest chain type protocol or maybe even something completely different? So these are the sort of fundamental questions we really want to get at. If you want a blockchain protocol with certain properties, like, say, consistency and liveness in the partially synchronous model, as good as what we're used to from the permissioned model, what does that dictate about the implementation of your protocol? Does it have to be proof of stake? Does it have to be BFT type, et cetera? Those are the kind of theorems we're going to pursue. So we've finally reached the point in the talk where I'm going to stop discussing kind of background review, examples, context, and I'm going to move on to really sort of new mathematical models and new mathematical results.
00:53:09.250 - 00:54:36.834, Speaker A: And as I said at the beginning, this is all going to be joint work with Andy Lewis Pi, a professor at the London School of Economics. We've written a sequence of three papers, all of which are available on archive. So, resource pools in the Cap theorem, how does blockchain security dictate blockchain implementation? And Byzantine generals in the permissionless setting? There's a bunch of results across these papers. I'm going to cherry pick one result from each of the three papers to tell you about. And really, this is all to illustrate the recurring theme in our research agenda, which is to what extent do the properties you would like a blockchain protocol to have? To what extent do the desired properties dictate what the protocol must look like, how you implement it, so in particular, seeking results of the form, does every blockchain protocol satisfying certain properties have to look a certain way? Does it have to look like a bitcoin? Does it have to look like algorand, depending on the kinds of mathematical properties that you're after? So our first contribution is a model that's general enough to capture most of the major blockchain protocols that are out there, including all of the ones we've talked about thus far in this talk. And this is sort of nontrivial, actually, to formulate a model that's a sweet spot of complexity in the sense that it's simple enough, you can prove theorems about it, make different comparisons between different types of blockchain protocols. On the other hand, general enough that it captures the sort of diversity in blockchain protocols that are out there, which we've already seen sort of a glimpse of.
00:54:36.834 - 00:55:15.454, Speaker A: So we wanted a model that would talk about not just proof of work and proof of stake, civil resistance, but other notions of civil resistance. And we wanted to be flexible about whether it was a longest chain type protocol, a BFD type protocol, a Dag based protocol, whatever. So that's the first contribution of the papers is a model that allows direct comparison of blockchain protocols of really quite different types. So if you want to compare for example, proof of work longest chain protocols to proof of stake BFT type protocols. Our model allows you to do that. So the model has a number of details as you can imagine. And I'll let you check out the papers if you want to do a deep dive on it here.
00:55:15.454 - 00:56:12.802, Speaker A: I just want to highlight one really quite simple I think, but also super important part of the model, which is the notion of a resource pool. So we will always be analyzing blockchain protocols relative to some resource pool. What's a resource pool? Well, it specifies at every moment in time what is the resource balance of each of the nodes who are running the protocol. For example, in the context of a proof of work blockchain, like Bitcoin, the resource pool will specify the hash rate that each of the nodes contributes to the protocol. In a standard proof of stake blockchain, the resource pool is going to be the locked up stake of each of the participating nodes in the protocol. And in general, the semantics of the resource pool and resource balances is they determine to what extent a node can contribute to the protocol's execution. So that definition is quite general.
00:56:12.802 - 00:57:09.686, Speaker A: And within that we want to differentiate between two different types of resource pools which roughly correspond to the idea of proof of stake blockchains and proof of work blockchains. But I want to emphasize nowhere in our papers do we define what it means to be a proof of work blockchain. That's sort of a little too operational for our tastes nor proof of stake. So rather we talk about blockchains that run relative to a resource pool that is sized versus one that is unsized with sides sort of meant to sort of represent typical proof of stake blockchains, unsized meant to represent typical proof of work blockchains. So in the sized setting, the resource pool is actually completely determined by onchain information. So the current state of the blockchain actually determines to what extent people can contribute to the protocol's execution. And if you think about it, standard proof of stake protocols are exactly like that.
00:57:09.686 - 00:57:58.866, Speaker A: The resource is just the stake that's locked up at some contract and then the state of the blockchain tells you exactly who has how much locked up in that contract. In the unsized setting, the resource balances will be independent of the blockchain state. It will evolve separately of the blockchain state. And if you think about it, that's very much true for a proof of work blockchain like the blockchain state tells you literally nothing about sort of everybody's contributing hash rate. I mean, you can guess based on who's mined how many blocks, but you don't literally know what people's resource balances are just from on chain information. Whereas you do know that in a proof of stake blockchain. And one thing that's come up over and over again in our research is that the size and unsized settings.
00:57:58.866 - 00:59:16.426, Speaker A: Are provably quite different and in particular there are things you can do in the size setting that you probably cannot do in the unsized setting. So the next theorem will clarify one result of that type. So to motivate the theorem that will show up at the bottom of this slide, let's go back to the very concrete question of comparing a typical proof of work longest chain protocol like Bitcoin to a typical proof of stake BFT type protocol like Algorand. As they saw, on the one hand they differ quite a bit in what they look like and how they operate, but they also differ quite a bit in their properties. And the question we then want to understand is in what sense do they have to look very different given that they have very different properties as state machine replication protocols? So, working toward the next theorem statement, let's articulate an interesting property that the bitcoin protocol has, which I'm going to call adaptive liveness, right? So liveness in the context of state machine replication just means that as long as there's transactions to add and confirm, they keep getting added and confirmed. So that's what liveness means. Adaptive liveness means you continue to have liveness even under sort of unbounded changes in the total resource balance.
00:59:16.426 - 01:00:20.674, Speaker A: So like in a proof of work chain, we'd be talking about fluctuations in the total hash rates. In a proof of stake chain we'd be talking about fluctuations in the total amount of currency staked. Now, this distinction between liveness and adaptive liveness is not a very interesting distinction in the sized setting because remember, in the size setting, by definition all of the resource balances are just directly observable on chain. So for example, if you have a sudden drop by a factor of 100 in the total resource balance, that's sort of immediately visible to the protocol itself and it can just take that into account in how it proceeds. For example, any proof of stake blockchain that's worth its salt is not going to blink an eye if there's suddenly a factor 100 difference in the total amount of stake, you're just going to adjust the random sampling procedure accordingly. By contrast, in the unsized setting, by definition the resource balances are independent of the blockchain state. So the blockchain state could be staying exactly the same.
01:00:20.674 - 01:01:17.670, Speaker A: It'll look literally identical. Yet there's been some factor 100 drop in the total resource balance like in the total hash rates in a proof of work blockchain context. So in the unsized setting, for example, for typical proof of work blockchains, we don't necessarily expect it to be automatic. That just because they're sort of live under a sort of steady total hash rate total resource balance that they would continue to be live under wildly fluctuating resource balances. Nevertheless, a very interesting property of, for example, the bitcoin protocol is it does have adaptive liveness. So if the total hash rate suddenly drops by a factor of 100 in bitcoin, it is true blocks will be produced more slowly than before, at least up until the next difficulty adjustment period, but blocks will continue to be produced infinitely often. So as long as there are transactions to process, they will eventually get processed.
01:01:17.670 - 01:02:25.354, Speaker A: Now, to be clear, the algorithm protocol is also going to satisfy adaptive liveness, but sort of for relatively sort of straightforward reasons, just by virtue of being a proof of stake blockchain operating in the sized setting, the random sampling adjustment will just sort of automatically handle big changes in the total amount of stake. Bitcoin, meanwhile, which operates in the more demanding unsized setting where it's not obvious how to get adaptive liveness, the longest chain consensus protocol that it uses nevertheless achieves that property. And so the first main theorem I want to tell you about is that for protocols that satisfy adaptive liveness like bitcoin and algorand, you really have to choose between either operating in the unsized setting as bitcoin does, or achieving consistency in the partially synchronous setting. You cannot have both. So formally this is an impossibility result. It states three properties and there's a proof that there does not exist a state machine replication protocol that satisfies all three. You have to pick two out of the three.
01:02:25.354 - 01:03:02.774, Speaker A: In that sense, this is in the spirit of the Cap theorem from distributed systems stating that among consistency, availability and partition tolerance you have to pick two. So it has a very similar sort of structure to theorem statement here. Specifically, property one is operating in the unsized setting which again sort of corresponds to a proof of work blockchain. Number two is liveness, including adaptive liveness, so even under fluctuating resource balances and three is consistency in the partially synchronous setting. These are three properties. You would like them all, you cannot have them all. Bitcoin, for example, satisfies the first two.
01:03:02.774 - 01:04:13.274, Speaker A: It's a proof of work blockchain. So it operates in the unsized setting and as we mentioned, it does satisfy adaptive liveness, continues to produce blocks even under large constant factor changes in the total hash rates. But the longest chain rule just badly loses consistency in the partially synchronous setting because if two groups of nodes can't speak to each other, they might just grow independent parallel chains which is a big violation of consistency. Algorand, as we discussed it kind of has adaptive liveness sort of trivially because it works in the size setting and because it sort of inherits and then from the permissioned protocols that it sort of reduces itself to it inherits consistency in the partially synchronous setting. However, as we mentioned, algorand is a proof of stake protocol, it does operate in the sized setting so it satisfies the second and third of these three properties. And the theorem states that there is no protocol which satisfies all three. So this theorem helps clarify what it is that's driving the different properties of bitcoin versus algorand and in particular, we see that to get Algorand's sort of consistency and liveness guarantees, it is absolutely essential that it operates in the sized setting.
01:04:13.274 - 01:04:50.570, Speaker A: For example, using proof of stake civil resistance. Put another way, it's the proof of work aspect of bitcoin I. E. The fact that it operates in the unsized setting that basically forces the compromises you make with that protocol, and in particular, the loss of consistency when you have network outages or unbounded delays. Next, let me sort of talk you through a cartoon sketch of the proof. As you can imagine, there's plenty more details here. I'll encourage you to look at the paper if you want to see more, but let's at least understand the high level intuition of why it is you can't have all three of these properties simultaneously.
01:04:50.570 - 01:05:15.380, Speaker A: So imagine you're in the unsized setting. Imagine you actually do have property one. So, like a typical proof of work blockchain even. Just think about bitcoin. Suppose you're a bitcoin miner and you're sort of mining away, trying to produce blocks. And suppose at some point you notice something curious, which is like, you literally stop hearing about anything from anybody else. No one is announcing new blocks at all.
01:05:15.380 - 01:05:57.934, Speaker A: And if you think about it, there's two plausible explanations for why maybe you're hearing nothing from anyone else. Plausible explanation number one is that everybody else just turned off their machines. Nobody else is actually trying to produce blocks. Everybody else's hash rate has dropped to zero. For all you know, that's the reason you're not hearing from anybody else. Plausible explanation number two is that actually you're in the middle of a network outage and you've somehow been kind of cut off from the rest of the network, for example, by a denial of service attack. Okay? And if the only thing you know is that you stopped receiving messages from everybody, you cannot figure out which of these two you were in.
01:05:57.934 - 01:06:51.722, Speaker A: Notice if you were in the sized setting, you could certainly figure out which of the two that you were in, because in scenario one, everybody else's hash rate that's dropped to zero, that would be visible from the onchain information that you yourself know. But in the unsized setting where everybody else's resource balance is independent of what you can see, independent of the blockchain state, you cannot distinguish between scenario one and scenario two. So now you're really damned if you do and damned if you don't, right? You're not hearing anything from anybody else, but you have to decide whether to keep producing blocks yourself. So either you stop or you continue. And if you stop your own block production, well, for all you know, you're in scenario number one. No one else is doing any block production either. And then you violate adaptive liveness.
01:06:51.722 - 01:07:38.170, Speaker A: It is true that the total resource balance sort of shifted by a lot to get you in scenario number one, but that's the whole point of adaptive liveness is you're supposed to continue block production in that case. So if you stop producing blocks, you violate the adaptive liveness property. On the other hand, if you carry on producing blocks even though you're hearing nothing from anyone else, as for example, a faithful bitcoin miner would do, well, then for all you know, if you're in the partially synchronous setting, for all you know, you are before global stabilization time. And actually lots of other nodes are in fact producing blocks. Trying to tell you about it, but because of the unbounded network delays, you're not hearing about any of their efforts. So then you're producing blocks. They're producing presumably different blocks and that's a violation of consistency.
01:07:38.170 - 01:08:28.762, Speaker A: We're talking here about the partially synchronous setting where unbounded network delays are something you have to worry about. All right, so that's the first theorem I wanted to mention. Between operating the unsized setting as a proof of work blockchain, would adaptive liveness even already in the synchronous setting and then consistency in the partially synchronous setting? You have to pick two. You cannot have all three bitcoin again, satisfies properties one and two. Algorithm satisfies properties two and three. And I hope you now see what I mean. That's sort of a recurring theme in our research is understanding must a blockchain protocol with certain properties be implemented in a particular way? So for example, what we just learned is that if you want to retain consistency in the presence of network outages and also have standard liveness properties, then actually you need to use proof of stake civil resistance.
01:08:28.762 - 01:09:27.666, Speaker A: Okay, not quite right, because remember we phrased it in terms of the sized setting. So for whatever reason, it must be the case the resource balances are observable from on chain information, as is typically the case in a typical proof of stake blockchain. So that tells us an aspect of the blockchain design which is forced by a consistency property that you might want. So next, what I want to do is tell you about another result, which is very much in this same theme. And what I want to do is I want to drill down on protocols that have the same mathematical guarantees as, for example, Algorand, and want to ask, are all protocols with those mathematical guarantees? Do they all kind of look in an implementation sense, do they look like Algorand? So in other words, I want to focus on protocols that meet the properties two and three adaptive liveness consistency in the partially synchronous model. As we now know, that means they're sized protocols. So we're going to be talking only about sized protocols for this result.
01:09:27.666 - 01:11:02.906, Speaker A: And to what extent do achieving properties two and three force you to resemble Algorand? We know you have to be in the size setting, but for example, do you have to use a BFD type consensus algorithm? Or alternatively, maybe some kind of longest chain protocol would work just as well is there a proof of stake longest chain protocol meeting the second and third properties from the last theorem? So the following definition is our proposal for the answer to that question. It's going to be a notion of a blockchain protocol producing certificates. And let me warn you that if you go to the paper, you'll find the exact, precise mathematical definition, which is a little bit different than what I've stated here on the slide for the talk. I've simplified it a bit, but I do think this captures the spirit of what's going on. So generally, if you're a node running a blockchain protocol, the very first time you hear about some block, generally you don't immediately rush to the conclusion that that block is confirmed and that all of those transactions really should be executed. For example, if it's a longest chain protocol like, say, Bitcoin, in addition to wanting to check that the block is indeed on the longest chain that you're aware of, you might want to wait until six or 20 or 100 blocks have been added after it so that you're sure it's going to stay on the longest chain for the foreseeable future. Or alternatively, in sort of a proof of stake BFT type protocol based on these sort of classical permissioned protocols, you might want to wait until you hear from enough votes in favor of the block from other nodes that are running the protocol.
01:11:02.906 - 01:12:00.098, Speaker A: And so that's fine. So generally, a block will transition from unconfirmed to confirmed once sort of sufficient support has been received for it. And so a protocol will, say, produces certificates if it can never go backward, if it can never be the case that with some set of messages, you regard a block as being confirmed, but then in the presence of additional messages, you change your minds, you flip your opinion into it being unconfirmed. So let's immediately observe that the usual bitcoin protocol does not produce certificates in this sense. So if you're a node running the bitcoin protocol and you believe that some block is confirmed, it's on the longest chain that you're aware of, and there's a bunch of blocks after it on the longest chain that you're aware of, therefore you think it's confirmed. If, hypothetically, someone showed you a sort of much longer alternative chain, you would change your mind. All of the blocks on what you used to think was the longest chain would be rolled back.
01:12:00.098 - 01:13:01.750, Speaker A: They would toggle from confirmed back to unconfirmed. So in that sense, a longest chain protocol like Bitcoin does not produce certificates in this sense. We can also observe, on the other hand, that BFT type protocols like, say, Algorand, for example, do produce certificates in this sense under the standard assumption that less than a third of the power behind the protocol is byzantine. And basically the reason it basically inherits that property from the permissioned consensus protocols from the 1980s and 90s that they're based on those are voting based protocols. So you consider a block confirmed once it has sufficient votes. So obviously, hearing more votes doesn't cause you to flip it back to unconfirmed. Moreover, under the standard sort of upper bound on sort of the Byzantine power, you're not going to have forks, so you're not going to have two different conflicting blocks, both of which have sufficient support to regard as confirmed.
01:13:01.750 - 01:13:57.174, Speaker A: So that, slightly informally, is what we mean by a blockchain protocol producing certificates. A node, once it believes that a block is confirmed, will always believe that the block is confirmed, no matter what future messages are ever received. So Algorand and other BFT protocols like that produce certificates and also they happen to satisfy consistency in the partially synchronous setting, unlike the longest chain protocols we've been talking about. Is that an accident? Very much, no. Our next result asserts that in fact, a blockchain protocol achieves consistency and liveness in the partially synchronous setting if and only if it produces certificates. Now, the if direction of this statement is not hard. So if your blockchain protocol happens to produce certificates, well, then you're good.
01:13:57.174 - 01:14:24.930, Speaker A: That's a sufficient condition for achieving consistency in the partially synchronous setting. The big challenge in the partially synchronous setting is that messages could be delayed by unbounded amounts. So what you're worried about is that a node that has not received messages because of message delays makes a bad decision. Actually consider something confirmed when it's not. But if you have certificates, then that's never going to happen. You will never flip from confirmed to unconfirmed. You only go in the other direction.
01:14:24.930 - 01:15:31.318, Speaker A: So the harder part is the only if direction that you give me a protocol that satisfies consistency in the partially synchronous setting and I will extract from that protocol for you what in effect is a certificate. In other words, certificates are not just sufficient for guaranteeing consistency in the partially synchronous setting, they're necessary. So in that sense, any protocol that shares Algorand's guarantees must resemble Algorand in the sense that it must produce certificates. All right, so I'm not going to say much about the proof. I mean, to be honest, I haven't even given you the fully precise version of the definition of producing certificates, but at sort of a very high level for the only if direction, the proof proceeds by contraposition. So consider a protocol which does not produce certificates, meaning there's some kind of non monotonicity in the confirmation rule. By definition, then we need to show a consistency violation and we're going to really kind of exploit the fact that we're thinking about the partially synchronous model.
01:15:31.318 - 01:16:36.634, Speaker A: So we need to show that an adversary can schedule message delays so that it basically triggers the non monotonicity in whatever confirmation rule your protocol is using. So that's kind of at a high level how the proof proceeds. So that's the result about certificates that I wanted to highlight in this talk I will say that if you go to the paper that we wrote on certificates, you will find a much deeper study and a bunch of other results. So for example, one thing we do do is study at some length the synchronous model, where you have an opera known bound on message delays and propose an analog of certificates for the synchronous setting. And so what's interesting about that is, right in the partially synchronous setting, for consistency, for consistency and liveness, we already know from our Cap type theorem, you have to be in the sized setting. So in the last slide for the last theorem, clearly we were only talking about protocols that operate in the sized setting. What's interesting in the synchronous model is you once again get a formal separation between what unsized protocols can do and what sized protocols can do.
01:16:36.634 - 01:17:29.274, Speaker A: So specifically in the unsized setting. So proof of work type blockchains, including bitcoin. In fact, we show that it is impossible for any such protocol to produce certificates in our sense, even in the synchronous setting, whereas in the sized setting you can indeed produce certificates in the synchronous setting, even with longest chain type protocols. So a big chunk of the proof of stake design space in fact leads to certificates, at least in the sense of the synchronous model. All right, so let me tell you one more result and then I'll wrap up the talk. And this again will be a result that's really I've really cherry picked to illustrate a broader research agenda. That agenda being to understand exactly when, meaning under exactly which conditions is permissionless consensus possible.
01:17:29.274 - 01:18:33.006, Speaker A: And I phrased this in a way that's meant to mirror the extremely rich literature on the permissioned case, where possibilities and impossibilities have been mapped out under all kinds of different nuanced sort of subsets of assumptions. And I think it's totally plausible that there is as rich a theory waiting to be developed for permissionless consensus protocols, just as in the permissioned case, one would expect the answer feasible versus infeasible to depend in an intricate way on the different assumptions that you make. So which consensus problem you're looking at? Obviously assumptions about the network, assumptions about the adversary's power, maybe, whether or not you allow protocols to use randomness, et cetera. All things that have mattered in the permissioned case also seems like they will matter quite a bit in the permissionless case. So let me provide some supporting evidence for that assertion. Just by telling you about one result we were able to prove, and I should say, as far as we can tell, this result is not that easy. Our proof certainly is not that easy.
01:18:33.006 - 01:19:14.618, Speaker A: The result concerns the Byzantine broadcast problem. So that's one of those famous single shot consensus problems we mentioned briefly at the very beginning of the talk. And what we have is an impossibility result for deterministic protocols in the permissionless setting. So there does not exist a deterministic permissionless protocol, satisfying all of the properties you generally want in Byzantine broadcast, namely termination validity and agreement. And this result holds even for sort of very weak adversaries. You only need to assume that the adversary has some nonzero fraction of control over the protocol's operation. And let me emphasize that this is a formal separation between the permissionless and permissioned models.
01:19:14.618 - 01:20:21.118, Speaker A: There's a formal sense in which Byzantine broadcast is a fundamentally more difficult problem in the permissionless model than in the permissioned model. Because in the latter model, in the permissioned model, we do have a solution under these same sets of assumptions, the dole of strong protocol which we mentioned way back at the beginning of the talk it's permissioned, but it is deterministic and it does satisfy Byzantine broadcast all of termination validity and agreement no matter how many of the nodes are Byzantine. Whereas here we're not solving Byzantine broadcast for any deterministic protocol with any nonzero fraction of the protocol control controlled by an adversary. As usual, I encourage you to check out the paper for more results along these lines. For example, we study randomized protocols and show that they're provably strictly more powerful than deterministic protocols in the permissionless model. All right, so we've had a lot to say in this talk. So let me just quickly review kind of the main high level points what you should sort of take away from this talk long term.
01:20:21.118 - 01:21:25.426, Speaker A: In the first part of the talk, we reviewed the classical permissioned consensus problem and again, before blockchains in the this was the sensible version of the problem to study, where all of the nodes running the protocol are fixed and known up front. And as we discussed, that problem was relatively well solved already by the late 80s. So Bitcoin then just viewed through the from a consensus perspective, the big breakthrough is achieving consensus under reasonable assumptions in the permissionless model. So here, not only is the set of nodes not fixed, not only is it not known in advance, it's generally not known ever. Literally anybody at any time can download a copy of the protocol software off of the web and spin up a node and start running the protocol. And even in that sort of Wild Wild West type setting, under reasonable assumptions, the Bitcoin protocol does achieve consistency and liveness. And there were two big ideas in the Bitcoin protocol which enabled this solution of permissionless consensus.
01:21:25.426 - 01:22:45.354, Speaker A: Idea number one was sort of beating civil attacks through the proof of work civil resistance mechanism, choosing block producers with probability proportional to their computational power. Big idea number two was to embrace forks, not worry about the fact that people might produce conflicting blocks and just have an in protocol method for resolving forks, namely by considering blocks on the longest chain to be the ones that actually count. We then discussed how actually, while Bitcoin showed that the permissionless consensus protocol design space is non empty, there were some very different looking points in that design space. So we talked about alternatives to proof of work like proof of stake stable resistance and alternatives to longest chain rules like instead piggybacking on the BFT type protocols. From the question then, given the sort of diversity of permissionless consensus protocols was how can we have a theory that allows us to make apples to apples comparisons between the two and really allow us to formally articulate the trade offs you make in some parts of the design space versus others? So, the first contribution of our work was a general model, a general formalism that allows you to discuss all of these different types of permissionless consensus protocols simultaneously. I then showed you three results that we've proved in that model. Again, there's many more in the papers that are referenced earlier.
01:22:45.354 - 01:23:54.530, Speaker A: So, the first result was a Cap type theorem saying that of three properties you might want of a blockchain protocol, you have to settle for only two. So there does not exist a protocol which works in the unsized setting like a proof of work protocol that achieves adaptive liveness in the synchronous setting, meaning it produces blocks even under sort of large fluctuations in the total resource balance and third consistency in the partially synchronous setting. And we interpreted Bitcoin as an example, satisfying the first two properties but not the third, and Algorand and other proof of stake BFD type protocols as satisfying the second and third properties but not the first. Then we drilled down into protocols that do achieve consistency in the partially synchronous setting, which by the captype theorem have to be sized protocols. And we asked to what extent those have to resemble BFT type protocols. Like, for example, algorand we extracted what it meant to look like a BFT type protocol with the mathematical property of certificate production. This idea that block confirmation is a monotone function of the messages that a node has received.
01:23:54.530 - 01:24:48.854, Speaker A: And what we saw there is that in fact, in the partially synchronous model, being able to achieve consistency is equivalent to producing certificates. So certificates are not just sufficient for consistency in the partially synchronous setting, it's also the only way that you can get it in a formal sense. And finally, I mentioned this impossibility result for the Byzantine broadcast problem for deterministic protocols showing a formal separation with a permissioned model where the duel of strong protocol is deterministic and solves Byzantine broadcast. And I showed you that result really just to illustrate this more general research agenda, which is just understanding when permissionless consensus is or is not possible, totally plausible that there is as rich a theory to be developed for the permissionless case as we've already seen over the last several decades for the permissioned case. So that wraps up the talk. Thanks very much for joining me. Hope you enjoyed it and I hope to see you again soon.
01:24:48.854 - 01:24:49.410, Speaker A: Bye.
