00:00:00.650 - 00:00:23.406, Speaker A: Hi, everyone, and welcome to this video that accompanies the epilogue of the book Algorithms Illuminated, part Four, about a field guide to algorithm design. So congratulations. You have now reached the end of the Algorithms Illuminated book series, and as a result, you now possess a rich algorithmic toolbox suitable for tackling a wide range of computational problems.
00:00:23.406 - 00:00:51.340, Speaker A: In fact, you may now have so many tools that it's just overwhelming so many algorithms to choose from, so many data structures, so many different design paradigms. What's the most effective way to put all of these tools to use? Well, to give you a starting point in this video, I want to tell you about the typical recipe that I use when I need to understand a new computational problem. As you develop more and more of your own experience, I highly encourage you to come up with your own recipe that works best for you.
00:00:51.340 - 00:01:09.282, Speaker A: So suppose you're confronted with a new computational problem that you haven't seen before. What should you do? Well, I hope you've learned a lot of things from this book series and from these video playlists. Probably one of the things you've learned is that designing algorithms from scratch can be pretty hard.
00:01:09.282 - 00:01:40.662, Speaker A: So the first several steps of this recipe are honestly just trying to be lazy, trying to get out of the difficult task of coming up with an algorithm from scratch and hopefully tackling the problem using the tools that you already have. In the best case scenario, your problem will literally boil down to a disguised version of, or maybe a special case of a problem that you already know how to solve. So just to give one example, lots and lots of problems turn out to be shortest path problems, and you can solve them quickly using, say, dejkster's algorithm.
00:01:40.662 - 00:02:09.458, Speaker A: Or maybe you can even just use breadth for search, depending on the situation. One of the cool things that will happen if you push past the scope of this book series and pursue a deeper study of algorithms is you'll learn about additional problems which are well solved, meaning we can solve them in polynomial time using pretty practical algorithms, but also problems that show up in disguise all over the place. So some good examples would include, like, the fast Fourier transform, which is another killer application of the divide and conquer algorithm design paradigm.
00:02:09.458 - 00:02:44.000, Speaker A: Or there's some ubiquitous graph optimization problems like, say, maximum flow, minimum cut bipartite matching, or you even have sort of very general optimization procedures, sort of in the spirit of the MIP solvers that we talked about, but for polynomial time solvable problems, namely linear programming and conducts programming. In any case, whenever you're tackling a new computational problem, the first thing to do is see if it fits into one of your off the shelf algorithms that you can just apply immediately. And if it does, among all the algorithms that apply, you want to use the fastest ones sufficient to solve that problem.
00:02:44.000 - 00:03:00.994, Speaker A: So if that first step fails, we have to work a little bit harder. But we still want to err on the side of being as lazy as possible and just sort of using our off the shelf tools as much as we can. So the next thing to consider is the array of four free primitives that we've studied in this book series.
00:03:00.994 - 00:03:18.346, Speaker A: So again, these are subroutines that run in linear or near linear time. So the amount of time is barely more than what you need to read the input anyways. And if a four free primitive has the potential to simplify the problem you're looking at, why not just apply it? So like, if you have an array, maybe it helps to sort the array first.
00:03:18.346 - 00:03:33.070, Speaker A: If you have a graph, maybe it helps to compute the connected components of the graph first. The third step again, is to try to be lazy, to not work too hard coming up with an algorithm. So you first just want to identify what would be the totally obvious naive way to solve the problem.
00:03:33.070 - 00:03:48.018, Speaker A: So maybe that would be something like exhaustive search and then you want to just double check. Maybe this naive solution is already good enough. Like if you just need to solve the traveling salesman problem on a graph that has ten vertices, exhaustive search is going to be just fine.
00:03:48.018 - 00:04:11.654, Speaker A: Or if you're counting the number of inversions in array that only has length, like 1000, you can go ahead and use the quadratic time of exhaustive search algorithm for counting inversions if the obvious solution is not good enough. Now you need to actually start thinking about algorithm design paradigms. And as we've said many times, often the best place to start brainstorming is the greedy algorithm design paradigm.
00:04:11.654 - 00:04:22.442, Speaker A: So for many problems, you can try to brainstorm a bunch of different greedy algorithms and try them out on small examples. Most likely all of those greedy algorithms will be wrong. They will fail on some inputs.
00:04:22.442 - 00:04:44.626, Speaker A: But actually seeing concretely the inputs on which those greedy algorithms fail and why they fail, that inevitably helps you understand the problem better and see more clearly what tools are actually going to be needed. So let's assume that you've made it through step four and you're not happy with the results. So maybe it was a problem where kind of you actually couldn't, didn't seem to lend itself to the greedy algorithm design paradigm.
00:04:44.626 - 00:05:01.062, Speaker A: Or maybe you did come up with a bunch of greedy algorithms, none of them work. You'd really like an algorithm that does work and you're willing to move on to graduate to a more sophisticated paradigm to come up with an exact algorithm. Well, depending on the type of problem you're looking at, you might next consider the divide and conquer algorithm design paradigm.
00:05:01.062 - 00:05:18.478, Speaker A: So this is the very first one we discussed, actually back in the first half of part one with merge sort being a completely canonical example. And of course, we saw many other examples quicksort Karatsuba multiplication, Strassen's matrix multiplication, closest pair, et cetera. So we've seen lots of examples of divide and Conquer.
00:05:18.478 - 00:05:39.078, Speaker A: That said, there's certain types of problems it tends to work well for, and other types of problems it tends to not work well for. So if you're dealing with a computational problem where the input is in an array, then it's sort of very obvious a way to split it in half, the left half and the right half. You can try recursively solving the problem on each subarray and combining the results that may or may not work, but at least it's sort of clear how you would approach the problem.
00:05:39.078 - 00:05:52.362, Speaker A: What you try. There's other problems like problems involving graphs, where honestly, it's not so obvious how to even get started with Divide and conquer. I mean, you could split the vertices into two groups of of size, n over two each if there's N vertices.
00:05:52.362 - 00:06:11.006, Speaker A: But like, which N over two do you put over here, and which N over two do you put over here? It's not totally clear. So for that reason, it's a little unusual to see Divide and Conquer applied to, say, graph problems. But if you're dealing with a kind of problem where divide and Conquer is natural, where there's a natural way to divide the input into multiple independent subproblems, give it a shot.
00:06:11.006 - 00:06:29.062, Speaker A: If it works, you'll probably wind up with a quite fast correct algorithm from the Divide and Conquer paradigm. It's very natural to segue into the dynamic programming paradigm because that also deals with smaller subproblems. And actually, especially if you're trying to apply Divide and Conquer, and it seems to not be working.
00:06:29.062 - 00:06:47.758, Speaker A: And the reason it's not working is because to combine the recursively computed solutions, you always have to seem to recompute a lot of stuff. That's often a signal that you should probably be looking at dynamic programming instead. So you're all graduates of the Algorithms Illuminated dynamic Programming Boot camp, so you've had lots of practice with this, and you know what the game is.
00:06:47.758 - 00:07:13.480, Speaker A: So to apply dynamic programming, you have to understand in what sense the optimal solution to the problem that you want to solve. How must it be built up from optimal solutions to smaller sub problems? And your task then is to prove that there's really only a limited number of candidates vying to be an optimal solution. And then in dynamic programming, you can exhaustively search over that small number of candidates for what the optimal solution could possibly look like.
00:07:13.480 - 00:07:44.830, Speaker A: And if you do have this type of insight, if you can see how optimal solutions must be built up from a small number of ways from smaller sub problems, then you're usually off to the races, right? So that usually lends itself naturally to a recurrence, which involves exhaustive search over all of the candidates for an optimal solution that usually sort of tells you what the subproblems have to be. And then of course, the dynamic programming algorithm writes itself. You just solve the subproblems from smallest to largest using the recurrence to solve each subproblem, given the answers you've already computed to the smaller subproblems.
00:07:44.830 - 00:08:05.302, Speaker A: Okay, the next two steps, step seven and step eight, they're going to be conditional on you having some success in the previous few steps. If you haven't, then we'll get to step nine where we start talking about what to do with NP hard problems. But let's consider the happy case where in one of these first six steps you actually came up with a reasonably good algorithm that solves the problem you care about.
00:08:05.302 - 00:08:13.130, Speaker A: Exactly. So maybe you actually came up with a greedy algorithm that winds up working. Or maybe you figured out how to apply the divide and conquer paradigm to this problem.
00:08:13.130 - 00:08:41.326, Speaker A: Or maybe you built on the skills you developed in the dynamic programming boot camp and you figured out how to design a dynamic programming algorithm for the problem. In any case, as soon as you have an algorithm for a problem, that's great. But again, the good algorithm designer is never complacent, is always asking can we do better? So then you want to ask can we somehow speed up the algorithm that you have so far? And one thing you always want to be on the lookout for is opportunities to deploy data structures in your programs.
00:08:41.326 - 00:09:11.260, Speaker A: And the clarion call for a data structure is repeated computations of the same type. So for example, suppose you're just doing minimum computations over and over and over again as we were doing in, for example, prim's algorithm and Dijkstra's algorithm. So repeated minimum computation, that kind of algorithm calls out for a heap data structure because the raison detriment of a heap is exactly to speed up minimum or maximum computations from linear time, which is what you'd get by exhaustive search all the way down to logarithmic time.
00:09:11.260 - 00:09:48.050, Speaker A: Similarly, imagine you had a program that over and over and over again did lookups so it wanted to know whether or not a given element was in a given set. Well then that's an algorithm that is calling out for a hash table because the whole raise on Detroit of hash tables is to keep track of an evolving set of objects while supporting effectively constant time insertions. And lookups, in any case, if you're deploying a data structure in your program, you want to remember the principle of parsimony which is loosely inspired by the famous quote of Albert Einstein, that a theory should be as simple as possible but no simpler.
00:09:48.050 - 00:10:23.550, Speaker A: Same thing in your programs, the data structure you use should be as simple as possible but no simpler. So identify what are the repeated operations that are being performed by your program? Exactly? Which operations is it that you want to speed up and then use the minimal most lightweight data structure that does support that collection of operations because that way those operations will be as fast as possible. So for example, if all your program does is repeated lookups and it doesn't need to, say, maintain ordering information over a bunch of keys, then you do not want to be using a balanced binary search tree for your lookups because that would be logarithmic time lookups.
00:10:23.550 - 00:10:48.042, Speaker A: You want to be using a hash table to get those constant time lookups. After you've scrutinized your program for opportunities to deploy data structures, now you want to scrutinize it again to see if there's any opportunities to simplify or speed up your algorithm using randomization. We of course, have seen a couple of killer apps of randomization in this book series sort of most famously The Quicksort Algorithm way back in part one but also in part four.
00:10:48.042 - 00:11:07.726, Speaker A: In this video playlist, we talked about the color coding algorithm for computing long paths and graphs that also crucially, used randomization to have a random coloring of the vertices so that you could then look for a panchromatic path. So for example, suppose at some point in your algorithm the algorithm needs to choose one element from among many. So like a position in an array.
00:11:07.726 - 00:11:34.220, Speaker A: See what happens if you choose it randomly? Maybe it'll speed things up if all of the first eight steps fail and you still do not find yourself with a correct algorithm solving your problem in a reasonable amount of time. Unfortunately, it's then time to contemplate the possibility, which, as we've seen is really quite a plausible possibility that there's no efficient algorithm for the problem that you care about. For example, because the problem is NP hard.
00:11:34.220 - 00:12:02.450, Speaker A: Step nine then is to diagnose the problem to see if it is NP hard so that you avoid wasting any more time trying to come up with a too good to be true algorithm for it. So one thing you can do is you can just ask a colleague who's an expert in NP hardness if they think your problem is NP hard. Or alternatively, if you made it through chapter 22 of this book, if you got your mastery of NP hardness up to what we were calling level three, you can try to prove the problem is NP hard yourself.
00:12:02.450 - 00:12:22.806, Speaker A: Remember, we have a two step recipe for doing that. You just choose a known NP hard problem and again, we've covered 19 in this book and there's hundreds more out there. So in the first step of the recipe you pick a known NP hard problem and then in the second step you reduce that known hard problem to yours because reductions spread intractability in the same direction of the reduction.
00:12:22.806 - 00:12:37.470, Speaker A: By reducing the hard problem to yours, you've proven that yours is also an NP hard problem. If you do in fact prove that your problem is NP hard, or alternatively, if a colleague tells you it is and you believe them. Now, as we discussed, you can't have it all with NP hard problems.
00:12:37.470 - 00:13:04.806, Speaker A: So you have to compromise. And so you're going to have to decide which compromise is most appropriate for your application. Do you want to compromise on correctness so the algorithm is going to be incorrect for some inputs, or do you want to compromise on speed and have an algorithm that's going to be slow on some inputs? For step ten, let's suppose you decided that you wanted to compromise on correctness, so you really want an algorithm that's fast and you're willing to be incorrect on some inputs in order to always be fast.
00:13:04.806 - 00:13:39.518, Speaker A: Well, actually, all of the algorithm design paradigms which we just iterated through, which are useful for the design of exact meaning polynomial time algorithms, all of them are also useful for the design of fast heuristic algorithms, including the divide and conquer and dynamic programming paradigms. However, for the design of fast heuristic algorithms, there really is one design paradigm that stands above the rest as being the most frequently useful, and that is the greedy design paradigm. So remember, always the flaw of greedy algorithms is that they're usually not correct, but with an NB hard problem, all of the polynomial time algorithms are going to be incorrect.
00:13:39.518 - 00:14:07.014, Speaker A: So the flaw of greedy algorithms is really a perfect fit for the design of fast heuristic algorithms. So that's probably the most sensible place to start in most cases. While all the algorithm design paradigms that we learned in parts one through three are relevant for the design of fast heuristic algorithms, there's actually one more design paradigm which we learned in this part that we learned in part four, which is also really useful for that purpose, namely the local search algorithm design paradigm.
00:14:07.014 - 00:14:41.734, Speaker A: So whenever you have a computational problem and it's clear in your mind what are the feasible solutions, so corresponding to the vertices of that metagraph we were talking about. So if it's clear in your mind what are the feasible solutions, what's the objective function that you want to minimize or maximize? And if you can come up with a natural notion of local moves, so how to go from one feasible solution to a different feasible solution that isn't too different from the first one? If you have clear answers to all three of those questions in mind, it's well worth taking a crack with local search. And remember, there's sort of two different ways you can apply local search.
00:14:41.734 - 00:15:07.434, Speaker A: So one is the same way you'd invoke any other fast heuristic algorithm, you'd just give the local search algorithm an input and you'd say, come up with the best solution that you can. And so local search can be very useful for that, coming up with solutions from scratch. But also don't forget that there's kind of a no brainer use of local search whenever you have a little bit of extra computation time, which is you can always tack it on as a post processing step to some other heuristic algorithm that you're using, like, say, a greedy heuristic.
00:15:07.434 - 00:15:22.750, Speaker A: So you run the greedy heuristic, it produces a pretty good solution. You feed that into a local search post processing, and that solution can only get better. So now let's consider the other fork in the road where instead of compromising on correctness, you decide to compromise on speed.
00:15:22.750 - 00:15:40.498, Speaker A: So you really want an exact algorithm. You want it to output the correct answer on every possible input, but you're willing to live with the fact that it's going to be slow, presumably exponential time on some of those inputs. Well, we did see in part four a couple killer apps of dynamic programming, a paradigm we were very familiar with from polynomial time solvable algorithms.
00:15:40.498 - 00:15:54.026, Speaker A: But now we've seen it applied several times again to come up with exact algorithms for NPH hard problems. I mean, they're dynamic programming algorithms. They still take an exponential amount of time in the worst case, for example, because they might use an exponential number of subproblems.
00:15:54.026 - 00:16:09.650, Speaker A: But still, we've had many examples where dynamic programming can beat the pants off of exhaustive search. So still, maybe the most compelling example, I would say, would be the but also, you know, we saw a nice dynamic programming algorithm for the traveling salesman problem, the Bellman Health Corp. Algorithm.
00:16:09.650 - 00:16:53.920, Speaker A: And finally, if you're after an exact algorithm but dynamic programming doesn't seem applicable, or if the dynamic programming algorithms are just too slow, then you probably want to consider using one of these semirelible magic boxes, for example, for mixed integer programming or Satisfiability. So remember, mixed integer programming, that's most natural for encoding optimization problems, where Satisfiability tends to be more directly useful for search problems, where you're just trying to find out whether or not there's a feasible solution, and if so, get your hands on a feasible solution. So whenever you have a problem which is easily encoded as either a MIP problem or a Sat problem, well worth throwing the latest and greatest MIP or Sat solvers at the problem and crossing your fingers and seeing how well it does.
00:16:53.920 - 00:17:19.930, Speaker A: That concludes the 13 step checklist that I wanted to tell you about, a checklist that I go through when I'm trying to make sense of a new computational problem that I've been confronted with. Needless to say, this is not gospel. This is just take this as a starting point or a model if you like, as you develop your experience with algorithms and as you acquire skills beyond what you've learned in this book series, you'll of course, want to develop your own personalized recipe.
00:17:19.930 - 00:17:33.694, Speaker A: And that, my friends, is the end of our journey, at least for now. And concludes the Algorithms Illuminated book series. And I've been teaching this stuff, I've been teaching algorithms now for many, many years.
00:17:33.694 - 00:17:50.866, Speaker A: And I got to tell you, it never gets old. It never stops being fun studying algorithms is an excuse to learn about many of the greatest hits of computer science. Many of the most brilliant ideas in the history of the discipline have made appearances in this book series and in these videos.
00:17:50.866 - 00:18:03.654, Speaker A: So along the way, you've learned a number of elegant and clever algorithms and concepts. You've learned a bunch of practical techniques that you can apply in your own programming projects. And there's always been a remarkable confluence between those two things.
00:18:03.654 - 00:18:18.774, Speaker A: So, look, I know it hasn't always been easy algorithms and data structures, especially once you get to the cutting edge. It's a quite difficult topic. After all those computer scientists that preceded us, those were some creative and brilliant individuals.
00:18:18.774 - 00:18:45.894, Speaker A: But you are now in a position to stand on the shoulders of those giants and apply their ideas in your own projects. And if I wanted to get a little bit starry eyed about things, I might even hope that these books or these videos have left you just a little bit changed, maybe a little bit more curious or passionate about computer science, or maybe just a little bit smarter than when we started. See you next time.
00:18:45.894 - 00:18:46.370, Speaker A: Bye.
