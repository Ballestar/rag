00:00:00.250 - 00:00:12.410, Speaker A: All right, so let's get started. So let me sort of just remind you where we are. So last week for the first time, we relaxed our incentive compatibility guarantee.
00:00:12.410 - 00:00:23.210, Speaker A: For weeks and weeks we'd been focused just on strong guarantees like dominance strategy. Incentive compatibility expose incentive compatibility. Last week for the first time, we started talking about Bayes Nash implementations.
00:00:23.210 - 00:00:41.878, Speaker A: So the current set of goals the following as usual, we want simple and polytime mechanisms. As usual, we want to have welfare as close to optimal as possible, at least subject to polynomial time for the case of NP hard welfare maximization problems. And we want mechanisms that are Bayesian incentive compatible for today or for this lecture.
00:00:41.878 - 00:01:00.606, Speaker A: And remember, what that means is, that means direct revelation mechanisms should form a Bayes Nash equilibrium. Now to talk about Bayes Nash equilibrium, there has to be a notion of a prior. So basically, when an agent is reasoning about whether or not direct revelation is a good idea or not, so what do they know? They know some common prior distribution capital F.
00:01:00.606 - 00:01:09.534, Speaker A: They're assuming that other bidders valuations are drawn from that distribution. They know their own valuation. And there are these strategies, so they know the strategies that everybody are playing.
00:01:09.534 - 00:01:43.450, Speaker A: So I don't know other people's valuations, but if I knew their valuations, then I would know their actions. Okay, so in algebra, so what does it mean that direct revelation is a Bay's Nash equilibrium? Just means that for every bitter i, whatever its valuation VI might be. So whatever its valuation might be, then direct revelation, meaning just report the true value, should maximize its expected utility.
00:01:43.450 - 00:01:59.102, Speaker A: And here the expectation is over the random draws of the other bidders. When I talked about the general Bayesnash setup last week, I allowed the prior capital F to be correlated until further notice, the distribution is a product distribution.
00:01:59.166 - 00:01:59.346, Speaker B: Okay?
00:01:59.368 - 00:02:06.946, Speaker A: So until further notice, bidder's valuations are independent. So I don't need the condition on VI when I write V minus I drawn from F minus i.
00:02:07.048 - 00:02:07.566, Speaker B: Okay?
00:02:07.688 - 00:02:36.460, Speaker A: So again, BIC means that direct revelation sigma I-V-I equal VI should maximize utility. So the UI. So here I'm assuming that other bidders valuations are drawn from F minus i, and I'm also assuming that they're doing direct revelation because it's an equilibrium concept, maximizes this over everything that bidderI could do, so over all reports bi.
00:02:36.460 - 00:02:50.494, Speaker A: So in other words, you can declare whatever you want, declare whatever bi you want, but under the following assumptions. First of all, other bidders valuations are drawn from F minus i. Second of all, the other bidders are doing direct revelation.
00:02:50.494 - 00:02:56.834, Speaker A: Under those two assumptions, your utility, expected utility is maximized by just direct revelation.
00:02:56.962 - 00:02:57.446, Speaker B: Okay?
00:02:57.548 - 00:03:22.490, Speaker A: So that's what a Bayesian cynic battle mechanism is, all right? And I don't think we didn't really talk about so we didn't really talk about concrete examples last week. The point of this lecture is going to be a very sweeping, really very satisfyingly, sweeping positive result. Basically saying whenever you can get a computationally efficient approximation algorithm, you can massage it in a way that it becomes a Bayesian incentive compatible mechanism.
00:03:22.490 - 00:03:57.990, Speaker A: Okay? So we'll get positive results that are simply impossible for the stronger notions of incentive compatibility. So what's sort of the coolest thing that could be true? So, the Holy Grail would be that we can achieve all three goals whenever we know it's possible to achieve only the last two goals. So whenever you have a polynomial time algorithm which has some guarantee on the welfare, we'd like to basically get Bayesian instead of compatibility for free.
00:03:57.990 - 00:04:00.102, Speaker A: And I'm going to show you how to do that in this lecture.
00:04:00.166 - 00:04:00.730, Speaker B: Okay?
00:04:00.880 - 00:04:16.080, Speaker A: Very cool. All right, so what's the sort of methodology that we'll use? Well, it's going to be this black box reduction, which I sketched last time. So let me remind you the blueprint of how this works.
00:04:16.080 - 00:04:32.818, Speaker A: Um, so there is going to be we're going to be given an algorithm A, which achieves only two and three.
00:04:32.904 - 00:04:33.154, Speaker B: Okay?
00:04:33.192 - 00:04:51.206, Speaker A: So you want to think about a canonical case like bidders with submodular valuations, something like that. That's a problem for which we know there are good approximation algorithms, and we totally failed to get a desicc mechanism outside of the special case of coverage valuations. So that's A A is something which is not suitable for a mechanism, but has a good approximation ratio.
00:04:51.206 - 00:05:06.938, Speaker A: And we want to design this algorithm B. And the way we're going to do this is we're going to preprocess valuations, okay? So the valuations will first go into initial subroutine. These are going to be resamplers.
00:05:06.938 - 00:05:25.274, Speaker A: And so this will somehow basically resample valuations and feed this new valuation profile into the old algorithm. And the algorithm will not be any of the wiser. The algorithm will just treat these resample evaluations as evaluation profile.
00:05:25.274 - 00:05:35.074, Speaker A: It will do its thing. And what we want is, even though A is unsuitable for a BIC mechanism, b can be implemented as part of a BIC mechanism.
00:05:35.202 - 00:05:35.880, Speaker B: Okay?
00:05:37.550 - 00:05:54.622, Speaker A: So the goal here is given an algorithm A and a prior capital F, construct an algorithm B, which has the form. First, you apply resampling. This is a subroutine R.
00:05:54.622 - 00:06:09.090, Speaker A: Then you apply A, where Ri is a polytime resampler.
00:06:12.230 - 00:06:12.930, Speaker B: Okay?
00:06:13.080 - 00:06:26.886, Speaker A: Mapping valuations to valuations. So that's the high level plan. Now, for this to give us the sweeping positive result that we want, basically two things should be true.
00:06:26.886 - 00:06:36.730, Speaker A: So first of all, the incentive failures possessed by A should go away. Secondly, we should inherit the approximation guarantee of A.
00:06:36.800 - 00:06:37.034, Speaker B: Okay?
00:06:37.072 - 00:06:52.582, Speaker A: So what's good about A, we retain what's bad about A, we get rid of. So primary goals for R. So, again, there's going to be one algorithm, Ri, for each of the N bidders.
00:06:52.666 - 00:06:53.298, Speaker B: Okay?
00:06:53.464 - 00:07:17.634, Speaker A: So Ri takes I's valuation and spits out a different valuation. So, first of all, the composed algorithm R followed by A is BIC implementable. All I mean here is that by BIC implementable, I just mean so B is basically an allocation rule.
00:07:17.634 - 00:07:28.106, Speaker A: It takes his input evaluation profile, spits out some allocation. I just mean I can couple it with some payment rule so that that allocation payment rule combo is indeed a BIC mechanism. So that's all I mean.
00:07:28.106 - 00:07:34.810, Speaker A: By BIC implementable, there exists a suitable payment rule and then it should be welfare improving.
00:07:34.970 - 00:07:35.680, Speaker B: Okay?
00:07:36.690 - 00:08:05.618, Speaker A: So if I look at the expected welfare of algorithm B, so equivalently, remember, B is just A applied to the resampled valuations. That should be at least as good as if I didn't resample. And remember sort of we're motivated by the situation where A is a good algorithm, okay? So we're thinking of this as being very close to the maximum possible welfare.
00:08:05.618 - 00:08:16.390, Speaker A: So maybe for every possible valuation profile, a is close to the maximum welfare, or at least maybe on average over the relevant distribution f, the expected welfare is close to optimal.
00:08:16.470 - 00:08:16.714, Speaker B: Okay?
00:08:16.752 - 00:08:21.542, Speaker A: And the point here is just that whatever guarantee this has, then our massaged algorithm has it as well.
00:08:21.616 - 00:08:22.240, Speaker B: Okay?
00:08:22.850 - 00:08:32.918, Speaker A: So if we can do these two things, I mean, that's great. That basically says this is the technical sense in which BIC will be without loss for approximation algorithms.
00:08:33.034 - 00:08:33.700, Speaker B: Okay.
00:08:35.510 - 00:09:03.942, Speaker A: Good. All right, so another really important property for making this work is we're going to have each resampler be distribution preserving. So remember, a resampler R sub i, we're envisioning it taking the actual valuation at Bitter IVI and remapping it to some other valuation.
00:09:03.942 - 00:09:13.482, Speaker A: So it gets an input at random. It remaps it so its output is also random. We would like the distribution over inputs to be exactly the same as the distribution over outputs.
00:09:13.482 - 00:09:24.954, Speaker A: That is, we would like the output of the I three sampler to be distributed according to F sub I, the same distribution from which VI is drawn.
00:09:25.082 - 00:09:25.760, Speaker B: Okay?
00:09:26.390 - 00:09:43.798, Speaker A: So this is another property we're going to use as sort of a guiding principle for how we're going to come up with these resamplers, these R sub I's. Now we'll kind of see in the proofs why this distribution preservation property is useful. But let me just say about why we might want it.
00:09:43.798 - 00:10:08.270, Speaker A: In fact, it's sort of hard to think about how you'd prove this result without this property. But so basically improving the key properties one and two distribution preservation will allow properties one and two to decouple across the bidders. So decouples proof of one two across bidders.
00:10:08.270 - 00:10:33.078, Speaker A: And so we'll see this in more detail later, but just roughly speaking, what's going on? So if everybody else's distribution stays exactly the same, remember the BIC implementability condition when a player I is reasoning about whether or not direct revelation is a good idea, it's only reasoning about other people in as much as we're sort of averaging over V minus I.
00:10:33.164 - 00:10:33.462, Speaker B: Okay?
00:10:33.516 - 00:10:43.834, Speaker A: So if behind the scenes there's some resampling going on, which leaves the distribution of what I see invariance, then the decision about whether or not something is the best response is also invariant, stays exactly the same.
00:10:43.952 - 00:10:44.266, Speaker B: All right?
00:10:44.288 - 00:10:57.246, Speaker A: So somehow, if other distributions stay exactly the same, it allows me to handle incentives separately for each of the bidders. That's the intuition for the first part, for the welfare part. Well, so we want the expected welfare to only go up.
00:10:57.246 - 00:11:07.362, Speaker A: And so if all the distributions are remaining unchanged, then by linear of expectation, I can just handle this bidder by bidder. That's all that matters. We'll return to that precisely later.
00:11:07.362 - 00:11:16.598, Speaker A: But that's the gist about why you might want this distribution preservation property as a tool for proving one and two. These are the two things we really care about.
00:11:16.764 - 00:11:17.480, Speaker B: Okay.
00:11:19.530 - 00:11:32.166, Speaker A: So that's the high level. So now the rest of this lecture, I want to show you the construction of these Ris, which is pretty nice, actually. Okay, so I gave you a hint at the end of last lecture.
00:11:32.166 - 00:11:52.510, Speaker A: Last lecture, I talked about the special case of single parameter settings and uniform distributions just to kind of as a warm up. So today I really want to talk, though, about multi parameter settings, right? We're really motivated by these commentary options problems where we just failed to come up with good Dsik mechanisms. So I'm really thinking about, like, submoduular valuations as a motivating problem.
00:11:52.510 - 00:11:56.862, Speaker A: So that's multi parameter. That's not single parameter. Okay, so it was really just a warm up, last lecture.
00:11:56.862 - 00:12:33.850, Speaker A: So here's the general setup. So bidder, I am, however, going to retain the assumption from last lecture that there's an explicit set of possible valuations that's finite and that's explicitly given, okay? And that's not that satisfying an assumption, but we're going to need it for today. Okay, so bidderI has a finite set ti of possible types pretty much in this class, whenever I say type, you should just think valuation.
00:12:33.850 - 00:12:59.374, Speaker A: So type is just the private information known to that agent that you don't know as the designer, but think valuation has a finite set of possible types or valuations. And you should also think about it that we're basically given as input just a list of the possible types and the probabilities of those various types.
00:12:59.422 - 00:12:59.586, Speaker B: Okay?
00:12:59.608 - 00:13:04.834, Speaker A: So this is a prior. So remember, all of this only makes sense with respect to a prior. I'm assuming it's a product prior.
00:13:04.834 - 00:13:11.194, Speaker A: So I only need to know the marginal distributions. So the probability for each bidder, what's the probability of VI? And that's just given as a list.
00:13:11.312 - 00:13:11.980, Speaker B: Okay?
00:13:21.870 - 00:13:42.414, Speaker A: And so what you might want to think about is you might want to think about, like saying submoduular valuations. Think about some concrete class that we've talked about, how to represent coverage valuations, k unit demand valuations, something like that. So just imagine we're given like, a polynomial length list of succinctly represented valuations of that type with some probability masses.
00:13:42.414 - 00:13:44.580, Speaker A: So that's what you should think about.
00:13:46.070 - 00:13:46.626, Speaker B: Okay?
00:13:46.728 - 00:13:56.818, Speaker A: And then also there's some just algorithm which again, we're thinking of as being a good approximation algorithm, which just maps evaluation profile or a type profile to an allocation.
00:13:56.994 - 00:13:57.720, Speaker B: Okay?
00:14:06.110 - 00:14:10.458, Speaker A: Yeah. So these are numbers that obviously for a fixed I, these are non negative numbers that sum to one.
00:14:10.544 - 00:14:11.180, Speaker B: Okay?
00:14:12.270 - 00:14:28.430, Speaker A: All right. And so again, think of A as either a good approximation algorithm, possibly in the worst case for every valuation profile, or at least on average with respect to the specified prior. Now I want to proceed for each bidder independently.
00:14:28.590 - 00:14:29.300, Speaker B: Okay?
00:14:32.150 - 00:14:33.810, Speaker A: Any questions about the setup?
00:14:35.450 - 00:14:36.200, Speaker B: Okay.
00:14:38.170 - 00:14:55.126, Speaker A: So here's the way I want you to think about this. What we're given is we're given an algorithm A, where if only people did direct revelation, we'd get great welfare. By definition, it's a good approximation algorithm.
00:14:55.126 - 00:15:06.154, Speaker A: So if we ran it on the true input, we'd do well, that's A. The problem is it's not a BIC mechanism. People aren't motivated to do direct revelation.
00:15:06.154 - 00:15:15.614, Speaker A: So let's just make a small change. Let's stick with algorithm A, let's fix a bidder I, and let's just try to correct the incentives for bidder I by itself.
00:15:15.732 - 00:15:16.238, Speaker B: All right?
00:15:16.324 - 00:15:22.338, Speaker A: So for every bidder other than I, let's imagine that they do direct revelation, even though it might not be in their best interest.
00:15:22.424 - 00:15:22.674, Speaker B: Okay?
00:15:22.712 - 00:15:27.682, Speaker A: So let's analyze the world from I's perspective, assuming everyone else does direct revelation.
00:15:27.826 - 00:15:28.520, Speaker B: Okay.
00:15:42.030 - 00:16:27.560, Speaker A: Looks like something died on the board here. Okay, so first step, so fix bidder I. And in effect, we're basically just assuming that all the other resamplers are the identity and everybody else is just directly revealing their information even though they might not be incentivized to for J not equal to I.
00:16:27.560 - 00:16:35.770, Speaker A: And so the question is just how to incentivize direct revelation for I under these assumptions.
00:16:43.730 - 00:16:44.640, Speaker B: All right.
00:16:47.090 - 00:17:04.158, Speaker A: Now what do we have under our control? So there's two things we actually need to design. The first thing obviously is the resampler, the Ri. But also, remember, ultimately this algorithm B, this is just the allocation rule.
00:17:04.158 - 00:17:12.200, Speaker A: Ultimately this is going to be embedded in some mechanism. So somewhere, either implicitly or explicitly, we're thinking about designing a payment rule pi as well.
00:17:15.210 - 00:17:15.960, Speaker B: Okay?
00:17:16.330 - 00:17:36.954, Speaker A: So the question is, how do we design just for I, the resampler Ri and the corresponding payment rule pi so that its expected utility is maximized by direct revelation. So this is something we talked about in the special case at the end of last lecture. So again, the special case I said let's just talk about single parameter settings.
00:17:36.954 - 00:17:49.054, Speaker A: This was the end of last lecture and let's just assume the uniform distribution. And the reason I started us with single parameter settings is because we have a very good sense for what's an implementable allocation rule. It's just monotonicity.
00:17:49.054 - 00:18:03.090, Speaker A: We talked about that a lot last quarter for DSIC mechanisms. But as we explained last week, it's basically the same story for BIC mechanisms, single parameter. So an allocation rule is implementable can be turned into a BIC mechanism if and only if it's monotone.
00:18:03.090 - 00:18:15.098, Speaker A: So if and only if as you average over all the V minus I's, the amount of stuff you get is only going up as you bid higher. So we didn't prove it, but the proof is the same. But we talked about, I called it the extended Myerson dilemma last week.
00:18:15.098 - 00:18:41.570, Speaker A: Monotonicity is sufficient, also necessary. But the point was, it was sufficient for BIC implementability. Now the whole point of today's lecture is to do things like commentary auctions, which are multi parameter, which means it doesn't make sense to talk about a monotone allocation rule, okay? There's multiple things you're bidding on, so what does it mean to just bid more? What if you bid more on one thing and less on another? So this is what's, a little intimidating, but actually it plays out really nicely in this context if you think about it.
00:18:41.570 - 00:19:10.794, Speaker A: What are we trying to prove? We're trying to massage algorithm A into a different algorithm and then we need to argue, okay, and now all of a sudden it's incentive compatible. So we need to think to ourselves like, what's that part of the proof going to look like? Where we write down this proof that says, oh, now we've got it, now we've got our incentive compatible mechanism. Because again, we don't have Myerson's lemma, it's not single parameter, okay? So we need some part of the proof where we argue we've achieved the desired incentive compatibility, okay? So that's something to watch out for.
00:19:10.794 - 00:19:19.626, Speaker A: That said, it's going to fall out quite nicely. We're again going to use basically a matching approach. So in the single parameter setting last week, we drew up a bipartite graph.
00:19:19.626 - 00:19:42.694, Speaker A: On the left hand side was the true type of a bidder, on the right hand side was a possible resampled type. And we talked about how distribution preserving resamplings corresponded to perfect matchings in that graph. We also argued that if the allocation rule was already monotone, then the maximum weight matching for the appropriate weights was just the sort of easy perfect matching from left to right.
00:19:42.694 - 00:20:00.022, Speaker A: So you just match the jth smallest valuation with the JH smallest allocation probability. And if you don't have a monotone allocation rule, then if you compute a maximum weight matching, it just automatically uninverts all the inversions and the allocation rule. And that gave us the monotone allocation rule at the end of the day, which gave us the implementable mechanism.
00:20:00.022 - 00:20:22.990, Speaker A: So we're still going to use this matching setup, we're still going to look at now it's going to be basically fractional matchings from true types to resampled. So we can't use Myerson's limit to argue monotonicity. But what turns out to be cool is the exact same linear programming duality we use to characterize well, RAW's uni equilibria will be able to use directly to argue that we're incentive compatible.
00:20:23.150 - 00:20:23.618, Speaker B: Okay?
00:20:23.704 - 00:20:29.810, Speaker A: So we're basically going to wind up proving it directly we're basically going to wind up proving it directly with tools we've already developed.
00:20:29.890 - 00:20:30.134, Speaker B: Okay?
00:20:30.172 - 00:20:35.846, Speaker A: So that's the good news. So pretty much all the pieces we're about to assemble you've seen in some form before.
00:20:36.028 - 00:20:36.760, Speaker B: Okay?
00:20:38.510 - 00:20:50.010, Speaker A: All right, so any questions about any of that? If not, I'll sort of set up the right analog of the matching for this more general setting.
00:20:51.010 - 00:20:51.760, Speaker B: Okay.
00:20:53.650 - 00:21:13.954, Speaker A: All right, so here's the idea. So remember, we have two things we need to design, all right? If we want to sort of establish incentive compatibility, we need to pick this resampler. So how do we remap playerized type? And then secondly, if we want, we can also talk explicitly about what the payments are going to be.
00:21:14.072 - 00:21:14.514, Speaker B: All right?
00:21:14.552 - 00:21:58.260, Speaker A: So the resampling algorithm R, that's going to correspond to a primal linear programming solution like fractional matchings, and the prices will just fall out of the dual. So in what way are resampling algorithms equivalent to some kind of fractional matching? What are called sometimes B matchings, which just means that the amount of flow into or out of a node might be something other than one. This is also called the transshipment problem sometimes.
00:21:58.260 - 00:22:45.090, Speaker A: So let me just show you the LP formulation, and it's basically the same as the LP formulations we were talking about for unit demand bidders. A couple of slight differences that we'll discuss, but no big deal. And so, again, the graph you should have in mind here is you want to have in mind on the left hand side true types, and on the right hand side, resampled types.
00:22:45.090 - 00:23:14.650, Speaker A: And you should think of a resampling algorithm remapping an input valuation VI to some other output valuation V hat I as like pushing flow from that true type VI to the resampled type V hat i. So the probability between the inputs and the outputs is going to correspond to flow in this graph from the left side to the right side. Okay, so let's fill in some details.
00:23:14.650 - 00:24:16.926, Speaker A: So first, what are the decision variables? So T sub I is the possible types of bitter i, and so basically they're going to correspond to flow variables. So for a choice of a left hand side and right hand side node, we're going to have a decision variable S sub t, which is meant to represent the probability that, first of all, the true type of bidder I is S, and then secondly, that gets remapped to t. Okay? So that is the semantics you should have for the decision variable XST in this linear program, okay? It's the probability that the bidder shows up with type S, and then it's fed to the resampling algorithm says, yeah, well, for today, let's report you as T to the algorithm A.
00:24:17.028 - 00:24:17.680, Speaker B: Okay?
00:24:21.010 - 00:24:52.220, Speaker A: So constraints, all right? So first, for every true type S, let's look at the sum of the excess T's over T. So this is the sum of the outflow on a left hand side node. So what should that be equal to? For this to make sense, for the semantics to make sense.
00:24:52.220 - 00:25:34.040, Speaker A: So looking at this, so XST is the probability that VI equals S and R-I-V-I equals T. So if we sum this over all possible T's, so we're summing this probability over all possible T's, then what are we left with? That's just the probability that VI equals S, right? And the probability that VI equals S that's given by the prior. So that should just be the probability that I has type S.
00:25:34.040 - 00:25:54.534, Speaker A: So this is going to be F I of S. Okay? Another way to think about it is you want to think about the initial flow. In this graph, you want to think of this one unit of flow, which is probability mass, and that one unit of flow or sort of initial commodity is distributed according to the true types, according to the prior.
00:25:54.662 - 00:25:54.918, Speaker B: Okay?
00:25:54.944 - 00:26:03.762, Speaker A: So at a given node S, here it starts. It's endowed with FIS units of stuff, okay? And we want everything to go from the left hand side to the right hand side.
00:26:03.816 - 00:26:04.034, Speaker B: Okay.
00:26:04.072 - 00:26:16.500, Speaker A: So the outgoing flow should also equal FIS. Okay, good. Now the second set of constraints is motivated by our distribution preservation property.
00:26:16.500 - 00:26:55.094, Speaker A: So remember, one of our guiding principles is that when we define these randomized algorithms ri that map types to types, we want the distribution on the output types to be the same as the distribution on the input types. So given that that's what we want, if we fix a resample type, so a right hand side node and we look at the sum of the flow going in or the total probability mass leading to T, what should this be equal to? F I of t good. F I of t.
00:26:55.094 - 00:27:12.734, Speaker A: Thank you. Okay, and that's just because that by definition is the input is the probability that the true type is T, and we want the probability of the resample type is T to be equal to the probability of the true type is T. Good.
00:27:12.734 - 00:27:30.318, Speaker A: So that's the entire primal, okay, the entire primal linear program. And the claim is that feasible solutions to this linear program are in bijective correspondence to distribution preserving resampling algorithms.
00:27:30.494 - 00:27:31.314, Speaker B: Okay?
00:27:31.512 - 00:27:57.002, Speaker A: Namely, so how would you define an algorithm from one of these LP solutions? Well, if someone with true type S shows up, you would just output a type with probability proportional to the excess t's. Where t's I'm ranging over all possible output types t. Okay, so the probability given that you have somebody with S, the probability that you up with T is just going to be XST normalized by f I of S.
00:27:57.002 - 00:28:13.520, Speaker A: Okay. So you want to think of probability that ris equal T given S. This is just going to be XST over F I of S.
00:28:13.520 - 00:28:19.566, Speaker A: Okay? So that's how, given these X's, you would define the Ri.
00:28:19.678 - 00:28:20.050, Speaker B: Okay.
00:28:20.120 - 00:28:27.798, Speaker A: Just output. So it's basically just it's almost like randomized rounding of this linear program, if you like, on the fly. Okay, good.
00:28:27.798 - 00:28:35.270, Speaker A: Everyone clear on that? Feasible solutions to this are exactly the same thing as distribution, preserving resampling algorithms.
00:28:35.610 - 00:28:36.086, Speaker B: Not clear.
00:28:36.108 - 00:28:42.486, Speaker A: This was helpful. Okay, but hope I just want to make sure the bijection is clear. Okay, but it turns out it is helpful.
00:28:42.486 - 00:28:55.914, Speaker A: It's really helpful, actually. Okay, so let's see why. So one question is, is there something that makes sense to optimize over this polytope?
00:28:56.042 - 00:28:56.720, Speaker B: Okay.
00:28:57.410 - 00:29:18.818, Speaker A: And there is. So in the same way, one thing I want to point out, even though we'll be using exactly the same math, the exact same complementary slackness conditions we used for unit demand bidders, but the semantics are pretty different. I mean, it's maybe like worth squinting and just not worrying about the differences in semantics for this lecture because it might get confusing.
00:29:18.818 - 00:29:29.574, Speaker A: That said, back a couple of weeks ago when we were talking about unit demand bidders and the LP duality, we had different bidders on the left hand side and various items on the right hand side.
00:29:29.692 - 00:29:30.022, Speaker B: Okay.
00:29:30.076 - 00:29:51.786, Speaker A: And the edges were kind of the value of a given bidder for a given item. And so now we've fixed a single bidder and we're just looking at all of the possible roles it might play. So these are kind of like possible lives that biddereye might be leading, and then the items are somehow corresponding to particular misrepresentations of that bitter designed to kind of coax some algorithm into a better solution.
00:29:51.786 - 00:30:00.914, Speaker A: Okay, so the semantics of what things mean is not well preserved. Okay, but it's the same linear program, the same duality, the same combination slackness. That's exactly what we need.
00:30:01.032 - 00:30:01.700, Speaker B: Okay.
00:30:03.270 - 00:30:07.240, Speaker A: Okay, good. So here's the objective we're going to want to study.
00:30:09.530 - 00:30:10.280, Speaker B: Okay?
00:30:11.370 - 00:30:26.646, Speaker A: So let me just write it down, and then as time proceeds, we'll see why it's natural. So I mean, in the same sense, in the same way unit demand with unit demand bidders, we wanted to do a welfare maximizing allocation here. We somehow want to maximize welfare on behalf of bidder.
00:30:26.646 - 00:30:37.454, Speaker A: I in some sense. So let me write down the objective for that. So I'm going to maximize overall edges, if you like.
00:30:37.454 - 00:30:51.090, Speaker A: So choices of a left hand side node s, a right hand side node t. And we're going to look at the amount of flow going from S to t. And so what's the coefficient? Okay, so there's going to be some edge weight.
00:30:51.090 - 00:31:15.466, Speaker A: I need to tell you what WST is. So WST is defined as says well, I's true type is s. We're thinking about the case where, right, so this says true type s, resampled or misreported type t.
00:31:15.466 - 00:31:32.554, Speaker A: So imagine that I's true type is s, but it says that its type is t to the algorithm and everybody else reports truthfully. So this is a valuation profile. The other bidders do whatever they're doing.
00:31:32.554 - 00:32:12.618, Speaker A: I says its value is T, its true value is S, and I average this over the other bitters. Okay, so WST, which sort of matches up with these semantics, right? So if you think about this resampling algorithm, an XST pair, that's how frequently somebody with true type S gets remapped to somebody with type T, and then that gets fit into the algorithm. What we're saying here is just like, oh, well, when the true type is S and it is actually misreported as T, how good is that for this bidder? Is that like a lot of expected utility or not so much expected welfare? Excuse me? Or not so much?
00:32:12.704 - 00:32:13.342, Speaker B: All right.
00:32:13.476 - 00:32:22.974, Speaker A: So as always, we use its true type to evaluate its value. But of course, the mechanism isn't privy to the true type. It only goes on what it's told.
00:32:22.974 - 00:32:35.246, Speaker A: And with this st pair, algorithm A is being told that the type is T. Okay? So this is how good this particular misreport is for this particular bidder, averaging over the valuations for all the other bidders.
00:32:35.358 - 00:32:35.826, Speaker B: Okay?
00:32:35.928 - 00:32:53.814, Speaker A: So if you think about it, that's the sensible coefficient, given our semantics, that XST is how frequently you lie about someone whose S is actually being t. This is just how good that is for that bidder. Now, why would we want to maximize this? Well, there's sort of two reasons.
00:32:53.814 - 00:32:59.942, Speaker A: So remember what our goals are. So we have this algorithm A. It's not incentive compatible, but it's a good approximation.
00:32:59.942 - 00:33:22.110, Speaker A: So first of all, we're trying to go towards something which is incentive compatible. And this is going to be vague, but I hope by now you have this sense that incentive compatible mechanisms always are basically optimizing on behalf of the participants, right? Why would you tell somebody what you really want? Well, you'd only do it if they actually act in your best interests. If they don't act in your best interests, you're going to misreport your interests.
00:33:22.110 - 00:33:39.042, Speaker A: So incentive compatibility somehow has the flavor of optimizing exactly on behalf of participants. And so that's what this objective is doing informally. This is just saying, given that we're going to remap eyes types in some way, let's make sure it gets the most expected utility possible, overall possible remappings.
00:33:39.042 - 00:33:50.454, Speaker A: Given that we're feeding it into algorithm A, that's the first thing. Secondly, the second property we want is we want welfare to only go up. And so at least from bitter eyes perspective, again, we're sort of maximizing overall remaps.
00:33:50.454 - 00:34:01.022, Speaker A: Okay? All distribution preserving remaps. But still there's this spirit of we're trying to make eyes welfare as high as possible. And so that should somehow also lead to kind of us doing only better than we were doing before.
00:34:01.022 - 00:34:06.514, Speaker A: Okay, so those are all the reasons this feels like should feel like kind of the sensible objective to write down.
00:34:06.632 - 00:34:07.300, Speaker B: Okay.
00:34:09.590 - 00:34:22.038, Speaker A: Okay, good. So let me just. Make an informal claim which basically is just that for this linear program.
00:34:22.038 - 00:34:39.334, Speaker A: So notice, I mean, this is the same objective as we had for our welfare maximization back with unit demand bidders, okay? We've seen this objective before. I'm just going to claim basically we know everything we could care to know about this linear program, okay? We know it's dual, we know the complementary slackness, et cetera.
00:34:39.462 - 00:34:40.054, Speaker B: Okay?
00:34:40.192 - 00:35:19.734, Speaker A: We already did all that back in lecture six, I think it was same LP as for unit demand bidders in lecture number six. Okay, aside from the semantics, which as we already discussed are quite different, there's two minor technical differences, let me just point out. So minor technical difference number one is that with unity demand bidders, we had ones here saying in that case that okay, I guess two things.
00:35:19.734 - 00:35:31.142, Speaker A: So first of all, we didn't have a quality, we had it most, okay? Because we were thinking about a case where we have N bidders M goods. N can be bigger than M or vice versa. So maybe some goods are unsold, maybe some bidders don't get an item.
00:35:31.142 - 00:35:45.422, Speaker A: So that makes almost no difference. It just means that in the dual, instead of having non negative dual variables corresponding to constraints, we have unrestricted dual variables, that's all. All right, so the equations also the right hand side was a little different before.
00:35:45.422 - 00:36:04.022, Speaker A: We had a one here saying that every bidder should get at most one item because it was unit demand that was sensible. And here we had a one saying every good should be allocated only once, which was also sensible. Here instead we have these kind of fractional things.
00:36:04.022 - 00:36:15.034, Speaker A: But if you think about it, it's basically the same thing, right? So imagine for example, all of these are multiples of one over 100. And so maybe one of these is like 17 over 100.
00:36:15.152 - 00:36:15.722, Speaker B: Okay?
00:36:15.856 - 00:36:28.938, Speaker A: So just make 17 copies of this guy, each with like one over 100. So first of all, I guess if all of these were the same, imagine it was a uniform distribution. Imagine these were all one over 100, then it's clearly just the matching LP kind of scaled.
00:36:28.938 - 00:36:45.938, Speaker A: It's just a change in units. And if these are different numbers, you could just imagine looking at the lowest common denominator or sort of the least common multiple and just making suitable replicas of everybody and then just doing matching with the replicas. Matching with the replicas is just going to correspond to fractional solutions over here.
00:36:46.024 - 00:36:46.514, Speaker B: Okay?
00:36:46.632 - 00:36:50.534, Speaker A: So that's sort of a vague but just intuitively, this is really going to be the same.
00:36:50.652 - 00:36:51.320, Speaker B: Okay?
00:36:52.570 - 00:37:13.550, Speaker A: All right, so that's the hand waving that I'm not going to again go through the same old duality for this. I'm just going to use it, but there's not really going to be any surprises. So in particular, what we're going to really find useful is the componentary slackness conditions.
00:37:13.970 - 00:37:14.720, Speaker B: Okay?
00:37:17.490 - 00:37:29.902, Speaker A: So remember what complementary slackness is. So if I give you a solution which is feasible for the primal, I give you a solution which is feasible for the dual componentary. Slackness is a necessary and sufficient condition that they're both optimal.
00:37:29.966 - 00:37:30.194, Speaker B: Okay?
00:37:30.232 - 00:37:39.030, Speaker A: If complementary slackness is satisfied, they're both optimal. If it's not satisfied, at least one of them is not optimal. So let's consider an optimal pair and invoke complementary slackness.
00:37:39.030 - 00:37:50.390, Speaker A: So let x star and Y star, p star be an opt primal dual pair.
00:37:51.050 - 00:37:51.462, Speaker B: Okay?
00:37:51.516 - 00:38:06.250, Speaker A: So x star, of course, is exactly what you see here. I'm not going to write it down, but let me sort of remind you the structure of the dual from before. So there's going to be a set of dual variables for this first set of constraints.
00:38:06.250 - 00:38:14.510, Speaker A: So we're going to be calling these the USS. Actually, these won't play any role. And then there's another set for these, the Pts.
00:38:15.170 - 00:38:15.920, Speaker B: Okay?
00:38:18.450 - 00:38:24.860, Speaker A: The constraints have exactly the same flavor as for unit demand bidders. So the dual variables are exactly the same. Only difference because.
