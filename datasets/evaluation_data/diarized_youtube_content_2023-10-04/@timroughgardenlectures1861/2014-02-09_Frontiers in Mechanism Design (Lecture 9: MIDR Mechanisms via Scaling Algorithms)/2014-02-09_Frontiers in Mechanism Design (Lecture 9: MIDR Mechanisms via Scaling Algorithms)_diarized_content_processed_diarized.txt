00:00:00.570 - 00:00:39.810, Speaker A: So at the end of last lecture, we were studying a particular scenario, a pretty new one. So we had nonidentical items, we had totally general valuations, so not sub modular, nothing like that. But the twist in scenario eight was that we had a lower bound on the supply of every item. Usually we were just thinking of one copy of every item. Scenario eight, we have k copies, and the problem gets easier the bigger k is, the bigger the supply is. So we assume each bidder only wants one copy of a given item, but other than that, it's totally arbitrary. So there's m sort of types of items, k copies of each Km items overall and valuations are defined only on subsets of U, and we just assume we only give one copy to a given bidder.
00:00:39.810 - 00:01:10.510, Speaker A: But actually we didn't even need that. These were monotone last week, if you go back and look at the proofs. So these are totally general vis, and for the purposes of talking about polynomial time, we're thinking of these as black boxes. All we do is we interact with them and they support value queries and demand queries. It's not important right now, but the reason we needed demand queries was to solve an LP relaxation. That was the separation oracle in the duel that we needed to solve the LP. And so what we did last week is we said, well, let's put incentives aside and just focus on the algorithmic problem of welfare maximization.
00:01:10.510 - 00:01:56.238, Speaker A: We postponed incentives until today, and for just the underlying welfare maximization problem, we showed that it can be solved in polynomial time almost optimally. So we had a really nice strong positive result. So like I said, the problem gets easier the bigger k is, but so we need k to be logarithmic. So if you have a log m over epsilon squared copies of every item, we gave a polynomial time algorithm that gets within a one minus epsilon factor of welfare maximizing allocation. Let me say the theorem here's what we actually proved, because this is sort of a stronger statement than what I put on the board last time, but the proof actually showed this. The algorithm takes as input, it takes some feasible linear programming solution. So this was for that LP with the decision variables corresponding to bitters I and bundles s.
00:01:56.238 - 00:02:40.880, Speaker A: And so it takes as input an LP solution. And then it did this randomized rounding, so it scaled down the solution slightly multiplied all the y's by one minus epsilon, and then for each bitter it gave it a bundle at random. And we did this independently. For each bitter i, we had these yis, and that we just interpreted those as the probabilities of getting a given bundle s. And what we proved is that the expected welfare of the solution was almost as good as the LP solution, almost as good as what we started with. And there was a decent chance of being feasible. As a result, if you just keep doing independent trials of this randomized routing algorithm, you simultaneously get an allocation which is feasible, meaning every good is allocated at most K times and also the welfare of that allocation is almost as good as that of the LP solution that you started with.
00:02:40.880 - 00:03:06.914, Speaker A: So the particular wise that we were working with last week was the optimal solution, the linear program, that's the obvious one to use. But more generally we didn't really use that fact. We just said, okay, give me a feasible LP solution, feed it into this randomized rounding and this is what you get as output feasible plus almost as good as the fractional solution. Okay, so that's the way we proved this can also be. So I gave you a randomized variant. We're sort of thinking of just doing independent trials until it succeeds. It can be derandomized.
00:03:06.914 - 00:03:41.890, Speaker A: I'm not going to talk about it in class though, but it's sort of nontrivial but standard techniques from 25 years ago, so this can be made deterministic as well. So that's specifically kind of where we left off last week. Now, as far as the big picture, just so you don't lose the forest for the trees, what are we doing? So this is the sort of second and final week of part two where we're looking at sort of NP hard versions of welfare maximization. I didn't say it, but this is also NP hard. We can get close to optimal, but you can't get optimal. And we're still looking for this very strong dominant strategy and center compatibility guarantee. So that's what we're doing now.
00:03:41.890 - 00:04:13.510, Speaker A: And this is hard. This is one of the most challenging goals we're going to see all quarter. And the reason being is as soon as you pass to approximation algorithms generally, the VCG mechanism no longer works. And even more generally we just kind of have a very sort of thin toolbox for designing multipramter DSIC mechanisms. And so the approach we're sort of doing is we're doing this bottom up approach where we're just sort of grasping for whatever DSIC mechanisms we can find. We started with just the VCG mechanism, that's sort of all we had. We said, okay, what could we do? Well, then the first baby step was maximal in range.
00:04:13.510 - 00:04:43.670, Speaker A: So we said, well, I guess if we wanted, we could throw out a bunch of the outcomes up front and then run VCG on what's left. It's clearly DSIC. I give you one application of that for multi unit auctions with non downward sloping valuations. We had a two approximation and so right now we're in the middle of the second baby step. Just like, okay, just these maximum range mechanisms. They have some applications but they're pretty limited. So the second baby step, which we introduced last week and we'll actually use in both of the lectures today, is the extension to maximal and distributional range.
00:04:43.670 - 00:05:06.030, Speaker A: So a maximal and distributional range or MIDR allocation rule. So this is going to be a randomized allocation rule, and instead of pre committing to a set of outcomes, it pre commits to a set of distributions over outcomes. Okay, so script D is the set of distributions.
00:05:11.010 - 00:05:11.422, Speaker B: Okay?
00:05:11.476 - 00:05:18.990, Speaker A: Whenever I talk about outcomes, I'm talking about them abstractly. But for our purposes, we're always thinking about allocations. So just a way of assigning the items to all of the bidders.
00:05:19.070 - 00:05:19.700, Speaker B: Okay.
00:05:22.230 - 00:06:00.874, Speaker A: So maximum range, that corresponds to the case where all the distributions are just point masses. So this is certainly at least as general. What might these be? What is this distribution? It could just be an explicit list output this allocation with 1% probability, this other one with 3% probability, et cetera, et cetera, et cetera. Or maybe it's defined implicitly, like for each item, assign it randomly to a bidder according to some distribution. Those are the kind of distributions we want to have in mind in script D. Now this is in the spirit of VCG, so there should be some flavor of exact optimization. So by definition, once you've settled on this set of distributions, the allocation rule is uniquely defined.
00:06:00.874 - 00:06:32.182, Speaker A: So basically, given that we're going to sample an outcome from one of these distributions, we insist on picking the distribution which is best for the bidders, just in the spirit of ECG. So what's best for the bidders? It just means maximizes their expected welfare, given that we're going to sample from the distribution. So it outputs. So I'm going to use sometimes Little Omega as shorthand for an allocation. But just think of always Little Omega as some S one, S two up to SN the items assigned to all of the bidders.
00:06:32.246 - 00:06:32.762, Speaker B: Okay?
00:06:32.896 - 00:06:59.566, Speaker A: So it outputs some outcome or some allocation drawn at random from the best distribution d star, again, meaning that among all of the candidates. So all of the distributions in script D star should give the bidders the maximum possible welfare, where here VI denotes the reported valuations of the bidders.
00:06:59.678 - 00:07:00.340, Speaker B: Okay?
00:07:00.950 - 00:07:36.090, Speaker A: So again, sort of given that we've pre committed to this distribution set script D, and given that we're going to sample an outcome from one of these distributions, we should pick the best one. And then what I asked you to do as an exercise for exercise set four is to verify that there's a natural analog of BCG payments, which when you couple them with this allocation rule, gives you a DSIC mechanism. So this is a randomized allocation rule. So here we're defining DSIC to be bidders maximize their expected utility where the expectation is over the coin flips in the allocation rule by revealing their true valuation. So we're thinking about risk neutral bidders.
00:07:36.170 - 00:07:36.800, Speaker B: Okay?
00:07:37.890 - 00:07:58.606, Speaker A: All right, so that's kind of where we were. Now that sort of everyone's here, let me just sort of pause for the quick course announcements. So there are only two required problems in exercise set four. Those are due today. I'm going to post an exercise set five soon. That's going to be completely optional. Just have two or three optional questions, so be on the lookout for that today or tomorrow.
00:07:58.606 - 00:08:31.840, Speaker A: So the big news is that all the project deadlines are now sort of settled, and all the project suggestions are up on the website. You should have an email with the link to that so by Friday. So this is for those of you taking the course for a letter grade, by the way, only. So if you're taking the course for a letter grade, send me at least two topics you're interested in by Friday. And then over the weekend, I'll sort of write people back with confirmations of the project topics. There's two deliverables, one just to sort of keep people on track, is I do want a one to two page outline. There's more detailed instructions on the website.
00:08:31.840 - 00:08:55.560, Speaker A: So that's going to be due March 5. That's a Wednesday, and that's two weeks before the final report is due. This is the Wednesday of finals week, March 19, and just ballpark twelve to 15 pages. There's both more detailed instructions. There's also a sample report, all posted on the website. And again, feel free to just get in touch with questions, either picking the projects or afterwards. Happy to help.
00:08:55.560 - 00:08:58.680, Speaker A: All right, so any questions?
00:09:00.570 - 00:09:01.320, Speaker B: Okay.
00:09:04.750 - 00:09:37.490, Speaker A: Right, so basically we have these MIDR allocation rules. We've defined them, we haven't used them. There's this problem where we know how to solve it without incentives, and so no prizes for what comes next. We're going to use these allocation rules to get just as good an approximation for that problem with incentives. Okay, that's the plan for this lecture. And then the second lecture today, we'll go back to what kicked off this whole MB hardness discussion, submodular valuations. And we'll give a second application of MIDR allocation rules to get a constant factor DSIC mechanism for a subclass of submodular valuations.
00:09:37.490 - 00:10:38.540, Speaker A: So that's the plan for today. Full disclosure, this is probably going to be the most sort of technically intense lecture of the quarter would be my guess. So I'm not going to say it's easy after this, but if you can survive today, I think it'll all be downhill from here. And also the lecture right now, this first lecture will be the longer one. The second lecture will be the shorter one for today, probably. So that's just what you can expect. All right, so let me just remind you, why are we doing this? So why do we define these MIDR allocation rules? Well, one thing is just we're just trying to brainstorm about what else might possibly be DSIC, but can we even think of why this might be useful? And so the hope which you shouldn't necessarily believe until I show you the actual examples, but the hope would be in the same spirit, that if you have a discrete problem, which is hard, like an integer program, and you relax it to a linear program, you allow more solutions, fractional solutions that can somehow create tractability when it wasn't there before.
00:10:38.540 - 00:11:14.930, Speaker A: So this is sort of the daydream now a couple of warnings. So first of all, this analogy is not perfect. So when you relax an integer program to a linear program, you keep all the integer solutions and you get a bunch more stuff. You get stuff which is usually strictly better than the integer solutions. We've seen that already. And that's not really what's happening when you pass to an MIDR allocation rule, if you think about it. So you have these integer solutions, these allocations, and we're passing to things which by definition are distributions over allocations.
00:11:14.930 - 00:11:52.974, Speaker A: So you're not going to have anything strictly better than an allocation, you're only looking at distributions of allocations. So in particular, if you think of an MIDR allocation rule, and it happens to be the case that in this set script d that includes all the point masses, all the integer solutions, well, you're not going to do better than an integer solution with a distribution. So if you have all the integer solutions in d, all the point masses, you haven't made your life any easier. Things have not gotten more tractable. You're optimizing over just as hard a set as before. So that's a little different. So somehow these set script d have to be chosen and a fair amount of work has to go into figuring out what the right script d is.
00:11:52.974 - 00:12:25.030, Speaker A: We'll see that today. And then the second comment is we will see a couple of nice applications of MIDR allocation rules, but they're really still pretty limited in use. And so it'll never be the case that you design some randomized algorithm and you get lucky and it happens to be MIDR that just basically never happens. You have to basically channel all your design creativity into ensuring the MIDR property. Now again, we'll see a couple of positive examples, but I mean, it's really a pretty limited kind of playground that we're working in to design these rules.
00:12:25.110 - 00:12:25.642, Speaker B: Okay?
00:12:25.776 - 00:13:06.642, Speaker A: That was true for maximal and range rules. Also, you never accidentally get a maximal and range approximation algorithm. You have to sort of begin the exercise trying to, trying to get that property. Okay, so today, so in the first lecture I want to focus on a special kind of MIDR rule. And these are going to, I'm going to call these scaling algorithms, which is a very restricted form of randomized rounding. Randomized rounding. We've already seen an example of last week in the just approximation algorithm.
00:13:06.706 - 00:13:07.320, Speaker B: Okay?
00:13:08.890 - 00:13:48.742, Speaker A: So let me explain how I want you to think about randomized rounding of linear programs for today. And this will be sort of our perspective in both lectures today. So we think about algorithms, approximation algorithms that are the composition of two steps. So in the first step, you solve a linear program. And in the second step, you round it to an integer solution. So the LP you should have in mind is just the one we were working with over the last couple of weeks with the Yis as decision variables. So the LP you can think of as that as taking these valuations as inputs as specified by a black box supporting demand queries or whatever.
00:13:48.742 - 00:14:32.270, Speaker A: So the first part of the algorithm takes as input the LP sorry, the valuations, and then it solves some LP optimally, and it spits out an optimal LP solution y star is. Okay, so this was exactly the first step in our algorithm last week. Then we have what I'm going to call an oblivious randomized rounding algorithm, R. So the responsibility of the first box is just to produce an LP solution, and the responsibility of the second box is to compile this LP solution, which in general is not a distribution of allocations, into a distribution of allocations.
00:14:32.350 - 00:14:32.594, Speaker B: Okay?
00:14:32.632 - 00:14:48.278, Speaker A: So that is it's going to randomly pick some allocation. So over here we're going to get a distribution, and I'm going to use the notation R of Y star as the distribution over allocations implicit in this rounding algorithm.
00:14:48.374 - 00:14:49.020, Speaker B: Okay?
00:14:57.470 - 00:15:27.982, Speaker A: All right, so a couple of comments. So what do we mean by oblivious? So what I mean by oblivious here is that once you give the rounding algorithm this LP solution, the Y's, it doesn't look at the original input, the valuations that generated these Y's. So basically, the rounding algorithm depends only on the Y's, not on the V's. That's what I mean by oblivious. In principle, you can imagine randomized rounding algorithms which not only use the Y's, but go back and look at the v's again to do clever rounding.
00:15:28.046 - 00:15:28.514, Speaker B: Okay?
00:15:28.632 - 00:15:38.482, Speaker A: So today it's going to be oblivious, and it doesn't do that. And if you think about the randomized rounding last week, it was oblivious in this sense. It just looked at the Y's, scaled them down, and then flipped coins according to the Y's.
00:15:38.546 - 00:15:39.160, Speaker B: Okay?
00:15:40.030 - 00:16:36.694, Speaker A: So that's what I mean by oblivious. And again, let me just remind you that this LP solution in general is strictly better, right? So when an LP relaxation, including the one we're looking at here, the Y star is strictly better than any integer allocation. So therefore it's not by itself a distribution of allocations. That's what happens in this final step. All right, good. So the question now is, what is the relationship between a randomized rounding algorithm and MIDR allocation rules? Are they the same or can they ever be the same? So at a high level, we might hope that randomized rounding is a good candidate class of algorithms to generate MIDR rules. Remember what MIDR says, it basically says you should always be picking the best distribution.
00:16:36.694 - 00:17:24.890, Speaker A: So you're optimizing over distributions. Randomized rounding has this sort of nice, there's some syntactic connections in that you are explicitly doing an optimization over fractional solutions and you're outputting distributions which have some connection to what you're optimizing over. So MIDR says optimize over distributions. This picture well, you optimize over the y's, and then the y's sort of generate these distributions. So that sounds like it's on the right track. The concern, the reason why randomized routing algorithms like this might not yield MIDR rules is because what we really want to be optimizing over, if you look at the definition of MIDR, we want to be optimizing over this output here. We always want to be outputting the best distribution of our allocations for the given v's.
00:17:24.890 - 00:18:05.910, Speaker A: But in this picture, the optimization is happening over here. The optimization is happening over the LP solutions. The optimization is not explicitly happening over the distributions. So the worry is that there's some disconnect between those two introduced by the routing algorithm r. So scaling algorithms are going to be the specific kind of randomized rounding algorithms where optimization over the fractional solutions is exactly the same as optimizing over the distributions. So it's exactly the class of capital r's that give us the MIDR property in this schematic picture. All right, so what is it simple enough to state? So an oblivious randomized rounding algorithm is an alpha scaling algorithm.
00:18:05.910 - 00:18:46.466, Speaker A: So here alpha is a number between zero and one if whatever you feed it. So whatever feasible LP solution, you give it y. If you look at any bitter and you look at any bundle, the probability that bitter I gets this bundle in the output of this routing algorithm is exactly what the LP suggests. Y star is multiplied by alpha. So think of alpha as like 0.9. So the probability and again, so what's the coin flips here? The coin flips is over what the randomized algorithm does. So remember, y is an LP solution that gets fed to this rounding algorithm.
00:18:46.466 - 00:18:51.960, Speaker A: It flips some coins, it outputs an allocation, and so an alpha, sorry.
00:18:54.170 - 00:18:54.438, Speaker B: The.
00:18:54.444 - 00:19:08.090, Speaker A: Probability that I gets s is exactly the suggested probability. Or we're going to interpret this as the suggested probability of that event by the LP solution times alpha.
00:19:08.990 - 00:19:09.740, Speaker B: Okay?
00:19:12.850 - 00:20:09.962, Speaker A: And again, let me remind you, we would not expect for some NP hard problem like this, we would not expect to ever have a one scaling algorithm, okay? Remember these LP solutions in general, they're going to be strictly better than any integer allocation. So if we had a one here, that would say we're giving people exactly what the LP is giving them in expectation. So we'd be getting in expectation the full value of the LP, and we don't expect to get that the LP is going to be strictly better than any integer allocation. So we want this to be scaled down, or we expect this has to be scaled down, but we're hoping alpha is as close to one as possible. Alpha will correspond to some approximation guarantee. All right, so is that definition clear? The motivation, I think will become more clear as you go along, but at a high level. The point of this definition is so that by optimizing over an LP, you wind up inadvertently optimizing over the distributions in your range.
00:20:09.962 - 00:21:05.250, Speaker A: And we'll see that happens, but everyone can parse this. Okay, all right, so let me now show you why. If you have such an algorithm, you're good to go. And then we'll worry about, okay, do such algorithms exist? And if so, how do we get our hands on them? Okay, so claim if we have an alpha scaling algorithm, then the induced allocation rule x is MIDR. And so what I mean by induced allocation rule, I mean the allocation rule where you solve the LP optimally and then you apply the scaling algorithm to generate a randomly generated allocation. That's what I mean by the induced allocation rule. So the claim is scaling algorithms are sufficient condition to turn for a randomized rounding algorithm to give you an MIDR rule and therefore a DSIC mechanism.
00:21:05.250 - 00:22:00.478, Speaker A: So, the proof is easy because we define scaling algorithms to make it easy. All right, so MIDR just means of all the distributions over outcomes that we might ever see varying over all valuations, for a given valuation, the algorithm picks the one that maximizes expected welfare. So what is the range, the distributional range of this allocation rule? Well, it's just as you range. So, conceptually, imagine just over the LP solver. Imagine feeding this every possible input in the world, every possible valuation profile. So you get back some set of Every possible output, every conceivable y's that you ever get from this LP solver. And from each of those Y's, you might get, imagine applying the randomized rounding and you get some corresponding allocation distribution over the allocations.
00:22:00.478 - 00:23:01.170, Speaker A: So, the range of x is simply the output distribution of the randomized rounding algorithm applied to any conceivable LP solution that might be Fed. Now, here's the key point. So, here's what's special about scaling algorithms. So no matter what feasible LP solution y, you feed the rounding algorithm R, you get that the expected welfare of the allocation at the end of the day is exactly the same as the LP solution you started with multiplied by alpha. So, the expectation here, let's look at the allocation. So, again, Omega drawn from R of Y. Just think of this as like the output of the randomized rounding algorithm, and it's expected welfare.
00:23:01.170 - 00:23:26.586, Speaker A: That's what we're saying. So this is just the welfare of Omega. And again, Omega is just the random output of the randomized rounding algorithm. So by linearity of expectations, we can just do this as a sum over bidders. And now for a given bidder, let's just kind of brute force sum this. Let's just say, well, think of every bundle s that bidder I might get. It has some value for that bundle.
00:23:26.586 - 00:23:32.380, Speaker A: And then it contributes to this expectation. The probability that I actually gets that bundle where again, this is over.
