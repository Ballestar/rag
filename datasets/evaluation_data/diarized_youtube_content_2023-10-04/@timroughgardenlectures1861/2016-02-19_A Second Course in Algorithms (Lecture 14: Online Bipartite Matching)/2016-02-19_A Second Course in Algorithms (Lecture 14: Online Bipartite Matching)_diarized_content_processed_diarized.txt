00:00:00.410 - 00:00:35.346, Speaker A: Ah, this is going to be our final lecture in the third section of the course, which has been on online algorithms. So we're going to culminate in this section with a discussion of the online bipartite matching problem. Let me just quickly remember you what that is. So it's going to be bipartite matching. So there's a left side, call it capital L, a right side call it capital R, you know, the whole left side up front. Okay, I tell you that before anything starts. It's the vertices of the right side that come online one at a time, okay? When a vertex on the right hand side shows up, it shows up with all of its incident edges.
00:00:35.346 - 00:00:52.154, Speaker A: It's a bipartite graph, so all the incident edges go back to capital L. You want a matching, you want a big matching. The rub is that you can only match a vertex on the right hand side at the moment it shows up. You can't delay and wait to see who else is going to show up and then match it later.
00:00:52.272 - 00:00:52.940, Speaker B: Okay?
00:00:53.790 - 00:01:26.050, Speaker A: So that's the definition of the problem. And the goal is just to compute a max matching. So there's no edge weights, just max cardinality matching. You'd like that to be as big as possible, as big as the max matching in hindsight, after you've seen all of the right hand side vertices. So any questions about the problem? This is what we had last lecture, last lecture. I also sort of told you about a killer app of online bipartite matching and web advertising. The interpretation being the left hand side vertices, which, you know, up front, those correspond to advertisers who have purchased a certain number of targeted ads.
00:01:26.050 - 00:02:02.754, Speaker A: And then people, potential customers come in online, like as soon as they type in a search query or as soon as they view a content page. And the edges correspond to the potential consumers on the right hand side who match the targeting criteria on the left hand side. So it's one reason why this problem has gotten a lot of attention over the past seven, eight years or so. A couple of quick announcements. Problems at number three, as I'm sure you know, is due Tuesday. I posted the 7th exercise set and then my office hours, they're normally three to four. Today there's a talk I have to be at, so they're going to be at four to five instead.
00:02:02.872 - 00:02:03.106, Speaker B: Okay?
00:02:03.128 - 00:02:36.720, Speaker A: So one time only, my office hours will be 1 hour later today. All right, so we'd like a good online algorithm for this online bipartite matching problem. So we'd like a good competitive ratio. So we'd like to always produce a matching which is almost as good as what we could have achieved with full hindsight. So what kind of competitive ratio can we expect to attain? So remember last lecture we saw online scheduling? We got a two competitive algorithm. We studied the online Steinertree problem, we got a log k competitive algorithm where k was the number of terminals. So here we're trying to maximize, so we'd like to get as close to one as possible.
00:02:36.720 - 00:03:26.510, Speaker A: And so an example that we went through briefly, last lecture, I'm going to go through it again. And so what the lecture shows is that with deterministic algorithms you can't beat a factor one half, you cannot have a competitive ratio strictly closer to one than one half. So node deterministic algorithm has competitive ratio bigger than a half. All right, it's a very simple example that shows this. So you're going to start with two vertices on the left hand side. So you're thinking about some arbitrary deterministic online algorithm. A vertex shows up on the right hand side connected to both of the vertices on the left hand side.
00:03:26.660 - 00:03:27.360, Speaker B: Okay?
00:03:27.890 - 00:03:40.382, Speaker A: So the online algorithm now has to make a decision. It can match it to neither one, but that would be a disaster if this is the only vertex that ever shows up. Or it can match it to the top vertex or to the bottom vertex.
00:03:40.526 - 00:03:41.026, Speaker B: Okay?
00:03:41.128 - 00:04:06.618, Speaker A: And the point is, no matter what the online algorithm does in the first step, there's a vertex, I can feed it in the second step so that it regrets what it did. So for example, if you have an online, the algorithm in question picks this edge to match, then what I'll do sort of nefariously is define the second vertex so that its only neighbor is to the one that you already matched to.
00:04:06.784 - 00:04:07.258, Speaker B: Okay?
00:04:07.344 - 00:04:22.462, Speaker A: And then that'll be the entire input. All right, so the algorithm will just have a matching of size one, whereas clearly a matching of size two is possible obviously by symmetry. The same thing is true for deterministic algorithms which pick this edge instead in the first step.
00:04:22.596 - 00:04:23.326, Speaker B: Okay?
00:04:23.508 - 00:04:32.290, Speaker A: So no matter how smart you are, just because you have to make this decision at times, step one without knowing what's going to happen later, deterministically, at least you're going to be off by a factor of one half. At least.
00:04:32.440 - 00:04:33.140, Speaker B: Okay.
00:04:36.090 - 00:05:19.778, Speaker A: So algorithm equals one, but opt equals two. Everyone clear on that argument? Just a simple adversary argument. Okay, so this raises a couple of questions. Question number one is, can we achieve one half? Is there a matching positive result? Would the deterministic algorithm say? And then the second question would be, can randomization help and allow us to beat the one half? Remember for example, that when we talked about online decision making, it was totally crucial that the algorithm had used randomization. Remember we had a lower bound for deterministic algorithms with respect to the regret benchmark, so we're used to the idea that randomization can help with online algorithms. So the answers to both those questions are yes. So you can get a 0.5
00:05:19.778 - 00:06:11.726, Speaker A: with a deterministic algorithm, that's a pretty easy result, which I'll show you next. And you also can beat one half with randomization and I'll give you the main ideas of that in the remainder of the lecture. All right? So as far as the matching positive result, all you need is greedy. So greedy has competitive ratio one half. What do I mean by greedy? I just mean when a vertex shows up on the right hand side, you look at its neighbors. Obviously, if all of its neighbors are already matched you can't do anything. But if at least one neighbor is unmatched, you match it to an arbitrary unmatched neighbor.
00:06:11.838 - 00:06:12.500, Speaker B: Okay?
00:06:13.190 - 00:06:35.382, Speaker A: So that's the greedy algorithm. And the claim is this always will output a matching with size at least half of the maximum possible. Now this is actually pretty easy to prove directly, but that's not what I'm going to do, okay? So I'm going to give you a more sophisticated than necessary proof of this claim because this argument will generalize usefully when we talk about beating the one half with the randomized algorithms.
00:06:35.446 - 00:06:36.060, Speaker B: Okay?
00:06:37.630 - 00:07:14.870, Speaker A: So the way I'm going to do it is actually by duality. So weak linear programming duality. So let me remind you what the Primal and dual linear programs look like for the max cardinality bipartite matching problem. So for the Primal, so the semantics are here is we're thinking of XE being one for an edge in the matching. XE is zero for an edge, not in the matching. We want to maximize cardinality. There's no cost notice, just maximize cardinality.
00:07:14.870 - 00:07:35.790, Speaker A: What are the constraints? Well, we're not insisting that it's a perfect matching, we're sort of hoping we compute a perfect matching, but it's not a feasibility requirement. We just want it to be a matching. So that means for all vertices on either side, if we sum over the edges incident to the vertex at most one of those should be in the matching.
00:07:36.450 - 00:07:37.200, Speaker B: Okay?
00:07:39.890 - 00:07:42.350, Speaker A: And then these should be non negative.
00:07:45.490 - 00:07:46.420, Speaker B: All right?
00:07:46.790 - 00:07:57.858, Speaker A: Now there's a few minor differences between the Primal dual pair we looked at back when we were talking about bipartite matching because back then we were thinking about the min cost perfect matching problem.
00:07:58.024 - 00:07:58.306, Speaker B: Okay?
00:07:58.328 - 00:08:32.430, Speaker A: So here there's no costs and there's no need to restrict attention to perfect matchings. So you see the two differences, instead of having a min and min cost perfect matching, we have a max. And instead of having equations here, which is when we're insisting on a perfect matching here we just have inequalities. Those are the two differences between this Primal linear program and the one we talked about a couple of weeks ago. So the dual is basically the same as what it was before, although again with just some minor tweaks. So before the dual was a maximization, but now the Primal is a maximization. So the dual is a minimization.
00:08:32.430 - 00:09:17.600, Speaker A: There's going to be one dual variable per constraint. So per vertex there's going to be one constraint per variable or per edge. So we want to minimize the sum of the vertex prices and so because it's minimization, these inequalities are going to be greater than or equal to inequalities. So this is going to be PV plus PW for all edges VW in the graph in case it wasn't clear. So we're thinking about sort of the graph after we see it at the end of the algorithm. These linear programs are going to be only for the analysis. So we just run the greedy algorithm, we let it run and then this is all sort of an expose facto analysis of what happens.
00:09:17.600 - 00:09:31.090, Speaker A: So this were every edge that ever arrived. And then previously, because these were equations, we had real value decision variables. Now these are inequalities, so we have non negative decision variables.
00:09:33.190 - 00:09:33.940, Speaker B: Okay.
00:09:35.750 - 00:10:35.266, Speaker A: So that's the primal dual pair for the max kernely bipartite matching, okay, which you've basically seen before, modulo these minor cosmetic differences. All right, so why did I write this down? Okay, so we're trying to prove a positive result. We're trying to prove that this algorithm did something good, that it got close to optimum. And remember, when you're trying to prove these kinds of things, the challenge is always how do you know when you're close to optimum? Okay, so last lecture we just had a couple of ad hoc arguments for online scheduling for online Steiner tree, but we figured out how to do it. But also if you think about it, the whole culmination of the first half of was sort of the message that duality is sort of the ultimate answer of how do you know when you're done? How do you certify your own correctness? And as we'll see in the rest of this course, it's also super useful for proving approximate correctness. So this will be our first sort of nice example of that today's lecture. So I'm writing down the primal dual pair because I'm going to show the way, I'm going to prove that the matching that the greedy algorithm outputs is good is I'm going to compare it to a dual feasible solution.
00:10:35.266 - 00:10:43.194, Speaker A: And remember, dual feasible solutions by definition, by construction of the dual, are bounds on what the optimal solution to the Primal could be.
00:10:43.312 - 00:10:43.980, Speaker B: Okay?
00:10:44.910 - 00:11:42.682, Speaker A: And so maybe that sounds kind of fancy, but actually exhibiting the dual is very simple for this claim. So here's what we do. Again, remember, we've run the greedy algorithm to completion and we're doing this sort of in hindsight analysis. So now what I'm going to do is I'm going to show you, okay, it's not quite a dual feasible solution, but it's on the right track. So for every vertex of the graph set QV to be equal to one half if V matched by greedy and equal to zero otherwise. Okay, so again, we ran greedy. It didn't necessarily output a perfect matching, might have output something worse than a perfect matching, but so after the fact we sort of scan through the vertices and say, well, whatever's matched, give it a q value of one half, whatever is unmatched, a Q value of zero.
00:11:42.682 - 00:12:47.086, Speaker A: Or if you want, you can think about it like we actually define this dual in tandem with running the online algorithm. So a new vertex shows up if the greedy algorithm matches the new vertex on some edge. At that point, we set the Q values of both endpoints to be one half. Okay, that's another way to think about it. Okay, so what can we say about these Q's? So call the greedy matching m. So the first thing to notice is that the sum of the Q values is what in terms of m cardinality of m, right? We're sort of imagine defining the dual in tandem with the algorithm. Every time the algorithm adds an edge, we set the q values to the two endpoints to a half.
00:12:47.086 - 00:12:53.166, Speaker A: So at all times, including at the end of the algorithm, the sum of the Q values equals the sum of the edges that are in the matching.
00:12:53.278 - 00:12:53.890, Speaker B: Okay?
00:12:54.040 - 00:13:31.950, Speaker A: We're using the fact that because it's a matching, no one's ever going to have their Q value set twice. You either just stay at zero forever more or you're set to one half exactly once when you're matched by the greedy algorithm. Okay, but here's the more important point. I'm not going to claim that the Q's form a dual feasible solution. Remember, this is the dual here. So dual feasible means the sum of the prices on the endpoints of an edge should be at least one. So I'm not going to claim that the Q's form a dual feasible solution, but if we double the Q's, I claim that's a dual feasible solution.
00:13:31.950 - 00:15:02.362, Speaker A: So I claim p, the vector defined by just doubling all the Q values is feasible for deasibility just means that if you look at any given edge of the graph, the sum of the p values of the two endpoints should be at least one. So really all this is saying is that for every edge, at least one of the two endpoints got a Q value of one half, at least one had a Q value of one half. Once we double the sum of the p values of this edge is at least one. Okay, so for all VW in the edge set, it's going to be true that QV plus QW is at least one half, and therefore once we double it's, at least one. So why is that true? Well, how could it be that both V and W, both QV and QW are zero? What would that mean? That would mean the only way it's zero is if you didn't get matched. So that would mean both V and W didn't get matched. But then what on earth were we doing when the vertex W showed up, right? So how could we not have matched W if V was available? Its neighbor V was available at that time.
00:15:02.362 - 00:15:12.742, Speaker A: Okay, so another way of thinking about it is the greedy solution outputs a maximal matching. So there's no way you're going to have two points connected by an edge, both of which are unmatched.
00:15:12.906 - 00:15:13.620, Speaker B: Okay?
00:15:14.950 - 00:15:50.330, Speaker A: Everyone okay with that? So now we're done. So now we can use so remember, the dual always bounds the best possible solution of the primal. So our primal is a maximization problem. So every dual is going to be an upper bound on how big the max matching could possibly be. So we're going to use this dual feasible solution to compare against our matching. So just copying this down. So rewriting this in terms of half the p's.
00:15:50.330 - 00:16:39.670, Speaker A: This is the objective function value of our dual feasible solution. Yeah, which is an upper bound on how big opt could be. Okay, and that's the proof of the claim. This is the size of the matching output. By greedy we proved it's at least half of the best possible, so it's a one half competitive ratio. Okay, so any questions about that? This idea we'll see a few more times where you have an algorithm. It's not to notice we didn't solve a linear program in our algorithm.
00:16:39.670 - 00:16:54.094, Speaker A: Our algorithm is just the greedy algorithm. We use linear program to analyze it to prove that it's approximately optimal. We'll see that several more times where we're looking at natural heuristics natural algorithms. We want to know how do we prove they're near optimal. We're going to use a suitably constructed dual solution.
00:16:54.222 - 00:16:54.900, Speaker B: Okay.
00:16:57.030 - 00:17:03.140, Speaker A: All right, so any questions about that makes sense?
00:17:04.950 - 00:17:05.700, Speaker B: Okay.
00:17:06.710 - 00:17:49.858, Speaker A: All right, so that was the answer to the first question. Could we actually get one half say, with a deterministic algorithm? The answer is yes. So can we do better with randomization? Okay, so I'm actually not going to answer that question for you in lecture. So rather than thinking about randomized algorithms for integral matchings, I'm going to think about deterministic online algorithms for fractional matchings. Okay, so remember, in a matching you have to pick an edge. Every vertex should have a most one edge picked incident to it. But then you can also talk about fractional matchings where you can have fractions on edges between zero and one.
00:17:49.858 - 00:19:09.622, Speaker A: So in some sense, feasible solutions that are not zero one to this primal correspond to fractional matchings. Okay, but let me tell you that on problems at number four. So it turns out that the same ideas I'll teach you in this lecture also lead to the same improvement for the integer matching case with a randomized algorithm. Problems at number four will walk you through the steps to translate the analysis I'll show you today to the randomized algorithm for the integral problem. All right, so I guess one quick thing to remember, and at first you might find this weird, actually, to talk about the fractional bipartite matching problem instead of the integral one, because wasn't one of the points a few lectures ago that the problems are the same? I don't know if you remember, but when we were talking about LP duality, one of our examples is we went back to the Hungarian algorithm and we used the Hungarian algorithm to prove that actually there's always a primal solution, which is, amongst all fractional solutions to the primal, there's an optimal one, which is zero one, corresponding to a perfect matching how do we prove it? Because the algorithm exhibited an integral matching, but it also exhibited a dual with exactly the same objective function value. So that shows that there's always a zero one optimal solution. Also, on exercise set, I think, number five, I ask you to prove it directly.
00:19:09.622 - 00:20:06.160, Speaker A: So for bipartite matching, there's always an optimal solution, which is zero one. So allowing fractions does not help you solve the problem if you're solving it optimally. However, allowing fractions can help online algorithms, so why is that? Well, let's return to our same example, right, so we have two vertices up front on the left. A new vertex comes on the right, connected to both. So before we had to pick, either we match it to the top vertex or we match it to the second vertex with fractions we don't have to pick, we can hedge. So in light of this example, it's tempting if our algorithm is allowed to use fractions to match this edge, 50 50 to each one. Okay, that's something this algorithm could do that the previous one could not.
00:20:06.160 - 00:20:41.558, Speaker A: And then an adversary can come along and say, oh, well, this one's only connected here, but then there's still one half room here for us to at least partially match this vertex. So at this point, the fractional online matching algorithm can assign a one half to that vertex. Okay, so opt is still two and we're still not as good as opt, but we got three halves. Okay, so that's only a ratio of three quarters, so that's not one half. So at least in this example, we're not stuck with the one half if we can have fractions.
00:20:41.734 - 00:20:42.460, Speaker B: Okay.
00:20:44.430 - 00:21:46.094, Speaker A: All right, so any questions about that? That's why it really is a different problem when you start talking about fractional matching. Just to kind of foreshadow what you'll see on problem set number four, notice that if you had to be integral but you were allowed to randomize, there's a sort of obvious analog to splitting one unit fractionally on these two edges, which is to flip a coin and pick one of the two edges at random. Okay, so building on that kind of idea allows you to translate solutions for the fractional problem like you'll see today, to randomized algorithms for the integer problem. Okay, so questions? Yes. For an arbitrary fractional matching, how do we know that it was actually implementable as a pure outcome? So we're actually not going to insist that. Yeah, as far as translating it to a randomized, translating it to randomized algorithm, it's not obvious. It's not obvious.
00:21:46.094 - 00:22:27.814, Speaker A: You can do it. I'm just sort of giving you some high level intuition about why it's conceivable as possible. But you're right, the question is, it's an excellent question. You're basically saying so what you'd like to do is kind of realize all the fractions as probabilities over some distribution over matchings. And so in some sense that's true. In some sense we know after the fact, again, just by knowing that this thing has zero one optimal solutions, that kind of tells us that any fractional matching is indeed the convex combination of integer matchings. But the question is, can you sort of construct that distribution online? So you need an online algorithm which actually at the end of the day has constructed the correct distribution and it's not trivial.
00:22:27.814 - 00:22:45.120, Speaker A: So again, the problem as usual will be broken down into a few steps so that you can see how it goes. But it is. So last year I actually didn't lecture but I just decided it was too much on the topic. It took another like maybe 20 minutes to do the randomized translation. Other questions?
00:22:50.690 - 00:22:52.786, Speaker B: Okay. All right.
00:22:52.808 - 00:23:01.206, Speaker A: So let me tell you the algorithm we're going to use and the algorithm we're going to use is really just sort of the natural extension of hedging. Okay, actually let me put this over.
00:23:01.228 - 00:23:01.800, Speaker B: Here.
00:23:04.250 - 00:24:23.210, Speaker A: And I'm going to call it sometimes this is called the water level algorithm because there's sort of a nice physical interpretation of what's going on. So, so the idea is you think of the left hand side vertices that are up front as containers for water, initially empty with capacity one and each arriving right hand side vertex is a source of one unit of water. And what's interesting is what happens when a new vertex shows up. So when the next right hand side vertex comes, let's call it oh yeah, W. You want to try to send W's water over to the left hand side to its neighbors. That's the analog of matching it actually. So you fill up the neighbors, the neighboring containers, always preferring the containers that are currently the least full with the lowest.
