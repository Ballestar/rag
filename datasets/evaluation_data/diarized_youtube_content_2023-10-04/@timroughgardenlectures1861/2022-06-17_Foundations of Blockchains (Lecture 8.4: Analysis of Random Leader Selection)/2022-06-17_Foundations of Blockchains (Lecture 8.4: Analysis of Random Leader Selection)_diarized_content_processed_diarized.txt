00:00:00.570 - 00:00:29.862, Speaker A: So what we saw at the end of the last video is that the key that unlocks all of the good properties of longest chain consensus, it really boils down to how balanced is the sequence of leaders. So just to remind you, what does it mean that a leader sequence is W balanced? Well, it means that if you look at any interval of W or more consecutive leaders, over half of them should be honest nodes. So for example, if you're 100 balanced, that means if you look at any stretch of 100 consecutive leaders, at least 51 of them should be honest.
00:00:29.862 - 00:00:49.382, Speaker A: Similarly, if you look at any stretch of 200 consecutive leaders, at least 101 of them should be honest, and so on. And in this video I want to explore what happens when in each round we choose a leader uniformly at random. So each of the N nodes is equally likely probability one over N to be the leader for that round.
00:00:49.382 - 00:01:13.022, Speaker A: The primary motivation for looking at randomly selected leaders is that's what's going to extend very easily to the permissionless setting that we'll start talking about in lecture nine. But also, if you think about it, this intuition that with respect to this balance condition seems like randomly chosen leaders should do pretty well. So to appreciate this, let's compare and contrast randomly chosen leaders with just a sort of simple deterministic round robin.
00:01:13.022 - 00:01:30.242, Speaker A: Imagine for example, N equals 3000, 3000 nodes. Let's say 1000 of them are Byzantine, so 33% Byzantine. Now imagine you do round robin leader selection and it just so happens that all of the Byzantine nodes occur consecutively in your ordering.
00:01:30.242 - 00:01:48.350, Speaker A: So maybe nodes 1001 through 2000 in your ordering are all of the Byzantine nodes all in a row. Well, this leader sequence is not going to be any better than 1000 balanced. On the other hand, suppose that each leader is chosen independently and uniformly at random from this pool of 3000.
00:01:48.350 - 00:02:01.810, Speaker A: Well, then any leader has a two thirds chance of being an honest node and a one third chance of being an adversarial node, a Byzantine node. So you're certainly going to sometimes see Byzantine leaders. You might even see two, three, four Byzantine leaders in a row.
00:02:01.810 - 00:02:28.678, Speaker A: It just seems pretty unlikely you're going to see 100 Byzantine leaders in a row, right? That would be sort of probability one third raised to the 100th, which is a really, really small probability. So while with the round robin order you're not even 2000 balanced, it seems like with randomly chosen leaders you'd be 100 balanced or even possibly quite a bit better than that. The catch, of course, with randomly chosen leaders is that anything in principle can happen with positive probability.
00:02:28.678 - 00:02:54.770, Speaker A: So even if you have like 3000 nodes and only one of them is Byzantine, there is always a chance, astronomically small, but a chance that actually you keep randomly selecting the one Byzantine node over and over and over and over again. It's very unlikely the probability would be one over 3000 raised to the length of the sequence that you're talking about. But you have to concede that is still a positive probability.
00:02:54.770 - 00:03:13.366, Speaker A: So by the same reasoning, with randomly selected leaders, there's no way we can try to argue that longest chain consensus is guaranteed, meaning with probability one guaranteed to have any of those three properties that we listed on the previous slide. Consider, for example, finality. You want to consider a block confirmed.
00:03:13.366 - 00:03:33.942, Speaker A: At some point it winds up on the longest chain. It gets extended like a huge number of times, a million times. There's some positive probability that the next 1,000,001 randomly chosen leaders are all Byzantine nodes, astronomically small, but some positive probability, in which case they may well roll back your block.
00:03:33.942 - 00:04:05.754, Speaker A: So if you want zero failure probability, you can literally never consider any block confirmed, which is not very useful. So the only option with longest chain consensus and randomly chosen leaders is you want to have probabilistic versions of the three guarantees in the theorem on the previous slide. So you want to say accept with astronomically small probability the longest prefix property will hold for any given block that gets confirmed, except with astronomically small probability it's not going to get rolled back.
00:04:05.754 - 00:04:11.722, Speaker A: Similarly for probabilistic liveness. So that's kind of a bummer. We'd love to have those properties with probability one.
00:04:11.722 - 00:04:21.934, Speaker A: But this is also reality. Once you move to the permissionless setting, you're basically forced to select leaders at random. There's always some tiny probability that things could go horribly wrong.
00:04:21.934 - 00:04:45.794, Speaker A: Bitcoin, for example, does not offer guaranteed finality, it only offers probabilistic finality. Now, just to be clear, probabilistic guarantees are basically as good as probability one guarantees if the failure probability is sufficiently small. So if the chance of something going wrong is smaller than the chance of being hit by an asteroid in the next 24 hours, well, maybe that's actually as good as just a deterministic guarantee.
00:04:45.794 - 00:04:56.074, Speaker A: So now let me switch to talking through why random leaders should typically generate a leader sequence that's reasonably well balanced. I'm not going to go through a rigorous proof here. I totally could.
00:04:56.074 - 00:05:09.338, Speaker A: It wouldn't necessarily be harder than some of the other proofs we've done in this lecture series, but it would take us a little bit far afield. Also, hopefully I can give you a good, strong sort of intuitive sense of how the analysis would go anyways. So first, just a tiny piece of notation.
00:05:09.338 - 00:05:24.514, Speaker A: Alpha is going to denote the fraction of the nodes that are Byzantine. And remember, we're really doomed unless alpha is less than 50%. So we really need at least 51% honest nodes to have any hope for our balancedness property to hold.
00:05:24.514 - 00:05:40.258, Speaker A: So here's what we have going for us. Given that alpha is less than a half, consider any sequence of a bunch of consecutive leaders, right? So maybe say, 100 leaders in a row. Suppose there's 49% Byzantine and 51% honest nodes.
00:05:40.258 - 00:05:57.370, Speaker A: Then we expect a strict majority of these 100 to be honest. Typically you would expect 51 honest and 49 Byzantine. If we look at 1000 consecutive leaders, we expect 510 of them on average to be honest, and 490 of them on average to be Byzantine.
00:05:57.370 - 00:06:15.894, Speaker A: Returning to our 51 49 example, if you look at a given window, you're not generally going to see exactly 51% of them being honest. There'll be a little bit of variation, right? So like, if you look at 100 leaders in a row, yeah, on average you expect 51 honest, 49 Byzantine. Sometimes it's going to be a little bit more, sometimes it's going to a little bit less.
00:06:15.894 - 00:06:43.062, Speaker A: And because there's only kind of a margin of error of two nodes between the averages, between the expected 51 and 49, we really wouldn't be that surprised to once in a while see 100 nodes in a row, 51 of which were Byzantine and 49 of which were honest, which of course would violate our balancedness property. Now, however, imagine we blew up the window length. We're looking at, say we increased it from 100 to 1000 and look at 1000 liters in a row.
00:06:43.062 - 00:07:08.814, Speaker A: Now, instead of a 49 51 49 split, we're expecting a 510 490 split, and that's a gap of 20 between the expected number of honest nodes and the expected number of Byzantine nodes. Now, again, typically you're not going to get exactly 510 honest liters. You might get a little bit more, you might get a little bit less, you might get 515, you might get 5518 5252-3497.
00:07:08.814 - 00:07:24.262, Speaker A: That would be bad for us because that would be 503 Byzantine, which would be a majority Byzantine. That would violate balancedness. But I hope intuitively it seems like this at least should be a little less likely than it was in the length 100 window, where we only had this margin of error of two nodes.
00:07:24.262 - 00:07:35.214, Speaker A: With this margin of error of 20 nodes, seems like we should be able to absorb the variation around the average a little bit more frequently. And that is in fact true. And obviously, if we blow up the window size again, it becomes only more true.
00:07:35.214 - 00:07:51.150, Speaker A: If we look at 10,000 consecutive liters, we're expecting a 5104 900 split on average. That's a gap of 200, giving us even more buffer to absorb the variation around the average that we're likely to see. All right, so that's intuition.
00:07:51.150 - 00:07:59.890, Speaker A: I hope you find it convincing. I mean, it is honestly really quite accurate intuition. If you want really sort of concrete numbers, you got to do some math.
00:07:59.890 - 00:08:28.382, Speaker A: And the very cool thing you discover if you actually do the math, is that not only is it getting less and less likely that you have 50% Byzantine nodes, as the window gets larger, the probability of at least 50% Byzantine nodes is decreasing exponentially quickly in the window length. W. So, formally, what I mean when I say the probability of seeing 50% Byzantine nodes is dropping exponentially quickly in the window length w is that that probability is bounded above by an expression of the form e.
00:08:28.382 - 00:08:38.530, Speaker A: Here. E is the base of the natural logarithm 2.7 118 E raised to the negative of some positive constant C times w.
00:08:38.530 - 00:08:47.442, Speaker A: So C here is just some constant independent of w. It's not that important for us. It is going to depend actually on alpha and so how close alpha is to one half.
00:08:47.442 - 00:08:57.030, Speaker A: So exactly how much margin of error you have between the honest and the Byzantine nodes. But for concreteness, maybe think of C as like 0.1, something like that.
00:08:57.030 - 00:09:07.610, Speaker A: So exponentially small probabilities are really good. Actually those are going to zero very quickly. Right? So this means that every time we look at slightly larger windows, we increase the window length by plus one.
00:09:07.610 - 00:09:19.310, Speaker A: We're actually cutting the failure probability by a constant factor. So that's quite nice. Now, if you remember the definition of a leader sequence being W balanced, it wasn't about just one length W window.
00:09:19.310 - 00:09:36.882, Speaker A: The definition asserted that every single window of length W or more should have a strict majority of honest leaders. So for example, imagine we're looking at a sequence of sort of 1000 leaders in a row, pretty long sequence. And imagine we're thinking of W equals 40.
00:09:36.882 - 00:10:04.314, Speaker A: So we want to look at all possible intervals of 40 or more consecutive leaders amongst the sequence of 1000. The event that we're interested in is not so much about having at least 50% Byzantine in a given length 40 window, but rather the probability that in any of the possible length 40 or greater windows among the sequence of 1000, if any of them have at least 50% Byzantine nodes. Because if that's the case, then we're not W balanced.
00:10:04.314 - 00:10:19.202, Speaker A: So previously we were worried about just one bad event, namely that a given window has at least 50% Byzantine. Now we're worried about any of all big number of bad events. Okay? So we have this sequence of length 1000, we have every possible window of length 40 or more.
00:10:19.202 - 00:10:40.926, Speaker A: For each of those windows we have a corresponding bad event that is at least 50% Byzantine. So at this point we can invoke what's known as the union bound in probabilistic analysis, which just says that when there's a bunch of bad events that you're worried about, you can at least upper bound the probability that any of them happen by the sum of their individual probabilities. That's an inequality that holds with equality.
00:10:40.926 - 00:10:54.986, Speaker A: When the events happen to be disjoint, when the events overlap, it's sort of overkill. But it's a very convenient upper bound on the probability that anything goes wrong. So look at each thing that could go wrong, we'll get its probability and sum up those individual probabilities.
00:10:54.986 - 00:11:27.430, Speaker A: So applying that here, we know an upper bound on the probability of any given bad event that's this E raised to the minus CW and a sort of sloppy upper bound on the number of bad events that we're worried about would be capital T squared, where capital T is the length of the sequence we're considering. So capital T would be what we were calling 1000, right? Any window of any length has a starting point of which there's at most T possibilities and an ending point of which there's at most capital T possibilities. So there's clearly at most capital T squared possibilities for windows of any length amongst a length T sequence.
00:11:27.430 - 00:11:49.422, Speaker A: If this event doesn't occur, which is with the remaining probability one minus this probability, if the event doesn't happen, then in fact we have a W balance leader sequence, which of course is what we wanted all along. Now this failure probability, you might stare at it and be like T squared E to the minus CW, like, is that a big number? Is that a small number? Well, here's the point. The point is it depends on little W.
00:11:49.422 - 00:12:02.562, Speaker A: So every time we increment W, we're cutting this term by a constant factor. So if we increase W enough, this is going to get really small. So the way to think about this is like, let's think about what failure probability we're comfortable with.
00:12:02.562 - 00:12:16.642, Speaker A: Okay? Maybe it's 10%, maybe it's 1%, whatever call that little delta the failure probability we're comfortable with. Now we can just solve for the appropriate window length little W. So you do that basically by sort of taking logs and rearranging.
00:12:16.642 - 00:12:47.070, Speaker A: Let me just write down the answer here on the bottom of the slot. So you do a little algebra and what you find is that you can get whatever failure probability delta you're comfortable with as long as you take the balance parameter W to be at least some constant times the logarithm of the time horizon, capital T plus the logarithm of one over the failure probability one over delta. Now, the key takeaway from this inequality is just to remember that the logarithm is a quite slowly growing function.
00:12:47.070 - 00:13:01.938, Speaker A: So the logarithm of like, big numbers tends to actually be a small number. So if we were talking log base two rather than the natural log base two of 1000 would be like around ten. Log base two of a million would be around 20, log base two of a billion would be around 30, et cetera.
00:13:01.938 - 00:13:20.298, Speaker A: So what that means for us is that even if you take capital T, the time horizon to be pretty long, and even if you take the failure probability little Delta to be pretty small, so that one over delta. Is pretty big, actually. You can still get away with reasonably palatable choices for the balance parameter little W.
00:13:20.298 - 00:13:41.070, Speaker A: And if you remember this parameter, little W is really important for us. Right? We had that theorem which basically said like, whatever your balance parameter little W is, that's also basically how many blocks you have to wait to extend the block before you can consider it finalized. So small little W's means sort of relatively short waits for blocks to be finalized in longest chain consensus.
00:13:41.070 - 00:13:55.650, Speaker A: So small W is what we want and what this shows is that with random leaders you do get decently small little W's. So the exact value of little W you can get away with. Like obviously from the expression you see, it depends on capital T, right? So no surprise.
00:13:55.650 - 00:14:15.854, Speaker A: So the longer period of time you want to sort of make sure that you're safe, the bigger you should take the balance parameter, similarly the smaller the failure probability you should be conservative and use bigger values of little W. It's also actually going to depend on alpha. Alpha, remember, is the fraction of nodes that are Byzantine and that's sort of hidden in here.
00:14:15.854 - 00:14:35.410, Speaker A: So the constants C and C two that I mentioned here, they can both depend on alpha. Basically, as alpha gets closer and closer to 50%, you are correspondingly going to have to take the window length W bigger and bigger. So I encourage you to plug in sort of different values of capital T and little delta and alpha and see what you get.
00:14:35.410 - 00:14:56.198, Speaker A: Again, if you just want sort of a concrete number to keep in mind, sort of heading out of this video, think about W as in maybe the low double figures somewhere in the dozens. So that wraps up our discussion about why randomly selected leaders actually do quite well with respect to this balance condition. This fact alone would be reason enough to think seriously about selecting leaders uniformly at random.
00:14:56.198 - 00:15:21.970, Speaker A: But as I said, the number one reason we care about random leaders is because that's what's going to generalize very easily to the permissionless case. So with an eye toward the permissionless setting that we're going to study in lecture nine, let me conclude by calling your attention to the fact that nowhere on this slide do we need to understand anything about the number of nodes N. It plays literally no role in this argument.
00:15:21.970 - 00:15:44.586, Speaker A: The only thing that mattered, the only thing that mattered was that when we select a leader at random, we had a bigger than 50% chance of getting an honest node and a less than 50% chance of getting a Byzantine node. That was the only property about random leader selection that we needed for this argument to be valid. This, of course is not at all true.
00:15:44.586 - 00:16:08.050, Speaker A: If you're doing round robin leader selection, if you're going through the nodes sort of in the same order, pass after pass, you have to know when to restart your pass. So in particular you know how many nodes you've processed. So this is our first big clue that random leader selection might actually continue to be useful in the permissionless setting where we don't have an operator known set of nodes, we don't even know the number little N of nodes.
00:16:08.050 - 00:16:43.338, Speaker A: The only thing we're going to need to implement in the permissionless setting to get the same argument to work is we're going to have to have some method of sampling a node from this unknown set of all the nodes that are out there so that we get Byzantine nodes with the appropriate frequency. So if 49% of the nodes out there are byzantine, we should be sampling byzantine nodes 49% of the time. As long as we can implement that type of sampling procedure in the permissionless setting, which we'll see we can do using, for example, proof of work, as long as we have that kind of accurate sampling subroutine, this exact same argument holds.
00:16:43.338 - 00:17:12.162, Speaker A: The probabilistic process generating leader sequences will be exactly the same, so the probability of being balanced will be exactly the same, so the probability of having those guarantees common, shared prefix, finality and liveness will also be exactly the same. Now that we know that random leaders are very likely to generate balanced sequences, let's move on to the proof of why balance sequences is all you need for the three properties. We want common prefix, finality and liveness.
00:17:12.162 - 00:17:15.300, Speaker A: We'll start with common prefix in the next video. I'll see you there.
