00:00:00.410 - 00:01:04.526, Speaker A: Any questions before we get started? Okay, so if not, then with this lecture, we're going to start on the fourth and sort of final module of CS 261, which is about NP hard problems and and pretty much all of CS 161 and all of the first half of CS 261 focus on problems that are solvable in polynomial time. Right? That's kind of what we've been doing. What you've been doing for like a quarter and a half is learning polynomial time algorithms for lots of different problems. Now, unfortunately, it's kind of a sad fact that in practice, there's actually a lot of problems which are both important, practically relevant, and pretty frequently occurring, which we do not believe are polynomial time solvable. So they're NP hard. So unless P equals NP, there's no polynomial time algorithm for them. And so the question is, but just because the problem is NP hard, that doesn't mean you don't have to do your best to solve it.
00:01:04.526 - 00:01:42.910, Speaker A: I mean, it may be some crucial optimization problem for your startup or whatever. So it's still important to kind of know what are the options you have available to yourself as an algorithm designer when a problem is NP hard. And so that's what we're going to focus on for the remaining lectures. And primarily we're going to focus on approximation algorithms. So I'm not really going to talk at all about MP completeness or MP hardness in this class in 261. I sort of assume that you've seen that in previous classes 154, or even before that. If you want sort of a refresher on NP completeness, how to define it, how to interpret it, I'll put some 161 level videos up on the course website if you're interested.
00:01:43.060 - 00:01:43.760, Speaker B: Okay?
00:01:45.090 - 00:02:24.006, Speaker A: All right. So what does it mean as an algorithm designer if your problem is NP hard? Well, one thing to remember is it's not a death sentence. It doesn't mean it's literally hopeless to do anything nontrivial for the problem, but it does kind of like throw the gauntlet to the algorithm designer. It kind of says, like, you're going to have to step up your game in various respects. In particular, you have to make some kind of compromises, right? So maybe the compromise is you work a lot harder, you spend a lot more time on the problem, you buy like, thousands of cores. Maybe the compromise is you only restrict attention to kind of relatively modest size inputs. Or maybe the compromise is you sort of relax, correctness, and look at approximation.
00:02:24.006 - 00:03:09.430, Speaker A: So those are the things we're going to be talking about. So possible compromises. Okay, so it's not a death sentence, but it does generally mean you need significantly more human and computational effort to get an acceptable solution to MP hard problems. Okay. And there is, of course, sort of a trade off, sort of the more work computational and sort of human thought you put into a problem, the better you expect to do. And so we're going to be exploring a continuum, like some things which are kind of quick and dirty, and maybe the results you get aren't that strong. And then if you work really hard, how do you get excellent results? So we'll talk about that whole continuum.
00:03:09.430 - 00:03:49.966, Speaker A: All right? So the first thing you can do is you can try to make the problem go away. You can say, okay, well, maybe this problem in general is NP hard, but maybe my application has some special structure. And with this special structure, the special case of the NP hard problem is actually polynomial time solvable, okay? And this happens a fair amount. In fact, you've already seen an example in this class. The vertex cover problem in general graphs is an NP complete problem. But as you saw in a problem set in bipartite graphs, the special bipartite case vertex cover reduces just to max flow mint cut, and it's polynomial time solvable. So sometimes when you do a restriction to a special case, you wind up in P, wind up being polytime solvable.
00:03:49.966 - 00:04:14.190, Speaker A: That's great. Sometimes you're still MP hard, but you're in a sense, still much easier than where you started. We'll see an example for that when we talk about the traveling salesman problem. Next lecture. In any case, I do want to point out that I mentioned that sort of doing well on MP hard problems takes not just computational, but human effort. And so implementing this step definitely takes sort of domain expertise and human effort. You have to understand your application.
00:04:14.190 - 00:04:20.810, Speaker A: You have to understand what special structure your application has, and then you have to figure out an algorithm which exploits that special structure.
00:04:20.890 - 00:04:21.374, Speaker B: Okay?
00:04:21.492 - 00:04:45.030, Speaker A: But when it works, that's a great resolution. So you don't want to solve NP hard problems if all you really care about is a polytime special case. All right, so you can also spend exponential time on the problem, but hopefully it's still better than brute force.
00:04:52.330 - 00:04:53.174, Speaker B: Okay?
00:04:53.372 - 00:05:50.634, Speaker A: So sort of we sometimes almost identify NP hard problems, which are those that seem to require brute force, but the real situation is actually a little more nuanced than that. There's a lot of MP hard problems where while we don't know any sub exponential time algorithm, you can do better than brute force. So instead of like, brute force might be N factorial, and we know an algorithm which is two to the N, or brute force might be two to the N, but we know an algorithm which is 1.5 to the N. Okay? So we don't know anything sub exponential for any natural, empty, complete problem, but sometimes there are shavings, significant shavings you can do with respect to just the most naive brute force search algorithm, time permitting. We'll see a couple of examples in 261 in traveling salesman problem, and in threesat as well, okay? And I should say, pretty much in practice, there's almost no NP hard problem. You would actually resort to just brute force search.
00:05:50.634 - 00:06:31.022, Speaker A: You would always try to do something smart. So in a class like 261, we sort of focus on stuff that works fairly generally and that you can prove guarantees about. But in practice, keep in mind that there's always heuristics, which usually do at least some successful pruning, at least some successful speed up over brute force, although in practice, those are often super application specific. And so for that reason, they don't really fit inside a course like 261, which is focused on general techniques. And you can also say, well, maybe we want to be fast, we want to handle large input sizes. And so then a possible compromise is to relax. Correctness.
00:06:31.022 - 00:07:14.706, Speaker A: So hopefully you relax it in some bounded way. Hopefully you're still in some sense approximately correct, at least on most of the instances, if not all of them. And this is what we'll be mostly focusing on for these last three weeks. So what this is called when you relax, correctness, at least for optimization problems, is approximation algorithms. Now, as you know, you can study approximation algorithms even for problems that are polynomial times solvable. So this particularly came up when we were discussing matching. So matching is polynomial times solvable in the non Bipartite case even, but it's fairly slow the exact algorithm.
00:07:14.706 - 00:08:00.022, Speaker A: So on the problem set, you explored some ideas where you prove approximate, say, near optimal matchings that run in near linear time. So we've already seen approximation algorithms for problems in P, but for NP hard problems, really, they take on a sort of new importance, because polynomial time exact optimization just isn't an option. All right, so what's the game in approximation algorithms? So we're going to take as a hard constraint polytime in the worst case. So in every single input, we want an algorithm that runs in polynomial time. Obviously, the smaller the polynomial, the better. But anything polynomial is a good start, much better than exponential brute force search. And then the goal, again, we're going to be talking always about optimization problems.
00:08:00.022 - 00:08:04.840, Speaker A: So there'll be a notion of an optimal solution, and we just want to get as close to it as possible.
00:08:10.330 - 00:08:11.080, Speaker B: Okay?
00:08:12.270 - 00:08:18.326, Speaker A: Close in the sense of having objective function value, close to the best possible objective function value of any feasible solution.
00:08:18.438 - 00:08:19.100, Speaker B: Okay?
00:08:20.430 - 00:08:51.890, Speaker A: So that's sort of at a high level of what approximation algorithms are about. You have an NP hard problem. You can't solve it exactly in polynomial time, presumably. But if I give you a constraint of polynomial time, how close can you get? How close can you get to full optimality? And so there is now a massive, massive literature on approximation algorithms. People in a good chunk of the algorithms research world has been obsessed with approximation algorithms for basically 25 plus years now. So as a result, there's tons of results. There's a lot of sort of really cool general techniques.
00:08:51.890 - 00:09:01.354, Speaker A: And so the goal on CS 261 is just to expose you to sort of a couple of the ones that seem most broadly applicable, the ones that you might be most likely to remember later on.
00:09:01.392 - 00:09:01.594, Speaker B: Okay?
00:09:01.632 - 00:09:55.420, Speaker A: But just know there's a huge, huge literature on approximation algorithms and I guess sort of one takeaway. We'll do lots of specific examples, including in this lecture. But one just thing to keep in mind, what should be sort of cool for you is that one thing approximation algorithm shows. It shows that literally every algorithm design paradigm that you've learned so far in your studies, which you probably only used for exact algorithms, is actually, again, useful for the design of approximation algorithms. So greedy algorithms, divide and conquer algorithms, dynamic programming, linear programming, and then even some other techniques like local search, which are also useful for heuristics, all of those are used to successfully design good approximation algorithms. So we're almost going to be doing kind of a nostalgic tour of all the techniques you would have learned in 161 and seeing them useful now in this other context for NP hard problems. So that's one thing to keep in mind.
00:09:55.420 - 00:10:37.282, Speaker A: All right, so that's kind of the philosophical preamble I had for you. So let's do some examples. So what are some examples of MP hard problems you might want to approximate, and how would you approximate them? So let me begin by just pointing out you actually already know several examples. Really? Let me just remind you of some of the ones that you know. So first, a problem we studied exactly a week ago, min makespan scheduling. So we were studying this at the time in the context of online algorithms. So the setting is you have M identical machines.
00:10:37.282 - 00:11:05.594, Speaker A: There are N jobs, each with a processing time, and the objective function is to minimize the make span. What does that mean? That means you want the most heavily loaded machine to be as lightly loaded as possible. So you want to minimize the maximum load equivalently. We're basically trying to distribute the jobs as evenly as possible between the different machines. So that was the problem. We were setting it online where the jobs come one by one. But if you think about it, it's a totally reasonable problem to study offline.
00:11:05.594 - 00:11:35.686, Speaker A: You could imagine cases where you actually know a whole batch of jobs that need to be scheduled, you know them at once, and then you want to do a more kind of centralized optimization to schedule them all in a smart way. So the offline problem also makes sense. Again, offline means you have all the jobs at the beginning. So that problem turns out to be NP hard computing the schedule with the best possible make span. We're not going to prove that. And really, we're not really going to prove any of these NP hardness results. I'm just going to state them.
00:11:35.686 - 00:12:01.154, Speaker A: You really already know how these things are proved. If you've taken, say, CS 154. So you take some known MP complete problem like stat, and then you reduce it to the problem that you're talking about, and that proves that it's MP complete as well. So you should have seen a bunch of reductions of that form in your previous courses. All of these are proved by exactly those same kinds of reductions. And I'm not going to do it, not going to do it here. All right? So for our purposes, let's take it as a fact that it's empty hard.
00:12:01.154 - 00:12:59.862, Speaker A: So we don't expect to get an exact solution in polynomial time. What would be a polynomial time algorithm that gets close to opt? Well, let's just revisit what we were thinking of at the time as our online algorithm, graham's algorithm and Graham's algorithm, and the Parlance of approximation algorithms is a two approximation. What does it mean that an algorithm is a two approximation? It means it runs in polynomial time. Okay? So remember what Graham's algorithm is. You go through the jobs in an arbitrary order, one by one, and you always schedule the next job on the machine that has the lightest load, clearly a polynomial time algorithm. Moreover, we proved back then, we proved that the schedule produced by Graham's algorithm makespan at most twice the best possible in hindsight. So the best possible in hindsight is the same thing as our optimum for the offline problem.
00:12:59.862 - 00:13:52.502, Speaker A: So now it's just that both the algorithm and the optimal solution have all the jobs up front. So because Graham's algorithm had a competitive ratio of two, because it always produces schedule no more than two, worse than the best in hindsight, reinterpreting it as an approximation algorithm, we can call it a two approximation always outputs a schedule within factor two of the best possible. And this two, this is usually called the approximation ratio of an algorithm. It plays exactly the same role as the competitive ratio for online algorithms. They just use a different word for it in the offline case. Okay? So you could say, all right, so for Graham's algorithm, if really what you care about is the offline problem, the fact that Graham's algorithm actually processes jobs online, that's just kind of a bonus. It wasn't really a constraint for the offline problem.
00:13:52.502 - 00:13:59.030, Speaker A: So it's natural to ask, well, maybe I can somehow do better if I have all of the jobs up front.
00:13:59.180 - 00:13:59.782, Speaker B: Okay?
00:13:59.916 - 00:14:50.410, Speaker A: And you can actually for this problem, and actually, a very simple idea, which you'll explore in Problems at Four already proves this, which is, before you do your single pass over the jobs, you just sort them. Process the biggest job first, then the next biggest job, ending with the smallest job. Now, if you think about it, sorting the jobs by processing time, that's definitely not an online computation, right? If I feed you the jobs one at a time, and you have to schedule in that order, you can't sort but if you have them all up front, you can sort them and then go in that order. That lets you beat the factor two, actually by a significant amount. Okay, so that's an example of you how offline algorithms can do things that online algorithms cannot. Yeah. So PSET four can do better offline.
00:14:50.410 - 00:15:26.002, Speaker A: Okay, so that's example one. I want to go through sort of two more examples that just to just reinterpret previous stuff you've seen as approximation algorithms. Then we'll do a new example. So any questions so far? Okay, example number two. So this is something you may or may not have seen before. It sort of depends on who you took 161 from. I teach this when I teach 161.
00:15:26.002 - 00:15:55.706, Speaker A: Some other people do too, but I know some people don't. So hopefully though, you at least heard of the knapsack problem, which is sort of a traditional killer app for dynamic programming. So what's the problem? So the input is n items. Each item has both a value and it has a size or a weight. And then they also give you a knapsack with some fixed capacity. And the goal is you want to pack the knapsack, you want to stuff it subject to the capacity. So the sum of the sizes of the items you pick should be amos the capacity.
00:15:55.706 - 00:16:30.090, Speaker A: Subject to that, you want the most valuable subset possible. So you pick the subset of items with maximum value that fits in this Napsack. And if you think about it, whenever you have a single shared resource with bounded capacity, it's basically a knapsack problem. So it comes up all the time. Okay, so what do we know about Napsack? And again, so if you want a refresher, if you never saw that, I'm not going to teach it again in 261 because I teach it in 161. But I'll put again some 161 level videos up on the course site if you want. Let me sort of remind you of the executive summary.
00:16:30.090 - 00:16:46.800, Speaker A: So first of all, like I said, the students usually learn this for dynamic programming. So you learn an exact dynamic programming algorithm that solves the Napsack problem. Now one thing which is pretty confusing until you get used to it, so the Napsack problem is actually NP hard.
00:16:50.530 - 00:16:51.326, Speaker B: Okay?
00:16:51.508 - 00:17:18.070, Speaker A: So there's no polynomial time algorithm for the Napsack problem. Let's remember what polynomial time means for a second. So polynomial always means polynomial in the input size. Now you just have a graph with like n nodes and M edges. It's clear what the input size is. It's like n plus m. But remember, there's a subtle point when the input has numbers like the weight of an edge, the capacity of an edge, or the value or the weight of an item in knapsack.
00:17:18.070 - 00:17:43.086, Speaker A: So if the weight of something is a million, what's the input size? Well, the input size is just how many keystrokes it takes to type in the input to feed it to a program. And you only need seven keystrokes to type down the value a million. Okay, so the input size is logarithmic in the magnitudes of the numbers. The input size is the number of digits needed to write down all of the numbers in your problem.
00:17:43.188 - 00:17:43.886, Speaker B: Okay?
00:17:44.068 - 00:18:05.394, Speaker A: So polynomial input size means polynomial in the number of objects plus the number of bits necessary to represent all the numbers. And the Napsack algorithm is not polynomial in that sense. So the typical running time for the Napsack dynamic programming algorithm would be n the number of items times or let's say n squared times the maximum item weight.
00:18:05.522 - 00:18:06.150, Speaker B: Okay?
00:18:06.300 - 00:18:16.646, Speaker A: So if I multiply all of the item weights by ten, this running time actually gets slower by a factor of ten, even though the input length only went up by a single digit per item.
00:18:16.758 - 00:18:17.322, Speaker B: Okay?
00:18:17.456 - 00:18:30.186, Speaker A: So that's called a pseudopolynomial time algorithm, where it's polynomial in the number of objects and the magnitudes of the numbers rather than in the logarithm of the magnitudes of the numbers rather than in the number of digits.
00:18:30.298 - 00:18:30.910, Speaker B: Okay?
00:18:31.060 - 00:19:04.206, Speaker A: So that's how you reconcile the fact that Napsack is, on the one hand, NP hard, you cannot solve it exactly in time, polynomial in the number of digits and the number of items unless P equals NP. On the other hand, there are and again, this is an example of beating brute force search. Brute force search for Napsack would be two to the n. Try all subsets of items. And this dynamic programming algorithm, in kind of almost all cases of interest, will be much faster than that brute force search algorithm. So this is an empty hard problem where you can really do something non trivial. Okay? So that's what's up with solving Napsack.
00:19:04.206 - 00:20:05.610, Speaker A: Exactly. So what if you really want to be in truly polynomial time and you're willing to give up a little bit on correctness? So you can also there's a very natural greedy algorithm where the gist of it is you try to pack orders, try to pack the items in order of what's called density. So the value per weight, bang per buck in a sense. So that will give you a one half approximation, meaning it's guaranteed to output a feasible solution, total value at least 50% of the maximum possible. So that's the application of the greedy idea. Let's say dynamic programming equals exact. And then if you use dynamic programming in conjunction with Truncation what do we mean by Truncation? Well, what's wrong with the old dynamic programming algorithm? If the numbers are too big, then it runs a long time.
00:20:05.610 - 00:20:43.050, Speaker A: So what if I just like, lop off all the low order bits of all of the numbers in the input so that actually that old algorithm all of a sudden runs fast because the numbers have many fewer digits or much smaller numbers. So that's what I mean by truncation. You take the input numbers, you chop them, you sort of divide them all by a million, something like that. And then you run the dynamic programming algorithm and then you scale back. So if you do that in just the right way, you can actually get a one minus epsilon approximation for any epsilon you like in time that's polynomial in n and one over epsilon.
00:20:45.950 - 00:20:46.700, Speaker B: Okay.
00:20:48.750 - 00:21:22.918, Speaker A: So now we know we can't get a one minus epsilon approximation for epsilon equals zero in polynomial time. But the point is you can get as close as you want. You can get 99% in polynomial time. It's just that as you get closer and closer to optimal, your running time is blowing up with a one over epsilon as part of it. Okay, so you can't go all the way to exact, but for any constant epsilon, you really can get a one minus epsilon approximation for napsack. Okay, so that's sort of the happiest case imaginable. So if your problem is NP hard, this is about the best you could hope for, that you get arbitrarily close approximation, many NP hard problems.
00:21:22.918 - 00:21:35.100, Speaker A: This is not possible. Okay, all right, so any questions about any of that? Again, this is all stuff I cover in 161, and I'll put videos up on the website if you want to check them out later. So any questions?
00:21:39.390 - 00:21:40.140, Speaker B: Okay.
00:21:41.310 - 00:22:43.650, Speaker A: All right, so now let me revisit something definitely all of you have seen, because this is again something we talked about exactly one week ago in the context of online algorithms, the Steiner tree problem. So again, let me just quickly remind you the setup. So the input is a graph undirected with edge costs for each edge. And one thing I asked you to think through last week on exercise set number seven was why there's no loss of generality in restricting ourselves to the metric special case. What's the metric special case? It means the graph first of all, is complete, every edge is there. Second of all, the edges satisfy the triangle inequality. So the shortest path between two points is just the one hot path between the two endpoints.
00:22:43.650 - 00:22:54.610, Speaker A: Okay, so we'll use that again here. So without loss of generality, g is complete and the triangle inequality holds.
00:22:55.370 - 00:22:56.120, Speaker B: Okay.
00:22:58.650 - 00:23:08.440, Speaker A: All right. And then the other part of the input were terminals. Okay, so let me call as a set, I'll call them capital R.
00:23:12.090 - 00:23:12.860, Speaker B: All right.
00:23:14.990 - 00:23:19.910, Speaker A: So maybe we have t one, t two, t three, for example.
00:23:20.080 - 00:23:20.800, Speaker B: Okay.
00:23:22.450 - 00:24:16.160, Speaker A: All right, so we studied this in an online context previously, where the graph is specified up front and these terminals, t one through Tk come in online one by one, and at each time it had to make a decision. Oh, what's the goal? So the goal is to output a subgraph of G that spans all of the terminals, and subject to spanning all the terminals is as cheap as possible, minimum sum of edge costs. So in an online context, we studied a greedy algorithm which just processes the terminals in whatever order they come. So in some arbitrary adversarially chosen order. But again, just like with scheduling, this problem makes perfect sense also as an offline problem, where I give you not just the graph, but also all of the terminals up front. Okay, so just tell you these are the 100 terminals, connect them as cheaply as possible, please. That's the offline standard tree problem.
00:24:16.160 - 00:24:58.950, Speaker A: This again? Is MP hard? Again, I'm not going to prove it. Pretty much all of the MP hardness results from today, I think, date back to the original paper on MP completeness, or rather on sort of the pervasiveness of MP completeness by Dick Carpin 72. So these are all sort of among the very first generation of NP hard problems. And we gave a greedy algorithm, which was a two log k competitive, or in the current context, we would call it a two of log k approximation algorithm.
00:24:59.110 - 00:24:59.770, Speaker B: Okay?
00:24:59.920 - 00:25:13.806, Speaker A: So remember what that online greedy algorithm was. It effectively processes the terminals in an arbitrary order. Each iteration it adds one new edge connecting the new terminal to some previous terminal. And amongst all of the possibilities, it does the obvious thing. It picks the cheapest of those edges.
00:25:13.918 - 00:25:14.580, Speaker B: Okay?
00:25:15.030 - 00:25:30.280, Speaker A: So that was the online algorithm. And what we proved is that the solution it outputs is within two log k times the best possible solution. In hindsight, I e. The optimal solution of the offline version of the problem.
00:25:31.130 - 00:25:31.880, Speaker B: Okay.
00:25:34.970 - 00:26:31.606, Speaker A: So in scheduling, I mentioned briefly that you could do better in the offline case if I give you all the jobs in advance, you can sort. And so the question arises is, if we study Steinertree in the offline case, can we do better than log k? Okay, you can't do better than log k with an online algorithm, it turns out, but you can with an offline algorithm. And that's what I want to show you next. And the high level idea of why you can do better is the same as before, which is if I give you all the terminals at once in advance, you can process them in whatever order you choose, okay? Not just in some arbitrary one that comes in one by one online. And again, we kind of want to sort of be greedy in some sense. So, okay, we're going to process the terminals one at a time. What's the best order for us? Well, probably the first thing you might think of would be, okay, among all the terminals that we might connect next, let's connect the one that we can connect as cheaper than any of the rest of them.
00:26:31.606 - 00:26:56.910, Speaker A: Okay, so if there's three terminals left, we could pick any of them next. The first one has a shortest edge of length three to a previous terminal. The second one has a shortest edge of length five to a previous terminal. And the third one has a shortest edge of length seven to a previous terminal. We pick the first one, we say, oh, let's go for the three. Maybe after we include the three, we'll be able to connect those other two cheaper than we could before. Okay, so that's an idea.
00:26:56.910 - 00:27:07.220, Speaker A: So do the same thing as in the online algorithm setting, except process the terminals in a greedy ordering in order of so basically at all time is picking the terminal that you can connect as.
