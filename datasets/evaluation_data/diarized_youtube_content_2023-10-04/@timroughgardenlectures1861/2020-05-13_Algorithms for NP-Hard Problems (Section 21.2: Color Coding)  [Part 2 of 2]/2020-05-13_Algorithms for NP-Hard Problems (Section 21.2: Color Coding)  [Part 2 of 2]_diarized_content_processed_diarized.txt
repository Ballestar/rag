00:00:00.570 - 00:00:29.766, Speaker A: Let's turn to the first step of the color coding approach. So the responsibility here is to color the vertices of a graph in k colors, where k is the target path length we're looking for, so that some minimum cost k path of the graph turns monochromatic, so that for some optimal path, each of its vertices gets a distinct color. Only problem is, how on Earth are we ever going to do that when we have no idea what minimum cost k paths look like? After all, that's what we're trying to find in the first place.
00:00:29.766 - 00:00:53.114, Speaker A: So here we're going to have to bring out another tool from our toolbox, one we actually haven't seen in a while, randomization. The hope is that a uniformly random coloring, meaning for each vertex, we independently assign it one of the k colors, each equally likely. The hope is that a uniformly random coloring actually has a decent shot at turning an optimal kpath to become panchromatic.
00:00:53.114 - 00:01:02.898, Speaker A: If that is the case and we get lucky, then we can relatively efficiently recover that path using the dynamic subroutine that we just dynamic programming subroutine that.
00:01:02.904 - 00:01:05.374, Speaker B: We just designed in this quiz.
00:01:05.422 - 00:01:09.726, Speaker A: Let's think through just what is the probability that a given kpath is turned.
00:01:09.758 - 00:01:13.650, Speaker B: Panchromatic under a uniformly random coloring?
00:01:38.920 - 00:01:43.252, Speaker A: All right, so the correct answer is the fourth one answer d k factorial.
00:01:43.316 - 00:01:45.608, Speaker B: Divided by k raised to the k.
00:01:45.774 - 00:01:52.420, Speaker A: So let's start with the number of different things that could happen. So we've got our kpath Capital P. It's got k different vertices.
00:01:52.420 - 00:01:55.096, Speaker A: Each of its vertices is going to be assigned a color from one through.
00:01:55.118 - 00:01:57.496, Speaker B: K uniformly at random, which means there's.
00:01:57.528 - 00:02:11.548, Speaker A: K different things that could happen to the first vertex of P k different things that could happen to the second vertex and so on up to K. Different things that could happen to the KTH vertex. Which means there's K to the K possible colorings of the K vertices.
00:02:11.548 - 00:02:21.040, Speaker A: In this path capital P. And by definition, each of those is equally likely. Each of the colorings happens with probability exactly one over the number of possibilities.
00:02:21.120 - 00:02:23.270, Speaker B: One over K raised to the k.
00:02:23.720 - 00:02:41.672, Speaker A: Question number two is about the numerator. So of all of these K to the k possibilities, and how many of them does this path P wind up being panchromatic? So the claim here is that the answer is k factorial. Why? Well, imagine we sort of first choose which vertex is going to get color.
00:02:41.726 - 00:02:43.640, Speaker B: Number one, say colored red.
00:02:43.790 - 00:02:45.736, Speaker A: There are k different choices for which.
00:02:45.758 - 00:02:47.516, Speaker B: Vertex winds up the red one.
00:02:47.698 - 00:02:56.616, Speaker A: Now we want to sort of figure out which vertex is green. Well, it has to be one of the k minus one uncolored vertices. So those are the number of choices we have for a green vertex.
00:02:56.616 - 00:03:08.096, Speaker A: Then we have k minus two remaining choices for a yellow vertex, and so on, all the way down to one choice remaining for that last color. So that gives us k factorial of the k to the k colorings give.
00:03:08.118 - 00:03:10.000, Speaker B: Us a panchromatic path.
00:03:11.160 - 00:03:15.780, Speaker A: So how should we interpret that answer? I mean, k factorial and k to the k are both growing pretty quickly.
00:03:15.850 - 00:03:18.516, Speaker B: With k. So what does their ratio look like?
00:03:18.698 - 00:03:24.168, Speaker A: So for short, let's denote this ratio by lowercase p. How can we get.
00:03:24.174 - 00:03:25.192, Speaker B: A feel for it?
00:03:25.326 - 00:03:31.316, Speaker A: Well, in the numerator we have this k factorial. And remember a couple of videos ago I showed you a really good approximation.
00:03:31.348 - 00:03:34.532, Speaker B: For the factorial function, sterling's approximation.
00:03:34.676 - 00:03:49.410, Speaker A: Back then, in the context of the tsp, I was just trying to illustrate how much faster a two to the n time algorithm is than an n factorial time algorithm here. Actually, sterling's approximation will play a much more direct role. Let me remind you what it says.
00:03:49.410 - 00:03:59.168, Speaker A: Strilling's approximation says that n factorial is very well approximated by n over e. Here, e is 2.7 118 dot, dot, dot.
00:03:59.168 - 00:04:04.384, Speaker A: So the ratio N over E raised to the Nth power times a leading.
00:04:04.512 - 00:04:07.190, Speaker B: Term square root of two pi N.
00:04:07.640 - 00:04:24.380, Speaker A: Before we were content to just notice that N over E to the N is a lot bigger than two to the N for even modest values of N. Here, let's actually plug in this formula for the factorial function to simplify our ratio p. So we're going to plug in Sterling's approximation for the numerator.
00:04:24.960 - 00:04:27.070, Speaker B: With K playing the role of N.
00:04:27.440 - 00:04:40.038, Speaker A: Noting that the two K to the K terms cancel out. We can simplify this expression as follows. So this looks pretty bad.
00:04:40.038 - 00:04:57.834, Speaker A: Our probability of success, p, meaning the probability that we transform a given k path into a pancreatic path using a uniformly random coloring that's decreasing exponentially fast with k, you can see that e to the k in the denominator. So in fact, even if you just plug in k equals seven, this is.
00:04:57.872 - 00:05:01.206, Speaker B: Already less than 1%, which is kind of a bummer.
00:05:01.398 - 00:05:25.842, Speaker A: On the other hand, who says we have to stop with just one uniformly random coloring? So, randomized algorithm, it's going to do different things the more times we run it. So we could just do a bunch of independent random trials, keep trying different colorings, keep invoking our dynamic programming subroutine for computing the min cost panchromatic path, and over all of our trials, we just remember the best of all of the panchromatic paths that we ever see. We only need to get lucky once.
00:05:25.842 - 00:05:53.994, Speaker A: If even one of our random colorings winds up turning an optimal kpath panchromatic, our dynamic programming subroutine is guaranteed to find it. So the question is not so much what is the probability that a single experiment succeeds? The question is how many trials do we need before we're going to succeed in at least one trial with probability at least, say, 99%? Well, here there's a very clean answer. Let's build it up step by step.
00:05:53.994 - 00:06:00.526, Speaker A: Let's start with just one trial so one trial succeeds with probability P, which is pretty small, so it fails with.
00:06:00.548 - 00:06:03.214, Speaker B: Probability one minus P, which is pretty big.
00:06:03.412 - 00:06:07.946, Speaker A: We're not going to stop at one trial. We're going to do capital T. Independent random trials.
00:06:07.946 - 00:06:21.762, Speaker A: Well, capital T is a parameter we get to pick. We want to know how big do we need to set capital T to get what we want? So if one trial fails with probability one minus P, the second trial also fails with probability one minus P, and so on. All of these trials are independent.
00:06:21.762 - 00:06:32.390, Speaker A: So the probabilities multiply, meaning that the probability that all capital T trials fail is one minus P, the failure probability of one trial raised to capital T.
00:06:32.460 - 00:06:33.990, Speaker B: The number of trials.
00:06:34.510 - 00:06:48.874, Speaker A: And if this does not happen, if it is not the case that all capital T of the trials fail, then at least one of them succeeded. And that's exactly what we care about. So the probability that at least one trial is successful is going to be one minus this quantity, quantity one minus.
00:06:48.922 - 00:06:51.246, Speaker B: P raised to the capital T. So.
00:06:51.268 - 00:07:06.706, Speaker A: That may look a little messy. One minus quantity one minus P raised to the T. To simplify things, let's remember something that actually came up a few videos ago, which is the close relationship between the linear function one minus X and the exponential function E to the minus X.
00:07:06.706 - 00:07:29.962, Speaker A: We were discussing this back when we were talking about why does that magical quantity, for maximum coverage and influence maximization one minus quantity one minus one over K raised to the K, why does that converge to 63.2%? So back then we were using that one minus X and E to the minus X are pretty close to each other when X is close to zero. Here we're going to use the fact that E to the minus X is always at least as big as one minus X.
00:07:29.962 - 00:07:45.566, Speaker A: So one minus X is a linear function, e to the minus X is a curve that kisses it right at zero. So if we plug in, in particular X equal to P, then what we discover from this graph is that one minus P is bounded above by E.
00:07:45.588 - 00:07:47.578, Speaker B: To the minus P. And now that's.
00:07:47.594 - 00:07:53.186, Speaker A: A lot easier to handle. We have E to the minus P raised to the capital T, but then that just becomes E raised to the.
00:07:53.208 - 00:07:56.610, Speaker B: Minus P times T, meaning our success.
00:07:56.680 - 00:08:03.474, Speaker A: Probability that at least one of the trial succeeds is at least? One minus E to the minus p.
00:08:03.512 - 00:08:06.246, Speaker B: T. And what's really important here is.
00:08:06.268 - 00:08:15.370, Speaker A: That the probability that all of the trials fail, that's decreasing really quickly with capital T. That's decreasing exponentially. As we take more and more independent random trials.
00:08:15.370 - 00:08:31.306, Speaker A: Going back to our original question, how big do we need to take capital T? How many trials do we need so that we succeed with probability at least 99%. Well, that's a failure probability of at most 1%. So what we do is we're just going to set this failure probability bound that we have e to the minus PT.
00:08:31.306 - 00:08:35.006, Speaker A: We're going to set that equal to a parameter delta, where here delta would.
00:08:35.028 - 00:08:36.414, Speaker B: Be like zero one.
00:08:36.612 - 00:08:54.546, Speaker A: So now we can solve for the number of trials capital T as a function of delta. We find that as long as we take at least one over P or p is a success probability times log one over delta, where delta is the failure probability. We're willing to tolerate that many trials is enough to get us at least one success with probability at least one minus delta.
00:08:54.546 - 00:09:13.386, Speaker A: So for example, if our success probability of one trial P was like 1%, that one over P term would turn into a factor of 100. And if we set delta to be zero one, meaning we want a 99% success rate, then that's going to multiply the 100 by something like five. So it's going to tell you that take 500 trials and you're good to go.
00:09:13.386 - 00:09:14.794, Speaker A: You should succeed almost all the time.
00:09:14.832 - 00:09:16.480, Speaker B: On at least one of them.
00:09:17.010 - 00:09:30.098, Speaker A: In the context of color coding, where we're taking these uniformly random colorings to turn kpaths panchromatic, we know what our success probability P is. It's root two pi k over e to the K. That was our sterling approximation that got us that.
00:09:30.098 - 00:09:49.870, Speaker A: And so that gets flipped in the number of trials. So the number of trials we're going to need, the number of times we need to experiment with uniformly random colorings before we're likely to have had at least one success. Where a given k path turned panchromatic that's going to be e to the k divided by root two pi k times this log of one over delta factor.
00:09:49.870 - 00:10:04.942, Speaker A: Now this may seem extravagant, this exponential in K number of trials, but don't forget, we're already in our dynamic programming subroutines spending time exponential in K. So this exponential in K is just going to multiply with that one.
00:10:04.996 - 00:10:07.600, Speaker B: And we'll have ballpark the same type of running time.
00:10:09.890 - 00:10:22.590, Speaker A: So just to make sure that it's clear how all the ingredients fit together, let me go ahead and show you the pseudocode. The first thing the algorithm does is compute how many random trials it needs. And that's what we just sort of figured out on the previous slide.
00:10:22.590 - 00:10:36.882, Speaker A: So it's going to be e to the K divided by root two pi k times log one over delta, where delta is this user supplied failure probability. Now we're just going to run capital T independent random trials. Each trial we pick a fresh, new, uniformly random coloring.
00:10:36.882 - 00:10:52.230, Speaker A: Each trial we invoke our panchromatic path subroutine to find the minimum cost panchromatic path for that particular coloring. And then we just remember the best path that we ever see over all of the trials. That is the color coding algorithm.
00:10:52.230 - 00:11:02.926, Speaker A: So it does a whole bunch of independent random trials. And in each independent trial it experiments with a uniformly random coloring of the vertices. Each trial might succeed or it might not.
00:11:02.948 - 00:11:04.030, Speaker B: It might fail.
00:11:04.610 - 00:11:34.294, Speaker A: What do I mean by that? Succeed means that at least one of the minimum cost k paths becomes panchromatic, in which case that path or some equivalently good path will be found by the dynamic programming subroutine. Or it could fail, meaning that this coloring actually winds up turning none of the minimum cost kpaths of the graph panchromatic, removing all of them from the subroutine's consideration. So in that failure case, the subroutine, maybe it returns plus infinity if in fact the coloring meant there were no panchromatic paths at all.
00:11:34.294 - 00:11:49.402, Speaker A: Or if the subroutine returns a panchromatic path, it can't be a minimum cost one because none of them were panchromatic in the failure case. So it's going to be some kpath of the original graph with strictly higher cost. But the point is, we only need one of these trials to succeed.
00:11:49.402 - 00:12:03.282, Speaker A: If at least once we wind up coloring the vertices so that some minimum cost kpath becomes pancreatic, then this algorithm will be correct. And of course, we've chosen the number of trials capital T, so that the success probability is exactly what we wanted.
00:12:03.336 - 00:12:06.050, Speaker B: It to be, at least one minus delta.
00:12:19.190 - 00:12:37.054, Speaker A: How about the running time of the algorithm? Pretty much all the algorithm does is run these capital T independent random trials. So the running time is just going to be the number of trials capital T times the running time per trial. So the number of trials we computed explicitly, it's e to the k divided by root two pi k times log one over delta.
00:12:37.054 - 00:12:44.138, Speaker A: Let's just be a little bit sloppy with the upper bound and forget about that root k factor. Let's just call the number of trials o of e to the k times.
00:12:44.224 - 00:12:45.710, Speaker B: Log one over delta.
00:12:46.130 - 00:13:02.578, Speaker A: The time of a trial is completely dominated by the invocation of the dynamic programming subroutine for computing a minimum cost panchromatic path. If you recall, the running time of that algorithm via a sort of Bellman Ford style argument was two to the k times m, where m is the.
00:13:02.584 - 00:13:04.110, Speaker B: Number of edges in the graph.
00:13:04.270 - 00:13:28.794, Speaker A: So multiplying out, that gives us a running time of quantity two times e raised to the k power times the number of edges m times log one over delta, where delta is the user supplied failure probability. So how should we feel about this running time? Well, it is beating the pants off of exhaustive search. Remember in exhaustive search you had to enumerate all ordered k tuples vertices of that's going to be scaling like n to the k.
00:13:28.794 - 00:13:35.962, Speaker A: Here we have a running time bound that scales like a constant raised to the k. Constant is not as small as it was before. Now the constant is like 5.5.
00:13:35.962 - 00:13:47.342, Speaker A: But still for the values of n and k that we're talking about k equal to like ten or 20 and n equal to, say, in the hundreds or the thousands. 5.5 to the k is way, way better than a running time of n to the k.
00:13:47.342 - 00:14:02.822, Speaker A: N to the k would be useless, really already for like, k equals five. There is a special name for algorithms of this type. So exact algorithms for NP hard problems, whose running time, while of course is exponential sort of exponential in only a rather restricted way.
00:14:02.822 - 00:14:15.434, Speaker A: So where the exponential dependence depends only on a particular parameter, sort of measuring the difficulty of the instance. So in the kpath problem, the parameter is just k. The longer the paths that you're looking for, the harder the problem gets.
00:14:15.434 - 00:14:26.838, Speaker A: In general, algorithms that have exponential dependence only on parameters and are polynomial otherwise in the input size, those are known as fixed parameter algorithms. I encourage you to do a web.
00:14:26.864 - 00:14:29.406, Speaker B: Search on that term if you want to learn more.
00:14:29.588 - 00:14:46.238, Speaker A: And this particular fixed parameter algorithm actually made a pretty big difference in the Motivating application. Remember, at the beginning of this section, we talked about the application of finding long linear pathways in protein protein interaction networks. So finding meaningful structure in biological networks.
00:14:46.238 - 00:15:14.106, Speaker A: And before color coding came along, the state of the art techniques were getting stuck for pretty small values of k, maybe k around ten or something like that. And with the invention of color coding so even all the way back in like 2007 or so computers at that time, this algorithm allowed computation biologists to find linear pathways of length up to 20, even in PPI networks that had thousands of vertices. So that really led them to understand the structure of these biological networks in.
00:15:14.128 - 00:15:18.286, Speaker B: A much deeper way than they could before. So that wraps up our discussion of.
00:15:18.308 - 00:15:44.562, Speaker A: The color coding algorithm and more generally, of exact algorithms for NP heart problems that have provable running time bounds better than exhaustive search. For the rest of this chapter, for the rest of chapter 21, I want to discuss state of the art technology that does not necessarily have provable running time bounds better than exhaustive search, but can be super effective for tackling NP hard problems in applications. State of the art solvers for mixed integer programming and satisfiability.
00:15:44.562 - 00:15:45.942, Speaker A: We'll start talking about that next.
00:15:45.996 - 00:15:47.570, Speaker B: I'll see you then. Bye.
