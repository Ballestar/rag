00:00:00.330 - 00:00:24.962, Speaker A: So for the formal proof, let's start by formalizing what I asserted that this influence function is nothing more than a weighted average of coverage functions, coverage functions from those event attendance problems. So formally, let's fix an input of the influence maximization problem. So we've got a directed graph g, we've got an activation probability p between zero and one, and a positive integer k, the number of seed vertices that we're picking.
00:00:24.962 - 00:00:41.974, Speaker A: And for convenience, on this slide, we're going to think about the version of the cascade model that includes that post processing step. So remember, that means that even after the process concludes, if some of the edges are still unflipped, just go ahead and flip them. So at the end of the day, all of the edges will be either active or inactive.
00:00:41.974 - 00:01:11.058, Speaker A: And the key thing to remember about that cascade process is that the vertices that wind up activated, given a set of seeds, are precisely those reachable from a seed vertex by a directed path of activated edges. So next, as a thought experiment, imagine that we actually had telepathy and we just knew in advance exactly which edges are going to wind up being activated. In effect, imagine that we just tossed all of the coins up front rather than on a need to know basis like we do in the cascade model.
00:01:11.058 - 00:01:37.130, Speaker A: Then, if we knew the activated edges capital H, the influence maximization problem would literally boil down to a maximum coverage problem. The ground set would just be the vertices V of the social network, and there would be one subset per vertex with the corresponding subset containing all of the vertices reachable from V by a directed path of activated edges. That is by a directed path in the subgraph V comma capital H.
00:01:37.130 - 00:01:53.298, Speaker A: So for example, going back to our four vertex graph we looked at earlier. So suppose that the activated edges happen to be the edge from A to B, the edge from B to D, and the edge from C to D. So those three edges constitute the set capital H.
00:01:53.298 - 00:02:14.658, Speaker A: Then we've got one subset corresponding to the vertex A, which includes all of the vertices reachable from A along activated edges. So that's going to be A, B, and D. Similarly, the set corresponding to C that's going to be all the vertices reachable from C along a directed path of activated edges.
00:02:14.658 - 00:02:35.082, Speaker A: So you can get from C to itself, you can get from C to D, and that's going to be it. So what you can then say is that the number of people that wind up activated is exactly the coverage of the subsets corresponding to the seed vertices. So for example, in this example, if you pick the subset corresponding to A and the subset corresponding to C, then you wind up activating all four vertices.
00:02:35.082 - 00:02:43.742, Speaker A: And that's because the coverage of those two sets is all four vertices. Now, of course, we don't know all of the Activated Edges In Advance. Capital H.
00:02:43.742 - 00:02:48.658, Speaker A: That's not fixed. That's some random set. Different h's have different probabilities attached to them.
00:02:48.658 - 00:03:25.886, Speaker A: But at the end of the day, the influence of a given set of seed vertices is just going to be the expected value of this coverage function where the expectation is over all possible subsets capital H. So all two to the M subsets of possible active edges. So what does that expectation look like? Well, we can just sum over all of the different subsets h of activated edges that we might encounter.
00:03:25.886 - 00:03:31.698, Speaker A: Again. There's two to the m of those where m is the number of edges. Each one of those has some probability piece of h.
00:03:31.698 - 00:03:43.334, Speaker A: Actually, there's a simple formula for that. We're not going to need the formula. But basically for the set H to be exactly the set of activated edges, you need to flip heads for all of the coins in corresponding to edges of H.
00:03:43.334 - 00:03:52.762, Speaker A: So that is a P to the size of H probability. You also have to flip tails for all of the edges outside of H. So that has a one minus P raised to the number of edges outside of H.
00:03:52.762 - 00:03:59.802, Speaker A: Probability. But whatever. There's some probability piece of H that H is exactly the subset of activated edges.
00:03:59.802 - 00:04:34.434, Speaker A: And then, as we saw, once you've fixed the activated edges, you just have a coverage function with the ground set being the vertices. Also one subset per vertex where the subset corresponding to a vertex is the other vertices reachable from that vertex along a directed path of edges that are active edges of capital h for the Rigor obsessed among you. What we're really doing is we're using what's called the Law of Total Expectation to write the original expectation, the definition of influence, as a probability weighted average of conditional expectations, where the conditioning is on the activated edges.
00:04:34.434 - 00:04:39.834, Speaker A: Capital H if that didn't make sense. Don't worry about it. I hope it's intuitively clear that there are these two to the M.
00:04:39.834 - 00:04:56.910, Speaker A: Possibilities for the different subsets of activated edges. Each of those possibilities has its own probability, each of those situations has its own coverage function, and the influence is just the corresponding weighted average of those two to the M coverage functions. All right, so that sounds like good news.
00:04:56.910 - 00:05:06.130, Speaker A: We already know that the greedy algorithm for one coverage function gets the approximate correctness guarantee that we want. All right? Now, we're not dealing with a one coverage function. It's an average of a bunch of them.
00:05:06.130 - 00:05:10.594, Speaker A: But hopefully the analysis still works. And so let's check. That is indeed the case.
00:05:10.594 - 00:05:23.106, Speaker A: The main thing we need is a new version of what we were calling the Key Lemma for the maximum coverage problem. So this is the lemma that asserts that the greedy algorithm makes progress in each iteration. So back in maximum coverage.
00:05:23.106 - 00:05:41.550, Speaker A: We wanted to say that each of the K iterations increases the coverage by a healthy amount. Here we want to argue that each iteration of the KKT algorithm increases the influence of the current set of seed vertices by at least a given amount. And the form of the key Lemo will be exactly the same as it was in the maximum coverage problem.
00:05:41.550 - 00:06:06.422, Speaker A: So we're going to lower bound the progress, the increase in influence in each iteration in terms of the deficiency of the current solution at that iteration. So we're going to let I star denote the maximum influence possible using a K seed vertices. And then the deficiency in a given iteration of KKT will be the extent to which the influence of KKT's current solution falls short of the maximum possible I star.
00:06:06.422 - 00:06:31.422, Speaker A: And the guaranteed progress is that in each iteration you will increase the influence by at least a one over k fraction of that deficiency. So this is literally exactly the same as the Lemma in the maximum coverage problem, except what used to be C star for the maximum coverage is now istar for the maximum influence. And what used to be the number of elements already covered is now just the current influence of the KKT solution, otherwise it's exactly the same.
00:06:31.422 - 00:06:51.118, Speaker A: Now, you might recall that in our approximate correctness guarantee for the greedy coverage algorithm, we had two parts. We had a key lemma like this one guaranteeing progress, and then we had a second part which was doing some algebra to get the final approximate correctness guarantee. Now this key lemma is exactly the same guarantee that we had for maximum coverage.
00:06:51.118 - 00:07:06.778, Speaker A: So the second part of the proof continues to apply completely unchanged. Given that this Lemma is actually true, you just do the exact same iterate at K times invoke the geometric series formula simplify. And if this Lemma is true, we will get what we want.
00:07:06.778 - 00:07:25.134, Speaker A: That the KKT algorithm guarantees at least a one minus quantity, one minus one over K raised to the k fraction of the maximum possible influence. So we're going to conclude this video by proving the key lemma. We will declare victory at that point because the approximate correctness guarantee follows in exactly the same way as before.
00:07:25.134 - 00:07:34.926, Speaker A: So some notation, let's consider an arbitrary optimal solution. So some K vertices call them capital S star. By optimal I just mean they have the maximum possible influence.
00:07:34.926 - 00:07:48.550, Speaker A: So influence istar. So the plan is we're going to fix some subset of the possible activated edges that gives us a coverage function as we've seen. Then we're going to just piggyback on the analysis that we already did for the coverage problem.
00:07:48.550 - 00:07:55.490, Speaker A: And then we'll take a weighted average at the end. So to get started, fix your favorite subset of edges. There's two to the M possibilities.
00:07:55.490 - 00:08:18.222, Speaker A: I don't care which one you pick, just pick one capital h. We then have a corresponding coverage function, which I'm going to call f sub h. And again, remember, what is that coverage function? That just says, given that h is exactly the active edges, and given that you picked a particular set s of seed vertices, how many vertices are reachable from a seed vertex along one of those active edges? And as we've seen, that's exactly a coverage function.
00:08:18.222 - 00:08:30.658, Speaker A: So that's what I mean by F sub H. Now, we worked pretty hard to prove that approximate correctness guarantee for the maximum coverage problem and that greedy algorithm. So we'd certainly like to reuse as much of that work as we can here.
00:08:30.658 - 00:08:53.014, Speaker A: And if you'll recall, or if you go back to that video, improving that approximate correctness guarantee, we had a key lemma just like this one involving the coverage rather than involving the influence, and then improving the key lemma. We had this key claim. So this really important inequality, and this is when we're talking about the green region, one thing being bigger than the green region, and the other thing being smaller than the green region.
00:08:53.014 - 00:09:23.874, Speaker A: So what did we say? Well, we said, on the one hand, let's consider the deficiency in coverage up to this point. And then on the other hand we said, let's look at how much extra coverage you would get under this thought experiment of adding each subset from the optimal solution to what you have so far, and the latter is at least the former. So, in other words, there's always going to be one option, one of the subsets from the optimal solution, which will get you a one over k fraction of your deficiency, the extent to which your coverage is less than the maximum possible coverage.
00:09:23.874 - 00:09:44.986, Speaker A: So we just copy that exact same inequality down here, where we instantiate it with the coverage function induced by the subset capital F of activated edges, so that's f sub H. And again, that's just for a given set. That's just the number of vertices reachable from S via a path of active edges, a path of edges that are in capital H.
00:09:44.986 - 00:10:03.282, Speaker A: So I've literally just copied that inequality down here using our current coverage function f sub H. The left hand side it's doing a thought experiment for each of the vertices v in the optimal solution I star. It's asking how much boost and influence would we get if we added to the greedy solution SJ minus one.
00:10:03.282 - 00:10:30.522, Speaker A: If we added this vertex v from the optimal solution S star, we do that thought experiment k times once for each of the k vertices in the optimal solution S star. We sum up the results so that's the bigger quantity and then the smaller quantity is our current coverage deficiency with respect to this coverage function F sub H. So the extent to which our coverage under f sub h is not as big as the coverage of S star would be.
00:10:30.522 - 00:10:42.426, Speaker A: All right, so we really haven't proven anything right now. This is all stuff we proved a couple of videos ago. This is literally just piggybacking directly on the key inequality from the key claim in our coverage analysis.
00:10:42.426 - 00:10:59.700, Speaker A: So let's now actually use the fact that influence is a weighted average of coverage functions and complete the proof of the guarantee for the KKT algorithm. All right, so that inequality, I don't expect you to remember it, so let me write it down again. Here at the top of the slide is exactly the same inequality we had before.
00:10:59.700 - 00:11:20.730, Speaker A: Now, this is an inequality for a fixed choice of capital H, a fixed set of edges that wind up being active. So we actually don't just have one inequality like this, we actually have two to the M inequalities like this, where M is the number of edges. So we have an inequality of this form for each choice of capital H, each subset of edges.
00:11:20.730 - 00:11:41.370, Speaker A: So now the trick we're going to do, we're like, well, we already know that weighted influence is a weighted sorry, that influence is a weighted average of coverage functions. So let's just take these inequalities one for each coverage function and take the weighted average of them using the exact same weights that we have in the definition of influence. So weighted by the probability, the capital H really is the set of activated edges.
00:11:41.370 - 00:11:59.910, Speaker A: All right, so let me show you exactly what I mean by taking a weighted average of these two to the M inequalities. Let me give myself a little more room to work with here on the right hand side. So whenever you have an inequality like this that some number A is at least as big as some number B, if you multiply both sides by the same positive number, the inequality is still true.
00:11:59.910 - 00:12:25.422, Speaker A: So if A is at least B twice A is at least twice B, one half A is at least one half B, and so on. So let's take the inequality corresponding to a subset H of active edges and multiply through by the probability that that set really is the set of active edges in the cascade model. So now we have the inequality, weighted by the probability that that actually is the world we're going to be living in, weighted by the probability p sub H.
00:12:25.422 - 00:12:34.386, Speaker A: Again, we don't really care what P sub H is, but there is that closed form formula. P raised to the number of edges in h times one minus p raised to the number of edges not in h. All right.
00:12:34.386 - 00:12:55.094, Speaker A: So again, we have one of these inequalities for each possible subset of edges, we want a weighted average. So finally, let's just sum up the two to the M inequalities. We get another inequality, right? If a one is at least as big as B one and a two is at least as big as B two, well then a one plus A two is definitely at least as big as B one plus B two.
00:12:55.094 - 00:13:05.446, Speaker A: So we did this just following our nodes we wanted to piggyback on our coverage analysis. We knew we had a coverage function only when we fixed a set h of active edges. And then we knew we needed to get back to the influence.
00:13:05.446 - 00:13:16.542, Speaker A: And we knew that influence was this weighted average of the coverage functions. So it made sense to just take the same weighted average of the inequalities that we got for each coverage function individually. So that was all a reasonable thing to try.
00:13:16.542 - 00:13:27.122, Speaker A: But now actually, it's really going to work out just magically. So watch what happens when we interchange a couple sums. So all I've done here is multiplied through by p sub h.
00:13:27.122 - 00:13:47.546, Speaker A: I've pushed the p sub h in as far as I can, and also on the left hand side, I harmlessly reversed the order of summation. So now I'm summing over the vertices in the optimal solution first, and then over the possible subsets of active edges second. Now, why was this such a good move? Well, we've seen this sum over h p sub h times f sub h before.
00:13:47.546 - 00:14:08.014, Speaker A: Remember, influence is exactly a weighted average of coverage functions. The coverage functions involved are exactly these f sub h's and the weighted the probabilities are exactly the p sub h's. So the influence f INF is exactly sum over Hphh, and that means we are good to go.
00:14:08.014 - 00:14:14.878, Speaker A: That means all of these sums over h's. We know another name for these things. This is just the influence of the corresponding vertex subset.
00:14:14.878 - 00:14:57.520, Speaker A: So for example, this first sum over h on the left hand side, that's nothing more than the influence of the first j minus one vertices chosen by the KKT algorithm, along with this one vertex v from the optimal solution S star. Similarly, that second sum over h on the left hand side, that's just the influence of the first j minus one vertices chosen by the KKT algorithm. And then on the right hand side, we get the influence of S star, and we get the influence of SJ minus one.
00:14:57.520 - 00:15:10.786, Speaker A: So that's the analog of the most important inequality that we had for the maximum coverage analysis. This is the analogous inequality that we now have for influence maximization. And now that we know this, we're pretty much good to go.
00:15:10.786 - 00:15:29.546, Speaker A: The rest of the proof goes exactly the same as it did for maximum coverage. So what does this key inequality tell us? It tells us that if we think about these k thought experiments so we have the greedy solution so far, s j minus one, the j minus one vertices it picked in its first j minus one iterations. We do k thought experiments one for each vertex in the optimal solution S star.
00:15:29.546 - 00:15:57.970, Speaker A: We say how much would the influence jump if we included that particular vertex right now as part of the greedy solution? And this inequality says the sum of those thought experiments can be bounded below by the current deficit in influence, the extent to which the maximum possible influence is bigger than the influence achieved by the KKT algorithm already. Now, the next step is we just use the same maneuver that the maximum of k numbers has to be at least the average. So on the left hand side of this inequality we have the sum of k things.
00:15:57.970 - 00:16:26.762, Speaker A: The average value is just one over k times the sum of k things, and so one of the numbers has to be at least that big. So again, remember that if I have ten numbers, that sum to 101 of them has to be at least ten, that's just one of them has to be at least the average. So that means that there exists a vertex in s star that captures at least a one over k fraction of that left hand side, which of course means it also captures at least a one over k fraction of the right hand side, because the right hand side is only less.
00:16:26.762 - 00:16:44.126, Speaker A: In other words, there's a vertex in the optimal solution s star. So that if you added it right now to the greedy solution so far, you're guaranteed the influence would go up by at least one over k times the current influence deficit. Now, KKT algorithm may or may not pick that vertex, but it's a greedy algorithm.
00:16:44.126 - 00:17:10.294, Speaker A: It picks the best vertex for this iteration that maximizes the influence increase. So whatever the KKT algorithm does, it's going to get at least this same lower bound on the influence increase at least one over K times the left hand side of this inequality, hence one over K times the right hand side of this inequality, also known as one over K times the influence deficit. F INF of s star minus f INF of s j minus one the j minus one vertices chosen in the first j minus one iterations.
00:17:10.294 - 00:17:26.474, Speaker A: If you go back and look at the statement of the key lemma, that is exactly it. We have now finished the proof of the key lemma. And once again, the approximate correctness guarantee for the KKT algorithm from here proceeds via the exact same algebra that we used for the maximum coverage analysis.
00:17:26.474 - 00:17:44.434, Speaker A: So this shows that even though influence maximization is a more general problem than maximum coverage, the analogous greedy algorithm, the KKT algorithm, gets us just as good a guarantee. Two seed vertices, you're going to get at least 75%. Three seed vertices, you're going to get at least 70.4%.
00:17:44.434 - 00:17:58.230, Speaker A: No matter how many seed vertices you're choosing, you will get at least 63.2%. And again, this is just an insurance policy. This is just sort of telling you what would happen in the doomsday scenario of a most contrived possible instance.
00:17:58.230 - 00:18:17.338, Speaker A: On more realistic instances, you should expect this heuristic algorithm to over deliver. You expect to actually get influence quite a bit closer to 100% than this worst case analysis would suggest. So that wraps up the part of our discussion of fast heuristic algorithms, which uses the greedy algorithm design paradigm.
00:18:17.338 - 00:18:39.780, Speaker A: It also wraps up the part of our discussion on heuristic algorithms where we have provable approximate correctness guarantees. But we're not done with chapter 20 yet because I do want to add one new tool, important tool to your toolbox, where even though there aren't usually provable guarantees, it's still extremely effective on MP hard problems in many cases in practice. So that's going to be local search and I'll see you there.
