00:00:00.250 - 00:00:25.574, Speaker A: So this video is going to be the third and final one about the details of coupling proof of stake civil resistance together with BFD type consensus in both of our first two videos. The last two videos we had the standing assumption, which was quite unsatisfying, which was that we were assuming all of the active validators in the Staking contract had exactly the same stake amounts. And as we saw in the last video, there's a number of challenges that come up even for that special case.
00:00:25.574 - 00:00:55.118, Speaker A: But now that we've talked through those issues, let's actually talk about what the protocol would look like in the more general case where validators have arbitrary stake amounts, arbitrary q sub eyes. So this is going to require making a few changes to the protocol as we've discussed it thus far. So remember, at a high level we're just trying to reduce permissionless consensus to permission consensus by using a random sampling procedure to select both a committee to do voting and then also a block proposer who are then going to run tendermit for some period of time.
00:00:55.118 - 00:01:20.246, Speaker A: And so if you think about the committee selection and the leader selection, if we're going to have non uniform stake amounts, presumably those things need to change a little bit. So presumably we want to treat active validators with a large stake differently than active validators with a small stake, both in the sense that they should be more likely to be elected to the Committee of Voters, they should have more expected voting weights and then also they should have a higher probability of being selected as a block proposer. Right.
00:01:20.246 - 00:01:56.900, Speaker A: The whole point is we're trying to do proof of stake, we're trying to sample with respect to the stake distribution when it comes to electing the block proposer. So let's start by reexamining committee selection when we have nonuniform stake amounts, when we have nonuniform Q sub I's. So to define the committee we're going to continue to use VRFs verifiable random functions, but we're going to use the version of VRF based sampling that samples with multiplicities and in particular we'll sample in a way such that the higher your stake amount, the higher q sub i, the higher multiplicity you will typically be sampled with.
00:01:56.900 - 00:02:18.920, Speaker A: So this is something we talked about, if you remember back in our part two videos about VRF based sampling. And so let me remind you how that works. Basically the idea is look, if someone stakes ten coins, let's just treat them exactly as if they registered instead as ten sybils with one coin each.
00:02:18.920 - 00:02:47.214, Speaker A: So more precisely we're going to use tau to denote the choice of sampling probability for the special case we've been talking about thus far for the special case where everybody just has the same minimum stake amount, a stake amount of one. So as usual, I'm going to be assuming stake amounts are integers, that is they're denominated in kind of the smallest unit of the native currency. So this parameter, tau, this is the same as what I meant by tau sub C in the protocol description.
00:02:47.214 - 00:03:14.170, Speaker A: So for example, if you have 1000 public keys each registered with a stake of one coin and you want committee sizes of 100, you might expect tau to be like 10%. If you have 10,000 public keys registered in the Staking contract with one coin each, and you want committee sizes of size 100, you might expect tau to be 1%. So in any case, tau is going to be chosen as a function both of the committee size you want and of the total amount of stake that's locked up in the Staking contract.
00:03:14.170 - 00:03:51.586, Speaker A: Now again, the plan is to treat an active validator with ten staked coins as if it had registered ten times under different public keys with one coin each. So what does it mean to treat it as if it was ten civils with one coin each? Well, we're going to be flipping ten coins, one for each of the civils it could have in principle created, and then each of those coins is going to come up heads with probability tau. So the idea is then we want the active validator with ten staked coins to inherit the results of these ten independent biased tau coins that we flipped.
00:03:51.586 - 00:04:19.034, Speaker A: What does that mean to inherit the results of those ten coin flips? The answer to that question is going to sort of depend on what the node wants. And if you recall back in our part two videos about VRF based sampling with general stake amounts, we talked through two different variants that are relevant. The first variant we talked about there, which is not the one we're doing here, the first variant we talked about back then, we thought about a node that only cared about whether or not at least one of its ten symbols got selected.
00:04:19.034 - 00:04:41.666, Speaker A: It didn't care if it was one or five, it just cared about whether it was zero or at least one. And we'll actually see that again later in part three when we talk about coupling proof of stake random sampling with longest chain consensus here. Meanwhile, in the context of BFT type consensus, a node actually is going to care about how many of its sybils get selected.
00:04:41.666 - 00:04:57.894, Speaker A: In this thought experiment we're doing where we say what if it had registered ten times with one coin each, right? Because each selected sybil is going to be a member of the committee. And remember, the committee, they're the ones responsible for running the tendermint protocol. And remember, each participant in the tendermint protocol gets a vote.
00:04:57.894 - 00:05:27.010, Speaker A: So the number of your ten sybils that get selected, right, if three out of the ten get selected, that would give you three different votes you could cast in the resulting tendermint protocol. So because of that, because nodes really care about exactly how many times they would have been selected, had they sibled. We're going to need to have our sort of sampling procedure output not just whether or not some public key is on the committee, but in some sense how many times, so the multiplicity with which they are selected to the committee.
00:05:27.010 - 00:06:03.178, Speaker A: So we're going to define a notion of the sampled stake of a public key. So for example, a public key with ten coins, its sampled stake might be any of eleven different possibilities, anything from zero to ten reflecting the eleven different possible outcomes of the thought experiment with the ten unit coin sybils. And so this is going to give us sybil resistance again under this assumption that what a node really cares about is sort of the multiplicity with which its public keys get sampled, right? So if you own ten coins total, it doesn't matter whether you stake it all under one public key, if you split it three, three, four under three different public keys, or if you split it ten ways between ten different public keys.
00:06:03.178 - 00:06:26.226, Speaker A: At the end of the day, ultimately what the protocol is going to do on your behalf is flip ten independent coin flips, each with bias tau, and give you a combined sample stake of the total number of those coin flips that come up heads. So it really doesn't matter how you split, you're going to have the exact same distribution of your total sampled stake. All right, so that's the first change concerning how the committee gets selected.
00:06:26.226 - 00:06:46.080, Speaker A: And again, this is really just kind of using exactly the second variant that we discussed back in those part Two VRF videos. Now that we are sampling committee members with multiplicities, we actually have to now revisit the consensus protocol tendermint that they're going to be running. So we need a version of tendermint that accommodates nonuniform voting power.
00:06:46.080 - 00:07:26.810, Speaker A: So what I mean here is just that, for example, if you have a participant in tendermint that was sampled with multiplicity three, so it has sampled stake equal to three, then its vote should count three times. So in a little more detail, so like, imagine you're one of the sort of participants in this tenement protocol and you're sort of collecting votes from everybody in one of the voting stages and you're trying to figure out sort of how much should everybody's vote count? Well, everybody with their votes for a block should be of course, including their signatures so that you know what public key in the Staking contract that vote is supposed to correspond to. And then also everybody's supposed to be submitting their credentials along with their votes.
00:07:26.810 - 00:07:49.090, Speaker A: So previously we were thinking that these credentials would just be used to kind of verify committee membership, right? So maybe there's 10,000 public keys in the Staking contract, you know who all of those public keys are. And now as you're collecting votes, those votes should have attached to them a signature which tells you which of those 10,000 public keys it corresponds to. And furthermore, the vote should include a Credential.
00:07:49.090 - 00:08:16.898, Speaker A: So given that Credential, you know whether or not to pay attention to somebody's vote, right? By the efficient verifiability property of verifiable random functions, you can check that the Credential was indeed computed correctly. And then you just check, is the Credential below the threshold of the tau sub C or is it above? And if it's below the tau sub C, you're like, AHA, you're one of the roughly 100 committee members out of the 10,000 that's actually allowed to vote. If you try to trick me by including a Credential that's bigger than that threshold tau sub C, I'm going to say, nah, nah, nah, nah.
00:08:16.898 - 00:08:36.030, Speaker A: You're among the other 9900 public keys that didn't make it onto the committee. So I know to ignore your vote. Now, in this revised version of the protocol, where we more generally have these multiplicities, these sampled stakes, we want to use credentials not just to figure out whether people are on the committee at all, but we really want to know exactly what is their W Subai, what is their sampled stake.
00:08:36.030 - 00:08:52.774, Speaker A: So now if I'm trying to figure out how many times to count the vote that I received from you, what I'm going to do is I'm going to take your Credential, and I'm not just going to compare it to some simple threshold. I'm going to take your Credential, your VRF output. I'm going to look at how many total coins you have in the Staking contract, right? I know your public key.
00:08:52.774 - 00:09:03.750, Speaker A: I can look in the Staking contract that you have, say, ten staked coins along with that public key. And based on the knowledge of your Q sub I like ten. And your VRF output that is your Credential.
00:09:03.750 - 00:09:24.554, Speaker A: Based on those two things, I will be able to infer what your sampled stake W sub I is. And that's because the protocol is just going to use some sort of publicly known mapping to take as input stake amounts and VRF outputs and map that back to an integer between zero and the full stake amount. There's various ways you can define that mapping.
00:09:24.554 - 00:09:46.214, Speaker A: I'll leave the details of that point as an exercise for the viewer. So that covers how a participant in this version of the tenement protocol can count votes. Again, votes weighted by multiplicity, just by using the submitted credentials along with the votes, plus sort of peeking into the publicly viewable Staking contract to see what everybody's Q sub eyes are.
00:09:46.214 - 00:09:56.330, Speaker A: So the next question is, okay, so you can do your vote counting, and suppose you have a total voting weight of 75. Again, this counts multiplicities. Suppose you have 75 votes.
00:09:56.330 - 00:10:16.240, Speaker A: All in favor of one particular block. Do you consider that a supermajority I-E-A quorum certificate or not? So the obvious answer there would be like, well, why not just do what we always do and sort of demand more than two thirds of the overall voting power of the committee. So the total sampled stake among the committee members.
00:10:16.240 - 00:10:26.798, Speaker A: So that's a fine idea. Honestly, we'd sort of love to be able to do that. Just say, look, if the total weight on the committee is 100, just define a supermajority of 67.
00:10:26.798 - 00:11:06.740, Speaker A: As always, the reason that doesn't quite work is because how is a participant in this protocol supposed to actually know the total voting weight of the committee? That participant knows a lower bound on the total voting weight, right? Maybe they got say, 75 votes counting multiplicities in favor of some bloc and then they got sort of 17 dissenting votes for other blocs. Well, then they certainly know that the total weight on the committee is at least 92. But how do they know the total weight isn't bigger than that? And it just so happens you didn't hear from some of the other committee members, maybe because they're Byzantine and they withheld their votes from you, maybe because there's network delays and you just haven't heard from them yet.
00:11:06.740 - 00:11:38.780, Speaker A: The one piece of good news is that everybody at least does know the expected value of the total amount of voting weight on a committee because that's completely deducible from information that is common knowledge. For example, the total amount of stake that's in the publicly visible Staking contract. So if there's 10,000 coins total in the Staking contract, and then the parameter tau, which again is just part of the protocol, so everybody knows that if tau is 1%, then you know the expected number of votes counting multiplicity on a committee is going to be equal to 100.
00:11:38.780 - 00:12:03.170, Speaker A: Now, knowing that sort of 75 votes, all in favor of a single block, that sounds pretty compelling, right? That's even like sort of three quarters. And so it might be reasonable to just call that a supermajority and proceed. So it's important to realize that while this might work fine most of the time, if you get unlucky, you really might get bitten and you actually are vulnerable to consistency violations.
00:12:03.170 - 00:12:25.034, Speaker A: Because, for example, for all you know, this is just kind of a weird time step where you had sort of more sampling error than usual and you wound up with a big committee. So let's say you wind up with a committee with total voting weight 120 rather than 100. Let's go ahead, assume that the committee is proportional, right? So maybe in the Staking contract there's two thirds honest stake, there's one third Byzantine stake, and maybe in the committee it's exactly the same.
00:12:25.034 - 00:12:41.066, Speaker A: It's a big committee, but it's still two thirds, one third honest Byzantine. So 80 votes from controlled by honest nodes, 40 votes controlled by Byzantine nodes. And the problem in this case is that the 75 votes that you collected, that does not actually constitute a supermajority.
00:12:41.066 - 00:13:17.050, Speaker A: That is not actually two thirds of the committee's overall voting weight. And that means you're going to wind up being too permissive with what you accept as a quorum certificate and potentially too permissive with the decision to finalize a block. So, for example, if the 80 honest votes are split 50 50 between two conflicting blocks, and then the 40 Byzantine votes vote on both, giving both of those conflicting blocks a total of 80, well, then actually both of those are going to cross this 75 threshold that we're using here as a supermajority, and that's going to be a consistency violation.
00:13:17.050 - 00:13:37.950, Speaker A: Now, one way you can sort of reduce the likelihood of such consistency violations is to use a higher threshold. So if you're expecting a total number of 100 votes, counting multiplicities on the committee, you can say, well, to be a quorum certificate we're actually going to require sort of 80 votes or even 90 votes. So that's going to help reduce the frequency of potential consistency violations.
00:13:37.950 - 00:13:59.618, Speaker A: On the other hand, you sort of suffer on the liveness side, right? There's no free lunch here. So for example, imagine that you're very conservative and you insist on 90 votes to constitute a quorum certificate. And imagine further that 75% of your stake is honest and 25 is Byzantine, which already is like a pretty strong assumption about how much is honest stake.
00:13:59.618 - 00:14:20.062, Speaker A: Well then in a typical round, even if you hear from all of the honest nodes and they all agree, it's still only going to be 75 votes and you're not going to meet your threshold of 90. So now you're only ever going to make progress if you happen to have like a super big committee so that there's enough honest votes on this crazy big committee to pass your threshold of 90. So to some extent you can't really get rid of this problem.
00:14:20.062 - 00:14:40.902, Speaker A: All you can do is trade off kind of the likelihood of a consistency violation with the rate at which you're likely to make progress. So ultimately you just have to pick some parameter like maybe 75 and live with the consequences. So this problem is actually pretty similar to the last of the three issues that we discussed last video.
00:14:40.902 - 00:15:16.298, Speaker A: Remember, that issue also concerned sampling error. And there we were, we were concerned that, you know, maybe just by bad luck you get a committee which is not representative, right? So maybe even though that in your staking contract you have 75% honest stake and 25% Byzantine stake, maybe you get unlucky in sample committee that is 60% honest and 40% Byzantine, which again is not good enough and tendermint is vulnerable to consistency violations if you have that 60 40 split. Now you can choose your protocol parameters so that the likelihood with which you get such a disproportionate committee is small, but you can't drive that probability to zero.
00:15:16.298 - 00:15:34.450, Speaker A: So that's why we talked in the last video the need to have an emergency backup procedure for resolving forks in protocol, right? So maybe sort of after an unlucky committee, hopefully you get like a normal committee that does have a portion of representation. So it's more than two thirds honest. They're going to run basically a consensus protocol to figure out how to resolve the previous fork.
00:15:34.450 - 00:15:59.742, Speaker A: And then once you have a canonical choice of which branch of the fork is the real one, then you can kind of sort of proceed as normal. And so here then, we're seeing a second reason sampling error is a problem not so much about the proportional representation of the committee, but just about the size of the committee. And that if you have a really big committee, it might turn out that whatever sort of threshold for a quorum certificate you set actually isn't two thirds of the committee size.
00:15:59.742 - 00:16:13.566, Speaker A: In which case, again, tendermint becomes vulnerable to consistency violations. So this is the second issue that is caused by sampling error. And again, it's a second reason why you probably want to have this emergency backup procedure for resolving forks in protocol.
00:16:13.566 - 00:16:44.374, Speaker A: It's just hard to make the probability of a fork as close to zero as you'd really like. So the good news is that if all of your committees are fine, meaning the size isn't like crazy different from what you expect, and sort of the proportional representation isn't crazy different from what you expect, then actually the consistency and liveness guarantees of the tenement protocol carry over to this weighted case straightforwardly. Literally all you have to do is redefine the definition of a quorum certificate from lecture seven so that takes into account voting weights rather than just counting the number of distinct votes.
00:16:44.374 - 00:17:03.422, Speaker A: And then with that new definition of a quorum certificate, basically those consistency and liveness proofs go through unchanged. And that would be a good exercise for you to actually check for yourself. All right, so while these first two changes concern the selection of the committee members and how to interpret their votes, we still need to address the issue of selecting leaders.
00:17:03.422 - 00:17:20.738, Speaker A: Because again, remember, the whole point in Proof of Stake is we want to select block proposers with probability proportional to the Q sub I's. So previously we were just defining the leader as whichever public key had the smallest Credential. So whichever one gave the minimum VRF output.
00:17:20.738 - 00:17:36.778, Speaker A: If you think about it, that's basically going to select one of the public keys in the Staking contract uniformly at random. That's actually what you want to do. If all of the stake amounts are exactly the same here with different stake amounts, we want to sort of bias toward those public keys that have a higher stake.
00:17:36.778 - 00:17:54.290, Speaker A: So in some sense we don't just want to use the raw VRF outputs, the raw credentials, we want to scale or adjust them somehow. So that basically the bigger your stake, the bigger boost you get, which basically means decreasing your Credential. You somehow want to scale down credentials in a way that's responsive to the stake amounts.
00:17:54.290 - 00:18:23.050, Speaker A: So we're going to refer to these modified VRF outputs as qi adjusted credentials. We're then going to be choosing the leader based on who has the smallest adjusted Credential. So obviously then, what we need to do is figure out what is this formula, how do we do this adjustment so that we actually faithfully implement proof of stake random sampling so that the probability that a public key is selected actually has the minimum adjusted Credential that should be proportional to their stake.
00:18:23.050 - 00:18:37.854, Speaker A: Now, if you've been following along really closely, you'll remember the credentials were actually used in two different ways in the protocol we've been talking about. The first is to select leaders. And so, as I said, we're going to be using adjusted credentials, not the old unadjusted credentials.
00:18:37.854 - 00:18:58.034, Speaker A: And secondly, actually, they provided our pseudorandum seeds also those R sub T's that we use as part of the input to our VRF evaluations. Remember R sub T, there's a pseudorandum seed of time T that's defined as the Credential of the proposer of the most recently finalized block. And there, there's no need to switch to adjusted credentials.
00:18:58.034 - 00:19:14.800, Speaker A: So for the purposes of the R sub T's, you can define them in exactly the same way as just the unadjusted Credential of the block proposer of the most recently finalized block. But for leader selection, really important, we use adjusted credentials instead. So let's now talk about exactly what that formula is going to look like.
00:19:14.800 - 00:20:01.082, Speaker A: So high level, we're going to use exactly the same approach that served us so well in achieving sybil resistance thus far. Meaning that if there's an active Validator that has, say, ten staked coins, we're going to treat it as if it had instead chosen to register ten times ten sybils with different public keys with one coin each. So the question then is what does it mean to treat them as if they were ten sybils with one coin each? Or maybe set a little bit more precisely, how do we make sure that this entity is indifferent, doesn't care at all between whether it stakes all ten coins under one public key or one public coin each under ten different public keys? Well, the answer to that question is going to depend on what indifferent means.
00:20:01.082 - 00:20:16.014, Speaker A: Like it's going to depend on what this person cares about. And indeed, we've already seen sort of two different examples, right? We talked about one version of VRF random sampling with general stake amounts where they only care whether or not at least one of their civils gets selected. That's not the version we're using in this video.
00:20:16.014 - 00:20:40.570, Speaker A: We will use it when we talk about longest chain consensus. The second version we talked about, which we are using here, right? We already used this in this video in change number one, when we were modifying the committee selection procedure so that it was civil resistant. So their indifference meant that your voting power in the tendermint protocol was the same whether or not you staked all ten coins in one account or whether you used one coin each in each of ten accounts.
00:20:40.570 - 00:21:08.398, Speaker A: So how did we make sure that its voting power was exactly the same either way? Well, we introduced this notion of sampled stakes. So we sampled with multiplicities and then we did this thought experiment and we said, well, look, if you did this ten sybil thing, there'd be some distribution over how many of your sybils actually get selected. So let's just kind of randomly draw from that distribution a sampled stake and then weight your vote in the tendermint protocol by that sampled stake.
00:21:08.398 - 00:21:25.714, Speaker A: So your power in tendermint is exactly the same whether or not you register once or ten times. That's how we addressed indifference in the context of committee selection. So here, in the context of leader selection, what someone cares about is not voting weights.
00:21:25.714 - 00:21:49.040, Speaker A: What they care about is the likelihood with which they will control the next leader, that the next leader will be one of the public keys that they own. So that's what indifference is going to mean here. The likelihood with which you own the public key of the next leader should be exactly the same whether or not you stake once with ten coins or ten times with one coin each under a different public key.
00:21:49.040 - 00:22:25.318, Speaker A: Now, if you did the latter, if you actually used ten different public keys, one coin each, that would give you ten different VRF outputs, ten different potential credentials you could use. But if you think about it, given that the leader is going to be whoever has the smallest credential, you're going to throw out nine of your sybils and you're only going to bother to report the credential, which is the smallest of the ten available to you. So what really matters in that thought experiment is the distribution of the smallest credential, the distribution of the smallest of those ten effectively independent VRF outputs.
00:22:25.318 - 00:23:03.080, Speaker A: Each of those VRFs can be is indistinguishable from a uniformly random output. So you can think of that distribution as the distribution of the minimum of ten samples from the uniform distribution on one. So what then does it mean to make someone indifferent between actually executing the thought experiment and actually having these ten symbols with one coin each, indifferent between the thought experiment and just staking once and for all with all ten coins onto a single account? Well, I guess its adjusted credential should be such that it has the exact same distribution as the minimum of ten uniform random variables on zero one.
00:23:03.080 - 00:23:24.438, Speaker A: If we define adjusted credentials in this way, then we will, for the usual reasons, get the civil resistance property that we want. We will be sampling with probability proportional to stake. Right? If you have ten coins, it doesn't matter whether you sort of stake them all in one account, whether you split them three, three four over three accounts, or whether you split them one coin each over ten accounts.
00:23:24.438 - 00:23:50.758, Speaker A: At the end of the day, the best of the credentials that you're going to have, best of the adjusted credentials that you're going to have over all of the accounts that you stake in is going to have the same distribution as if we just sampled ten times from the uniform distribution. And you got to keep the smallest of those ten. So that's why you get civil resistance, right? So the probability that you're selected, it doesn't matter how you split your stake among multiple counts, it only depends on the total amount of stake that you have.
00:23:50.758 - 00:24:29.854, Speaker A: Now you might ask why is a public key being sampled proportional to stake? Well, imagine you had say, ten coins, and imagine there's maybe 1000 coins total in the Staking contract, right? So fundamentally, however you split your stake, fundamentally, you're basically getting the minimum of ten independent draws from the uniform one distribution. Meanwhile, everybody else in the Staking contract collectively are getting 990 independent draws from the uniform one distribution, again, independent of how they might split stake across multiple accounts. So total there's 1000 uniform one random variables being drawn, one for each coin.
00:24:29.854 - 00:24:43.990, Speaker A: Each of those coins is equally likely to lead to the smallest draw. So to lead to the smallest adjusted credential, you own ten coins out of 10,000. So there's going to be a 1% chance that one of your ten coins happens to be the minimum among all thousand.
00:24:43.990 - 00:25:06.746, Speaker A: So that's why with 1% of the stake you're selected with 1% probability. So similar solution that we had in change number one, to make committee selection civil resistant, again, intuitively treat someone with ten coins as if there were ten people with one coin each. The details are different because the distribution that the person cares about is different, right? So in change number one with voting power, you really care about this binomial random variable.
00:25:06.746 - 00:25:38.018, Speaker A: So how many of your sibils have a VRF output that's below some threshold, whereas here you care about a different distribution, the minimum of ten different independent samples from the uniform distribution. So finally you should be wondering, okay, how does this idea, which hopefully sounds attractive, but how does this idea actually get turned into a formula? Like what literally is the formula by which you are going to be adjusting people's credentials? There's not a unique answer to that question, but let me give you one slick one. So this solution is going to be based on the exponential distribution.
00:25:38.018 - 00:25:52.810, Speaker A: So let capital F sub lambda of x. Let that denote the CDF, the cumulative distribution function of the exponential distribution with parameter lambda. And if you don't have that memorized, let me remind you that that is one minus e.
00:25:52.810 - 00:26:01.598, Speaker A: Here e is the base of the natural logarithm 2.718 dot dot dot one minus e raised to the minus lambda times x. So for example.
00:26:01.598 - 00:26:17.294, Speaker A: The probability that a random variable from this distribution has value at most two, that probability is going to be one minus E raised to the minus two lambda. The input to this CDF is a non negative real number. Because this distribution is supported on the non negative reals.
00:26:17.294 - 00:26:32.646, Speaker A: And as a cumulative distribution function as a probability, the output is going to be between zero and one and it's increasing, right. So the bigger the threshold, the more likely it is that a sample is going to be below that threshold. So what the graph of the CDF looks like is going to depend on the parameter lambda.
00:26:32.646 - 00:27:01.394, Speaker A: As you jack up lambda, make it bigger and bigger, then this thing is going to approach one very quickly. So it'll be very steep at the beginning and then the asymptote at one, if lambda is smaller, then it's going to grow more slowly, but again it will have the same asymptote at one. So given a Credential, how do we define the adjusted Credential? Well, we're going to feed the Credential through the inverse of this CDF, where the parameter lambda corresponds to your stake amount Q sub i.
00:27:01.394 - 00:27:19.538, Speaker A: That's going to be the slick solution. And it's also easy to give a closed form expression for this quantity, which turns out to be the natural log of quantity one divided by one minus the Credential, viewed as a number in zero one. And then that natural logarithm is scaled by the stake amount Q subi.
00:27:19.538 - 00:27:35.038, Speaker A: So one over Q sub I times the natural log of one over one minus the Credential. Now when we're thinking of the Credential, the VRF output, we're thinking of that as being between zero and one. And that's because if we're taking the inverse of the CDF, the input better be between zero and one as it is here.
00:27:35.038 - 00:27:47.886, Speaker A: Now the output of F inverse, that's going to be a non negative number, right? The exponential distribution is supported on the non negative reals. So F minus one is going to be somewhere between zero and infinity. So that's where the adjusted credentials lie.
00:27:47.886 - 00:28:18.746, Speaker A: If you think about it, that means we're not literally implementing what I said was the idea on the second to last line on this slide, but still it's a transformation of that idea with exactly the same properties. So for those of you who are irritated by the complexity of this formula, right, which isn't that bad, right, one over Q times log, one over one minus X, where X is the Credential. You probably also think it's like, well, why not just do something simpler than this? Forget about this natural log, why not just do like the Credential divided by Q sub i.
00:28:18.746 - 00:28:35.306, Speaker A: So if you have ten times as many coins, then we're just going to sort of scale down your Credential by a factor of ten. Therefore obviously making it more likely that you're going to be the one with a minimum credential. But if you think about it, that's not going to give us the indifference property that we want that's important for civil resistance.
00:28:35.306 - 00:29:04.822, Speaker A: So think for example about someone that just has two coins, right? So one option they have is having two sybils each with one coin each, in which case their best Credential is going to be the minimum of two different draws from the uniform distribution. On the other hand, if they staked twice just both on the same account, and you just gave them a Credential of the sort of VRF output over two, that then would be some uniform draw between zero and one half. So the simplistic formula where you just divide by the stake amount gives you the wrong distribution.
00:29:04.822 - 00:29:15.882, Speaker A: It gives you uniform on the interval one over Q. It's not correctly simulating the minimum of Q different draws of the uniform distribution. So that's why we're doing this somewhat more complicated formula.
00:29:15.882 - 00:29:28.994, Speaker A: So that explains why the simple formula doesn't work. You're probably wondering why the more complicated formula here does work. And fundamentally that's driven by a super nice property that exponentially distributed random variables have.
00:29:28.994 - 00:30:19.150, Speaker A: Namely, if you take the minimum of two exponentially distributed random variables that's just itself an exponentially distributed random variable with rate equal to the sum of the rates of the two that you're taking the minimum of. So two questions you might have is like first of all, why is this property true? Second of all, why is this property useful for us? For the first question, it's probably easiest to think of in terms of a poisson process, right? So if you think about sort of a poisson process that at any instantaneous moment in time fires with some rate lambda, well then the exponential distribution is just the distribution on the time you have to wait until the first firing of a poisson process. Meanwhile, the minimum of two exponential, exponential random variables, that's like waiting until the first moment in time at which one of two different independent poisson processes fires.
00:30:19.150 - 00:30:42.482, Speaker A: So if one of them is firing sort of at each moment with rate lambda one, the other one's firing at each moment with rate lambda two, then the combined Poisson process fires with rate lambda one plus lambda two. So that's why the distribution on how long you have to wait until the minimum of those two firings is going to be exponentially distributed with parameter equal to the sum of lambda one and lambda two. The second question, I'm mostly going to leave those details to the interested viewer.
00:30:42.482 - 00:31:12.098, Speaker A: Let me just at least point out it seems like we're on the right track, right? If you look at the idea on the second to last line on the slide, we somehow wanted to adjust credentials so that they were reflecting the minimum of a bunch of independent random variables. And this orange fact is basically saying exponential random variables are particularly convenient to take the minimum over, you just get another exponential random variable with a higher rate. That's also why it's sort of convenient to do a transformation from sort of zero one to the non negative reels to implement this idea on the second to last line.
00:31:12.098 - 00:31:42.358, Speaker A: So that's one approach to adjusting the credentials so that indeed you are selecting leaders with probability proportional to stake as desired. And that now finishes the modifications I wanted to talk through of the basic protocol from two slides ago, two videos ago, how you extend it to accommodate general staking amounts, general Q sub eyes. And that in turn wraps up everything I wanted to say about coupling proof of stake random sampling, kind of a hard problem in its own right, and a permissioned consensus protocol like tendermint.
00:31:42.358 - 00:31:53.678, Speaker A: Again, sort of a hard problem in its own right. Now we've talked through a number of the big challenges that come up at the edges when you try to paste those two things together. And as you see, it's not that easy.
00:31:53.678 - 00:32:10.118, Speaker A: So what we're going to do next, the other half of part three is going to be about stitching together proof of stake random sampling with longest chain consensus. And again, actually the earlier proof of stake blockchain protocols, this was largely the route that they went. The emphasis on BFT is more from the last maybe four years or so.
00:32:10.118 - 00:32:25.620, Speaker A: And unfortunately, when we try to pair proof of stake civil resistance with longest chain consensus, we kind of inherit all of the problems that we talked about here for BFT protocols, plus some additional ones which I want to tell you about. So that's what's coming up starting in the next video. I'll see you there.
