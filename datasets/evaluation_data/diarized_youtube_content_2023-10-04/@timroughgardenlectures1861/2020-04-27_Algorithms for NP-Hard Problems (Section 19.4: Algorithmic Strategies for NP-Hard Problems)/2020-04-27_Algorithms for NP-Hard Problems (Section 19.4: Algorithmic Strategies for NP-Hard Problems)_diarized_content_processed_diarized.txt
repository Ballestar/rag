00:00:00.410 - 00:00:35.778, Speaker A: Hi everyone and welcome to this video accompanying section 19.4 of the book Algorithms Illuminated, part Four section about algorithmic strategies for NP hard problems. So suppose you've been working on a project and a few weeks ago you identified the computational problem that's really intrinsic to the success of this project. Because of its importance, you've spent the last few weeks throwing the kitchen sink at it. But not thing seems to work. You've tried all the algorithm design paradigms you know, you've tried to speed things up with data structures. You've tried to hit it with four free primitives but still no efficient algorithm.
00:00:35.778 - 00:01:18.866, Speaker A: Finally you realize, or maybe someone tells you that the problem is actually NP hard and therefore at least assuming the P not equal to NP conjecture, there is no guaranteed correct and guaranteed fast algorithm for the problem. But that doesn't make the problem go away. It explains why all your efforts have come to not. But it doesn't change the fact that this is the problem which really governs the success of your project. What should you do? So the bad news is that NP hard problems are pretty ubiquitous and it would not be surprising at all if you encountered one in your own work. The good news is that NP hardness isn't a death sentence. Doesn't literally mean it's completely hopeless to solve an NP hard problem in practice.
00:01:18.866 - 00:02:21.546, Speaker A: And indeed often not always, but often, MP hard problems can be solved in practice, at least approximately, and at least if you use invest sufficient resources and algorithmic. Sophistication NP hardness does throw the gauntlet down to the algorithm designer, however, and it tells you where to set your expectations. You should not be expecting some super fast and always correct algorithm akin to the ones that spoiled us for problems like sorting shortest paths, sequence alignment and so on. Unless you're lucky enough to deal only with very small instances or very well structured instances, you're probably going to have to work pretty hard to solve the problem and maybe even be ready to make some compromises. So what kinds of compromises? NP hardness rules out algorithms that share three desirable properties, or at least assuming the P not equal to NP conjecture is true, they rule out having all three of these simultaneously. So the first desirable property is you'd like an algorithm, which is general purpose, meaning it doesn't matter. You make no assumptions about the input, no matter what the input is, you want the algorithm to solve the problem.
00:02:21.546 - 00:02:58.826, Speaker A: This is as opposed to solving only a special case of a problem, handling only a subset of the instances that you might encounter. So obviously, general purpose would be nice. Second, we would of course like the algorithm to correctly solve the problem, ideally for every single input. Similarly, we would like the algorithm to be fast, ideally something like linear time, but at the very least polynomial time. Again, without any extra assumptions about the inputs no matter what the inputs are. And accordingly, you can choose from three types of compromises. You can compromise on generality, give up on being general purpose, and handle only a special subset of all of the possible instances.
00:02:58.826 - 00:03:37.326, Speaker A: You can compromise on correctness and actually not solve the problem correctly, at least on some of the inputs. Or you can compromise on speed and run in super polynomial time, at least on some of the possible inputs. For the rest of this video, we're going to elaborate on all three of these algorithmic strategies. All of them are quite common and useful in practice, and then later the next two batches of videos in the playlist corresponding to chapters 20 and 21. They're deep dives on the latter two types of compromises. So, a deep dive on how to compromise on correctness, and a deep dive on how to compromise on worst case polynomial running time. As always, our focus is going to be on powerful and flexible algorithm design principles.
00:03:37.326 - 00:04:28.402, Speaker A: They're applicable to a wide range of problems. As always, you should take these principles as a starting point and run with them, augmenting them with whatever domain expertise you have for the specific problem that you need to solve. So the first step of compromise is to compromise on generality. So to give up trying to solve the NP hard problem on all inputs and instead restrict your attention to a subset of all possible inputs, in the best case scenario, for the subset that you restrict your attention to, it will actually become polynomial time solvable. You'll be able to come up with an always fast and always correct algorithm for that special subclass of inputs. Now, if you've been following along with this book series or with these video playlists, you've actually already seen a couple of examples of exact and fast algorithms for what, if we zoom out, are really special cases of NP hard problems. So let me remind you of two of the problems you might have seen in the past.
00:04:28.402 - 00:04:53.942, Speaker A: If you haven't heard of these two problems before, don't worry about it. Feel free to skip on to the next slide without loss of continuity. So the first example is the weighted independence set problem. So this is a graph problem. In the input you're given an undirected graph, and also each vertex comes with a non negative weight. So for example, one input could be the five cycle, as you see on this slide. And I've labeled the vertices with their weights.
00:04:53.942 - 00:05:50.490, Speaker A: So the vertex weights are 12345 as you go around the cycle. Now, what's an independent set? An independent set is a subset of mutually nonadjacent vertices, so vertices that do not have edges between them. So, for example, if you want to think about the vertices as representing people or maybe tasks, and you want to think of the edges as conflicts, like two people who don't get along, or two tasks that can't be done at the same time, then the independent sets the mutually nonadjacent vertices. Those correspond to conflict free subsets of tasks or of people. So for example, in the five cycle, there's no independent set that has three different vertices. If you had three vertices, two would be neighbors, and that's not allowed, but there's a bunch of different pairs of vertices you could pick and that would be an independent set. And if you wanted to maximize the total weights of the independent set, which is the objective in this problem, in this example, you would choose the vertices of weights five and three.
00:05:50.490 - 00:06:31.586, Speaker A: You can't pick the five and the four because those are adjacent. So the weighted independent set problem turns out to be NP hard in general. So just like the Tsp equally so the weighted independent set problem is NP hard. We will actually see a proof of that later on in this video playlist. Now, for those of you who are graduates of the Dynamic Programming Boot Camp in part three of the book series, you might recall that the way I introduced you to dynamic programming was actually with this exact same weighted independent set problem. And in fact, we use dynamic programming to give a linear time algorithm for computing the maximum weight independent set. So what's the catch? The catch, if you remember, is that we only talked about path graphs.
00:06:31.586 - 00:07:03.406, Speaker A: So you just had a bunch of vertices on a line with an edge between each adjacent pair of vertices and that was it. So it was a super simple class of graphs. Even that class was a little bit tricky. We needed dynamic programming, but that's all we talked about. We didn't talk about five cycles, we didn't talk about more complicated graphs. So in the special case of path graphs, this Nphard problem flips and becomes polynomial time solvable, even linear time solvable. In fact, that dynamic programming algorithm for path graphs can be extended to tree graphs.
00:07:03.406 - 00:07:26.538, Speaker A: That's one of the end of the problem exercises in chapter 16. Again, in linear time. So in general, graphs, weighted independence, that is NP hard. We don't believe it's polynomial time solvable. But if you can get away with just thinking about tree graphs, boom. Linear time solvable. So a second example of a polynomial time solvable special case of an NP hard problem that you might have seen in the past concerns the Napsack problem.
00:07:26.538 - 00:08:17.818, Speaker A: So as a reminder, in the Napsack problem, you're given n items, each item has an integer value and an integer size, and you're also given an integer knapsack capacity. And the goal basically is to stuff the knapsack in the most valuable way possible. So you're looking for a subset of the n items whose total size is at most the Napsack capacity. So a subset that fits in the Napsack. And subject to that constraint, you'd like to maximize the value of the items that you choose. So Napsack problems show up all the time in real life whenever you have basically a single scarce resource that you want to spend in the smartest way possible, that's going to be a Napsack problem. So, for example, if your boss gave you an operating budget to hire people, and you have a bunch of candidates that differ in their productivity levels and in their requested salaries, figuring out how to hire the most productive group of people subject to your budget, that's exactly a knapsack problem.
00:08:17.818 - 00:09:27.342, Speaker A: So the Napsack problem is another canonical and killer application of the dynamic programming algorithm design paradigm. And so in particular, if you read part three or saw the corresponding videos, I showed you a dynamic programming algorithm which runs in big O of n times capital c time, where n here denotes the number of items and capital c denotes the Napsack capacity. On the other hand, as we will ourselves prove later in this video playlist, the Napsack problem in general is an NP hard problem. Now, you might be puzzled when you look at these two statements that the Napsack problem is NP hard and that on the other hand, we gave an O of n times c time algorithm for it using dynamic programming. So why doesn't this O of n c time algorithm for the Napsack problem refute the P not equal to NP conjecture? That would be a pretty big deal. The reason is that running in time O of n times capital c is not a polynomial time algorithm. I would agree with you that it's a polynomial time algorithm if in the special case where the Napsack capacity capital c is bounded by a polynomial function of n.
00:09:27.342 - 00:10:06.094, Speaker A: So for example, if capital c was n to the fifth, we'd have a running time of n to the 6th. And yes, I would definitely agree that's a polynomial running time in general, however, there's no reason to think that the Napsack capacity is going to be merely polynomial in the number of items n. It could be two to the n, for example. And if the Napsack capacity is two to the n, then the running time of this dynamic programming algorithm is also going to scale. With two to the n, it'll be exponential in n. On the other hand, the input size is not exponential in n. And the reason is, remember, what does input size mean? It means the number of keystrokes that you need to specify that input to a computer.
00:10:06.094 - 00:10:44.854, Speaker A: And to specify a number to a computer, the number of keystrokes is not proportional to the magnitude of the number, it's proportional to the number of digits in that number. So the logarithm of the magnitude. So for example, if you want to describe the number 1 million to a computer, you don't need 1 million keystrokes, all you need is seven. Or if you're working base two, you'd need 20. You need much, much less than the magnitude of the number. So in our example, where we have n items and all of the numbers are of magnitude roughly two to the n, we would be using n digits for each of the roughly n numbers. So that would give us n squared digits overall.
00:10:44.854 - 00:11:34.118, Speaker A: So the input size would only be polynomial in n quadratic in n, while the running time of this dynamic programming algorithm would be exponential in n. So that shows why running in time O of n times capital C, that is not a polynomial time algorithm in general. It is a polynomial time algorithm when capital C is not too big, when it's bounded by a polynomial function of n, but it is not polynomial time in general. And that is why, even though Napsack is NP hard, and even though we have this algorithm, this does not refute the P not equal to NP conjecture. So those are two examples you may already be familiar with of special cases of NP hard problems that can be solved in polynomial time. There's many more examples, and we'll actually see a couple other examples as we go through the video playlist. For example, to graph coloring and satisfiability, we'll see polynomial time solvable special cases.
00:11:34.118 - 00:12:23.834, Speaker A: We're not going to have any dedicated portion of the book or of the playlist to compromising on generality. And that's because work in that direction looks exactly the same as all the work we've been doing in parts one through three. The whole point of parts one through three was to develop a toolbox to design algorithms that are always correct and always fast. And that toolbox in particular can be applied to special cases of NP hard problems when they are, in fact polynomial time solvable. So this is a good point to make a sort of general comment and give a bit of a bit of a pep talk, which is that if you have to tackle an NP heart problem kind of in real life, really it's important that you're persistent and that you don't give up. Of course, you already know that as algorithm designers, but it's sort of more important than ever for making progress on NP hard problems. Often you have to throw the kitchen sink at the problems to really get the kind of progress you want.
00:12:23.834 - 00:12:49.086, Speaker A: So let me just give you like a random example of how you could combine different tools in the toolbox. Imagine your boss has given you a pretty big graph. Let's say it's like 10,000 vertices or something, and they want to know the maximum weight independent set. So they want a lot of value without any conflicts. That's exactly the maximum weight independent set problem. Now, you can't use exhaustive search because the graph has 10,000 vertices. If it was a tree, you could solve the problem in linear time using dynamic programming.
00:12:49.086 - 00:13:28.910, Speaker A: But let's suppose it's not a tree. Let's suppose the graph has a bunch of cycles, so then maybe it seems like you're stuck. But then imagine you use your sort of domain expertise and you realize actually there's kind of like of these 10,000 vertices, there's 20 vertices that are really kind of the most important ones. And imagine that these 20 vertices actually intersect every cycle of the graph. In other words, imagine that when you remove these 20 vertices from the graph, the graph that remains is acyclic. It's just a collection of trees. Well, if that's the case, if there are these 20 vertices whose removal makes the graph a cyclic, all of a sudden, you really could solve this problem.
00:13:28.910 - 00:14:15.422, Speaker A: Exactly. You can compute a maximum weight independent set by using a hybrid of exhaustive search and dynamic programming. So the way it would work is you would do exhaustive search over all of the subsets of the special vertices. So there's 20 vertices and you're just going to guess or enumerate over which of those 20 vertices belong to the independent set and which ones do not. Then you take those vertices out and you're left with an acyclic graph. And now you can apply dynamic programming and solve the residual problem in linear time. So how long would this take? Well, if you have 20 special vertices, two to the 20 things to enumerate, that's like a million different subsets to look at, which is not actually that bad, then you're left with an Acyclic graph with less than 10,000 vertices on which you can compute an independent set in linear time.
00:14:15.422 - 00:15:05.130, Speaker A: So if you put all that together, you'd require maybe tens of billions of operations overall to compute the Maxwell independent set. Maybe hundreds of billions depending on the implementation. And that may sound like a lot, but that's really something that your modern laptop could do in not that much time in a sort of acceptable amount of time. Meanwhile, if you just tried to do just direct exhaustive search without any of the cleverness, you would be over a trillion operations already when N exceeds 40. But again, that's sort of the takeaway. Point is that even if your application doesn't literally boil down to one of these computationally tractable special cases of an NP hard problem, you might still be able to use a solution to the special case as a building block in a more sophisticated algorithm. So the second algorithmic strategy for NP hard problems is to compromise on correctness.
00:15:05.130 - 00:15:49.270, Speaker A: And this is an especially popular choice in time critical applications. You really need your algorithm to be fast and you're willing to give up a little bit on correctness in order to make it happen. Algorithms of this type which are not guaranteed to be correct, they're often called heuristic algorithms. So we have not really seen many examples in this book series of any kind of algorithmic solution that was not guaranteed to be correct. In fact, the only thing I can think of is our discussion of Bloom filters back at the end of our data structures discussion at the end of part two. Remember, a Bloom filter is sort of a cousin of a hash table which uses less space, but an exchange has a small rate of false positives. So that was an example of a data structure that didn't always give correct answers.
00:15:49.270 - 00:16:42.986, Speaker A: So I think will be the first time we talk about algorithms that don't always give the correct answer. So when you're designing heuristic algorithms, you're intentionally giving up on correctness, but of course you'd like to give up on correctness as little as possible. You'd like a heuristic algorithm, which is still approximately correct in some sense of the word. So maybe it's correct on most of the inputs that you're ever going to encounter. Or maybe even you have some kind of provable guarantee that for all inputs, the algorithm is guaranteed to be at least approximately correct. So what do I mean by almost correct on every input? Well, that's probably easiest to interpret for optimization problems, where the goal is to compute a feasible solution, like, say, a traveling salesman tour with the best objective function value, like, say, you want to minimize the total cost of the tour. Almost correct then means that the algorithm outputs a feasible solution with objective function value close to the best possible, like a traveling salesman tour with total cost not much more than that of an optimal tour.
00:16:42.986 - 00:17:35.774, Speaker A: Your existing algorithmic toolbox for designing fast exact algorithms is directly useful for designing fast heuristic algorithms. So, for example, later in the video playlist, we'll be looking at greedy heuristics for problems ranging from scheduling to team hiring, problems to influence maximization in social networks. All of these heuristic algorithms that we'll discuss come with proofs of approximate correctness guaranteeing that for every input, the output of the heuristic algorithm is within a modest constant factor of the best possible objective function value. I should say that some authors call algorithms of this type algorithms that guarantee a constant factor within the optimal objective function value. Sometimes those are called approximation algorithms, and those authors reserve the term heuristic algorithms. For algorithms that do not have such provable guarantees, we won't be making that distinction. For us, a heuristic algorithm will be something which is not always correct.
00:17:35.774 - 00:18:19.334, Speaker A: It may have a provable guarantee of approximate correctness, or it may not. So those examples are really revisiting a tried and true part of our algorithmic toolbox greedy algorithms and repurposing them, not for exact algorithms, but for fast heuristic algorithms. The other thing I want to tell you about is a technique that we haven't discussed previously in the book series, which is particularly well suited for lots of different NP hard problems. Even though it often doesn't have provable guarantees, it often is unreasonably effective in making progress on MP hard problems in practice. And that technique is local search. So the third and final strategy for MP hard problems that we'll talk about is compromising on speed. So here we're going to be looking at exact algorithms.
00:18:19.334 - 00:19:04.426, Speaker A: So this is suitable for applications where you really cannot compromise on correctness subject to being correct, you'd like to have an algorithm which is fast as possible. Now, with an NB hard problem, again, assuming the p not equal to NB conjecture, you're not expecting polynomial time, you're not even expecting sub exponential time. In the worst case, you really got to be ready for an exponential time, worst case, running time. But the hope is that you still do quite a bit better than exhaustive search a lot of the time. So that could mean a couple of different things. So one thing it could mean is that the algorithm typically runs quickly like say in polynomial time, or maybe even a low polynomial time, at least for the inputs that tend to show up in your own application. So maybe not always, but most of the time you're seeing very fast running times.
00:19:04.426 - 00:20:06.506, Speaker A: You might also hope for a provable guarantee which says that for every single input of the problem you are guaranteed to run faster than exhaustive search. Now, in the second of these two cases where we're beating exhaustive search for every single input, we should still expect the algorithm to run an exponential time in the worst case. After all, the problem is NP hard. But we'll see a couple of examples where while still being an exponential time, algorithms that do significantly better than exhaustive search guaranteed. The first example will be to the traveling salesman problem where as we've seen, exhaustive search takes time scaling with n factorial. And we'll use dynamic programming to come up with an algorithm that instead scales in the smaller running time two to the n still exponential, but better than n factorial two to the n times a polynomial function of n. We'll also look at a dynamic programming algorithm, which, coupled with randomization, gives a very beautiful algorithm for finding long paths in graphs, which has been applied to finding signaling pathways in protein protein interaction networks.
00:20:06.506 - 00:21:07.566, Speaker A: Specifically, if we're looking for a path of length k in a graph, meaning a path that spans k vertices with k minus one edges, naive exhaustive search would scale as n to the k, where n is the number of vertices and k is the path length we're after. But our combination of randomization and dynamic programming will bring the running time down to e raised to the k times linear in the graph size, where here e is indeed 2.7 118. So those are two super cool examples where we again revisit one of the tools that was already in our toolbox, dynamic programming. We worked very hard to master that skill. And so here we're seeing a couple more really nice applications, applying it actually to NP hard problems and speeding up even in the worst case over exhaustive search. So making progress on relatively large instances of NP hard problems, say of problem sizes in the thousands or more that typically requires additional tools that do not have better than exhaustive search running time guarantees but are nonetheless unreasonably effective in practice.
00:21:07.566 - 00:22:04.466, Speaker A: And so there's two of those tools I want to tell you a little bit about later on in the video playlist, namely solvers for mixed integer programming problems or so called MIP solvers and also solvers for Satisfiability problems or Sat solvers. Here a solver is just kind of a ready made implementation that's been expertly implemented and sort of finely tuned to work really well in practice. So it turns out that many NP hard optimization problems, the traveling salesman problem, and many others can be encoded as MIPS as mixed integer programming problems. Meanwhile, a lot of yes no questions like checking whether a bunch of requests for scarce resources can all be fulfilled or not. A lot of those yes no questions naturally translate to Satisfiability problems. And whenever you're faced with an NP hard problem that can be easily specified as a MIP or Sat problem, it's really worth trying to apply the latest and greatest solvers to it. There's no guarantee that a MIP or Sat solver will solve your particular instance in a reasonable amount of time.
00:22:04.466 - 00:22:47.230, Speaker A: The problem is NP hard, after all, but they constitute cutting edge technology for tackling NP hard problems in practice. So two videos ago, I talked about the various levels of expertise you might want to have in the mastery of NP hardness level one, sort of cocktail party awareness so that know if you're a program manager, you know what one of your engineers means. If they tell you that they're working on an NP hard problem, we're pretty much now up to level one. We have a couple more videos to go in this chapter, but you're pretty much up to level one. So just to kind of consolidate, let me tell you what are the three biggest takeaways from what we've discussed so far. So things you just really have to know about NP hard problems. So first of all, it's important to know that NP hard problems are everywhere.
00:22:47.230 - 00:23:47.666, Speaker A: Despite the fact that most algorithms textbooks talk mostly about polynomial times solvable problems in real life, you are very likely to encounter NP hard problems. So no one did anything wrong. If an NP hard problem all of a sudden shows up in some important project, it's just inevitable. So the second thing to know is sort of at a high level, what does it mean that these NP hard problems are hard? What it means that under a technically mathematically open but widely believed mathematical conjecture, the P not equal to NP conjecture. If that conjecture is true, then no NP hard problem has any algorithm which is always guaranteed to be correct and always guaranteed to run in polynomial time, which is a big contrast to the problems that we've talked about in the previous books in this series. So if there's a silver lining to all this, it's that empty hardness generally isn't a death sentence. And people do often not always, but often have successes making progress on NP hard problems in practice, provided they're willing to apply some serious algorithmic.
00:23:47.666 - 00:24:36.902, Speaker A: Sophistication and provided they invest enough resources into the project human resources, computational resources, and financial resources. So if you have an NPR problem that you really care about, you're going to want someone on your team who's well versed with the tools that we're going to be describing in the rest of this video playlist. You're going to want to give that person the time and money that they need to apply that toolbox. After all, it's not called an NP hard problem for nothing. So we'll get back to algorithmic strategies for tackling NP hard problems soon enough. But in the next video, I want to talk a little bit about this question. How can you recognize MP hard problems in your own work so that you don't waste time trying to design a perfect algorithm for them? Turns out there's a very simple two step recipe for recognizing that problems are NP hard, and I want to give you glimpse of that in the next video.
00:24:36.902 - 00:24:38.690, Speaker A: So see you then. Bye.
