00:00:00.410 - 00:00:56.234, Speaker A: So the last video we gave a high level description of what a proof of stake BFT type blockchain protocol might look like. As we talked through that description, we noticed along the way various points that were under specified, various points where it was clear there were some missing details. And so in this video, what I want to do is take three of the issues that come up with the approach we discussed in the last video and talk about how you address them it. So remember the basic idea which is to basically reduce permissionless consensus to permissioned consensus through the invocation of a civil resistant random sampling procedure. In this case based on proof of stake random sampling. So for concreteness, we're just going to think about using the tendermint protocol, the permission to protocol that we talked about at length back in lecture seven. Now to run tendermint, you need to know what are the nodes running the protocol and also which of the nodes acts as the leader or the block proposer of each round.
00:00:56.234 - 00:01:40.086, Speaker A: And so that's where the proof of stick random sampling comes in. We use the random sampling procedure both to pick a committee so a subset of the public keys registered in the Staking contract and then one of those public keys in particular to act as the leader. And in particular we're exploring using the VRF. So verifiable random function based random sampling. So the protocol is going to proceed in rounds or in time steps. At each time step, each of the owners of a public key in the Staking contract is supposed to evaluate a verifiable random function using the private key corresponding to the public key registered in the Staking contract. Everybody's going to evaluate their VRFs on the same input, but they have different private keys so they can get different outputs.
00:01:40.086 - 00:02:14.760, Speaker A: And those are going to be called their credentials for a given round. Now, things simplify somewhat if you make the super unreasonable assumption that all of the public keys are staking the exact same amount. So let's say the minimum possible amount, let's call that one. Now that's an assumption we certainly want to get rid of. We want to be able to accommodate sort of arbitrary stake amounts. But as we'll see in this video, a surprising number of issues already come up. Even in the special case where all of the active validators have exactly the same stake amount, all three of the issues we talk about in this video will be relevant even in that special case.
00:02:14.760 - 00:03:25.200, Speaker A: So under this standing assumption, which again we'll retain for this video, but we'll get rid of in the next video, it's pretty natural to translate the VRF outputs into roles for the various public keys in the Staking contract. So the first question would be like, who gets to vote in tendermint? And your first thought might be like, well why not just let everybody vote? So like if you have a public key in the Staking contract, then you're allowed to broadcast a vote signed with the corresponding private key. And that would be fine if there weren't too many active validators. Like if there's only N, the number of public keys in the contract was sort of 100 or less, that would be fine. But because protocols like tendermint don't scale infinitely big, if you've got more like 10,000 active validators in your Staking contract, you want to pick out a subset, a committee, and it's only going to be the committee members, maybe of size 100, say, that are going to be actually voting in tendermint. So the obvious way to take the VRF outputs and define a committee is just like any public key whose VRF output is sufficiently small that's going to be one of the members of the committee. Sufficiently small here means less than some difficulty threshold tau sub C, which is tuned to target a particular committee size, like maybe C equals 100.
00:03:25.200 - 00:04:20.986, Speaker A: So the committee members are just those public keys with small VRF outputs. And so why not just have the block proposer the leader of a round be the committee member that has the smallest credential of them all? So at that point, once you've assigned public keys to the roles of voters and you've assigned one public key to the role of the black proposer, it makes sense to start talking about running the tendermint protocol. Now, when I say run tendermint, you should be asking, okay, but for how long? Right? This one committee presumably is not going to be running the tendermint protocol in perpetuity. Ultimately, we're building a permissionless blockchain protocol where the set of active validators, the public keys in the Staking contract are going to be changing over time. So presumably the committee also has to change periodically to reflect the changes in the set of active validators. So the committee is going to run tendermint for some finite period of time. For concreteness, we're saying, well, maybe they just run it for like one round of tendermint.
00:04:20.986 - 00:05:17.858, Speaker A: And the four stages that if you will call, happen in a single round of the tendermint protocol. In any case, once this committee's job is done, once that finite period comes to an end, at that point you're going to do sort of a new batch of VRF random samplings. You're going to reevaluate everybody's VRFs outputs at that point, and that will give you a new committee that will then run tendermint again for some next block of time. So, as we discussed at the end of the last video, it can't possibly be this simple. Because if you go back to the analysis of the tendermint protocol and you look at sort of fundamentally, why does it satisfy consistency and liveness even the partially synchronous setting? A big part of the reason is that the nodes running the protocol retain enough memory from round to round to avoid getting confused. For example, to avoid finalizing two different blocks at the same block height. So to make this idea to work here where you sort of change the committee, you need to somehow ensure there's enough kind of shared information between the previous committee and the next one.
00:05:17.858 - 00:05:50.318, Speaker A: So I'll leave it for you to think that through to some extent in the synchronous model, if you're happy just having consistency and liveness in the synchronous model, it's not too complicated. If you also want to have it in the partially synchronous model, that becomes a hard problem. And I'm not going to talk further about how you would do that. That's going to be just way too far out into the weeds for us. So at a high level, this is the kind of protocol we want to be thinking about, right? At each time step you do VRF based sampling that tells you who the committee members are. They're the ones with sufficiently small VRF outputs. It tells you who the block proposer is, that's the person with the smallest VRF output it.
00:05:50.318 - 00:06:57.410, Speaker A: And then the nodes that control those selected public keys, they're tasked with running one round of tendermint to try to finalize one new block. So let's now talk through three issues that come up when you're trying to, like we're doing here, stitch together VRF based random sampling with a permission to consensus protocol like tendermint. So one dissonance that comes up between the high level protocol description I showed you on the last slide and tendermint, as we were thinking about it back in lecture seven, concerns the identity of the leader. So in tendermint previously, we were thinking of the identity of the leader being common knowledge. Like if it's round 117, everybody just knows that the leader is node number 17, the 17th public key in the list that everybody started the protocol with. And that means that if nodes happen to hear about multiple block proposals from different nodes in the same round, they know which ones to discard and which ones to pay attention to, right? A block proposal by node number 17 they should take seriously. A block proposal by node number 18 should be discarded because everybody knows they're not the leader of this round.
00:06:57.410 - 00:08:05.094, Speaker A: So assuming, as usual, unforgeable signatures, that means node number 17 and only node number 17 is going to be able to make a legitimate block proposal in round 117. On the other hand, with this sort of VRF leader selection that we're proposing here, not only is the leader not common knowledge, right? So in round 117, you don't know who the leader is because it depends on everybody's VRF outputs, which you don't know, but even if you are the leader, you don't know that you're the leader, right? You might know you have a really low VRF output, but you don't know whether some other public key gives you a still smaller VRF output. So that means we're going to have to modify the tenement protocol a little bit and ask participants to make block proposals even when they're not sure that they're actually the leader of the round. In the most extreme form, we could just ask all committee members to make a block proposal. With that block proposal, they should of course attach their credentials. This version is probably overkill, right? So if you have someone who's a committee member but just barely, like their VRF output is just barely below the threshold tau subsea, very, very unlikely they're going to be the leader. If committee sizes typically have size 100, for example.
00:08:05.094 - 00:09:29.298, Speaker A: So a natural optimization is just say, well, look, if your VRF output is not just below the difficulty threshold but by a significant amount, like maybe it's sort of less than a 10th of the difficulty threshold, then you should make a block proposal. But in any case, even with the optimization generally, there's going to be sort of block proposals by multiple different public keys in the same round, which means committee members have to somehow decide which ones to pay attention to. Given that the intent is that the leader of the round should be the one with the smallest VRF output, the obvious thing to do as a committee member is just listen to all of the block proposals and then remember the one like believe in the one that has the smallest credential attached to it and discard all of the block proposals that have larger credentials attached to it. So you might hear block proposals from seven different public keys, but you're going to pretend like the only one you ever heard was from the one of the seven with the smallest attached credential. And the intuition here is that participants are sort of figuring out the leader of the round after the fact. So while there's strong intuition behind this idea, it shouldn't be obvious that this actually gets you out of the woods, that this actually repairs tendermint enough to use in our protocol from the previous slide. For example, when you have network delays, it's not guaranteed that a participant will actually know who the real leader is, right? So maybe all of the messages that came from the node that actually had the smallest VRF output, maybe none of those messages got delivered to you on time.
00:09:29.298 - 00:10:14.610, Speaker A: So you're mistakenly thinking that the person with the second smallest VRF is actually the leader of the round. So that would be a legitimate concern. But it turns out the tenderman protocol with this modification does retain its consistency and liveness properties. And actually if you really want to get into this, a great thing to do is go back to lecture seven, look at the analysis and convince yourself that that is in fact the case. So let me talk through some of the intuition for why that's true. And for this specific point, I want you to think not so much about the permissionless protocol that we sketched on the previous slide, I really want you to sort of rewind back to the permission to tenderman protocol we talked about in lecture seven. And so we're talking here about making exactly three changes to that protocol from lecture seven.
00:10:14.610 - 00:11:01.038, Speaker A: So change number one is that every round, each node gets a credential. So perfectly random bits that fall from the sky and a node can't manipulate it and they basically have to report those random bits honestly. That's basically what VRF outputs give you in our permissionless protocol. So nodes every round just magically get these random credentials. Change number two is that there's no leader per se, but rather every node running the protocol in every round should make a block proposal, attaching its credentials that it got for that round. Third change is that every honest node should remember only the block proposal in a round that comes with the smallest credential. So again, if it gets seven different block proposals, it throws out six of them and it just remembers the one that had the smallest credential attached to it.
00:11:01.038 - 00:12:04.206, Speaker A: So that's a well defined permissioned protocol. And we can ask whether or not this variant of tendermint continues to be consistent and live in the partially synchronous setting and it does. So why is that the case, at least intuitively? Well, let's start with consistency. So we want to argue that it can't be the case that two different blocks get finalized at the same block height. Well, what could go wrong? I guess what could go wrong is there's like all these different block proposals floating around and nodes might get confused by multiple proposals from multiple different nodes. But the intuition here is that, look, whatever could go wrong with multiple block proposals by multiple nodes that could basically be simulated in the original tendermint protocol when you have a Byzantine leader, right? The classic move of a Byzantine leader in a permission protocol is to make is to send conflicting messages, like conflicting blocks, to different honest nodes to try to mess them up. So by virtue of tenderament being consistent, despite the fact that there's going to be Byzantine leaders periodically, one would hope that it would be consistent also here when nodes have to are sort of attempting to break ties lexicographically among multiple proposals.
00:12:04.206 - 00:13:03.782, Speaker A: That's not a proof, but that intuition is quite accurate. And if you go back to the proof of consistency from lecture seven, you'll see, you really only need to make very minimal modifications for it to apply. Also here now for liveness, what we've got going for us is if you look at what we argued back in lecture seven, we actually didn't bother to argue anything unless, first of all, we were past global stabilization time, so no more unbounded message delays, all messages being delivered within delta time units at most. And furthermore, we didn't argue anything unless we happened to have two rounds in a row with honest leaders and as long as less than a third of the rounds have Byzantine leaders, then you will infinitely often have consecutive rounds, both of which would have honest leaders. And in that case, we will also be good to go here. Basically, in the first of the two rounds that has an honest leader, a bunch of different things could happen. But the main thing is that the honest notes conclude that first round kind of with a clean slate, nobody is still holding on to some block from the past.
00:13:03.782 - 00:13:43.294, Speaker A: Everybody's ready in that second round to vote on any new block that they hear about. In the second round with an honest leader, then everybody broadcasts block proposals. Everybody hears about those proposals on time. So everybody hears about the honest leader's block proposal because it's the leader, it has the smallest credential, so everybody's going to accept only its block proposal. And then the two stages of voting, all the honest nodes will again just sort of agree on that block and they'll finalize it in the fourth stage of that round of tendermint. So that's how this first issue gets addressed. And notice this actually ties nicely back into a discussion we had in those part Two videos about VRF based sampling.
00:13:43.294 - 00:14:21.694, Speaker A: We talked about issues with the VRF approach and one of the issues we mentioned was that you in effect have a variable number of winners. So if you're thinking about sort of electing exactly the public keys whose VRF output is below some threshold, you can sort of tune the threshold so that you get a given number in expectation. Like you get one winner in expectation. But because different people's VRF outputs are effectively independent random trials, there's going to be variability. Sometimes you're going to have more than your expectation, sometimes you're going to have less. This and so back in those VRF videos, we said there's kind of like three approaches, there's three ways to kind of deal with this problem. One is to kind of give up and kind of say like, well, maybe we don't care about the benefits of verifiable random functions that much.
00:14:21.694 - 00:14:59.466, Speaker A: Maybe we don't actually care about secrecy that much. Let's sort of get rid of it so we actually can guarantee exactly one winner of our sampling procedure. A second approach would be to try to deploy a solution to a problem called single secret leader election Ssle. That's actually a quite experimental problem at the moment. We may see deployments soon, but at the moment that would be kind of even past the cutting edge. And then the third option was to use VRF based sampling but to modify whatever consensus protocol you're working with to accommodate its quirk of having multiple possible winners. And that third approach is exactly the one that we're illustrating right here.
00:14:59.466 - 00:15:38.930, Speaker A: In effect, you can think about it that like everyone who's making a block proposal corresponds to a winner of a lottery. And basically we modified the consensus protocol just to kind of break ties among block proposals. Lexographically and happily that actually works. So this first issue, right, that's kind of pretty fundamental to VRF based random sampling a variable number of winners. You have that issue even if you have access to an ideal randomness beacon. So even if the inputs to everybody's VRFs are sort of random bits that just fall from the sky, you're still going to have this variable number of winners problem. And in fact, if you remember our part two videos about Verifiable random functions, we're actually at that time assuming access to an ideal randomness beacon.
00:15:38.930 - 00:16:27.750, Speaker A: Now here the protocol that we sketched in the previous slide. That's a concrete implementation, right? So that's not going to have an ideal randomness beacon. It has to somehow approximate it or simulate it. So the inputs to the VRFs are not going to be perfectly random bits that fall from the sky, but rather they're going to be pseudorandom seeds derived from the blockchain state at that time and back. In part two, after we discussed the challenges of using verifiable random functions, then we discussed the challenges of pseudorandom beacons. In particular, if the random seed is derived from the blockchain state in some way, well, guess who's responsible for maintaining and adding to the blockchain state? That's the nodes running the protocol. So it opens the door of nodes manipulating the blockchain state in order to manipulate the pseudorandomness for their future gains.
00:16:27.750 - 00:17:19.926, Speaker A: Now, depending on the details of how you derive the pseudorandom seed r SubT from the blockchain state, it might be more or less manipulable. So like a really bad idea would be to have R sub T depend on say, the transactions included in the previous finalized block. That would give the previous block proposer way too much power, right? There's a zillion different blocks they could put together that gives them a zillion different options to choose between for what the pseudoraminance is going to be on the next time step. So they're probably going to be able to choose something which is beneficial to it and harmful to others. The good news is that here we're doing something less stupid. We're using something that itself is sort of a VRF output that happened earlier, namely the credential attached by the block proposer of the most recently finalized block. So in particular r sub T is sort of independent of the transactions in the last block.
00:17:19.926 - 00:18:15.770, Speaker A: It's independent of everything other than kind of this one field in the block's metadata, namely the credential of its proposer. So this approach to defining pseudorandum seats can also be manipulated, as we'll see. But in some sense it's a lot less manipulable than a lot of other things that you might try to make this concrete, let's talk through the canonical way that you would manipulate this particular approach to defining the R sub T's. So I want you to think about a node that has registered some sybils in the Staking contract. So for simplicity, let's just assume that the first two public keys in the Staking contract, PK one and PK two, let's suppose those are both owned by the same entity by the same node. So that means each time step, I mean this node is basically going to be computing two different VRF outputs, right? One with the private key corresponding to PK one, one with the private key corresponding to PK two. Some time steps, those VRF outputs are going to be big.
00:18:15.770 - 00:18:57.254, Speaker A: Some time steps, those outputs are going to be small. Sometimes both of those outputs are going to wind up being quite small. So in fact, let's think about some time step where this node gets really lucky and actually both of its VRF outputs are smaller than everybody else's VRF outputs. So in this case, this node actually will be able to choose from among two different options for the pseudorandom seed at the next time step, which is exactly what we mean by manipulating the R sub T's, right? These VRF computations are secret. Nobody knows what they are except for the people who did them. So this node is in a position to reveal whichever of the two credentials that it wants. It can reveal the smaller one, it can reveal the bigger one.
00:18:57.254 - 00:19:36.110, Speaker A: Either way, it's going to have the smallest Credential that anyone knows about. So it will be able to propose the block at this round. If it uses its smaller Credential, then the pseudorandum seat of the next time step will be that smaller Credential. If it instead uses the larger but still super small Credential, the pseudorandum seat of the next time step will be that larger Credential. The node, of course, could also choose to reveal both. But because everybody ignores the larger of the two, that would be the same as only revealing the smaller Credential. So what good is this sort of power to choose from among two pseudorandom Teeds for the next time step to this node? Well, the obvious thing to do is just to sort of release the Credential.
00:19:36.110 - 00:20:39.586, Speaker A: Now for timestep T, that's going to give you the better meaning. The smaller credentials at timestep t plus one. That's the way to sort of maximize the likelihood that you're going to wind up being the leader also of timestep t plus one, not just the current timestep t, in case you're wondering, well, why would the Node care so much about sort of boosting its likelihood of being the leader again in the next round? Remember, there's a lot of different reasons, right? If there's a block reward, there's a financial incentive. If there are transaction fees paid to the block proposer, that's another financial incentive. Maybe they have a sort of other incentives like they want to somehow censor certain transactions. Who knows? Generally it's viewed as a bad thing for any node to be overrepresented in the frequency with which it gets to make block proposals proposals. So if we think about that specific case where there are block rewards and nodes are acting to maximize the amount of block rewards that they get, this manipulation starts feeling kind of similar in spirit to the selfish mining deviations for Nakamoto Consensus that we talked about back in lecture number ten, and indeed very similar mathematics.
00:20:39.586 - 00:21:30.280, Speaker A: So if you remember sort of that whole Markov chain analysis that we did back in lecture ten, very similar mathematics can actually be used to quantify sort of the benefit you get from this manipulation of these pseudorandum seeds as well. That's a fairly recent paper by Ferreira, Han, Weinberg and Yu. Sort of an open question how much of a big deal one should make of this sort of limited manipulation? The answer probably depends a little bit on sort of what the fraction of stake that's controlled by any one entity. If nobody has more than 1% of the stake, it's just kind of going to be pretty rare that you actually wind up with sort of the two smallest credentials overall. And maybe it's kind of safe not to worry about it. But if there's someone with like 20% of the locked up stake, you are going to be getting then periodic opportunities to do these manipulations. And that's something you might potentially be concerned about.
00:21:30.280 - 00:21:59.700, Speaker A: Now speaking at the time of this recording, this is in February 2023, VRF based sampling. It's reasonably widely used. Multiple major proof of stake blockchain protocols do in fact use VRF based sampling. And all of those protocols are indeed, at least to a little bit extent, manipulable in a similar way. So clearly all those projects are well aware of that fact. They've clearly all sort of made peace with that, at least temporarily. Sort of, yes, you can have a small amount of manipulation, but hopefully it's not that big a deal.
00:21:59.700 - 00:22:58.734, Speaker A: As this decade goes on, I do think you may see upgrades that actually totally sort of squash any possibility of manipulation. That would probably require one of the two experimental approaches that we talked about in part two. So either using these sort of new solutions to single secret leader election and or the use of verifiable delay functions. The last issue I want to discuss in this video is probably also the least subtle one to discuss, namely just the sampling error that inevitably gets introduced when you pick a subset, when you pick a committee from all of the active validators in the Staking contract. And the big worry here concerns the overrepresentation of Byzantine nodes on whatever committee you. You know, we're running kind of a BFT tech consensus protocol, so we're definitely expecting to have some assumption like less than a third of the overall participation is controlled by Byzantine nodes. It's a proof of stake protocol.
00:22:58.734 - 00:23:38.626, Speaker A: So participation is going to be measured by the stake amounts. So what we're sort of expecting to see is an assumption that as long as less than a third of the overall locked up state is controlled by Byzantine nodes, then lots of good things happen. But here's the problem, right, which is that suppose 70% of the locked up state is honest and 30% is controlled by Byzantine nodes. Any given committee in expectation you expect the same 70 30 split honest Byzantine. But whenever you're doing sampling there's going to be some variance. So sometimes you'll get a 75 25 split which is fine. Sometimes you'll get a 60 40 split which is no longer fine.
00:23:38.626 - 00:24:23.550, Speaker A: At that point you have a committee that's because of the sampling error is actually more than a third Byzantine. And remember, it is the committee that is tasked with running the tendermint protocol. So if you get one of these 60 40 committees you tell them to run the tendermint protocol, none of the consistency and liveness guarantees of that protocol apply. If 40% of the participation in tendermint is Byzantine then by double voting they actually might be able to trigger a consistency violation. They might actually be able to cause two different blocks to be finalized at exactly the same block height. So that is something we obviously want to avoid. So how should we go about doing that? Well, the recommendation here is to take a three pronged approach to do all of the following.
00:24:23.550 - 00:25:12.254, Speaker A: So the first is you just got to be honest and make a stronger assumption about what fraction of the overall locked up stake is controlled by honest notes. Maybe you assume it's for example, a 75 25 split at least so that you have a decent amount of wiggle room to absorb sampling error once you pass to the random committees. So the second thing you can do in addition to giving yourself wiggle room to absorb sampling error is to just try to reduce the sampling error. And the way you do that is by having sufficiently large committee sizes. So by the law of large numbers, as the committee size grows big you're going to be seeing closer and closer to proportional representation. Exactly the same principles we talked about when we analyzed longest chain consensus under random leader selection back in lecture eight. So how big is big enough? Sort of depends on your wiggle room.
00:25:12.254 - 00:25:55.920, Speaker A: The stronger your assumption in the first fix, the smaller the committee size you can get away with in the second. But for concreteness you might want to think about committee sizes as being at least in the hundreds. Now growing the committee size is not without cost and at some point it just starts being infeasible. And that's because remember, it is exactly the committee that's going to be tasked with carrying out the permission consensus protocol like tendermints. And so tolerating 100 nodes there may be not a big deal, but tolerating kind of thousands of nodes is going to be tough. So you basically want to. Pick these committee sizes as big as possible to reduce sampling error subject to the constraint that you should be able to implement the permission to consensus protocol at that scale, at the scale of a committee size.
00:25:55.920 - 00:26:35.690, Speaker A: And finally, whatever you do in those first two parts, however much wiggle room you have and however big your committee size, once in a while you just will have a bad committee. You're just going to have a committee that's more than a third Byzantine. And whenever that happens you are vulnerable to actually having a consistency violation. You might have a fork, you might have two finalized blocks at the same block height. So you just got to be ready to have an emergency backup procedure to resolve forks if necessary. The really Kludy way to do that would be to do that out of protocol. Like if you have a fork you just sort of shut down the protocol and then somehow reboot in some sort of canonical state that you have to sort of get everybody, all of the stakeholders to agree upon.
00:26:35.690 - 00:27:16.582, Speaker A: Maybe sort of more ambitious but also maybe kind of more appropriate would be to somehow resolve forks in protocol. So maybe if at some time step you get unlucky, you have a bad committee, they create a fork. Maybe the next committee who presumably are going to be, unless you're really unlucky, they're going to be more than two thirds honest. Maybe the next committee, rather than trying to finalize a new block, maybe they just finalize which branch of that fork is going to become the canonical one. That's what I mean by an emergency backup procedure. An optional additional protection you could take here would be to incorporate slashing. So slashing, we'll have a whole video about that in part four.
00:27:16.582 - 00:27:52.770, Speaker A: But basically the idea is if you detect sort of validators misbehaving, you confiscate part or all of their locked up state. And the canonical sort of smoking gun of a validator misbehaving is double voting. So having casting votes on different conflicting blocks at the same block height. So for these sort of forks you could then slash a bunch of stake that hopefully would be a deterrent. So that even when your sampling procedure winds up being unlucky, the economic cost of Byzantine sort of entities taking advantage of it is just too high. So they don't bother. So that'd be an additional potential layer of protection.
00:27:52.770 - 00:28:30.590, Speaker A: So those are the three issues that I wanted to talk through in this video. As promised, these are all issues for proof of stake BFT type blockchain protocols. Even under our standing assumption that all of the stake amounts, all of the QIS are exactly the same. Now that we understand these issues and the ways that they can be addressed, let's actually now turn our attention to just getting rid of that assumption. So let's actually talk through the changes you would make to the protocol if you wanted to accommodate arbitrary stake amounts. So arbitrary q sub eyes. And of course, we want to make sure that everything is sybil resistant.
00:28:30.590 - 00:28:44.460, Speaker A: So that's going to be the subject of the next videos. That's the third video in part three of lecture Twelve. It'll also be our last video about proof of stake BFT type consensus protocols before moving on to proof of stake longest chain type protocols. So I'll see you there.
