00:00:00.250 - 00:00:26.610, Speaker A: In this lecture we're going to be having on our hat where we seek to explain sort of an empirical ground truth about algorithms. So specifically the fact that when you look at hash functions empirically, they behave as well as if they were fully random functions, even though in theoretical worst case that's not always true. So we're going to be seeking some kind of theory which explains why simple hash functions work so go well in practice.
00:00:26.610 - 00:00:46.010, Speaker A: And the solution we're going to put forth is similar to the spirit of smooth analysis. We're going to assume that the data has at least a little bit of randomness in it. And under that assumption we'll see that indeed simple universal hash functions are guaranteed to do as well as fully random hash functions.
00:00:46.010 - 00:01:09.298, Speaker A: So just a little bit of just jog your memory about hashing and the specific motivating application of hashing I'm going to use, which you may not be familiar with, linear probing. So first, just hash functions. So we're thinking about there being a big set, capital M, maybe really, really big, maybe astronomically big, bigger than the number of atoms in the universe even.
00:01:09.298 - 00:01:21.160, Speaker A: And a hash function is responsible for basically compressing those. So giving each element of capital N a nickname that lies somewhere in capital M. So this is what a hash function does.
00:01:21.160 - 00:01:35.194, Speaker A: Maps elements of capital M to elements of capital M, where capital M is much smaller. Again, depends on the application. Maybe today think of capital M as being like logarithmic in the size of capital N, for example.
00:01:35.312 - 00:01:35.980, Speaker B: Okay?
00:01:36.830 - 00:01:53.466, Speaker A: Now what else do I want you to remember? So because we're compressing mapping a big set to a small set by the pigeonhole principle, inevitably there will be collisions. There'll be distinct elements of capital N that get mapped to the same element of capital M. And in your data structure, think of it as an array, say where you have a bucket.
00:01:53.466 - 00:02:08.098, Speaker A: You have to somehow deal with these collisions, right? So that's part of the implementation details of a hash table. One of the simpler ways, and usually what's emphasized in an undergraduate course, for example, is chaining. So this is where in each bucket you have a linked list.
00:02:08.098 - 00:02:20.338, Speaker A: So everything that hashes to the same element just gets concatenated to each other in a linked list in that bucket. That's not what we're going to be talking about today. The motivating example is going to be a style of open addressing.
00:02:20.338 - 00:02:28.278, Speaker A: So this is a different way of resolving collisions where you maintain the invariant that in your hash table. There's only going to be one item stored in each bucket.
00:02:28.374 - 00:02:28.778, Speaker B: Okay?
00:02:28.864 - 00:02:45.130, Speaker A: So clearly the load better be less than one, the number of things inserted better be at most the number of buckets because there's only one per bucket. And so that means is when you hash something and you go to put it in a bucket, in a slot, if it's already full. You need to look elsewhere to find an empty slot to put this element that you're inserting.
00:02:45.130 - 00:03:13.020, Speaker A: Okay, so there's many flavors of open addressing depending on how you search, how you do this search for an empty bucket. So you have to, formally speaking, define a probe sequence. What I'm going to use as motivation is a particular probe sequence, a particular way of doing this search for an empty slot in a hash table called linear probing, maybe the simplest thing you could think of.
00:03:13.020 - 00:03:27.502, Speaker A: So you're given some element X in capital N, and you apply your hash function H, so that maps it to capital M. What do you do if that slot is full? You just search subsequent slots until you find an empty one.
00:03:27.556 - 00:03:27.726, Speaker B: Okay.
00:03:27.748 - 00:03:59.858, Speaker A: And you just dump it in the first empty slot that you find, wrapping around to the beginning of the array, the beginning of the hash table, as needed. Okay, so start at a bucket, H of X, and then you look at H of X plus one, h of X plus two, et cetera, as needed to find an empty slot. Okay, so on an insertion, so think about the case where there's no deletions.
00:03:59.858 - 00:04:09.786, Speaker A: I don't want to deal with deletions. For an insertion, you just start at H of X and you just go forward until you find an empty slot and then that's where you put it. And then if you're doing a search, you start looking at H of X.
00:04:09.786 - 00:04:17.934, Speaker A: You keep looking forward. If you ever find that element, obviously you're done and you return it. If you find an empty slot, then you can conclude that the element isn't there and it's an unsuccessful search.
00:04:17.934 - 00:04:47.960, Speaker A: Okay, so that's linear probing. So why do this and why not do this? What are the pros and cons? So it's obviously simple, and the main reason that this is useful in practice beyond simplicity, is it is primarily sequential accesses to a disk or to the data structure, as opposed to random accesses. Okay, so mostly sequential access.
00:04:47.960 - 00:04:55.830, Speaker A: Okay. So I'm thinking here about a hash table, which is an array just stored contiguously in memory.
00:04:55.910 - 00:04:56.394, Speaker B: Okay?
00:04:56.512 - 00:05:07.098, Speaker A: So that interacts well with the memory hierarchy. It interacts well with things like prefetching. So empirically, you often get quite good performance from linear probing, and it is fairly common in practice for those reasons.
00:05:07.098 - 00:05:22.558, Speaker A: Contrast that with, say, chaining, right? If you have a linked list, it can be hard to make sure that your linked list is contiguous in memory. So that might be jumping around in memory as you follow the linked list. Similarly, if you tried to do something more clever as a probing strategy that took you to and fro within the hash table, that, again would be lots of random accesses.
00:05:22.558 - 00:06:02.702, Speaker A: Okay, all right, but what should one be concerned about with linear probing? Well, intuitively, here's what you're sort of worried is going to happen. So you start inserting things into your hash table randomly and for a while, when it's very sparse, you're not even going to have any collisions, okay? So it's not a big deal. And then maybe by chance, just two things map into, say, adjacent buckets, but then you're worried about this sort of amplification of clumps, okay? So now as soon as you have two things in a row, now all of a sudden there's a double the chance.
00:06:02.702 - 00:06:22.866, Speaker A: So two out of the number of buckets that the next thing will hit either of these. And whichever of these attempted inserts you try to do, you're going to wind up searching forward and putting the new element here. Now all of a sudden there's three different slots where if you try to insert in any of these, it's going to wind up inserting something here and then maybe even these things join.
00:06:22.978 - 00:06:23.542, Speaker B: Okay?
00:06:23.676 - 00:06:52.094, Speaker A: So somehow the bigger the blocks get, it feels like the more likely the blocks are to get even still bigger. Okay, so intuitively, linear probing is vulnerable to clumps and obviously the insertion time or the search time is degrading as you have these big clumps, okay, because on average you'll sort of find yourself somewhere in the middle as a starting point and then you have to search through half of this clump. So that's what you're worried about.
00:06:52.094 - 00:06:56.542, Speaker A: That's why it's not actually so clear that the performance is going to be especially good for linear probing.
00:06:56.606 - 00:06:57.220, Speaker B: Okay?
00:06:58.310 - 00:08:10.554, Speaker A: But there's an old result of my colleague, the living legend, Don Knuth, I forget the exact date, it's something like 1960 or so, where he said, well, let's ignore the details of what the hash function is and let's just assume that actually the hash function is totally random, okay? So that every time some new X shows up and you look at h of X, let's assume that's uniform, not only is it a uniformly random bucket, but it's also independent of everything that's happened in the past, okay? So that's a fully random function sends everything to a uniformly at random place independently. So under the best case scenario of a fully random H, and again, fully random independent H of X's, then it turns out the expected time of the teeth insertion is a function of the load only.
00:08:10.752 - 00:08:11.210, Speaker B: Okay?
00:08:11.280 - 00:08:24.080, Speaker A: So the exact function turns out to be basically one over one minus alpha, that's an alpha alpha squared, where here alpha denotes the load of the hash table, okay?
00:08:25.330 - 00:08:27.950, Speaker C: Not dependent on how the elements are distributed.
00:08:29.010 - 00:08:49.250, Speaker A: Well, so it's an expectation, right? So there's always like the really unlucky case where they're all in a row, but so on average over the randomness in the hash function, if the tables say half full, so that corresponds to alpha being 0.5, then you're expecting to do basically four probes on the teeth insertion.
00:08:49.330 - 00:08:49.910, Speaker B: Okay?
00:08:50.060 - 00:09:13.326, Speaker A: So alpha here is t the number of things you've inserted over the number of buckets. The cardinality of N. Okay? And so the takeaway here is that the reason this is so cool is it says basically you pick whatever expected insertion time you want.
00:09:13.428 - 00:09:14.126, Speaker B: Okay?
00:09:14.308 - 00:09:28.670, Speaker A: And independent of how many insertions you're ever going to see, as long as you scale your hash table proportionally to the number of elements that you're going to have to hold, then you can control the expected insertion time.
00:09:28.740 - 00:09:28.926, Speaker B: Okay?
00:09:28.948 - 00:09:44.370, Speaker A: So the point is, it depends on T only in as much as it depends on alpha, right? So obviously if, if you want to store double the number of elements, you should expect to sort of double the hash table. But what's great here is that actually the performance remains unchanged as long as you scale the hash table with the number of insertions.
00:09:44.450 - 00:09:44.694, Speaker B: Okay?
00:09:44.732 - 00:09:59.990, Speaker A: And that's sort of what you'd love to have this to be even smaller, like one over one minus alpha, that would be even better. But to have this be a function of alpha only otherwise independent of TNM is great. Okay, so it really just says scale the hash table with the data set, you'll be fine if it's totally random hash functions.
00:10:00.070 - 00:10:00.700, Speaker B: Okay?
00:10:01.070 - 00:10:19.442, Speaker A: And I encourage you to check out if you look at Knuth's Art of Computer Programming volume three, he has a footnote where he basically says after he worked out this theorem, he found the beauty so overwhelming, he had no choice but to devote the rest of his life to the analysis of algorithms. I'm not kidding. That's what he said.
00:10:19.442 - 00:10:31.880, Speaker A: Okay, so this seduced Knuth to basically work on this stuff for the rest of his life. Okay, so we're not going to prove this theorem. It's not sort of outside the scope of this course.
00:10:31.880 - 00:10:38.230, Speaker A: So what I want to talk about is I want to evaluate this hypothesis.
00:10:39.230 - 00:10:39.690, Speaker B: Okay?
00:10:39.760 - 00:11:10.190, Speaker A: So the best case scenario of a fully random hash function. Now in practice, hash functions are not fully random, okay? You might want to think about what it would even mean to use a completely random hash function. Basically, it means you'd have to store it as a lookup table explicitly, all right, which for any end of even moderate size is ridiculous.
00:11:10.190 - 00:11:25.634, Speaker A: Okay? So it's really not an option to use fully random HS in interesting applications, so we can't use a truly random.
00:11:25.682 - 00:11:26.280, Speaker B: H.
00:11:28.170 - 00:11:41.914, Speaker A: So this is false in practice. The point is we're not using these hash functions. So then the question is, all right, is there some other way we could kind of justify the mathematics and kinemus's derivation? Maybe there's alternative assumptions under which it would be true.
00:11:41.914 - 00:12:12.294, Speaker A: This one over one minus alpha squared would be true. For example, instead of having a totally random hash function, you could have just sort of any old hash function and assume that the data is totally random, that each thing you want to insert is sort of uniform at random from capital N and also independent from all the previous things you've wanted to insert. Okay, so the math would remain valid if you switched from thinking about a fully random hash function to fully random data sets, any hash function you want, it's pretty easy to find hash functions that would be fine.
00:12:12.294 - 00:12:22.970, Speaker A: The constant function would not be so good. So you need some mild assumptions on the hash function, but many hash functions will be fine if the data is totally random. So it's not hard to define a hash function that works well with random data.
00:12:22.970 - 00:12:39.374, Speaker A: On the other hand, it's not fully convincing. I mean, that's good. It's good to have a second interpretation under which this is true, but we don't really think data usually is kind of totally random and that everything being inserted is independent from what's being inserted before.
00:12:39.374 - 00:13:10.662, Speaker A: It seems sort of implausible. Okay, so given that we can't assume fully random hash functions, nor fully random data, sort of, the hope is that simple hash functions, namely ones that we can store explicitly in small space and ones that we can evaluate quickly, work just as well. Okay, so simple hash functions work just as well as fully random ones.
00:13:10.662 - 00:13:22.766, Speaker A: So we want to just say, look, this is true under the random function assumption, but that hypothesis is sort of overkill. Okay, just use sort of a small family of simple ones and you'll still get the same bound. That's what we'd love to be true.
00:13:22.766 - 00:13:41.102, Speaker A: Okay, all right, fine. It's a nice idea. So how should we define simple hash functions? So what's a good definition? So in this lecture, we're just going to use what should be the definition you all already know from undergraduate algorithms and data structures, namely universal hashing.
00:13:41.166 - 00:13:41.394, Speaker B: Okay?
00:13:41.432 - 00:14:13.694, Speaker A: So that's the only property of hash functions we'll be using. So recall so that if you have a set of family h of hash functions, all of these mapping the same domain N to the same range, capital M, we call this universal if what? So we want to think about collisions. So collisions are important.
00:14:13.694 - 00:14:29.810, Speaker A: So we say think about an arbitrary pair of distinct elements in the domain. Okay, so for all X, Y and N distinct. And we talk about the probability and this is over the choice of h.
00:14:29.810 - 00:14:40.870, Speaker A: Okay, so X and Y are fixed and different. We're picking a random hash function. Or put differently, we're looking at the fraction of hash functions in this family under which these two elements collide.
00:14:40.870 - 00:14:49.506, Speaker A: So again, X and Y are not random. H is what's random in here. So collisions are bad.
00:14:49.506 - 00:15:07.146, Speaker A: So presumably we want an upper bound in this probability. Now, what kind of upper bound would we be happy with? Well, suppose actually h was a totally random function. Okay, so suppose every H of X, h of Y, h of Z was chosen independently and uniformly at random.
00:15:07.146 - 00:15:09.280, Speaker A: What would this probability be?
00:15:12.050 - 00:15:14.142, Speaker C: One over the size of the range, right?
00:15:14.196 - 00:15:18.290, Speaker A: One over the size of the range. So I'm calling capital M. The range.
00:15:18.290 - 00:15:32.838, Speaker A: Why would that be true? Well, just first you pick H of X. It goes wherever it goes, slot number 17, okay? H of Y is independent of that and uniformly at random. And so there's a one in M chance that it goes to slot 17 as opposed to some other one.
00:15:32.838 - 00:16:02.826, Speaker A: So universal hash functions just says for the purposes of a pair of elements, x and Y, the collision probability is as low as the gold standard set forth by fully random hash functions, okay? That's what universal hash function universal hashing means, all right? Now, fully random functions have properties other than this one. So for example, if we looked at the probability that three elements collide, then we'd get a one over M squared here. And I'm not asking for that, but I'm saying if you just look at pairs of elements, then it should be the random type collision probability.
00:16:02.938 - 00:16:03.214, Speaker B: Okay?
00:16:03.252 - 00:16:06.530, Speaker A: So sometimes this is called two universal because it's about pairs of elements.
00:16:07.750 - 00:16:08.354, Speaker B: Okay?
00:16:08.472 - 00:16:12.034, Speaker A: So you should have seen this definition before.
00:16:12.232 - 00:16:16.310, Speaker C: What's the intuition behind why this is the right definition? A good definition?
00:16:17.770 - 00:16:24.082, Speaker A: Yeah. So, I mean, collisions are bad, want collisions to be unlikely. And so you're saying collisions is as unlikely as random hashing?
00:16:24.226 - 00:16:29.766, Speaker C: Yeah. How much looser is this than saying that it's a perfect hash function?
00:16:29.868 - 00:16:34.426, Speaker A: Well, if you're just focusing on pairwise collisions, it's exactly the same as saying it's a random hash function.
00:16:34.608 - 00:16:37.686, Speaker C: But if you look at like three elements or four elements, it could be significant.
00:16:37.718 - 00:16:53.306, Speaker A: Then there's no guarantee. So you should know this definition, I hope. And what else should you know? So you should know that there are plenty of constructions of small families, small script, h's.
00:16:53.306 - 00:17:13.878, Speaker A: So in other words, hash functions that can be stored in small space, for which you can also evaluate the function little h on any domain element X quickly. So, for example, the one I usually teach in undergrad algorithms is the one where you take a domain element like an X, you break it into small blocks, and then you take a random linear combination of the blocks of X.
00:17:13.964 - 00:17:14.214, Speaker B: Okay?
00:17:14.252 - 00:17:19.778, Speaker A: So you just pick coefficients in a linear combination at random, and then you take that modulo of prime.
00:17:19.874 - 00:17:20.230, Speaker B: Okay?
00:17:20.300 - 00:17:33.600, Speaker A: That's a universal family of hash functions. I'm not going to prove that now. I'm just saying that orally, hopefully to remind you, but also just to give you kind of a verbal proof that it's not hard to come up with functions that satisfy this property, okay? Simple classes of functions satisfy this property.
00:17:33.600 - 00:17:55.410, Speaker A: There are even simpler hash functions that don't satisfy this definition. But I'm just saying it's not unreasonable to take for, especially for the purpose of proving a theorem, a simple hash function. We're going to take this as the definition, okay? It's kind of more or less the minimal property under which you could hope any theorems would be provable under any kind of reasonable assumptions.
00:17:55.410 - 00:18:17.094, Speaker A: All right? So that's one thing you should know about these families is they do exist and they're small ones and they're practical ones. Another thing that you might have seen is that actually this property alone is sufficient to justify the performance of chaining. So remember, chaining is where you have a hash table and you have a linked list in each bucket.
00:18:17.094 - 00:18:40.766, Speaker A: And you can talk about things like what's the expected, say, unsuccessful search time in a hash table with chaining, okay? And it turns out if you tell me nothing about the hash functions that you're using, other than that the hash function was chosen at random from a universal family, that alone is enough to argue that as long as the hash function has constant load, the expected unsuccessful search time is constant.
00:18:40.878 - 00:18:41.202, Speaker B: Okay?
00:18:41.256 - 00:18:47.230, Speaker A: So universality guarantees constant operation time in hash tables with chaining with constant load.
00:18:47.310 - 00:18:47.698, Speaker B: Okay?
00:18:47.784 - 00:18:58.690, Speaker A: And again, don't forget the order of quantifiers here. When you talk about hashing, you think about the data set being worst case and fixed up front. And then you think about picking a random hash function from a family for this worst case data set.
00:18:58.690 - 00:19:27.914, Speaker A: That's the usual quantifiers when you're talking about universal hashing, okay? So for chaining, you don't need anything more than this assumption. But it turns out the plot thickens for some open addressing strategies and in particular for linear probing. So what would be great? Again, so to summarize, for chaining, universal simple hash functions in this sense are as good for the performance metrics that we care about as fully random hash functions.
00:19:27.914 - 00:19:56.300, Speaker A: So we'd love to just say exactly the same thing. For linear probing, don't use a random hash function, just use a universal hash function and you still get the one over one minus alpha squared upper bound on expected search and insertion time. However, not that long ago, just maybe a little over five years ago, we have the following result.
00:19:56.300 - 00:20:12.202, Speaker A: The point of this result says that conclusion for chaining is not true for linear probing. If all you tell me is that your hash functions are universal, then it is not the case that you get an upper bound. Forget about one over one minus alpha squared, you don't get any bound.
00:20:12.202 - 00:20:26.682, Speaker A: That depends only on the load alpha. So formally, there exists a sequence of ever. We're going to let the range grow larger and larger.
00:20:26.682 - 00:21:20.486, Speaker A: There exists universal families, script h of hash functions and worst case data sets, meaning things to be inserted into the hash table under a random hash function from this family of arbitrarily large size, such that the expected time of the teeth insertion grows with T even as the load stays fixed. So despite alpha T over M staying constant. So I'm being a little sloppy with the way I'm writing this.
00:21:20.486 - 00:21:33.398, Speaker A: So really what this means is there's a sequence of domain and ranges, capital N's and capital M's, and they're going to infinity. Also going to infinity is so M is going to infinity. So the hash table size is going to infinity.
00:21:33.398 - 00:21:35.446, Speaker A: So is the number of items that we're inserting.
00:21:35.478 - 00:21:35.674, Speaker B: Okay.
00:21:35.712 - 00:21:37.514, Speaker A: And they're going to infinity at the same rate.
00:21:37.632 - 00:21:37.962, Speaker B: Okay.
00:21:38.016 - 00:21:40.614, Speaker A: And their ratio is alpha, and that stays fixed.
00:21:40.742 - 00:21:41.322, Speaker B: Okay.
00:21:41.456 - 00:21:55.626, Speaker A: As T goes to infinity, despite the fact the load is constant, the expected time of the teeth insertion is also going to infinity. In other words, the expected insertion time cannot be bounded above as a function purely of the load alpha. It also depends on the size of the data set.
00:21:55.748 - 00:21:56.034, Speaker B: Okay.
00:21:56.072 - 00:22:04.910, Speaker A: And that's different than what we have for linear probing, for fully random hash functions. It's also different from what we have for chaining, for just universal hash functions.
00:22:05.070 - 00:22:07.106, Speaker B: Okay, Elliot, is this a time where.
00:22:07.128 - 00:22:09.910, Speaker C: You can apply smooth analysis to kind of get rid of this?
00:22:10.060 - 00:22:24.074, Speaker A: Yeah. So that's sort of the spirit of this lecture. Okay, so smooth analysis per se, it's not totally clear what that means, because for smooth analysis, at least as the way we've been talking about it and everyone else talks about it, there's numbers, and you add these small perturbations to numbers.
00:22:24.074 - 00:22:26.358, Speaker A: Everything with hashing is fully abstract.
00:22:26.454 - 00:22:26.714, Speaker B: Right?
00:22:26.752 - 00:22:40.480, Speaker A: There's just an abstract set N and abstract set M. So one way to interpret this lecture is what would it mean to analyze hashing in the spirit of smooth analysis? And so I'm going to give you what I think is a satisfying answer to that question.
00:22:42.690 - 00:22:43.006, Speaker B: Yeah.
00:22:43.028 - 00:22:50.420, Speaker A: So it's not an accident that this lecture is adjacent to the smooth analysis ones very much in the same spirit. Okay. Other questions.
00:22:51.670 - 00:22:53.566, Speaker C: Is it constructive proof?
00:22:53.758 - 00:23:09.560, Speaker A: Yes. I even was tempted to put it on homework, and I may torture some future year of this class with that. Yeah, it's just like, a little too long for a problem on a problem set, but it's close.
00:23:09.560 - 00:23:28.590, Speaker A: I'd probably have to give it, like, 50 points over all the parts or something. If I put it on the problem set and you've seen some 40 pointers, I still might do it sometime. Anyways, I encourage you to read it if you're interested.
00:23:28.590 - 00:23:42.734, Speaker A: It's just roughly about a page of a journal paper, maybe a page and a half. Okay, so what do we want? So empirically, I should say empirically, this does not happen. And I guess this is the other point.
00:23:42.734 - 00:23:50.740, Speaker A: I mean, this is the other so if empirically we also saw this, then we wouldn't expect some kind of smooth analysis to help us. Right. So the other sort of.
