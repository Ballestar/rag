00:00:00.410 - 00:01:06.606, Speaker A: Um, so as far as the lecture, let me remind you what we did last time. So we're in the middle of learning dynamics and we had this really cool result last lecture, which is that course correlated equilibria are tractable. And they are tractable in a very satisfying sense, in the sense that there are simple, certainly computationally efficient learning algorithms. Specifically, we looked at multiplicative weights on Monday, and if all players use a no regret algorithm like multiplicative weights, then the history of joint play will converge to this set. Of course, correlated equilibria, the biggest of the four sets in the equilibrium hierarchy that we've been discussing. Okay so last time we proved in here that's easy now the bigger an equilibrium concept the more things there are to find the easier it is to find one. So if any of these four sets of equilibrium were going to be computationally tractable it was going to be the course correlated equilibrium that's a necessary condition for anything else to be tractable.
00:01:06.606 - 00:01:41.226, Speaker A: So today I want to look inward a little bit I want to get greedy and ask are any of these more refined equilibrium concepts also tractable? And so I want to cover two results today. The first result is we're going to get an analogous tractability result for correlated equilibria. So the next smallest set, and I'll remind you what these are in a second. So the first main result is correlated equilibrium are attractable. Again the sense will be similar. So there are again going to be learning algorithms. They're not going to be quite as simple.
00:01:41.226 - 00:02:12.818, Speaker A: We're going to have to work a little bit harder, but they'll still be not too bad. They'll certainly be polynomial time. And the convergence will be in the same sense. If all players use these sort of more refined what are called no swap regret algorithms, then the history of joint play will in fact converge not just to coarse code equilibria, but to the smaller set of correlated equilibria. So that's the first result I want to talk about. One quick aside. If you wanted to just establish polynomial time tractability, you don't actually need to resort to learning algorithms for course correlated equilibrium or correlated equilibria.
00:02:12.818 - 00:02:43.998, Speaker A: You can formulate them as linear programs which we can solve in polynomial time. So that has the benefit. You even get an exact equilibrium, which you might recall from Monday, we only got approximate course correlated equilibrium. It'll be similar today. But the drawback is solving a linear program doesn't really bear any resemblance to actual behavior of players and games. Whereas the learning dynamics we're discussing, while not maybe exactly how people act, they at least bear some resemblance to actual behavior. So that's sort of the trade off between the linear programming approach and the learning dynamics approach we're talking about this week.
00:02:43.998 - 00:03:32.918, Speaker A: So looking ahead a little bit we will talk some about a special case of Nash equilibria today but next week we're actually going to talk about intractability of mixed Nash Equilibria and pure Nash Equilibria. So that's just to give you a little appreciation for it's really cool we can get tractability results for these slightly bigger sets or nontrivially bigger sets. Okay. So let me remind you what a correlated equilibrium is and I'm not going to give you the definition I gave you in lecture. Rather I'm going to cite a definition that you proved as equivalent in an exercise a couple of weeks ago. So this was exercise 59. So the original definition was in terms you think of a player sort of conditioning on its recommendation from a mediator and then switching and can it do better condition on its recommendation.
00:03:32.918 - 00:04:15.386, Speaker A: This is the definition in terms of a switching function. So a distribution sigma over the outcomes of a game I'll again use the notation for cost minimization games. So as a correlated equilibria or just a ce if for all players I and for all switching functions. So I'm going to use delta for that notation. This maps actions or strategies of player I. Back to strategies of player I. It need not be a bijection it just maps actions to actions.
00:04:15.386 - 00:05:24.050, Speaker A: So if for all switching functions player I is no better off switching than it is just following the distribution sigma. So on the one hand we look at I's expected cost according to the distribution sigma and that should be no more than I's expected cost if every time it's supposed to be playing some strategy si it instead plays the strategy delta of si. So this is the definition of correlated equilibria that maps most naturally to learning algorithms. Another thing just to remember about correlated equilibria. So if you just want a concrete example this was the traffic light example where we had the stop go game and it had two pure Nash equilibria and we observed that you could implement a 50 50 mixture of the stop go and the go stop outcomes using a traffic light. That was essentially the mediator which would recommend strategies via a green light or a red light whether to go or stop respectively. Okay so that's the equilibrium concept we're going to talk about in the first part of this lecture.
00:05:24.050 - 00:06:03.290, Speaker A: All right. So last time we were talking about course quality equilibrium we made this connection to these no regret algorithms. So let me just remind you sort of what was the online learning setting we were talking about on Monday. So remember this is where we have this known time horizon capital T and then on every day little T from one up to capital T. First you have to pick a distribution. Again we're just thinking about a single player at the moment. The single player has to pick a distribution over its actions and then after deciding on the distribution an adversary reveals a cost vector c sub t and then the goal is to minimize your cost relative to some benchmark.
00:06:03.290 - 00:06:57.330, Speaker A: And the benchmark we used on Monday was external regret, meaning you want to do at least as well as the best fixed action. In hindsight, there's a connection between competing with the best fixed action and coarse correlated equilibria. So the motivating question for this first result is, all right, we have this refined equilibrium concept, correlated equilibrium. Is there an analogous notion of regret, analogous to external regret so that we again get this connection between the static equilibrium concept and a no regret notion. And there is that's the next definition, it's called swap regret. And again, the point of this is so that minimizing this regret notion via dynamics will lead us to this equilibrium concept. So definition, so an algorithm, and here when I say an algorithm, I'm talking about in this online decision making setup that we talked about on Monday.
00:06:57.330 - 00:08:13.340, Speaker A: So an algorithm has no swap regret if for all cost vectors that the adversary might throw at us and for all switching functions. So here A is the set of actions of this fixed player, this one player that we're thinking about the expected value of how well you do. So again, we're going to look at a time average. So how well you do is just the cost vector on time T evaluated the action you play at day T, which is drawn according to this distribution that you chose at the beginning of day T. Now, previously what we did is we looked at the best performance of any fixed action. Here we're going to look at the best performance under any fixed switching function delta. So we have a delta fixed and we think about how well we would have done if every time on some day we played a given action, a sub T, we instead play the alternative action delta of a sub T.
00:08:16.590 - 00:08:17.340, Speaker B: Okay?
00:08:22.030 - 00:09:17.338, Speaker A: So this is a random variable because our actions are chosen at random from the distributions that we pick each day. Remember, our algorithms are randomized, but it has some expected value. And it should be the case that the expected value of this quantity is going to zero for every switching function delta. I forgot to say that goes to zero as T goes to infinity. So this is only more stringent than the notion of external regret that we discussed Monday. External regret corresponds to needing to worry only about a subset of the switching functions delta, namely which ones the constant functions, okay? The function's delta that no matter what the input is, always outputs a fixed action. So if delta is a constant function, this is always the same action.
00:09:17.338 - 00:09:24.734, Speaker A: So this is just saying your cost should be competitive with that of every fixed action as the time horizon goes to infinity.
00:09:24.862 - 00:09:25.700, Speaker B: All right?
00:09:26.310 - 00:09:47.110, Speaker A: So note, if you have an algorithm with no swap regret, meaning it vanishes, the time average vanishes in the limit, then it also has no external regret. In the same sense, no swap regret means no external regret.
00:09:50.030 - 00:09:50.780, Speaker B: Okay?
00:09:54.110 - 00:10:28.440, Speaker A: And basically this definition is engineered to make the following theorem true. So last time external regret was about competing with all fixed actions. Coarse coil equilibria is just about not having any unilateral deviations that help you. Here we're talking about competing with all switching functions. Correlated equilibria is just talking about no switching function can help you. So the same theorem connecting the two holds today. So I'm not going to prove it again, but I'll state it informally because it is important, but the argument is exactly the same.
00:10:28.440 - 00:11:06.538, Speaker A: So theorem. So what's not at all obvious is whether any algorithms of this type exist in the world, okay? For all you know, there could be an impossibility result saying this is impossible. But suppose for the moment that there did exist such algorithms and all players of now a multiplayer game used them. So if all players use no swap regret algorithms, then the history of joint play is indeed converging to a core lit equilibrium.
00:11:06.634 - 00:11:07.280, Speaker B: Okay?
00:11:07.750 - 00:12:09.650, Speaker A: So then sigma. So again, this is the same as on Monday. We just pick it as a uniform distribution over the T outcomes is an approximate correlated equilibrium, okay? And the regret which is going to zero as T is going to infinity, the regret with respect to a given switching function in the learning algorithm just corresponds exactly to the error in the corresponding equilibrium condition for the exact same switching function. So as the regret is going to zero, the extent to which the error with respect to the correlate equilibrium conditions is also going to zero. Okay, good. So they do exist no swap regret algorithms. And what's great is we're going to be able to piggyback on the work that we already did on Monday.
00:12:09.650 - 00:13:10.726, Speaker A: So I'm not going to show you the first ever no swap regret algorithm. There's some cool ones, but I'm going to show you a more recent meaning, last decade reduction. And so we're just going to prove that if there exists no external regret algorithms, and I gave you one Monday multiplicative weights, if there exists a no regret external regret algorithm, then there exists a no swap regret algorithm. So it's going to be a black box reduction from minimizing external regret to minimizing swap regret. So this is by Blum and Mansour five box reduction on from no swap regrets to no external regrets. Okay, so a problem we don't currently know how to solve to a problem that we do currently know how to solve.
00:13:10.838 - 00:13:11.500, Speaker B: Okay.
00:13:15.230 - 00:13:49.378, Speaker A: All right, so in particular, there exists no regret algorithm, no swap regret algorithms. This reduction will be polynomial time. Multiplicative weights is polynomial time. So there's even computationally efficient such algorithms. So corollary there exists polytime no swap regret algorithms. And combining this corollary with this theorem, we conclude that corolla equilibria are also tractable in the same sense as coarse correlated equilibria.
00:13:49.474 - 00:13:49.734, Speaker B: Okay?
00:13:49.772 - 00:14:19.258, Speaker A: There exist computationally efficient learning algorithms. So basically those that are output by this theorem. And if all players use these computationally efficient no swap regret algorithms, then we converge the history of play converges to the set of correlated equilibria. So this is the big picture. So there's a few moving parts here, but you should know all the details for everything except for the proof of this main theorem.
00:14:19.354 - 00:14:19.854, Speaker B: Okay?
00:14:19.972 - 00:14:51.020, Speaker A: So I'm asking you to remember from Monday that we know that there exists no external regret algorithm, specifically multiplicative weights. I showed you the full description, the full proof, and then you should understand the connection. You should understand that no swap regret, if players use them as learning algorithms, lead to correlated equilibria in this sense and with the same proof as on Monday. So that's why we get this conclusion. If we can prove this main theorem, then we get tractability of correlated equilibria. Okay, so that's where we are now. Questions about that.
00:14:55.470 - 00:14:56.220, Speaker C: Change.
00:14:57.310 - 00:15:36.550, Speaker A: No, it's a fixed game. Yeah. So it's the same story with the number of actions. So we think of the number of players and the number of actions of each player, meaning the game, that's a fixed thing and it's being played over and over again. So like you might recall from multiplicative weights, we asked how long does it take to get down to regret epsilon? And the answer was log of the number of actions n over epsilon squared. But we think of n as fixed and then t growing large. And then the question is how large does t have to be as a function of the other parameters before you get a target regret? Okay, then let's do the reduction.
00:15:36.550 - 00:15:58.240, Speaker A: It's a cool reduction. It's something that you would hope would work. And so it's really cool that it does work. It does need one kind of great trick at the very end. But the high level idea I think is very natural. It's sort of what you would want, it's the proof that you would wish would work. So here's how it goes.
00:15:58.240 - 00:16:28.380, Speaker A: So like last time, n is the number of actions. And so remember, a hypothesis is that no external regret algorithms exist. Just think multiplicative weights, I'm going to need n copies of them. N totally separate instantiations of a no regret algorithm. So m one to m, n are different instantiations of if you like multiplicative weights, but it doesn't matter which one.
00:16:29.150 - 00:16:29.900, Speaker B: Okay.
00:16:32.830 - 00:17:22.650, Speaker A: Intuitively, the jth no regret algorithm MJ, you can think of it as being responsible for protecting against deviations from action j to other actions. Remember, a switching function specifies so what do we need to do to minimize swap regret? We need to say that our cost is at least as good as it would be if you applied any switching function to the actions that we took. And so a switching function takes as input and action and it outputs an action. So there's sort of n squared things going on. And the Jth no regard algorithm is in some sense responsible for paying attention to the switches from the Jth action. That's sort of an intuition to keep in mind. All right, so let me describe this reduction.
00:17:22.650 - 00:18:06.434, Speaker A: Now, remember how a neurograd algorithm works, what its input and what its output is. So something like multiplicative weights, it spits out a distribution over actions. Then you feed it in a new cost vector, it changes its internal state. Like, for example, it modifies the weights, it spits out a new distribution. You feed it a cost vector, it spits out a distribution, and so on. So that's how we're going to be interacting with these M one through MN black boxes. All right, so these are subroutines at our disposal and one up to MN.
00:18:06.434 - 00:18:54.626, Speaker A: So they minimize, they have no external regret. Now, at the same time, we're designing this master no swap regret algorithm, which is responsible for outputting distributions over actions and receiving cost vectors. So at the beginning of a time step, we ask all of our no regret algorithms, the external ones, for their opinion. What do you think we should do right now? Tell us. A distribution over actions. So we're going to get, at a given time step, t an opinion from the first no regret algorithm. This is a distribution over the actions.
00:18:54.626 - 00:19:39.750, Speaker A: This is what M one thinks we should do. M two will give us its opinion and so on. Okay, so Q one, T through Qnt. Each of these is a distribution over the N actions. We have N distributions over the N actions. So we receive distributions QT one to QTN from M one up to MN. The next step I am going to, until the end of the proof, leave under specified, under determined.
00:19:39.750 - 00:20:10.894, Speaker A: So the problem is, all these NeuroGate algorithms have their own opinion about what we should do about the distribution from which we should pick an action. And in general, these are not going to be the same distribution. They'll have different opinions. But we're designing this master no swap or good algorithm, hopefully. And so we're responsible for outputting a single distribution over actions. So we have to somehow take these N distributions that are different and compile them into some consensus distribution over actions, which we then report back. Okay, so it's not clear how to do that.
00:20:10.894 - 00:20:27.270, Speaker A: I mean, you could think about various ways, but it's not clear what the right way is. But that's the key trick, is the right compilation procedure. So for now, there's going to be some method by which we take these N distributions and compute a consensus distribution PT.
00:20:32.010 - 00:20:32.760, Speaker B: Okay.
00:20:35.370 - 00:21:24.822, Speaker A: So in our master algorithm, this is the central processing unit. If you like, it takes as input the N distributions and it will somehow figure out the distribution from which an action eventually gets chosen. So this pink box is the actual no swap or good algorithm. So its job is to spit out distributions and accept cost vectors. Okay? So at time one, it's just going to do something like pick an action totally at random. So then we find out a cost vector. And now, of course, each of our subroutine no regret algorithms M one through MN, they're also expecting a cost vector, right? That's how these guys work.
00:21:24.822 - 00:21:47.258, Speaker A: So M sub J, it's unaware of anything else in the world. It doesn't know it's part of this big machine. It just is expecting to get a cost vector, spit out a distribution, it's QT, get a new cost vector, and so on. So the next step in our reduction is to take the real cost vector from nature, from the adversary, and apportion it amongst these no regret algorithms, m one through MN.
00:21:47.354 - 00:21:48.000, Speaker B: Okay.
00:21:52.530 - 00:22:21.666, Speaker A: So after we say what our distribution of reactions is, PT, we receive a cost vector CT. And to the outside world, this is all that the outside world sees. We're outputting a distribution of reactions PT, and then it gives us a cost vector CT. Everything else is internal to the guts of our algorithm and how we're interacting with our subroutines. Okay, so what do we do now? We're going to take this cost vector CT, and we're going to feed this into all of the neural regret algorithms.
00:22:21.778 - 00:22:22.006, Speaker B: Okay?
00:22:22.028 - 00:22:39.870, Speaker A: Well, not quite. We're going to split this. We're going to portion it according to the probabilities that we chose our different actions. So if we picked a given action with 10% probability, then the corresponding no regret algorithm gets 10% of the cost vector. So give the cost vector.
00:22:44.690 - 00:22:45.018, Speaker B: Press.
00:22:45.044 - 00:22:59.750, Speaker A: Remember, PT is a distribution of reactions. Our consensus distribution maybe action J, we pick with 10%. So we just multiply the real cost vector by 10%, and we give that to the Jth no regret algorithm.
00:23:01.210 - 00:23:01.622, Speaker B: Okay?
00:23:01.676 - 00:23:29.710, Speaker A: So in this picture, the overall algorithm gets CT as a cost vector, and now it's going to feed in. So this is going to be PT one times CT that's going to feed in PT two times CT. PTN times CT.
00:23:30.610 - 00:23:31.214, Speaker B: Okay.
00:23:31.332 - 00:24:12.300, Speaker A: And that's the entire reduction. So you have these N no regret algorithms, each sort of responsible for switches from a corresponding action. The master algorithm is responsible for outputting a distribution and inputting a cost vector. So what's missing is to specify the interaction. So how do you connect the outside world to your n no regret algorithms? Internally and going outward, there's this magical box which has a consensus distribution PT from QT one through QN and then going backward. I've told you exactly how we split up the cost vector to go back just according to the probabilities that we play the different actions, question distribution over action. Right?
00:24:14.610 - 00:24:27.318, Speaker C: So when you're feeding back boxes, is it like each black box is kind of assigned to a single action?
00:24:27.514 - 00:24:47.270, Speaker A: Roughly. Roughly. That's the rough intuition. So you could think about MJ as sort of paying attention or sort of guarding against switching functions that do really well by switching action J to any other action, which is one of the things we need to worry about for swap regret.
00:24:54.010 - 00:25:00.460, Speaker C: Boxes operating under the assumption that the distribution they give you is the one to be used.
00:25:00.990 - 00:25:42.630, Speaker A: Well, they don't really know, frankly, right? I mean, if you really think about just the I O behavior of one of these neural grid algorithms, they literally just spit out a distribution and give you a cost vector and it just goes back and forth. And as a function of the cost vectors that you tell it, it's going to modify what it tells you to do accordingly. You're correct in saying that this is fictitious. So these are not the actual costs. We're, in effect, lying to each no regret algorithm about what the actual costs were. And it's not clear that's a good idea right now. But I guess one thing that does seem sort of natural, perhaps another thing you could try is just feeding in this entire cost vector to each of the neurograd algorithms.
00:25:42.630 - 00:26:43.850, Speaker A: But it also seems natural to kind of have the overall cost that you face to be reflected kind of in the superposition of the neurograd algorithms, to split up the cost vector between them. And then the probabilities that you were playing the various actions is a reasonable way to split that up. I'm not saying that's the only way you could do it, but I think it's reasonably natural as well. Okay, so let's just take stock of what we've got, and once we've actually figured out what we've got, we'll realize we just need one key trick and we'll be done. So this reduction works for a suitable implementation of how to compute PT from the QTS. All right? So let's take stock. All right, so first of all, what is the expected cost of our algorithm always going to do? Time average costs, as usual.
00:26:43.850 - 00:27:29.754, Speaker A: Okay, well, we incur cost every single day, days from T of one up to capital T, and then write expected cost. So there's a probability we play the various outcomes and the probability that we play the various outcomes on a given day T is by definition, this distribution PT. That's what our algorithm decides to do. So we look over all of the actions. We look at the probability that we play a given action, and if we play that action, we just look at what cost we get. So that's what our algorithm that's just its expected cost. Now, we want to say this is good.
00:27:29.754 - 00:28:12.790, Speaker A: We want to say this is small. We want to say it's small, though, remember, relative to some benchmark. Okay, so relative to the expected cost under some switching function delta, that's our competition. So let's try to understand what those are. So what would our expected cost have been with some particular switching function delta? Okay, well, it's almost the same thing. Let me call this expression two dot delta, because it's with respect to a particular delta. So if every time we were supposed to play an action K, we instead play delta of K, we just do the same sum.
00:28:12.790 - 00:28:36.700, Speaker A: We just say, well, for any what was the you know, look at the probability there are action sorry. That our algorithm played action I. We're doing the counterfactual where instead of playing I, what if we had played delta of i? So over here, we just look at the cost if instead of playing I, we played delta of i. So that's the.
