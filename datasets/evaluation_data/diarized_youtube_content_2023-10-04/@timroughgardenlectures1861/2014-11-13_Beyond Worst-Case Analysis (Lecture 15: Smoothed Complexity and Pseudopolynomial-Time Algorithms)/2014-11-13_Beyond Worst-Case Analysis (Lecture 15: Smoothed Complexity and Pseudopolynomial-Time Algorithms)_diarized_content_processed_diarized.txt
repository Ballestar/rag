00:00:00.250 - 00:00:28.162, Speaker A: Today we're going to be having our final lecture on smooth analysis. So this idea of problem instances which are sort of chosen worst case by an adversary but then subjected to a small perturbation by nature and smooth analysis. You know, historically and also the last couple lectures, the emphasis has been on you study some fixed algorithm of interest, for example, the simplex algorithm or local search or whatever and you want to understand that algorithm through the lens of smooth analysis.
00:00:28.162 - 00:00:50.922, Speaker A: Whereas in this lecture, rather than trying to understand algorithms, we're going to try to understand computational problems through the lens of smooth analysis. So we're going to see what we can learn about essentially computational complexity by seeking out algorithms with polynomial smooth complexity. The answer will actually be very crisp, at least within a certain natural class of problems.
00:00:50.922 - 00:01:02.160, Speaker A: The answer will essentially be if and only if. That is, the smooth complexity is polynomial if and only if the worst case complexity is pseudopolynomial. And I'll remind you what that means in a second.
00:01:02.160 - 00:01:18.120, Speaker A: Okay? But to make this a crisp if and only if statement, we should zoom in on some class of problems so we know what we're talking about. So let me start with that. So we're going to talk about binary optimization problems of a particular form.
00:01:18.120 - 00:01:29.610, Speaker A: And there's many, many canonical examples. For now, think of knapsack as being a canonical example. One of the things that we've talked about before that we want to generalized.
00:01:29.610 - 00:01:49.850, Speaker A: So there's going to be n variables and they're binary. So x one up to x n, these are either zero or one. For example, Xi could denote whether or not you're including item I in some knapsack and then some of the zero one vectors are feasible.
00:01:49.850 - 00:02:13.986, Speaker A: And we're going to handle that in a very abstract way. So we just have some feasible set f, which is some arbitrary subset of all possible zero one n vectors. For example, in the Napsack problem, f would denote the characteristic vectors of all of the subsets of items who fit in the Napsack.
00:02:13.986 - 00:02:30.960, Speaker A: That'd be one example. There are values again, just like in the Napsack problem. And the goal then abstractly is just to compute the feasible solution with the maximum possible value.
00:02:30.960 - 00:02:59.650, Speaker A: So compute x and F. And again, remember here x is an n bit vector maximizing v transpose x, also known as some over the n coordinates of VI times x i. Again, remember, xi is always going to be zero or one in this lecture, okay? So I'd like to make that as big as possible subject to the feasibility constraints.
00:02:59.650 - 00:03:11.260, Speaker A: Actually, minimization would be fine too. For everything we're going to say in this lecture. I'll stick with maximization for consistency, but you could even think of the V's as being negative if you like, and then it's basically a minimization problem.
00:03:13.230 - 00:03:13.980, Speaker B: Okay?
00:03:15.790 - 00:03:33.994, Speaker A: So like I said, one example I want you to keep in mind for concreteness is knapsack. But I mean, let's point out that you can encode many, many problems with this formalism. And in fact, the worst case complexity of the problem will vary depending on how you choose capital F, the feasible region.
00:03:33.994 - 00:03:54.290, Speaker A: There'll be some choices of capital F that lead to a much easier problem than knapsack. So, for example, I could define capital f to just be at most k of the coordinates of x, say at most ten of the coordinates of x are one that would be very easy to solve. You just sort by the vis and pick the top ten vis, and that would give you an optimal solution.
00:03:54.290 - 00:04:04.854, Speaker A: Or you can encode problems far harder than Napsack in this formalism. For example, capital F could denote the characteristic vectors of independent sets in some graph.
00:04:04.982 - 00:04:05.610, Speaker C: Okay?
00:04:05.760 - 00:04:19.120, Speaker A: So as you vary f, you get different problems, right? But so there are these end things, you choose them or not. But then there are also these global constraints encoded by capital f on which combinations are allowed and which are not. Any questions on that?
00:04:19.570 - 00:04:22.750, Speaker C: Is there a general approach to solving binary optimization problems?
00:04:22.900 - 00:04:38.902, Speaker A: Well, so it depends if you mean in polynomial time or not. I mean, if you don't mean in polynomial time, then integer programming would be one obvious approach. If you mean in polynomial time, well, then you get this plethora of different MP complete problems and as we know, different heuristics seem appropriate for different problems.
00:04:38.902 - 00:04:58.890, Speaker A: So you would not expect some useful meta algorithm covering all of them, okay? And in this lecture in particular, we're not going to be able to say, we're not going to say some sweeping positive result for all of these. But what we'll be able to say is among problems of this form, one good thing is true if and only if a different good thing is true.
00:04:58.960 - 00:04:59.290, Speaker C: Okay?
00:04:59.360 - 00:05:03.462, Speaker A: Smooth polynomial complexity if and only if pseudopolymail time, worst case complexity.
00:05:03.526 - 00:05:03.718, Speaker C: Okay?
00:05:03.744 - 00:05:14.740, Speaker A: So that statement will make within sort of the realm of these binary optimization problems. And of course, there'll be plenty of problems for which both of those are false, like independent set, for example.
00:05:15.990 - 00:05:16.740, Speaker B: Okay.
00:05:21.110 - 00:05:29.998, Speaker A: Let me give you another. I do want to point out that there will be implications for the main result of this lecture, not just for Napsack. So we'll also get results that we don't know.
00:05:29.998 - 00:05:49.450, Speaker A: So let me just sort of sketch a quick second application now, and I'll ask you to think about it a little bit more on homework number eight. So let me talk about a scheduling problem. So suppose you're given n jobs, which each has three parameters.
00:05:49.450 - 00:06:00.650, Speaker A: So depending on which homework problems you've done, you may have seen things like this. So let's say each job has a processing time PJ. That's how long it takes to execute.
00:06:00.650 - 00:06:10.498, Speaker A: We're thinking about just having a single machine, okay? So you're just trying to figure out how to sequence end jobs on a single machine. Each machine has some amount of time that it takes to run. Assume you even know what that is.
00:06:10.498 - 00:06:23.410, Speaker A: Each job has a deadline. So you're welcome to think of these as the homework assignments that you have at any given time. You have an estimate of how long it's going to take, you know when it's due.
00:06:23.410 - 00:06:41.050, Speaker A: Now, unfortunately, you're so over scheduled, you're actually not going to be able to complete all of the jobs by all of the deadlines. So you're going to have to make some hard decisions. And there's a cost CJ, which is the penalty you incur if you fail to complete job J by the time of deadline DJ.
00:06:41.050 - 00:06:55.310, Speaker A: And then the objective here is to minimize the total cost that you incur for all of the jobs that you fail to fail to meet by their respective deadlines. Okay, so you want to minimize the cost of the late jobs.
00:06:58.390 - 00:06:59.140, Speaker C: Right?
00:07:00.470 - 00:07:07.894, Speaker A: So I hope you all agree that's a well defined computational problem may not be obvious how to solve it, but I hope it's clear what it is, what's it asking you to do.
00:07:08.012 - 00:07:08.680, Speaker C: Okay?
00:07:09.290 - 00:07:21.926, Speaker A: So on the homework, on homework eight, I'll ask you to give a pseudopolynomial time algorithm for this problem. So a dynamic programming based algorithm, very much sort of in the spirit of what you know from knapsack problems.
00:07:22.028 - 00:07:22.630, Speaker C: Okay?
00:07:22.780 - 00:07:36.720, Speaker A: And the reason I'm going to ask you to do that is because we're going to, in this lecture, have a positive result for all problems that have pseudo polynomial time algorithms. We already know Napsack does. This is just meant to show you a second application of sort of the meta theorem, the general theorem we're going to have today.
00:07:36.720 - 00:07:44.370, Speaker A: So that's all I was planning on saying about this. But here's just again, proof of concepts. There's multiple things we're talking about at once.
00:07:44.370 - 00:07:50.690, Speaker A: Probably the simplest thing for you to do is just keep knapsack as the canonical example in your mind as we go through the lecture.
00:07:53.510 - 00:07:54.260, Speaker B: Okay?
00:07:55.110 - 00:08:11.878, Speaker A: All right, so homework, pseudopolynomial time. Okay, so it is NP hard, but it can be solved in pseudopynomial time. All right, so back to the question about this lecture.
00:08:11.878 - 00:08:35.490, Speaker A: Which problems admit smooth polynomial time algorithms? Specifically, which binary optimization problems admit them? So as usual, we have to be precise about what we mean by smooth complexity. And it's going to mean the same thing as in the last two lectures. So we're going to have this very pleasingly kind of agnostic or general notion of smooth instances.
00:08:35.490 - 00:09:16.366, Speaker A: So each VI, these are pretty much the only numbers, right, in this abstract binary optimization problem. So if something's going to be perturbed, at least the way I've written it so far, it seems like it's got to be the vis, right? So each VI is drawn independently from a density function f sub I from zero one to zero one over sigma. Okay, so this is exactly what we were doing last lecture when we talked about pareto curves of Napsax is basically what we were doing when we were talking about Tsp in the plane as well.
00:09:16.366 - 00:09:29.326, Speaker A: So notice that I'm scaling the values, in effect, to lie between zero and one. It's more or less without loss of generality. And then again, the assumption is that the distributions are not too spiky.
00:09:29.326 - 00:09:35.390, Speaker A: The density is everywhere bounded above by one over sigma. So the area of support in particular has to be at least sigma.
00:09:35.470 - 00:09:35.726, Speaker C: Okay?
00:09:35.768 - 00:09:39.190, Speaker A: So it's at least a little bit spread out everywhere.
00:09:40.010 - 00:09:40.760, Speaker B: Okay?
00:09:42.170 - 00:10:04.222, Speaker A: And then, as usual, what are we going to mean by having polynomial smooth complexity? We mean there exists an algorithm with smooth polynomial time running time, meaning its running time is polynomial in the input size n and in one over sigma. Okay, so we mean the same thing as in the last couple of lectures. So that's what we're asking.
00:10:04.222 - 00:10:16.480, Speaker A: Which binary optimization problems admits an algorithm whose expected running time where the expectation is over the perturbation, whose expected running time is polynomial and n and one over sigma? That's the formal question.
00:10:18.210 - 00:10:19.120, Speaker B: All right?
00:10:21.650 - 00:10:36.770, Speaker A: And so, as I said, there's a very satisfying answer. It to that question on the left if and only if solvable in randomized pseudopolynomial.
