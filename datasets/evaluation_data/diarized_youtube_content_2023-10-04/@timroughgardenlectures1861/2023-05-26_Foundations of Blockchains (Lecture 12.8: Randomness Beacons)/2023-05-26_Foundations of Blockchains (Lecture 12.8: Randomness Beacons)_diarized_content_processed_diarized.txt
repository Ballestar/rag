00:00:00.170 - 00:00:22.750, Speaker A: So in this video, we'll continue to explore approaches to proof of stake random sampling. Remember the problem we're given this list of active, validators, presumably kept track of by some designated staking contract. We have a list of N public keys, each of those with an associated stake amount. And what we're trying to do is sample one of those public keys with probability proportional to stake.
00:00:23.810 - 00:00:25.778, Speaker B: So in the last video we explored.
00:00:25.794 - 00:01:09.662, Speaker A: A sort of quick and dirty solution which basically solves a relaxed version of this problem. We never in the last video said how to just pick a single random sample from the public keys in the list. But we said, oh, okay, let's just do batches at once. Let's just sort of schedule 10,000 liters at a time. And just each time we do that, each time we sort of schedule a new batch of 10,000 liters, we'll make sure that the various public keys are represented proportionally to their staking amounts. So what we'd like next is a better solution that really only samples public keys on a need to know basis. So at a moment in time, like when a round begins, when you need to pick a leader, at that point you're going to invoke this procedure, it's going to give you back one random sample chosen according to the state distribution.
00:01:09.662 - 00:01:45.070, Speaker A: That's actually what we really want. As we discussed, an issue with that sort of batching solution is you wind up knowing leaders well in advance, which opens the door to sort of bribery or coercion or denial of service attacks. And so really what we want to explore next is can we have a sort of stronger solution which really just gives you samples from the stake distribution on a need to know basis. So, like when a new round begins and you need to have some public key, who's going to be the block proposal proposer for that round? At that point, we want a procedure you can invoke that gives you back a single random sample chosen according to the stake distribution.
00:01:45.970 - 00:01:46.958, Speaker B: So what I want to do in.
00:01:46.964 - 00:02:20.070, Speaker A: This video is think through that problem under an unreasonably strong assumption, an assumption that the Protocol has access to something known as a randomness beacon. In other words, the Protocol has access to sort of perfect randomness just falling from the sky viewable to everybody. So slightly more formally, we can think of a randomness beacon as a box which periodically periodically, I mean, once a second, once every 10 seconds, something like that. A box that periodically emits perfect randomness.
00:02:21.130 - 00:02:23.146, Speaker B: So for example, maybe each output of.
00:02:23.168 - 00:02:37.310, Speaker A: A randomness beacon is a 256 bit string, a string of 256 zeros and ones. And for an ideal randomness beacon, that's going to be just equally likely to be any of the two to the 256 256 bit strings.
00:02:38.290 - 00:02:40.094, Speaker B: And again, part of that definition is.
00:02:40.132 - 00:02:49.700, Speaker A: This randomness is just common knowledge. So it's almost like a plane flies, fly in the sky with the 256 bit number trailing behind it and everybody can see it.
00:02:50.390 - 00:02:50.818, Speaker B: All right?
00:02:50.824 - 00:03:10.860, Speaker A: So the bad news is it's not clear where you're going to get an ideal randomness beacon from. Let's actually set that issue aside for this video and let's think through suppose we actually had access to such an ideal randomness beacon. How would we use it to carry out proof of stake random sampling? And as we'll see, actually, it wouldn't be that hard at all.
00:03:11.630 - 00:03:13.034, Speaker B: So let me tell you one way.
00:03:13.072 - 00:03:20.170, Speaker A: You could take each of these random samples, each R sub t, and turn that into a sample from the stake distribution.
00:03:23.290 - 00:03:24.986, Speaker B: So at a given time, T the.
00:03:25.008 - 00:03:50.320, Speaker A: Beacon by assumption is going to emit some uniformly random output r sub t. Maybe it's like a 256 bit string. So the first step is just to interpret R sub t as a real number between zero and one. So for example, if RT is a 256 bit string, you can just interpret that as an integer on the denominator. You put two to the 256 boom, you've mapped outputs to the interval zero one.
00:03:51.010 - 00:03:52.366, Speaker B: So we're going to interpret R sub.
00:03:52.388 - 00:04:08.440, Speaker A: T as like a uniformly random number from the interval zero comma one. I'm taking a little bit of liberties there, right? I mean, R sub t isn't really a real number. It's one of a finite number of possible outputs of the randomness beacon. But for all practical purposes, we can think of r sub t as just a uniformly random real number from the interval zero comma one.
00:04:09.450 - 00:04:10.978, Speaker B: And then the second step is we're.
00:04:10.994 - 00:04:49.730, Speaker A: Just going to partition the same interval zero comma one into subintervals corresponding to the public keys of the active validators. The links of those subintervals are going to be proportional to the stakes of the various active validators. So for example, the first interval subinterval is going to correspond to the first public key p sub k one. The second interval is going to correspond to p sub k two and so on. The length of these subintervals are going to be proportional to stakes. So let's use capital q to denote the normalizing factor. Capital Q is just the total stake across all of the active validators.
00:04:55.000 - 00:04:56.196, Speaker B: So then the length of the first.
00:04:56.218 - 00:05:03.370, Speaker A: Subinterval is just going to be little q one over big Q, the length of the second subinterval is going to be little Q two over capital Q, and so on.
00:05:07.030 - 00:05:08.418, Speaker B: And now you can imagine how the.
00:05:08.424 - 00:05:35.020, Speaker A: Random sampling procedure is defined. So R sub t, that's just a dot on this line. It's sort of a uniformly random number in one, or we're thinking of it that way. So it's going to land in exactly one of these buckets, a bucket corresponding to some public key PKI. And that will be the public key. That's the output of the random sampling procedure. So for example, if this gray x.
00:05:35.090 - 00:05:38.984, Speaker B: Denotes r sub t. In that case.
00:05:39.042 - 00:05:52.530, Speaker A: The output of the random sampling procedure would be PK sub two would be the second public key. If you prefer a little more algebra, we can write this equivalently in the following way.
00:05:55.980 - 00:05:58.008, Speaker B: Right, so the left endpoint of one.
00:05:58.014 - 00:06:20.580, Speaker A: Of these subintervals, that's just going to be the sum of the interval links that come before it. So if we're talking about the ith interval, that's going to be the sum of the links of the first I minus one intervals, and then the right endpoint of the ith subinterval is just going to be the same thing, but then with the length of the ith subinterval added, namely q sub I over capital Q.
00:06:24.290 - 00:06:26.778, Speaker B: And this procedure does indeed implement the.
00:06:26.804 - 00:07:00.810, Speaker A: Property that we wanted. Right? So r sub t, it's uniformly at random from zero one, or at least close enough. And so that means the probability that r sub t lies in a given subinterval is just going to be the length of that sub interval. So given that the sub intervals are defined to be proportional to the stakes, the selection probabilities are themselves going to be proportional to stakes, which is exactly what we wanted. That was that property star from our original proof of stake video. All right, so should we just declare victory? It seems like we got the property that we wanted. We have our proof of stake random sampling procedure.
00:07:00.810 - 00:07:25.970, Speaker A: Well, not quite. Right. We're not done for a couple of reasons. The biggest reason, which I'm sure you're wondering about, is like, well, where would we get an ideal randomness beacon from? And honestly, we don't have access to an ideal randomness beacon. We're going to need to somehow approximate it in a pseudorandom way. And that's going to be important for literally any sort of practical deployment of proof of stake random sampling. So that's going to be a big issue and we will talk about it.
00:07:25.970 - 00:07:53.946, Speaker A: But actually, there's a second issue that I want to talk about first. The second, much more subtle challenge. It's kind of a variant of the predictability issues that we talked about in the context of weighted round robin in the last video. Now, mind you, we've definitely made some progress. So just to remind you the situation in the weighted round robin solution. So there we had these epics. An epic would correspond to 10,000 rounds, 10,000 leaders, all chosen at the same time in a batch.
00:07:53.946 - 00:08:45.550, Speaker A: And basically at the beginning of an epic, everybody in the world would find out at the same time the information, namely the leaders for the rest of the epic. And we talked in the last video why that sort of predictability of future leaders is a prop. And again, an epic might be it's probably at least minutes long, it might be hours, it might even be days. And if you know sort of leaders minutes or hours or days in advance, you really run the risk of having bad actors trying to mess around with leaders. So, for example, they could try to coerce a leader into producing a block that the bad actor is interested in, or producing no block at all. They could try to bribe the validator into producing, for example, no block at all. Or they could just launch a denial of service attack, basically forcing that leader to skip its turn because it's basically disconnected from the rest of the network.
00:08:46.530 - 00:08:48.026, Speaker B: And if you think about it, there's.
00:08:48.058 - 00:09:35.438, Speaker A: Still kind of a version of that same issue plaguing the solution on this slide. So it's true we're only determining leaders on a need to know basis, kind of one by one. So definitely nobody's learning what the next 10,000 leaders are. But when a leader is chosen, notice everybody, both the leader itself and anybody watching, everybody learns the leader at exactly the same time, right? The output of the random beacon R sub T, it falls from the sky at the moment. It falls from the sky and is viewable by everybody. Everybody knows who the leader is. So why is there still kind of a predictability issue? Well, the leader themselves just found out that they're the leader and the protocol has to give them at least a little bit of time to assemble a block, right? At least, I don't know, tens or hundreds of milliseconds or something like that.
00:09:35.438 - 00:10:22.502, Speaker A: And so that means there's a small window there in which a bad actor could try to manipulate the block that leader puts together, or try to prevent it, for example, in a denial of service attack from being able to broadcast any block whatsoever. So, tens of milliseconds, hundreds of milliseconds, right? That's a pretty small amount of time. But things happen fast on the internet, so it is definitely a potential problem. So that is an issue. And it's an issue that, if you think about it, did not come up when we talked about proof of work and Nakamoto Consensus, right? So in that context, being elected as a leader basically means you're the first one to solve one of these hard crypto puzzles. You are checking yourself whether you've solved the crypto puzzles before you broadcast any information to anybody else. So you find out you're the winner, you've got your block.
00:10:22.502 - 00:10:43.730, Speaker A: Other nodes find out you're the leader only when they hear about the block that you've proposed. So the question is, can we have that same strong kind of privacy property or secrecy property in a proof of stake context where a leader, someone who's tasked with assembling a block, nobody knows that they're the one assembling the block until they actually hear about the block from that node.
00:10:52.260 - 00:10:54.416, Speaker B: So that's definitely an interesting question, kind.
00:10:54.438 - 00:11:58.320, Speaker A: Of from a computer science perspective, can we have a better version of proof of stake sampling with this additional secrecy property? You could ask, do we really need this? Do we really need to work hard just to take away this kind of window of maybe hundreds of milliseconds for denial of service and other kinds of attacks. And once again, different proof of stake blockchain protocols have very different philosophies, very different answers to that question. As mentioned, there are some protocols that actually view predictability of leaders as to some extent a feature enabling honest, validators to better coordinate with each other. There's other protocols that my interpretation is they'll say, well, this would be a property that would be nice to have. Maybe we'll have it at some future point in the roadmap, but it's not really kind of essential to the success of the project. And then there's other proof of stake blockchain protocols, major ones, that are deployed out there in the wild, which have taken the secrecy property very seriously and have worked hard to establish it. And we're going to be covering in the next couple of videos, the key tool for establishing that secrecy property, namely verifiable random functions, or VRFs.
00:11:58.320 - 00:12:22.600, Speaker A: So we're first going to address the second question how do we get secrecy while still having this unreasonable assumption of access to an ideal randomness beacon? And then once we understand that part, how we would get secrecy, we'll go back and turn our attention to how can we have a practical approximation of this ideal randomness beacon? So that's what's coming up. I'll see you in the next video to start talking about VRFs. Bye.
