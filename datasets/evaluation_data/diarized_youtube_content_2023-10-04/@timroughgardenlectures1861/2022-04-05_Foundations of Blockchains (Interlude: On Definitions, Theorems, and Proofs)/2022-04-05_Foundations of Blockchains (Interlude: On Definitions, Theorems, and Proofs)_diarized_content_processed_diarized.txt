00:00:00.410 - 00:00:43.318, Speaker A: Okay. And welcome to this interlude video. In this lecture series on the foundations of blockchains. In this interlude video, we'll be discussing who cares about all this math, who cares about definitions, theorems and proofs. And spoiler alert, the message of the video is going to be, you should care, really, all over computer science, the best practitioners, the best builders tend to be be very aware of the foundations of that area of computer science. Take, I don't know, like machine learning, for an example. The top machine learning practitioners have certainly mastered fundamentals like gradient descent, the backpropagation algorithm, the idea of generalization error, the notion of sample complexity.
00:00:43.318 - 00:01:45.806, Speaker A: All of the top practitioners are going to have familiarity with all of those concepts. And so too, should the best designers of blockchain protocols be aware of the relevant fundamentals. So, for example, the power and limitations of consensus protocols, what we're really sort of in the thick of right now, but also things like basic cryptographic, primitives basic game theory and incentives and so on. And depending on how much sort of time you've spent in the blockchain world, perhaps you've had the experience of coming across some people who don't really seem to know what they're talking about and other people who seem to really, really know what they're talking about. So members of that latter group inevitably have mastered the relevant fundamentals. So wouldn't you like to join them? So a few comments about this video before we get into it. So thus far to this point in lectures two through seven, we were a very heads down understanding exactly what byzantine fault tolerant consensus protocols can and cannot do.
00:01:45.806 - 00:02:45.982, Speaker A: And we learned an amazing number of things. For example, that magical threshold of 33% that you see mentioned in so many, for example, blockchain white papers, we learned exactly mathematically where that threshold comes from. Back in lecture six. And after this little breather, after this interlude video in lecture eight, we're going to go heads down again. We're going to study a different genre of consensus protocols, longest chain protocols, which are super relevant for blockchains. For example, Bitcoin famously, is a longest chain protocol, and we want to again understand in some detail the mathematical properties of longest chain protocols. But before doing that, before sort of rushing into another set of theorems and proofs, I thought it was worth spending a little time taking a step back and kind of reminding ourselves, why is it that we're doing what we're doing? And I should say that I've positioned this interlude in between lectures seven and eight in the lecture series, and I will be referring back to lectures two through seven just to make examples of some of my points.
00:02:45.982 - 00:03:28.586, Speaker A: But to be honest, you can kind of watch this video anywhere, including at sort of the beginning of the series, if you prefer. This video is optional. I guess in some sense, all videos are optional, right? They're just free videos hanging out on the web. What I mean is you could skip this interlude if you want, without any sort of loss of continuity in the rest of the lectures in particular. If you feel like this is going to be preaching to the converted, definitely feel free to skip and just sort of dive back into the longest chain protocols we'll discuss in lecture eight. And I do promise that now that you've finished this sort of boot camp on classical consensus, things will be getting quite a bit more blockchain specific from here for those of you who are sort of hungry for that. So, like I said, we're going to be moving on to longest chain protocols.
00:03:28.586 - 00:03:54.754, Speaker A: We'll talk about them first in a permissioned setting, but then we'll talk about permissionless consensus protocols. We'll talk about civil resistance strategies like proof of work and proof of stake. We'll talk about transaction fee mechanisms. We'll talk about sort of the economics of blockchain security. We'll do deep dives into bitcoin and ethereum in particular. So lots of very cool blockchain specific stuff coming up in the next, I don't know, ten to twelve lectures. Final comment.
00:03:54.754 - 00:04:36.454, Speaker A: It's sort of more of a disclosure. So I identify primarily as a computer scientist, but within computer science I identify very strongly as a theoretician. So this video may have a little bit of a justify your existence kind of vibe going for it. Now, to be clear, I don't think everybody should be a theoretician. In fact, to be honest, you have to be a little bit strange to become a theoretician. I'm not trying to argue that people who do theoretical work are better or worse than people, more on the engineering or more on the hacking side. The only point I'm trying to make is that whoever you are, I think it's important to appreciate theoretical work and how it can contribute to the advancement of science and technology, and in particular to blockchain technology.
00:04:36.454 - 00:05:21.374, Speaker A: So that's going to be my pitch in this video. So in general, across computer science, not just talking about blockchains, when you study the foundations of some area of computer science, it's always a really interesting blend of mathematics and engineering. And so it's actually really fun if you enjoy both of those two things. Sometimes there's a lot of math, but it's pretty much never math for math's sake. It's always math in the service of trying to sort of better understand and hopefully solve some really hard computer science problems. Now, unfortunately, sometimes the math is pretty hard and it can be frustrating and time consuming to understand it properly. You may have already had that experience thus far in this lecture series.
00:05:21.374 - 00:06:27.970, Speaker A: You might have it again in later lectures in this lecture series. I certainly have those emotions all the time in my own research, where the mathematics that I have to grapple with at least, I find quite difficult. So it's really kind of part of the price of doing business for understanding things on a deep level, much the way that no one can really run a sort of six minute mile or even a seven minute mile without sort of a fair amount of work involved. So too, to really understand the fundamental drivers of what's possible and what's impossible in computer science, there's really no excuse for taking the time to think it through carefully. So just as I imagine it's extremely satisfying to say, run a six minute mile, not that I know I've never run one, but so too, it's very satisfying when you get through some mathematics that you found quite difficult, get to the other side, look back and realize sort of how much better you understand things. So it's sort of very hard work along the way, but you do wind up feeling like you've gained some IQ points in the process, which is kind of priceless. All right, so let's talk a little more concretely.
00:06:27.970 - 00:07:18.778, Speaker A: What can mathematics, for example, definitions, theorems and proofs, what can it do for you? Let's say you don't want to be a theoretician. Maybe you're not even sort of building stuff. You're really just trying to sort of understand the space and sort of the pros and cons of different technologies in it. Well, for example, when you're assessing kind of multiple solutions to the same problem, I would encourage you to prefer solutions that come backed by mathematical reasoning. So ideally by a proof that the protocol or the algorithm or the software does what it's supposed to be doing. Always prefer solutions backed by math, backed by proofs over solutions that appear to be backed only by intuition and or hope or failing. Kind of an airtight proof that the solution is always going to do exactly what you want.
00:07:18.778 - 00:08:14.406, Speaker A: At least a proof that under certain assumptions, precisely articulated assumptions, the solution will have the properties that you're looking for. So for example, maybe you're using digital signatures and you do have to make the mathematical assumption that the discrete logarithm problem is hard. Fine, that's an assumption at least it's very clear what the assumption is that you need for your solution to work correctly. Similarly, maybe you have an assumption on what fraction of the nodes running your protocol are Byzantine versus honest. But again, it would be a clearly articulated assumption so that you know that if the solution fails in practice, it must be because one of the assumptions did not hold. I would also encourage you to be very skeptical, at least initially, of any solution presented to you that does not come with mathematical reasoning about why it has the properties it's supposed to have. For example, correctness.
00:08:14.406 - 00:08:54.230, Speaker A: In fact, any project where there is no such mathematical reasoning backing it up, I think your working hypothesis should be that it's probably not correct, that it's probably broken in some way. So a couple comments on this. So first of all this rule of thumb is not always correct. I will freely concede sometimes solutions get rolled out that initially there's no mathematical understanding of to what extent they work, and then sometimes years later, people figure out mathematically why actually it was a really good idea why it works really well. That does happen. I don't think it happens as often as alleged solutions with no backing actually being flawed. I think that's the much more common case.
00:08:54.230 - 00:09:39.538, Speaker A: But sometimes the solution does come before any kind of proof that it has the desired properties. A second point I would freely concede is the word broken. That can have a lot of meanings, right? You can be broken to different levels of severity, right? To be honest, it is actually often the case that solutions with no mathematical reasoning behind them are badly broken and just have to be thrown out and retired. That is actually quite common. It's also, I think, reasonably common that something sort of broken in a way that doesn't preclude its practical use. I think a good example maybe would be the Border Gateway protocol. So the BGP routing protocol, this is what governs sort of the routes along which data packets travel in the internet.
00:09:39.538 - 00:10:48.826, Speaker A: And at least certainly by any strict measure, the BGP routing protocol is broken. You can't prove nice mathematical properties about it, and that's because it doesn't possess nice mathematical properties, it can get confused, it can use terrible routes. And that actually does happen on bad days, in practice, on good days, and most days are good days, the BGP routing protocol winds up working reasonably well. So it's not to say that you can't be sort of super important even if you're sort of broken in this strict sense, but it's still really important to know the difference between something which is just sort of airtight. Guaranteed to work the way you want it to work or something else which is not so guaranteed and maybe at least on good days, will get you the kind of solution that you want. It's actually a pretty big difference if you think about it. I think this is a pretty good rule of thumb, frankly, across all of computer science, right? So like when I teach algorithms to undergraduates, I introduce them to the idea of a greedy algorithm, which are often sort of very simple kind of three line algorithms that sort of make a sequence of decisions in what seems like the kind of obvious way to solve some problem.
00:10:48.826 - 00:11:28.550, Speaker A: And the first thing I tell students when they study greedy algorithms is like your greedy algorithm is wrong. You believe in your heart of hearts because you came up with it, that it must be correct, but almost surely it's wrong. Sometimes you get lucky, like the minimum spanning tree problem kind of any algorithm you write, greedy algorithm you write down winds up being correct. But that's totally the exception that proves the rule greedy algorithms. They're often useful in practice, but they're almost always, at least in some instances, incorrect. So that's with algorithms. And I actually think this rule of thumb is even more important when you're talking about distributed computing, when you're talking about protocols running on potentially thousands or more nodes scattered all over the globe.
00:11:28.550 - 00:12:26.890, Speaker A: So I hope that at this point you actually have kind of like a quite visceral appreciation for this point, having just in the last lecture, in lecture seven, gone through the tenderman protocol as well as its proofs that it satisfies liveness and consistency. And as we discussed in that lecture, it's the kind of thing where the protocol, it must have been written kind of simultaneously with writing the proofs of correctness, writing the proof of liveness, writing the proof of consistency, because really lots of sort of implementation details in the protocol, they're there exactly so that you get these guarantees. If you mess around with those details a little bit, you'll probably break liveness and or consistency. So just tiny changes in the protocol's code can make a big difference about what kind of mathematical properties it has. And that makes distributed protocols extremely difficult to reason about. And it means if you don't have a proof, there probably isn't a proof to be had. You probably actually have a flaw in that protocol.
00:12:26.890 - 00:13:31.760, Speaker A: I think this rule of thumb has really been borne out in practice. I think if you look at sort of like the history of failures in layer one blockchain protocols over the years, I think it really provides a lot of support for this rule of thumb. For this point, pretty much every major sort of blockchain, every major layer one you might have ever heard of has at some point basically been broken, had some problem with its consensus protocol. Now it's important to distinguish between two things, right? So you can have a flaw in the design of the protocol or you can have a flaw then in the implementation of that design. So there are cases where the intended design itself was flawed and for example, an adversary could stall the blockchain and sort of disrupt liveness more common. The original design was actually correct, but there was some subtle bug in the implementation which meant that a consensus protocol got implemented which was slightly different from the one that was intended. And so not only did it make it wrong, but it made it wrong in a way that actually at some point that blockchain actually had to restart something like that.
00:13:31.760 - 00:14:50.506, Speaker A: So one point worth discussing, and this is kind of related to that sort of BGP routing protocol discussion we had earlier, is you might say, is it really fair to so directly equate having a mathematical proof with operating correctly in practice? Couldn't it be possible that technically a protocol isn't correct? Like there's some sort of crazy scenario under which something goes wrong, but that crazy scenario is never going to be going to happen. So it'll be as if the protocol actually was correct because the incorrectness sort of never shows up in reality. So this is a good question. And actually if we go back to the traditional theory of algorithms, I think there it can actually be a fairly tricky question. Like is it really important that your algorithm has a proof? I still think it's a good rule of thumb in algorithms, but there's more exceptions. Like, for example, famously, the simplex method for linear programming in principle runs in exponential time in the worst case, but in practice always it runs super quickly. So maybe you don't care that there's no proof of an efficient runtime for the simplex method if it simply just always does exactly what you want really quickly.
00:14:50.506 - 00:15:40.010, Speaker A: Similarly, if it's an algorithm you're just using over and over again and like exactly the same types of inputs, right? So maybe it's just like Google running some algorithm every time there's a search query, it's being run a billion times a day. They've been happy with the performance empirically so far. There's no reason to think it's not going to continue to perform well in the future, given that it's going to be solving exactly the same types of problems. So in algorithms, I would still stand by this rule of thumb, but I would concede there's sort of more cases in which maybe there are exceptions. Distributed Protocols I still think this rule of thumb is a really good one to have. As we've seen, distributed protocols can be flawed in extremely subtle ways. Moreover, there isn't really a strong analog of like in algorithms where you can sort of run it on a bunch of representative data up front.
00:15:40.010 - 00:16:50.990, Speaker A: Obviously you can roll out a distributed protocol on a testnet, but in practice it's difficult to stress test a distributed protocol on a testnet to the extent that it would be stress tested on main net, at least assuming it's a high value project. So point being, if you have a distributed protocol that doesn't have proofs backing up what it's supposed to be doing, even if it survives on a test net, I wouldn't have so much confidence once it hits main net just because it's so hard to do the stress testing on a testnet. And then even if it operates, let's say for a year with no problems on main net, I still would not have very high confidence in it. It's not clear you can use past performance to sort of predict future behavior of the protocol. One is just these protocols are operating over the internet, the patterns of delay can be changing all the time. Who knows what they're going to look like a year down the line. Also, obviously, maybe there weren't any sort of particularly motivated attackers in year one of the protocol's life, but maybe it grew and it got more valuable all of a sudden year two people start attacking it, that means things could turn out completely differently.
00:16:50.990 - 00:17:44.020, Speaker A: Now, granted, if there was some protocol and it had been running for 20 years and it had been securing like $100 billion of value for those 20 years, you would start to wonder if there was simply no way to attack the protocol. You would think probably someone would have done it by then. That's okay reasoning. To be honest, it's not a lot unlike us assuming the discrete logarithm is a computationally hard problem, right? Really? That's based purely on that. It seems like if it was easy, someone would have done it in the past 30 years. So you could similarly just basically assume that some blockchain that's been around for a long time probably is guaranteed to work. But up until that point, up until there's years and years of evidence in really high value, high stress situations, I would not feel very comfortable, in the absence of mathematical backing, of why that protocol supposedly does its job.
00:17:44.020 - 00:18:46.650, Speaker A: Now, let me drill down further into what I'm going to somewhat cheekily call the three pillars of math, by which I mean definitions, theorems, and proofs. To be clear, I just totally made up this phrase just now. I don't think anyone talks about the three pillars of math. If anyone was talking about it, they'd probably be referring to three areas of mathematics, like the three pillars of math or analysis, algebra, and geometry. And that's not what I'm talking about. I'm really trying to tease out three different aspects of mathematical language, how we talk about mathematics. They obviously work together closely as a team, but each has its own sorts, its own purposes, not just for the person doing the math, not just for the theoretician, but also for the person who wants to use the math, for the people who are building things informed by the math.
00:18:46.650 - 00:19:39.802, Speaker A: So first of all, definitions, which frankly, I think are taken for granted a little bit more than they should, without the right definitions, you don't get interesting theorems, you don't get clean proofs. Definitions are actually sort of a prerequisite for a mathematical theory to really mature in a nice way. But in any case, for us, probably the most important purpose that mathematical definitions serve for us is they allow us to precisely articulate what we're talking about and communicate our ideas to others. Notice in the boot camp on permission consensus that we just completed, we were very careful with our definitions. Probably everyone has an intuitive sense of what consensus means. But we did not stop there. We really said no, we're looking at, for example, the state machine replication problem or for example, the Byzantine broadcast problem.
00:19:39.802 - 00:20:54.338, Speaker A: We said exactly what those problems are. We said exactly what a protocol was in a mathematical sense, and we said exactly what we meant by correctness of a protocol for one of those consensus problems. So without definitions, there'd really be no hope of speaking clearly about whether or not a proposed solution is a good one, or for example, comparing the relative merits of two competing solutions to the same problem. They really are the bedrock of the language that we use to discuss the problems that we're solving and the quality of the solutions that have been proposed. So it's very common for people to arrange debates between two sort of technical experts, maybe sort of the founder of sort of two different competing projects, to have sort of a debate, maybe on a podcast, maybe at a conference or on a video or whatever. And if you've listened to enough of these, you'll notice that there's a bit of a dichotomy, right? Sometimes there's kind of a very sort of deep, interesting technical discussion where it seems like there's a lot of information being conveyed between the two. And sometimes you just get two people that seem like they're talking past each other completely, right? They're sort of arguing about their technology, but the one person's points don't seem to rebut the other person's points and it's hard to know what to take away from it.
00:20:54.338 - 00:21:52.208, Speaker A: And honestly, the biggest reason you have these two different types is it boils down to whether they are agreed upon definitions or not. When there are agreed upon definitions, you generally get a really interesting nuanced debate. If people aren't actually agreeing on the definitions, that's when they tend to be talking past each other. So again, it's really a common language which is what we need to make the kind of progress that we want. And a great example of this would be the by now extremely highly overloaded word decentralization. I mean, we may all have a sort of very informal sense of what we mean by decentralization, right? Way back in lecture one, I said I want you to think of blockchains as like a big computer that lives up in the sky and that isn't owned by anybody, or rather is owned by tons of people. And you can be one of them if you want.
00:21:52.208 - 00:22:40.850, Speaker A: So intuitively, decentralization refers to the fact that this kind of abstract computer that lives in the sky has sort of a million different owners and operators, that's informally what we mean by decentralization. But you've probably noticed a lot of arguments over which blockchain is more decentralized than another one. And there's been frankly, some pretty bitter battles about that across different projects. And usually it just boils down to different communities sort of using different definitions of decentralization. Obviously, if you have one definition in mind, you're going to design your protocol in a particular way. If you have a different definition in mind, you're going to design your protocol in some other way. So you wind up getting these disagreements over kind of the tech, when really, actually, it's just that they made different decisions about what decentralization meant to them before they sort of mapped out what the rest of the design was going to look like.
00:22:40.850 - 00:23:46.076, Speaker A: So that's my basic pitch around definition. So even if a definition, even if you never see it get used in a theorem statement, even if you never see it gets used in the proof, it still provides value in giving us this precise language to discuss these hard computer science problems that we're all trying to solve. Moving on to theorems, meaning formal mathematical statements that are asserted to be true and hopefully backed up by a proof and therefore actually true for theorems, let me differentiate between possibility and impossibility results. We've already seen several examples of each. So for example, our most recent example of a possibility result is for the tenement protocol. We just saw that in lecture seven. So, remember, we had this theorem statement which said that assuming PKI, that you've had this distribution of everyone's public keys up front, assuming the partially synchronous model meaning attacks eventually end, and assuming that less than a third of the nodes were Byzantine, we proved that that protocol satisfied two properties consistency, always, and then eventual liveness.
00:23:46.076 - 00:24:32.790, Speaker A: Liveness after the global stabilization time. And that's a kind of canonical type of possibility result. It articulates assumptions under which a specific solution has specific properties. So PKI partially synchronous model less than a third Byzantine, and then the properties are consistency and liveness. So possibility results, they're pretty great. Very clarifying, right? Because if you have a rigorously proved possibility result for your protocol, it's basically telling you as long as you're willing to have faith in your implementation of your protocol, of the design, and as long as you have faith in all of the assumptions that you made, then there's nothing else you need to worry about. It's just math tells you the thing is going to work, it'll have the properties that you want.
00:24:32.790 - 00:25:33.620, Speaker A: So flipping that around, that means if you deploy your protocol and it doesn't have the properties that you want, well, now you have like one of a very small number of smoking guns for what went wrong and then it becomes sort of very clarifying as far as what you should be doing next. For example, suppose you deploy sort of the tenement protocol and it breaks badly. Like there's a consistency violation, sort of two different, the same block height, block height number nine, two different blocks wind up getting finalized, each with sort of a supermajority of the votes. And you're like, wow, what happened? Well, first thing that could have happened is your implementation was buggy. Frankly, that's the most common thing, that's the most common source of why you have sort of bugs in consensus protocols or sort of why you have failures of consensus protocols. But if you have a bug free implementation, then it must be that one of your assumptions was incorrect. And some of the assumptions you're probably feeling pretty confident in, like the fact that nobody broke cryptography the fact that nobody was able to forge Ecdca signatures, that seems pretty unlikely.
00:25:33.620 - 00:26:20.564, Speaker A: So maybe it was your assumption that less than a third of the nodes were Byzantine. Maybe actually more than a third of the nodes were know, and Byzantine here could be malicious or buggy or something else. And so now you can start focusing on like, okay, how do we get back to having a supermajority of sort of honest, correct nodes? That's your next step if that's the reason that the protocol deployment failed. So those are possibility results where in principle, you maybe could have come up with the solution without doing the math. Again, distributed protocols are so sophisticated, usually you probably need the math to even come up with protocol. But in principle you could come up with the sort of code without ever doing any proofs impossibility results. There's no way you can do that without math, right? You need to have a mathematical model of everything that is possible.
00:26:20.564 - 00:27:12.840, Speaker A: And then you need to sort of formally prove that none of the things that are possible works, none of them have the properties that you're looking for in your particular model. For example, the FLP impossibility result. We had to precisely define the Asynchronous model. We had to precisely define what we meant by protocol and the properties that we wanted. And then there's no way we're going to be ruling out all protocols without doing some math and having like a hard math proof that in fact none of the protocols have the properties that we're looking for. But despite being sort of intrinsically very mathematical things, impossibility results are incredibly clarifying for the builder, for someone who's actually designing protocols. So again, even if you have no interest in proving these things yourself, even if you have no interest in reading the famous proofs that exist, it's still worth your time to at least know the statements, the theorem statements of the most important impossibility results.
00:27:12.840 - 00:27:44.272, Speaker A: Because if you don't know those impossibility results, you might wind up wasting an unbounded amount of your time trying to come up with something which simply does not exist. This may also be familiar kind of from the algorithms world. NP hardness. If a problem is NP hard, it tells you don't waste your time trying to come up with an always correct, always fast algorithm. You've got to compromise. So too, here with distributed protocols, the impossibility results says you must make compromises. Don't beat yourself up just because your protocol isn't perfect.
00:27:44.272 - 00:28:28.908, Speaker A: It's not that you're not smart enough, it's that that sort of perfect protocol you want simply isn't out there. Computer scientists understandably, they tend to get a little depressed by impossibility results which tell you you can't solve some problem. Much more exciting, you have a positive result which actually says, like, here, here's an algorithm, or here's a protocol which has all of the properties you want. Inevitably, that's for most people, sort of a. Better feeling. But don't forget that these are really two sides of the same coin and that really for people who do prove these things for a living. These are inextricably linked, right? And we saw that, we saw how the FLP impossibility result for the Asynchronous model told us we had to add back in some assumptions so that we could recover possibility results.
00:28:28.908 - 00:29:08.184, Speaker A: And that's how we arrived at the partially synchronous model. So the impossibility result, the FLP one, actually sort of guided us down a road, leading to some very satisfying possibility results, specifically the one we saw for the tendermint protocol. All right? So to summarize, everybody should learn the definitions. That's the basic language by which we communicate our ideas and have sort of healthy nuanced debates. Everyone should sort of learn about the theorem statements, right? The possibility results sort of tell you how to solve problems. The impossibility results tell you not to waste your time solving something that's unsolvable. So the third pillar, if you will, is proofs.
00:29:08.184 - 00:30:07.528, Speaker A: So the proofs being the arguments of why we know these theorems are actually true statements. I don't think everybody needs to necessarily know the proofs of all of theorems that we're talking about. But let me tell you the reasons why at least some of you should be motivated to spend some quality time with them. One reason is that in some cases, in possibility results, like the one for the tenement protocol, the proof actually guides the solution itself, right? So like we said, it's not clear how you'd ever come up with that protocol unless you were writing out the proofs of correctness at the same time. So that's in a case where actually to just come up with a good protocol, you should be thinking about the proof. So another thing proofs can do for you is to just really give you a really deep understanding about why something is true. So, for example, why a particular protocol satisfies certain properties, or why there's no protocol that satisfies some collection of properties that you'd want.
00:30:07.528 - 00:31:01.756, Speaker A: And just like we were talking about sort of the six minute mile at the start of this video, there's no way to get that depth of understanding without putting in the work, putting in the time, and really internalizing the proof. So why would you want to spend the time to get that depth of understanding? Is it to brag to your friends? Is it to kind of be able to think of really hard questions to ask in technical interviews? Maybe. But I think a far more interesting reason to gain that depth of understanding is it'll allow you to very quickly understand whether changes to the theorem statement will still be true or not. Like, take the tenement protocol, for example. So suppose you deployed it and you'd spent some quality time with the proofs of consistency and liveness that we saw in lecture seven. And suppose your friend or your coworker comes along and says, OOH, I see a way to make this protocol much more efficient. We're going to be able to save on a bunch of messages, and we kind of simplify this part of the protocol here.
00:31:01.756 - 00:31:53.740, Speaker A: Because you've spent quality time with those proofs, you will in most cases, be able to very quickly assess whether your friend has a good idea and you could say, like, oh, yeah, even the more efficient version of the protocol will still have consistency and liveness the same proofs will work. Or maybe I see the small modifications to the proofs required so that theorems will still be true versus frankly, this is probably the more common case. You say OOH, that change would kind of mess up the proofs. Like this step of the argument would break down and it's not clear how to salvage those proofs. So I'm not so confident you'd still be consistent live if you made that modification. For another example, you could imagine saying, oh well, let's actually relax 33% Byzantine to 34% Byzantine, would the theorem still be true? And given that you know the proof so well, you'd be able to say like no, definitely not. And I actually really know why the proofs still don't work.
00:31:53.740 - 00:32:57.170, Speaker A: It's because now all of a sudden you can have two supermajorities that don't actually overlap in any honest node. And so basically there's enough Byzantine nodes to double vote on blocks so that when joined with the honest nodes, actually you can finalize two different blocks of the same block height and you won't have consistency. So just by knowing how the proof works in that depth, you can very quickly answer the question like what's so special about a third? What goes wrong when there's more than a third Byzantine nodes? It should also go without saying that of course, by understanding lots of proofs you get better at sort of generating proofs for yourself. So if you design some totally new protocol and want to prove properties about, it certainly will be helpful that you've built up muscles by going through proofs of lots of other things in the past. Granted, that's a fairly esoteric skill. Like I say, you have to be a little weird to want to be a theoretician and sort of prove hard math theorems, but it's a critical skill, right? We really need these theorems and we do need sort of 1% of the people out there working hard to produce them. So if you're in that 1%, then obviously this is time well spent as well.
00:32:57.170 - 00:34:03.876, Speaker A: Finally, I do realize I'll probably lose some of you with this one, but at least for me, at least for the very best proofs, to me there's really kind of a virtuosity there. They really kind of sort of highlight kind of the limits of human understanding at that time and great sort of ingenuity and rigor was needed to actually achieve that understanding at that time. So as much as it's amazing to kind of go experience the Mona Lisa firsthand at the Louvre or see the Pyramids or see Petra or whatever, I think really internalizing some of the best proofs out there is an experience that's, to me, at least, just as profound. So that wraps up everything I wanted to say in the video. Thanks for sticking with me. Thanks for hearing through my pitch about why definitions, theorems and proof s are important not just for theoreticians, but they're also extremely useful for everyone else. So now that my conscience is clear, and I've told you a million reasons why it is we're doing what we're doing, let's go back to doing it.
00:34:03.876 - 00:34:08.080, Speaker A: I'll see you in lecture eight to talk about longest chain consensus. Bye.
