00:00:00.490 - 00:00:11.600, Speaker A: Okay, so welcome to this somewhat advanced and optional video, but one that nevertheless, I think very satisfyingly ties together a bunch of different threads that we've been talking about thus far.
00:00:12.690 - 00:00:14.798, Speaker B: So hopefully you hopefully you remember that.
00:00:14.804 - 00:00:35.526, Speaker A: When we talked about longest chain consensus initially in the permission setting in lecture number eight, and then earlier in this lecture in lecture number nine, when we extended the lecture number eight guarantees to permissionless longest shankansensensis coupled with proof of work, civil resistance, all of that analysis, finality liveness, et cetera. It was all under these assumptions. A one through a five.
00:00:35.526 - 00:00:46.614, Speaker A: And as we've discussed a one, that was a trusted setup assumption. Byzantine nodes don't have advanced, advanced knowledge about the genesis block. So that's just an assumption that we're making assumptions a two through a four that we used in the lecture number eight analysis.
00:00:46.614 - 00:00:59.310, Speaker A: We talked through why all of those definitely are satisfied by the proof of work version of longest chain consensus that we've discussed. But there's still this final assumption, assumption number five. So we made it back in lecture number eight.
00:00:59.310 - 00:01:14.546, Speaker A: We've continued to use it thus far here in lecture number nine. And that's an assumption we would really like to dispense with. So what was assumption A five? That was the assumption that we were operating in what we maybe call the super synchronous model or the instant communication model.
00:01:14.546 - 00:01:30.780, Speaker A: But basically that sort of all of the honest nodes kind of share a hive mind, right? So basically it's like the synchronous model with delta equals zero. All the honest nodes can talk instantaneously to each other. So if they all share pool all of their information, all of them will have exactly the same view of the world.
00:01:30.780 - 00:01:45.626, Speaker A: That was unsatisfying for two reasons. Reason number one is just that obviously the real world is not an instant communication model. Even on its best days, there is some nonzero message delay, say for messages sent across the internet.
00:01:45.626 - 00:02:03.406, Speaker A: The second reason it might have bothered us is that it seems to trivialize the consistency properties. Remember state machine replication? What's the point? The point is to keep a bunch of nodes in sync. And if you can assume that they can kind of telepathically kind of talk to each other instantaneously, well, it doesn't seem that hard to keep them in sync.
00:02:03.406 - 00:02:38.010, Speaker A: Now, there was still a lot of work to do in lecture number eight, right? There was still kind of a finality or self consistency, right, nodes being consistent with future versions of their self. So that was not a trivial property to prove even under assumption A five liveness and the stronger chain quality property, they were also not trivial to prove even under assumption A five. Now, to assuage your fears, I've said multiple times throughout lecture, aidan, earlier this lecture, that all the stuff we were proving in the supersynchronous model, it sort of still holds in the synchronous model as long as the parameter capital delta is small relative to the average duration of a round.
00:02:38.010 - 00:02:56.770, Speaker A: And so this is the video where I'm finally going to fulfill that promise and really show you sort of the math that backs up that claim that it is in fact true, that what we've talked about thus far is really true. More generally in the general synchronous model, as long as that message delay parameter delta is small compared to the average time you see between blocks being produced.
00:02:57.670 - 00:02:59.218, Speaker B: The other threads that this video kind.
00:02:59.224 - 00:03:11.590, Speaker A: Of importantly ties together is, remember, the whole point of proof of work is you have this puzzle difficulty, you have this threshold tau and you're trying to find something which hashes to something which is at most tau, like something that starts with 80 zeros in a row.
00:03:12.170 - 00:03:13.946, Speaker B: And a really obvious question is like.
00:03:13.968 - 00:03:48.050, Speaker A: How should you set tau? Do you set tau to like, zero? Do you set tau to like a super big number? Or how should we think about it? And what was weird is that when we were working in the supersynchronous model, we were kind of suppressing the role that tau and the puzzle difficulty actually plays in longest chain consensus. And this is the video where we finally get some good advice about how tau should be set. So as I said, the main punchline of this video is that we get all the guarantees we want as long as the max message delay delta is small relative to the typical duration between two successive blocks.
00:03:48.050 - 00:04:12.910, Speaker A: What determines how long it takes between two successive blocks? Well, it's exactly the puzzle difficulty. So the guidance is going to be set the difficulty parameter to target a particular rate of block production. And that rate of block production you're going to go after is going to depend on both kind of as we'll see how much byzantine hash rate you want to be able tolerate and exactly how big maximum message delay bound delta you're comfortable with assuming.
00:04:13.650 - 00:04:15.018, Speaker C: So here's our strategy.
00:04:15.114 - 00:04:27.940, Speaker A: So we had this really cool analysis in this supersynchronous model under assumption A five. Let's go back to that really cool analysis and let's actually kind of carefully see exactly where that assumption was used.
00:04:35.800 - 00:04:37.376, Speaker B: So at this point, I would encourage.
00:04:37.408 - 00:04:47.172, Speaker A: You to go back and review all of the lecture number eight analysis that we did. So we had our theorem one, which was the common prefix property. We had our theorem two, which was our finality property.
00:04:47.172 - 00:04:51.850, Speaker A: We had theorem three, which was liveness, and then we had theorem three prime, which was the chain quality.
00:04:52.700 - 00:04:53.816, Speaker B: So I'm not going to go back.
00:04:53.838 - 00:05:09.384, Speaker A: And review all of that analysis now, but let me just sort of highlight kind of the two key points where our analysis really leaned on assumption A five. Let's start with actually the theorems three and three prime. So the liveness and chain quality analysis.
00:05:09.432 - 00:05:12.336, Speaker C: Back from lecture number eight if you.
00:05:12.358 - 00:05:25.780, Speaker A: Go back to that analysis, the property we really needed is that every time we have an honest leader, every time there's a round with an honest leader, the length of the longest chain is going to grow by one, right? Just because honest nodes extend the end of the longest chain.
00:05:26.840 - 00:05:28.836, Speaker B: So why was this property useful in.
00:05:28.858 - 00:05:50.830, Speaker A: That context, in the context of liveness and chain quality? Well, it gave us a lower bound on the length of the longest chain and that meant that the longest chain was so long some of the blocks on it had to have been contributed by honest nodes. All right? And that's exactly what you want for liveness. You want to make sure that honestly produced blocks get into the longest chain, get finalized because honest nodes are always including all of the transactions that they know about.
00:05:50.830 - 00:06:21.824, Speaker A: So for example, if you're in a setting with 51% honest nodes or 51% honest hash rate and you look at sort of 1000 rounds in a row, you're expecting to have honest leaders in say 510 of those thousand rounds. And in that case this will guarantee that the longest chain will grow by at least 510 over that period. Now, some of the nodes on that chain may be contributed by Byzantine nodes, but any given Byzantine node in any given round is going to contribute at most one block to that chain.
00:06:21.824 - 00:06:32.200, Speaker A: You might remember that was actually our assumption a four that we had in lecture number eight, which as we've argued is sort of almost trivially satisfied in the proof of work implementation of longest chain consensus.
00:06:32.940 - 00:06:34.648, Speaker B: So given that the longest chain would.
00:06:34.654 - 00:07:02.356, Speaker A: Have grown by at least 510 and at most 490 of those blocks would have been contributed by Byzantine leaders, you have 20 on it that were in fact contributed by honest nodes. And generalizing that same argument for sort of an arbitrary fraction alpha of Byzantine nodes or byzantine hash rate you get the more general chain quality guarantees that we had in theorem three prime. And I want to emphasize that this property that every honest leader increases the length of a longest chain, that really is only because we were in the instant communication model.
00:07:02.356 - 00:07:20.900, Speaker A: That is just not going to always be true in the synchronous model with a nonzero value of the parameter delta. So even imagine like the super simple case where there's no Byzantine nodes at all. And moreover, sort of thus far the blockchain is just this very simple sort of orange chain that I've drawn on the lower right corner.
00:07:20.900 - 00:07:36.220, Speaker A: Now, every honest node will be dutifully following its instructions and its instructions are to try to add a new block at the end of the longest chain and at some point some honest node is going to solve a puzzle solution and it's going to add a block to the end of this longest chain.
00:07:36.880 - 00:07:38.716, Speaker B: Now, this node being honest will of.
00:07:38.738 - 00:08:02.208, Speaker A: Course, rush to tell all of its colleagues about the new block that it just created. But now that we're in the general synchronous model, now that messages can actually be delayed up to delta time units, there will be up to delta time units before any other honest node is aware that there's now a new longest chain. And so in that intermediate period where all of the other honest nodes have stale information, they're unaware of this newly produced honest node.
00:08:02.208 - 00:08:06.084, Speaker A: They will still be working on their old puzzles. They will still be trying to extend.
00:08:06.132 - 00:08:09.160, Speaker C: What they perceive as the longest chain.
00:08:09.580 - 00:08:10.996, Speaker B: And if one of those honest nodes.
00:08:11.028 - 00:08:31.440, Speaker A: Solves a puzzle before they hear about the newly honestly produced block, that's going to be a second block produced with exactly the same predecessor. So two honest nodes come up with puzzle solutions at roughly the same time. There isn't enough time for them to share notes, so they both wind up choosing the same block as the predecessor, the block that had previously been the end of the longest chain.
00:08:31.440 - 00:08:54.470, Speaker A: And you can see in this example that this statement I've made in dark blue at the top of the slide, actually isn't true. So that first honest leader, the first one to solve the puzzle, does actually successfully increase the length of the longest chain. But the second one, the one that's sort of slightly later actually does not adds a block to the blockchain, but does not increase the length of a longest chain at all.
00:08:55.080 - 00:08:56.624, Speaker B: All right, so that's going to be an issue.
00:08:56.682 - 00:09:04.620, Speaker A: Like if we really want to argue that liveness and chain quality still holds in the general synchronous setting, we are really going to go back and have to modify the proof.
00:09:05.120 - 00:09:05.564, Speaker B: All right?
00:09:05.602 - 00:09:12.270, Speaker A: Now the other place we really leaned on assumption a five was in our proof of the common prefix property.
00:09:13.840 - 00:09:15.484, Speaker B: So you might recall that the common.
00:09:15.602 - 00:09:26.720, Speaker A: Prefix property says that as long as you choose your security parameter k appropriately, all of the longest chains are going to agree, except possibly on their last K blocks.
00:09:27.380 - 00:09:28.688, Speaker B: So the last k blocks of a.
00:09:28.694 - 00:09:41.944, Speaker A: Longest chain should be regarded as still under negotiation. But anything further back on that longest chain is also going to be present on every single other longest chain that's been constructed. So that was our theorem one from option number eight.
00:09:41.944 - 00:09:57.870, Speaker A: Theorem number two was the finality guarantee. And if you go back and look at it, the proof of theorem two was really just a very simple that was a simple consequence of theorem one. So we didn't need to use assumption a five in the proof of theorem two in the proof of finality, except in as much as we needed it to prove the common prefix property.
00:09:58.720 - 00:09:59.996, Speaker B: And if you go back to our.
00:10:00.018 - 00:10:29.680, Speaker A: Proof of theorem one, you will see that really fundamentally we used assumption a five to argue that in a given height so a given number of hops away from the genesis block, at a given block height, you will never see more than one honest block. If you're trying to remember why was this property useful in our proof of theorem one? Well, you might remember we had that sort of potential function argument. We had a potential function that kind of at one moment in time is going to be zero and then at a later moment in time sort of grows to some big number.
00:10:29.680 - 00:10:50.670, Speaker A: And we argued that only Byzant teen nodes were going to be able to increase the potential function. Honest nodes would never increase that potential function. And then we did accounting argument to say like, oh, well, we must have had this sort of long window where we had sort of the Byzantine leaders outnumber the honest leaders, which was a contradiction to the W balancedness assumption, which is the key hypothesis you make when proving theorem one.
00:10:51.200 - 00:10:52.684, Speaker B: In any case, it's not actually important.
00:10:52.722 - 00:11:00.100, Speaker A: That you remember that now. But anyways, we did need that. We used the property in that proof that you never have two honest blocks at exactly the same height.
00:11:00.100 - 00:11:22.740, Speaker A: And looking at this picture in the lower right, we immediately see that in the general synchronous model with nonzero delta, in fact, you can indeed have two honest nodes at exactly the same height. You might have two different honest nodes inadvertently create a fork if they saw the same puzzle sort of very close to each other, and both produce blocks extending the same predecessor, which would have been the previous end of the longest chain.
00:11:23.400 - 00:11:24.916, Speaker B: So once again, if we want to.
00:11:24.938 - 00:11:51.250, Speaker A: Argue that the common prefix property continues to hold in the synchronous, the general synchronous model, we're going to have to go back and modify the proof of theorem one. I guess in some ways there was maybe like a third consequence of assumption a five, which was, as we said, it kind of trivialized consistency. It didn't trivialize finality, so it didn't trivialize nodes being consistent with their future selves, but it did trivialize making sure all of the nodes stay in sync with each other.
00:11:51.250 - 00:12:15.860, Speaker A: I'm actually not going to talk much, or at all really about this third point in this video. It is true, if you want to argue that longest chain consensus works well in the synchronous model, you do have to explicitly state and prove a consistency property. So that'd be a theorem four to join our theorems one, two, three and three prime.
00:12:15.860 - 00:12:52.052, Speaker A: But to be honest, that theorem four consistency. Once you've sort of reproved the common prefix property in the synchronous model, not only can you sort of derive finality or self consistency pretty easily from theorem one from the common prefix property, but you can also derive consistency between honest nodes pretty easily from that common prefix property for much of the same reasons. So the essence of the consistency argument really both for the self consistency and the consistency between different nodes, by which I mean at any given moment in time, any two honest nodes, either they have identical local histories or one of them has a local history which is a prefix of the other.
00:12:52.052 - 00:13:16.552, Speaker A: That's what I mean by consistency between honest nodes. Either way, the essence of that argument really lies in the common prefix property. So I hope that already at this point, sort of your intuition is building about why the ratio between, on the one hand, message delay capital delta and on the other hand, the average duration of a round, which, remember, for proof of work, is also just the average length of time that elapses between two successive puzzle solutions.
00:13:16.696 - 00:13:21.036, Speaker C: Why that ratio might be really important for us, right?
00:13:21.058 - 00:14:02.860, Speaker A: Because think about this orange inadvertent fork on the lower right part of the slide and remember, this is sort of the picture which shows why these properties, one and two that relied on assumption A five, why they're no longer true in the general synchronous model. So what went wrong in this sort of orange fork is that two honest nodes came up with solutions to basically the same puzzle, meaning solutions with the same predecessor at basically the same point in time. So the honest node that found their puzzle solution earlier certainly tried to communicate that to all of its colleagues, but it was delayed by up to delta time units and the second node actually had already solved the puzzle by the time that message arrived.
00:14:02.860 - 00:14:34.660, Speaker A: So in other words, in this orange fork, it was crucial that two puzzle solutions were found less than delta time units apart. So if you expect rounds to be long, if you expect the number of time units between successive puzzle solutions to be well, more than capital delta, well, then hopefully, while you will once in a while have these inadvertent forks, hopefully they don't happen very often. And so if they're sufficiently infrequent, the hope would be that they don't sort of fundamentally mess up sort of the arguments that drove all of our analysis in lecture number eight.
00:14:34.660 - 00:14:57.416, Speaker A: And as we'll see, that is in fact true. So sort of the key intuition to remember from this video is that our lecture eight analysis, pretty much the only thing that goes wrong with it is that you will sometimes have these inadvertent orange forks. But then, on the other hand, as long as those are very infrequent, which will be the case if puzzle solutions are typically discovered well more than delta time units apart.
00:14:57.416 - 00:15:15.330, Speaker A: So as long as those inadvertent orange forks are infrequent, they don't really mess up our lecture number eight analysis very much. It changes the parameters by a little bit, but fundamentally the exact same guarantees hold. And moreover, they hold for exactly the same reasons that we discussed in lecture number eight.
00:15:16.420 - 00:15:18.016, Speaker B: And that, of course, fulfills a promise.
00:15:18.048 - 00:15:38.970, Speaker A: I made you way back in lecture number eight, that assumption A five is actually not that big a deal. And moreover, it's very convenient to first study finality liveness et cetera under assumption A five because the actual conceptually important parts of the arguments of those properties stand out. As we'll see in this video, it's just kind of some extra details to handle the general synchronous model.
00:15:38.970 - 00:15:59.440, Speaker A: All right, so it should be clear from this discussion there's two key parameters we really need to keep track of. So one is just capital delta in the synchronous model, the maximum delay that any message might experience and secondly is the average length of a round or equivalently the average amount of time between two successive puzzle solutions if we're thinking about the proof of work version of longest chain consensus.
00:16:00.740 - 00:16:02.796, Speaker B: So the maximum message delay as usual.
00:16:02.828 - 00:16:08.384, Speaker A: Will denote by capital delta for the typical amount of time between successive puzzle solutions.
00:16:08.512 - 00:16:11.828, Speaker C: Let's call that capital L. So in.
00:16:11.834 - 00:16:16.120, Speaker A: Other words, capital L is the average length in time units of a round.
00:16:17.020 - 00:16:18.168, Speaker B: And one thing I want you to.
00:16:18.174 - 00:16:43.312, Speaker A: Notice we're thinking here about proof of work civil resistance along with longest chain consensus, capital L is determined by the difficulty threshold and only by the difficulty threshold. So to explain, let's go ahead and think of the node's hash rates as being completely fixed. Right, in the last video we were talking about difficulty adjustment and sort of varying hash rates.
00:16:43.312 - 00:16:53.210, Speaker A: We could incorporate that into this video if we wanted. But I want to sort of just zoom in on what's important here about the synchronous model. So let's just think about the case where nodes hash rates just stay exactly the same all the time.
00:16:53.210 - 00:17:10.856, Speaker A: Under this assumption, every single time step there will be exactly the same probability that somebody finds a puzzle solution. So for example, let's go back to those sort of parameters we were using where it's sort of shot 256. So 256 bits of output let's suppose the difficulty threshold was two raised to the 176th.
00:17:10.856 - 00:17:48.200, Speaker A: So that means the probability that a given dart hits the dartboard that's going to be two to the -80 and now suppose moreover that maybe in some given time step the total hash rate is two to the 60 hashes per timestep. And so that means you're going to have two to the 60 over two to the 80 or two to the -20 that will be the probability that some puzle solution will be found at a given time step. In general, the probability of a puzzle solution is extremely well approximated by just the number of guesses that nodes make during that time step times the probability that any given guess actually is a solution.
00:17:48.200 - 00:17:59.736, Speaker A: So that means we can think of this process as like a coin flipping experiment. So like Time step 100, you flip a coin. These two to the 60 darts get thrown at the dartboard two to the -80 chance that each of them hits the bullseye.
00:17:59.736 - 00:18:13.136, Speaker A: So two to the -20 chance essentially that at least one of them hits the bullseye. So that's sort of a coin with bias two to the -20 that's the probability with which it comes up heads. If it comes up heads that corresponds to a puzzle solution being found in that Time Step.
00:18:13.136 - 00:18:17.988, Speaker A: Next time step exactly the same thing two to the -20 chance do you have a puzle solution same thing at.
00:18:17.994 - 00:18:22.276, Speaker C: Timestep 102, 103, 104 and so the.
00:18:22.298 - 00:18:31.604, Speaker A: Average time between successive solutions this parameter capital L. That's just going to be the typical number of coin flips. You need to wait until you get aheads.
00:18:31.604 - 00:18:46.264, Speaker A: And that's something known as a geometric random variable. It's a simple type of random variable, you can look it up on Wikipedia. And the point is, if you have a coin with probability P of coming up heads, you expect one over P coin flips before you see a heads.
00:18:46.264 - 00:18:59.536, Speaker A: So here, given that we're expecting to see capital L, time units elapse before finding a puzzle solution, that means that the probability of finding a puzle solution per time step that corresponding to the bias of the coin, that should be.
00:18:59.638 - 00:19:03.520, Speaker C: One over capital L. So tying all that together.
00:19:03.590 - 00:19:03.824, Speaker B: Right.
00:19:03.862 - 00:19:30.796, Speaker A: So if you're designing a blockchain protocol and you decide you want to target some rate of block production, some average length capital L between blocks, and we'll see from this video's analysis why you may want to do that, well, then this tells you exactly how you should be setting the difficulty threshold. Tau in other words, this really does answer the question we asked a while ago, which was, how should you set tap so what I'm saying here is that given a target capital L, it does actually, as a function of the hash rate, tell you exactly how to.
00:19:30.818 - 00:19:33.564, Speaker C: Set the threshold tap right.
00:19:33.602 - 00:19:49.170, Speaker A: So, for example, if you wanted capital L to be two raised to the 20, which is roughly a million. So if you wanted a million time steps between successive puzzle solutions, that means the probability of a puzle solution at any given time should be one over that so two raised to the -20.
00:19:50.020 - 00:19:51.216, Speaker B: And then if you know that the.
00:19:51.238 - 00:20:18.190, Speaker A: Current hash, rate is sort of two to the 60 guesses per time step. Well, then that says you should set tau exactly. So that the probability that any given guess, any given Dart hits the dartboard is two to the -80 right because then the probability that any dart hits the bullseye in one time step will be roughly two to the 60 over two to the 80 or two to the -20 and then that gives you back the average length two to the 20 between different puzzle solutions that you were looking for.
00:20:19.040 - 00:20:19.468, Speaker B: All right?
00:20:19.474 - 00:20:36.396, Speaker A: So it's convenient to carry out this analysis in terms of the parameters capital delta and capital L, rather than explicitly referencing some difficulty threshold tau. But don't forget that there's really like a one to one for a fixed sort of overall hash rate. There's basically a one to one correspondence between capital L and little tau.
00:20:36.396 - 00:20:46.630, Speaker A: So when we talk about sort of like, oh, you should set capital L to be at least this big, that corresponds to saying the puzzle should be at least so difficult that tau should be at most so large.
00:20:47.800 - 00:20:49.024, Speaker B: All right, so now that we're hopefully.
00:20:49.072 - 00:21:02.520, Speaker A: Sort of very clear on those two parameters, I want to revisit our intuition that hopefully the only issue with our lecture number eight analysis is these inadvertent forks like we have on the lower right hand part of the slide and that hopefully those should be infrequent.
00:21:03.500 - 00:21:05.384, Speaker B: So the next definition is going to.
00:21:05.422 - 00:21:13.900, Speaker A: Articulate conditions for a block that's honestly produced and that cannot possibly sort of get wasted in one of these inadvertent orange forks.
00:21:14.480 - 00:21:15.772, Speaker B: So I want you to think about.
00:21:15.826 - 00:21:31.920, Speaker A: Some round with an honest leader. Again, the honest leader proposes one block extending what it perceives to be the current end of the longest chain. Let's, let little T denote the time step at which this block was created at which the node found its puzzle solution.
00:21:32.920 - 00:21:34.560, Speaker B: So we're going to call this block.
00:21:34.640 - 00:22:11.478, Speaker A: Safe if no other puzzle solutions have been found in the recent past, meaning at any point in the last delta time steps. So why is this a natural definition? Well, it makes sure that the honest node creating that safe block is up to date, at least with respect to other honestly created blocks. Right, because think about it, you're at time T, this honest node is producing a block you have to rewind back to time step T minus delta or earlier for anybody honest or Byzantine to be producing any puzzle solutions.
00:22:11.478 - 00:22:31.126, Speaker A: So in particular, any honestly solved puzzles, they must be from time steps T minus delta or back. Which means all of those puzzle solutions will have been received by this current node at time T. So the point of a safe block is it's an honestly produced block where that honest node is completely up to date with respect.
00:22:31.178 - 00:22:33.330, Speaker C: To other honestly created blocks.
00:22:34.230 - 00:23:11.000, Speaker A: And what makes this definition so useful is that these sort of key properties one and two on the top of the slide that played the central role in our liveness and finality analyses, respectively, those properties one and two actually do hold if you restrict attention just to the subset of honestly produced blocks that are safe. For example, consider the second property, which was central in our proof of the common prefix property. So it's not the case in the general synchronous model, as we've seen, that two honest nodes can never be at the same height.
00:23:11.000 - 00:23:17.164, Speaker A: But the claim is that it is actually true in the general synchronous model that no two safe blocks can wind.
00:23:17.202 - 00:23:20.332, Speaker C: Up at the same height, right, because.
00:23:20.386 - 00:23:59.656, Speaker A: Imagine some safely produced block and for example, maybe it gets added at height 100 in the blockchain at some period in time. And now fast forward to some subsequent safe block, which by the definition of being safe, that has to happen delta or more time units later, so that later produced safe block, that honest node will be aware of the height 100 block that had been safely produced previously. So there's no way that second honest node is going to be producing another block at block 100 because it has the option of extending the height 100 block that was safely produced earlier, right, because it's had enough time to learn about it.
00:23:59.656 - 00:24:12.096, Speaker A: So it might create something at height 101 or maybe a bunch of other stuff has happened in the meantime and it sort of creates a block at an even higher height. But there's no way it's creating a second block at height 100. It's going to be 101 or more.
00:24:12.096 - 00:24:16.188, Speaker A: So that's why the second property holds. If you restrict attention to the subset.
00:24:16.204 - 00:24:20.432, Speaker C: Of safely produced blocks for property One.
00:24:20.486 - 00:24:25.488, Speaker A: Actually, this can be a little bit simpler if rather than talking about property one directly, we talk instead about a.
00:24:25.494 - 00:24:29.204, Speaker C: Simple consequence of property one, namely that.
00:24:29.242 - 00:24:36.470, Speaker A: The length of the longest chain at any given moment in time is going to be at least the number of honest leaders that we've had thus far.
00:24:37.560 - 00:24:39.136, Speaker B: The statement in blue is the stronger.
00:24:39.168 - 00:24:52.510, Speaker A: Statement that sort of like clockwork, every honest leader sort of increases the longest chain length. But actually, if you go back to sort of our liveness and chain quality proofs, all that matters is that the longest chain is sufficiently long that it's at least as long as the number of honest leaders that we've had thus far.
00:24:53.120 - 00:24:54.732, Speaker B: Now, we of course know that this.
00:24:54.786 - 00:25:06.396, Speaker A: Statement in gray doesn't hold in general in the synchronous model. That's what the orange fork on the lower right shows. But I claim that that gray statement is true if we restrict attention to just the safely produced blocks.
00:25:06.396 - 00:25:11.376, Speaker A: So the claim is that the length of the longest chain is at least the number of safely produced blocks up.
00:25:11.398 - 00:25:13.184, Speaker C: To that point, right?
00:25:13.222 - 00:25:37.476, Speaker A: Like, imagine at some point in time you've constructed 117 different safe blocks, right? So we know from property two, they all have to be at distinct heights. And so if you're thinking about how short could the longest chain be? Well, the most smooshed that all of these safe blocks could be to the Genesis block, they have to occupy distinct heights. So you could have one at height one, one at height two, one at height three, one at height four, et cetera, all the way up to one at height 117.
00:25:37.476 - 00:25:52.210, Speaker A: But even then, you know, the longest chain is going to be at least 117. So in general, the longest chain length is going to be at least the number of safely produced blocks. And that was actually if you go back and look, the key statement that we needed for our theorems three and three prime in lecture number eight.
00:25:53.300 - 00:25:54.944, Speaker B: Okay, so moving forward, right?
00:25:54.982 - 00:26:08.556, Speaker A: So under assumption a five in the instant communication model, we really only had two different types of blocks. Those that were created by honest nodes, those that were created by byzantine nodes. As long as the former kind of generally outnumbered the latter, we had all of these nice properties.
00:26:08.556 - 00:26:27.960, Speaker A: Now here in the general synchronous model, we see we're actually going to have sort of three types of nodes. We have the Byzantine nodes as usual, and then among the honestly produced blocks we're going to have the ones that are safe, meaning no other puzzle was solved recently. And the ones that are not safe, meaning some other puzzle solution was actually solved within the last capital delta time steps.
00:26:28.700 - 00:26:30.084, Speaker B: So in the rest of the analysis.
00:26:30.132 - 00:26:48.828, Speaker A: We'Re basically going to pessimistically treat the unsafe blocks as byzantine, right? They were not produced by Byzantine nodes, they were produced by honest nodes. You know, but you know, just to be safe, to be conservative, let's think of them as if they were produced by Byzantine nodes. Byzantine nodes always have the option of acting like they're an honest node that produced an unsafe block.
00:26:48.828 - 00:26:58.310, Speaker A: And then what we're going to see is that the safe blocks play exactly the same role as honest blocks did back in our original analysis in lecture number eight under assumption A five.
00:26:59.240 - 00:27:01.396, Speaker B: So in other words, we have our.
00:27:01.418 - 00:27:11.332, Speaker A: Honest blocks and then some portion of them are basically going to start batting for the other team. They're going to be unsafe. So we're actually going to treat them not as honest blocks, but instead as adversarial blocks.
00:27:11.332 - 00:27:29.900, Speaker A: So some of the things we were previously thinking of as h's are now going to become a's so obviously a really important question is of all of the honestly produced blocks, how many do we lose to the other team? How many of those are going to be unsafe? Hopefully a lot of them are going to be safe. So we'll see. That is in fact true on this next slide.
00:27:32.160 - 00:27:33.916, Speaker B: So the key lemma is to understand.
00:27:34.018 - 00:28:06.650, Speaker A: What'S the expected fraction of blocks that are created which are safe. So blocks that not only were created by an honest node, but also it just so happened that no other puzzle solutions were found in the delta time units preceding it. Now, if we only cared about the fraction expected fraction of blocks that were created by honest nodes, which back when we had assumption A five in the instant communication model is in fact all that we cared about, that's really easy, right? So by assumption, alpha fraction of the hash rate is Byzantine, which means a one minus alpha fraction of the hash rate is honest.
00:28:06.650 - 00:28:19.144, Speaker A: Because with proof of work, simple resistance nodes are selected with probability proportional to their hash rate. That meant that just the probability that a leader is honest is going to be one minus alpha. Now, here where we also care about safe blocks.
00:28:19.144 - 00:28:33.280, Speaker A: It's going to be right, the fraction of safe blocks is going to be less than one minus alpha. It's going to be one minus alpha, sort of minus these sort of honestly created blocks that happen to be unsafe, which we're going to treat like actually blocks that were treated by Byzantine nodes.
00:28:33.940 - 00:28:35.136, Speaker B: And so what we're going to prove.
00:28:35.168 - 00:28:42.068, Speaker A: Is the following lower bound, that the fraction expected fraction of safe blocks amongst all of the blocks created was at.
00:28:42.074 - 00:28:45.940, Speaker C: Least one minus alpha times a number.
00:28:46.010 - 00:28:56.676, Speaker A: Which is less than one. And so that number is going to be quantity one minus delta over capital L. The maximum message delay divided by the average duration of a round.
00:28:56.676 - 00:29:00.440, Speaker A: That crucial ratio that we were expecting to enter into the picture.
00:29:01.100 - 00:29:02.376, Speaker B: And so this Lemma, I mean, we.
00:29:02.398 - 00:29:21.468, Speaker A: Still have to prove it, but this Lemma, given that it's true, is good news, right? So we recover what we had before. So if we said delta equal to zero, that would correspond to assumption A five, the instant communication model. So then we get what we expect, a one minus alpha fraction of the blocks are honest or equivalently safe.
00:29:21.468 - 00:29:31.152, Speaker A: And then what we're seeing is that that's almost true. That's approximately true. As long as the maximum message delay, delta is small relative to capital L, the average length of a round.
00:29:31.152 - 00:29:45.800, Speaker A: If delta over L is close to zero, then one minus that is close to one. So this lower bound is going to be close to one minus alpha. In other words, if delta is small relative to L, we have the same thing of before, basically, just with a little bit of loss.
00:29:46.620 - 00:29:48.168, Speaker B: The proof, it's not hard, it's just.
00:29:48.174 - 00:30:01.660, Speaker A: Really some sort of simple probability. So what I want you to think about, I want you to think about zooming in on a moment in time where some node, maybe honest, maybe Byzantine, some node, actually comes up with a new puzzle solution.
00:30:02.740 - 00:30:03.920, Speaker B: And what we're going to be interested.
00:30:03.990 - 00:30:11.520, Speaker A: In is the probability that that particular puzzle solution results in the creation of a safe block.
00:30:13.540 - 00:30:14.944, Speaker B: And in order for this to happen.
00:30:14.982 - 00:30:27.584, Speaker A: Really just by the definition of a safe block, two things need to be true. So first of all, the node that create that actually found this puzzle solution better be an honest node. That's a prerequisite for being safe.
00:30:27.584 - 00:30:32.600, Speaker A: And then moreover, there should be no other puzzle solutions from the previous delta time steps.
00:30:33.340 - 00:30:35.844, Speaker B: Now, the sticklers amongst you may complain.
00:30:35.892 - 00:30:59.212, Speaker A: That that second term, really, that should be probability of no other puzzle solutions in that window of delta time steps. Conditioned on that, the current puzzle solution was found by an honest node, right? But if you think about it, people are just looking for puzzle solutions by throwing darts at the dartboard, hoping to hit the bullseye. The fact that this particular puzzle was found by an honest node has no bearing on other nodes sort of probability of hitting the bullseye.
00:30:59.212 - 00:31:09.700, Speaker A: So that conditioning is actually not important. So I'm just going to write the second term as unconditional probability that no puzzle solutions were found in the previous delta time steps.
00:31:10.280 - 00:31:10.676, Speaker C: All right?
00:31:10.698 - 00:31:26.920, Speaker A: So of the two terms on the right hand side, the first one is extremely easy to understand. So given a particular puzzle solution, we're asking for the probability that it was found by some honest node. And that's just going to be equal to the fraction of the overall hash rate, which is honest, which is one minus alpha.
00:31:27.740 - 00:31:29.412, Speaker B: So how about the second term?
00:31:29.476 - 00:31:41.820, Speaker A: What's the probability that in a given window of delta time steps, setting aside sort of the one puzzle solution that we know was found, what's the probability that no other puzzle solutions were found in that window of delta time steps?
00:31:42.320 - 00:31:43.984, Speaker B: Well, so here, remember on the last.
00:31:44.022 - 00:31:57.396, Speaker A: Slide we noted that if the average duration between consecutive puzzle solutions is capital L, what that means this was the story with the geometric random variables. That means the likelihood of finding a puzzle solution at any given time step.
00:31:57.498 - 00:32:00.916, Speaker C: Is one over l. So probability one.
00:32:00.938 - 00:32:15.924, Speaker A: Over l for each time step and we're looking at a window of delta time steps. So there's delta different chances to hit the bullseye, to have a puzzle solution. So we can upper bound the probability that we find a puzzle solution at any of those delta time steps by the sum of the probabilities.
00:32:15.924 - 00:32:22.350, Speaker A: So the probability that a solution found at any of those timesteps is at most delta over l.
00:32:27.490 - 00:32:28.446, Speaker B: So technically, here.
00:32:28.468 - 00:32:49.730, Speaker A: I'm using something which is usually called the union bound, which just says that the probability that any of a bunch of events occurs can be bounded above by the sum of the individual probabilities of those events. We have seen the union bound before. You might remember it from when we were talking about the analysis of random leader selection in lecture number eight.
00:32:49.730 - 00:32:52.770, Speaker A: So the fourth video of lecture number eight, we also use the union bound.
00:32:52.850 - 00:32:57.814, Speaker B: Same thing we're doing here. So we're interested in the complementary probability, right?
00:32:57.852 - 00:33:08.940, Speaker A: So given that the probability that you do find at least one puzzle solution is the most delta over l, that means the probability that you don't find any puzzle solutions is at least one minus delta over l, which is what we wanted.
00:33:12.520 - 00:33:14.004, Speaker B: And so that's exactly what we wanted.
00:33:14.042 - 00:33:24.440, Speaker A: To prove in the Lemma. So a one minus alpha fraction of the blocks are created by honest nodes. Of those, at least a one minus delta over l fraction of those honest blocks are going to be safe.
00:33:24.440 - 00:33:50.322, Speaker A: So this Lemma, this is good news. This says as long as delta is small relative to capital L, most of the blocks created by honest nodes will in fact be safe blocks. And that's really good news, right? Because the unsafe blocks are pretty annoying because they hurt us in two different ways, right? So first of all, we can't really kind of count their progress in our sort of usual analysis the way we do for honest blocks.
00:33:50.322 - 00:34:13.120, Speaker A: You might remember that sort of inadvertent orange fork that we had on the previous slide, right? That last honest block doesn't really sort of do us any good. And furthermore, because we don't know what's going on with these unsafe blocks, the most conservative approach in the analysis is just to treat them as Byzantine. So it's a block that used to be honest and doing good things and now we're going to treat it as a Byzantine created block and so it could do bad things.
00:34:13.120 - 00:34:36.870, Speaker A: Now, unsafe blocks created by honest nodes sort of they can't do any more damage than Byzantine nodes are already doing with their blocks. A Byzantine node always has the option of sort of pretending like it's an honest node. But to keep it simple and to be conservative, let's just go ahead and think about the worst case where unsafed honest blocks, let's analyze them as if they were produced by Byzantine nodes.
00:34:37.690 - 00:34:39.206, Speaker B: So again, we used to have this.
00:34:39.228 - 00:35:15.010, Speaker A: Leader sequence of H's and A's and now some of those H's have basically turned into A's. But if you think about it, as long as after that sort of conversion, after a small fraction of the H's have sort of gone over to bat for the other team and become A's, if it's still the case after that sort of betrayal, that the number of H's outnumbers the number of A's. So in other words, that the number of safely created blocks outnumbers all of the rest of the blocks, both the Byzantine blocks and the honest unsafe blocks, then we're good, right? Again, that's sort of as good as if we were in the lecture number eight analysis.
00:35:15.010 - 00:35:50.794, Speaker A: So we can express this magenta inequality, right, which is saying the probability that a given block is safely produced should be higher, higher probability than the alternative that it's either a Byzantine created block or an honestly created unsafe block. We can express this inequality between probabilities in terms of our other parameters. Alpha, the fraction of Byzantine hash rate delta, the maximum message delay and capital L, the average length of time between successive puzzle solutions.
00:35:50.794 - 00:35:57.958, Speaker A: In particular, the Lemma tells us that we can lower bound the probability on the left hand side of the Magenta inequality by one minus alpha times quantity.
00:35:57.994 - 00:35:59.700, Speaker C: One minus delta over L.
00:36:02.390 - 00:36:02.898, Speaker B: Now, on.
00:36:02.904 - 00:36:17.560, Speaker A: The right hand side we have two terms. So first of all we have the fraction of Byzantine blocks and that of course is just alpha in expectation because an alpha fraction of the hash rate is Byzantine. And in proof of work, blocks leaders are selected with probability proportional to hash rate.
00:36:18.330 - 00:36:19.766, Speaker B: And then in addition, we have the.
00:36:19.788 - 00:36:21.426, Speaker A: Honestly created unsafe blocks.
00:36:21.458 - 00:36:21.654, Speaker B: Okay?
00:36:21.692 - 00:36:38.480, Speaker A: So that's going to be there's going to be a one minus alpha expected fraction of the blocks being honestly created. And of those, as we see at most, a delta over L fraction of them will be unsafe. So we have to add that additional one minus alpha times delta over L to the right hand side.
00:36:38.480 - 00:36:53.842, Speaker A: So now of course, we can just solve for an upper bound on alpha the maximum amount of Byzantine hash rate that we can tolerate. That's going to be a function of both delta and L. So if you want, you can sort of do the algebra and the privacy of your own home.
00:36:53.842 - 00:37:20.586, Speaker A: I'm just going to cut to the chase and tell you what you get if you rearrange this inequality as an upper bound on alpha. And so if you do this, what you find is that a sufficient condition to get everything that we want in the general synchronous model with arbitrary maximum message delay bound capital delta. A sufficient condition for getting everything we want is that alpha, the fraction of Byzantine hash rate is less than one minus two times delta over L.
00:37:20.586 - 00:37:23.882, Speaker A: Divided by two minus two times delta over.
00:37:23.936 - 00:37:24.540, Speaker C: L.
00:37:26.270 - 00:37:27.854, Speaker B: So to interpret this so first.
00:37:27.892 - 00:37:34.782, Speaker A: As a sanity check, we can consider plugging in. Delta equals zero. So that would be back in the supersynchronous instant communication model.
00:37:34.782 - 00:37:47.650, Speaker A: And then we just get the condition that alpha is less than a half I-E-A strict minority. The hash rate should be Byzantine. That's very reassuring because that's actually exactly what we proved is true in lecture eight in the beginning part of this lecture.
00:37:47.650 - 00:38:06.060, Speaker A: But what's cool about this general expression is you can see that it's not just if delta is zero. As long as delta over L is pretty small, pretty close to zero, then this ratio is going to be pretty close to one half and we will be able to provably tolerate a pretty close to pretty close to 49.9% Byzantine Hash rate.
00:38:06.060 - 00:38:29.822, Speaker A: So summarizing Nakamoto consensus, by which I mean longest chain consensus coupled with proof of work, civil resistance, nakamoto Consensus satisfies consistency and liveness in the sense of state machine replication. Again, it's probabilistic. So with high probability as long as the hash rate is sort of a little bit below 50% and as long as delta is small relative to capital.
00:38:29.886 - 00:38:30.500, Speaker C: L.
00:38:46.360 - 00:38:55.808, Speaker A: So it's instructive to interpret this for the specific case of Bitcoin. So as we mentioned in Bitcoin, capital L is set to ten minutes. That's really part of the protocol description.
00:38:55.808 - 00:39:06.040, Speaker A: Now, Nakamoto in the white paper didn't write about capital delta. So it's not totally clear if they had a specific value in mind. You could imagine taking capital delta to be like 10 seconds or something.
00:39:06.040 - 00:39:31.552, Speaker A: Except on bad days, messages do get delivered over the internet and usually well less than 10 seconds. And if you do that so if delta is 10 seconds and capital L is ten minutes, then delta over L is something like between one and 2%. And then if you plug that into this formula, you get that even in a synchronous model with the ten minute delay between blocks, on average, you can tolerate the Byzantine Hash rate pretty much all the way up to 49%.
00:39:31.552 - 00:39:36.390, Speaker A: Not all the way up to 49.9%, but up to more or less 49%.
00:39:37.400 - 00:39:39.152, Speaker B: So meanwhile, if you tried to modify.
00:39:39.216 - 00:39:57.060, Speaker A: Bitcoin by increasing the rate of block production, so making the puzzles a little bit easier, maybe you sort of instead think about tuning it. So you get a block on average every five minutes, or a block on average every 1 minute. So that would have the effect of decreasing capital L making capital L smaller.
00:39:57.060 - 00:40:09.340, Speaker A: Which means that ratio delta over capital L is going to be getting bigger, right? Delta is a property of the network that's independent of the rate of block production. So delta is fixed. If you make L smaller, that ratio is going to be getting bigger.
00:40:10.320 - 00:40:11.468, Speaker B: And if you look at this, the.
00:40:11.474 - 00:40:40.584, Speaker A: Right hand side, the right hand side is going to be getting smaller as delta over L gets bigger. So the requirement on the Byzantine Hash rate is going to be getting sort of more and more stringent as you have faster and faster block production. So, for example, if you ratcheted up the rate of block production, so the delta overl was something like 10% rather than in the one to 2% range, you'd still get provable guarantees, but you then need the amount of Byzantine Hash rate to be something like less than 44%.
00:40:40.584 - 00:40:45.096, Speaker A: You'd potentially be in trouble. If the Byzantine Hash rate was more than that, you could not go all.
00:40:45.118 - 00:40:48.776, Speaker C: The way up to 49% if you.
00:40:48.798 - 00:40:59.916, Speaker A: Set capital L to be 1 minute. And then if we're thinking of delta to be 10 seconds, that means delta over L would be equal to one 6th. And so then that would sort of require that you have less than 40% Byzantine Hash rate.
00:40:59.916 - 00:41:14.976, Speaker A: And if you notice, if you sort of actually take L to be only twice delta, for example, so that would be like 20 seconds. If delta is equal to 10 seconds, then actually, at least from this bound, you get no guarantees at all. So I hope you find this pretty satisfying.
00:41:14.976 - 00:41:40.844, Speaker A: Like, maybe you heard about the sort of ten minute per block average with bitcoin. Maybe you always wondered, why is that true? Why ten minutes? Why not something else? And it was clear Nakamoto had the it's clear that they had the intuition, right, which is that you wanted to have blocks produced relatively slowly so that longest chains sort of grow in an orderly fashion. But this math actually sort of really confirms what Nakamoto presumably knew in quite a precise way.
00:41:40.844 - 00:42:03.368, Speaker A: And in particular, we can see that with the particular parameter choices. For example, in the Bitcoin protocol, actually, you do get provable probabilistic consistency and liveness with a bound on Byzantine hash rate pretty close to 50%. I should mention the analysis I've given you is not the tightest possible with respect to the bound on alpha.
00:42:03.368 - 00:42:20.072, Speaker A: In terms of the parameters delta and L, it is sort of qualitatively correct? It is actually a pretty decent bound, but you can get sharper bounds. And there's been quite interesting research on that over the last maybe five years or so. So I'll put the exact citations in the lecture notes that accompany this video.
00:42:20.072 - 00:42:45.580, Speaker A: But there's one paper by Pass, Seaman and Shalat from 2017 that's what sort of my analysis here is probably closest to. And then a good entry point, if you want to see kind of really sharp analyses, is a paper by a whole bunch of people with the memorable title everything is a Race and Nakamoto Always Wins. So those are good starting points if you really kind of want to absolutely nail this bound.
00:42:45.580 - 00:43:08.292, Speaker A: But I think this qualitative understanding in terms of delta and capital l is already perfectly sufficient for our purposes, and one that I hope you find quite satisfying. So now we'll move on to the 6th video where we'll talk about the impossibility result, which kind of explains why you almost never see black chains. That couple, on the one hand, proof of work, civil resistance, and on the other hand, BFT consensus.
00:43:08.292 - 00:43:09.110, Speaker A: So I'll see you there.
