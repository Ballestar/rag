00:00:00.330 - 00:00:11.182, Speaker A: Okay, so last week we started our discussion about online problems and online algorithms. We talked about the multiplicative weights algorithm. That's an online algorithm for online decision making.
00:00:11.182 - 00:00:33.602, Speaker A: And we talked about its guarantee. And then we kind of had a sort of a detour last Thursday where we didn't talk about online problems per se, but we talked about applications of multiplicative weights so both to zero sum games and to the fast approximation of certain linear, linear programs. So this week, these two lectures are going to be sort of on the more traditional, quote unquote part of online algorithms.
00:00:33.602 - 00:00:48.300, Speaker A: So we're going to study a couple of the traditional killer application domains, things like scheduling, things like matching. So there's two problems and two algorithms I want to discuss in full. If there's time, I'll introduce you to a third problem as well.
00:00:48.300 - 00:01:02.062, Speaker A: So the first problem is about scheduling. It's called min make span scheduling. And so recall what an online problem means in general.
00:01:02.062 - 00:01:18.162, Speaker A: This is where you do not have all of the data in your hands. Up front, data arrives piecemeal, one piece at a time. And as the algorithm, unfortunately, you can't just wait till you have all the data and then solve it like you've been solving problems in this class and in 161.
00:01:18.162 - 00:01:32.838, Speaker A: Rather, you have to make an irrevocable decision each time a new piece of the input shows up. So like an online decision making, every day, you had to take an action without knowing the cost of that action. So to specify an online problem, I need to tell you sort of the way in which the input arrives.
00:01:32.838 - 00:01:51.134, Speaker A: What do I mean by piece by piece? And then what decision do you have to make when a new piece arrives? So for this problem, we're going to be thinking about jobs being scheduled on machines. So there's going to be M machines. All of them are exactly the same.
00:01:51.134 - 00:02:11.350, Speaker A: And you know about these up front. Okay, so you know you've got your M machines and then what arrives online are the jobs. So jobs arrive online one by one and job J has a processing time PJ.
00:02:11.350 - 00:02:22.314, Speaker A: Okay, so this is the input coming in one at a time, jobs one at a time. What decisions you have to make when you get a new job. You have to figure out which machine to schedule it on to assign it to.
00:02:22.432 - 00:02:23.100, Speaker B: Okay.
00:02:28.030 - 00:02:36.718, Speaker A: So you wish you could just wait till you had all the jobs and then you just assign them in a globally optimal way, but that's not going to be allowed. So this is like real time scheduling, if you like.
00:02:36.804 - 00:02:39.838, Speaker B: Okay, all right.
00:02:39.924 - 00:02:49.090, Speaker A: And so again, by schedule. So feasible solutions correspond to schedules, and that just means an assignment of each job to a single machine.
00:02:54.150 - 00:02:55.720, Speaker B: It okay.
00:02:58.650 - 00:03:33.200, Speaker A: All right, so what's the objective? What do we want? So people have thought about a lot of different objective functions in a scheduling context, but probably the most practically relevant one is the one we'll talk about now called the makespan. So to define the makespan, I need to tell you what the load on a machine is in a schedule. So the load on machine I is just the sum of the processing times of jobs assigned to i.
00:03:34.530 - 00:03:35.280, Speaker B: Okay?
00:03:36.930 - 00:03:51.590, Speaker A: So each job is a processing time of given machine. Out of a bunch of jobs assigned to it, you sum up those processing times that's its load, and then the make span is just the minimum sorry, the maximum over machines of the machine load.
00:03:52.410 - 00:03:53.160, Speaker B: Okay.
00:03:54.250 - 00:04:14.380, Speaker A: So for example, if you have four jobs, two machines, and you schedule them like so 2213 here, you're going to have a make span of four. Right? Both machines have load four, so the max is four. On the other hand, that's a three.
00:04:14.380 - 00:04:30.770, Speaker A: On the other hand, if you sort of screw it up a little bit and you put say, a two and a three on the same one, and then over here you have a one and a two. So here the makespans five, which is worse. We want a small makespan.
00:04:33.430 - 00:04:34.180, Speaker B: Okay.
00:04:37.190 - 00:04:49.014, Speaker A: So, yeah, so we want to minimize this. So basically the goal is to load balance as evenly as possible.
00:04:49.132 - 00:04:49.414, Speaker B: Okay?
00:04:49.452 - 00:04:55.322, Speaker A: So that's the way we're going to minimize the maximum load is we're basically going to try to get it as even as possible.
00:04:55.456 - 00:04:56.140, Speaker B: Okay.
00:04:56.910 - 00:05:21.646, Speaker A: Again, in many scheduling contexts, makespan is really what you care about. So like, if you want to think about, say, you're spawning off a bunch of MapReduce jobs or hadoop jobs to do a bunch of parallel processing, well, in many cases you don't know the final answer until the last of those parallel processes has completed. So it's really governed by the latest job to finish that corresponds to the last job on the machine with the biggest load.
00:05:21.758 - 00:05:22.226, Speaker B: Okay.
00:05:22.328 - 00:05:39.740, Speaker A: So it's really very practical objective function. Same questions about the setup, either about sort of what I mean by the data arrives online or by what the objective function is or anything else. Clear so far?
00:05:46.590 - 00:05:47.340, Speaker B: Okay.
00:05:48.030 - 00:06:09.970, Speaker A: All right, so this is one of those cases where sort of possibly the first algorithm you'd think of actually does great, so it's sort of a happy case. So here's the natural approach. And this was actually analyzed exactly 50 years ago by Ron Graham when he was at Bell Labs.
00:06:10.630 - 00:06:11.154, Speaker B: All right?
00:06:11.192 - 00:06:14.082, Speaker A: So jobs coming online. You got to schedule it right now. Right.
00:06:14.082 - 00:06:18.838, Speaker A: This job shows up. So you got to pick a machine. So, I mean, there's a few things you could think to do.
00:06:18.838 - 00:06:38.902, Speaker A: You could pick a machine uniformly at random and hope it gets pretty load balanced. But actually, which machine would seem to make sense given that you want everything to be even at a given time, which machine should you put the new job on? Right, exactly right. So I heard many people say the one with a minimum load.
00:06:38.902 - 00:06:59.090, Speaker A: Okay, so the one which is sort of under full compared to everybody else. So that's what we're going to study. So when new job J arrives, assigned to the machine with smallest current load.
00:07:03.110 - 00:07:03.860, Speaker B: Okay.
00:07:10.890 - 00:07:41.550, Speaker A: Now how are we going to measure the quality of this online algorithm? So we need a benchmark a yardstick against which we compare our own performance, and we're going to use the strongest possible benchmark. We're going to say, well, imagine we actually could wait until we had all the jobs and then we just computed the optimal schedule, the schedule with the minimum make span. Or if you prefer, imagine that we had full knowledge of the future and then we could schedule the jobs in a way that would get us the minimum possible make span.
00:07:41.550 - 00:08:17.702, Speaker A: So we define opts to be the minimum make span of any schedule in hindsight. So when we're talking about online decision making, we talked about the best fixed action in hindsight. That doesn't really have a natural analog in this context, but we really can just what we're going to do successfully is say that we're not too much worse off than this very strong benchmark.
00:08:17.702 - 00:08:21.950, Speaker A: Okay, so what an all powerful, all knowing opponent could accomplish.
00:08:22.850 - 00:08:31.310, Speaker B: Okay, it all right.
00:08:31.460 - 00:08:52.278, Speaker A: So we have our problem, we have our algorithm, we have our benchmark. We want to know how close is our algorithm to this benchmark. So what are the challenges involved in trying to prove some kind of guarantee for an online algorithm like this? Well, the challenge is kind of the same thing as it was when we were trying to design optimal algorithms for max flow and bipartite matching and so on.
00:08:52.278 - 00:09:07.558, Speaker A: We need to know how do we know that we're done? And back then it meant how do we know when we're optimal. Now here where we're making decisions online and the optimum has full hindsight or full foreknowledge. We're not going to expect to be optimal.
00:09:07.558 - 00:09:56.118, Speaker A: We expect we have to make some compromise in our guarantee reflecting the fact that our algorithm is much less powerful than the benchmark. Okay, so we're not going to say that the algorithm is optimal, but we still want to say that the scheduler produces is close to optimal. So then the question for the analyst is how do we know if the solution is close to optimal? So how do we know if we're close to the optimal solution? So how might we prove this? So the idea in the analysis, so the algorithm is just Graham's algorithm.
00:09:56.118 - 00:10:10.270, Speaker A: So the algorithm is just as is. So we're talking now about strategies for the analysis. So the plan is rather than try to understand opt itself in any detail, we're going to define some simple lower bounds on how small opt could possibly be.
00:10:10.270 - 00:10:18.420, Speaker A: So these lower bounds will be things that could only be better than opt. Sort of like when we were talking about dual feasible solutions. It's kind of a similar idea.
00:10:18.420 - 00:10:25.140, Speaker A: So next what I want to show you is I want to show you two different simple lower bounds on this number.
00:10:25.750 - 00:10:26.500, Speaker B: Okay?
00:10:28.070 - 00:10:50.874, Speaker A: Then we're going to compare the output of Graham's algorithm to those lower bounds on the optimal solution that will give us the guarantee. All right? So then the question is, where do these lower bounds come from? Actually, one of the most powerful tools for generating these lower bounds you already know, which is just linear programming duality. So remember, this is a minimization problem.
00:10:50.874 - 00:11:04.526, Speaker A: So we're looking for lower bounds, and that's exactly what dual feasible solutions give you for a minimization problem. Now the two online problems we're going to talk about today, actually, we don't need to be so sophisticated. We're going to be able to just kind of make up good enough lower bounds for our purposes anyways.
00:11:04.526 - 00:11:18.840, Speaker A: But a little bit later, we will use linear programs for this purpose. Again, just in the analysis, not in the algorithm. Okay, so the first lower bound I hope, is kind of obvious.
00:11:18.840 - 00:11:41.402, Speaker A: The min make span is at least the size of the biggest job. Any schedule has to put the biggest job somewhere, and that machine is going to have load at least equal to the size of that job. Okay, so that's our extremely easy lower bound.
00:11:41.402 - 00:12:03.790, Speaker A: Then we have a still quite easy lower bound. That's limit two. So limit two says not only does the min make span in hindsight have to be at least the size of the biggest job, but it also has to be at least the average sort of the sum of the processing times divided by the number of machines.
00:12:03.790 - 00:12:22.140, Speaker A: So in some sense, the best possible scenario would be like all the jobs get divided across the machine so that everybody's load is exactly the same. So formally, this is one over m. So remember, m is the number of machines and then the sum of the processing times.
00:12:23.550 - 00:12:24.300, Speaker B: Okay?
00:12:25.790 - 00:12:43.070, Speaker A: The way to think about this is sort of like the gold standard for load balancing is you make everything exactly the same, the load of everybody exactly the same. And this is what that would mean if you made everybody exactly the same, the total load on everybody divided by the number of machines. So just to make it formal.
00:12:43.070 - 00:12:59.910, Speaker A: So consider any feasible schedule again in hindsight. Well, the max load of a machine, i, e. The make span, that's only more than the average load of a machine.
00:12:59.910 - 00:13:33.884, Speaker A: You can't have everything be smaller than the average, and this is just equal to this quantity. So those are our two lower bounds, both of those clear. So neither of these in general is going to be exactly the same number as opt, but they can only be better than opt.
00:13:33.884 - 00:13:48.464, Speaker A: They can only be smaller than opt. So if we can actually compare our algorithm solution to one of these lower bounds, then we know we also can similarly compare ourselves to the only worse actual optimum solution. Okay, so we're going to see this a lot for the remainder of 261.
00:13:48.464 - 00:14:12.250, Speaker A: This idea where you don't exactly characterize what the optimum is, but you get some handle on it, you get some bound on it, and you compare the output of an algorithm to your bounds. So this is canonical in that sense. And then the theorem is that actually this online algorithm does pretty well given how strong the benchmark is.
00:14:12.250 - 00:14:29.264, Speaker A: So no matter what happens, your schedule at the end of the day, it's not going to be optimal. We sort of expected that because we're forced to be online, but it's not going to be more than double optimal. So you lose a factor two from not knowing the future.
00:14:29.462 - 00:14:30.210, Speaker B: Okay.
00:14:36.340 - 00:15:01.000, Speaker A: So in online algorithms terminology, this is called being too competitive. And then sometimes this number is called the competitive ratio. Okay, so if the online algorithm is always within an alpha factor of the optimum, then we call it alpha competitive or it has a competitive ratio of alpha.
00:15:01.000 - 00:15:07.390, Speaker A: Okay, so any questions about that before we prove it? Proof is not difficult.
00:15:11.840 - 00:15:12.590, Speaker B: Okay.
00:15:16.260 - 00:15:47.800, Speaker A: So what's the proof? Well, consider the worst load in the online solution. Okay, so let I so again run the online algorithm to completion and now in hindsight looking at what it did, focus on the machine that wound up being the biggest that determines the make span. Okay, so call that machine I and we're going to be interested in the last job that ever got placed on machine I.
00:15:47.800 - 00:16:23.956, Speaker A: Okay, so in different iterations of the online algorithm, different jobs got assigned. We're sort of fast forwarding to the last iteration where some job was assigned to this worst machine i. Now somewhere we better use the algorithm as being at least slightly clever, right? It's not an arbitrary algorithm, it's an algorithm which is greedy in the sense that when a job comes up, why did it schedule it, where it scheduled it? Well, it's because that job had the minimum load at that time.
00:16:23.956 - 00:16:53.244, Speaker A: That's the definition of the algorithm. You schedule something on the machine that currently has the minimum load. So prior to Jay's assignment was the smallest load at that time.
00:16:53.244 - 00:17:03.090, Speaker A: So we're thinking about some iteration where this job j gets scheduled on i, iteration 117. So in a iteration 117, I had the smallest load. That's why j got scheduled on it.
00:17:03.090 - 00:17:17.008, Speaker A: And then kind of the same argument here but going in the other direction. Well, the minimum load can only be less than the average load in iteration number 117.
00:17:17.184 - 00:17:17.910, Speaker B: Okay.
00:17:20.140 - 00:17:36.860, Speaker A: So you can't have everything bigger than the average. And so this is just equal to continue this here. Well, let's think about all the jobs that were scheduled prior to this iteration.
00:17:36.860 - 00:17:59.632, Speaker A: So currently we're scheduling job j. So the jobs which have been scheduled so far are jobs one through J minus one. That is the average load of a machine in iteration 117 just before I gets scheduled.
00:17:59.632 - 00:18:21.850, Speaker A: Okay, so what does that mean? So that means so fast forwarding all the way to the end of the algorithm. The final load on this machine is at most what its load was before it got its last job, plus.
00:18:24.380 - 00:18:24.696, Speaker B: The.
00:18:24.718 - 00:18:34.750, Speaker A: Processing time of that last job that it received. So this was an upper bound on before job J was assigned to it. This is an upper bound on after job J is assigned to it.
00:18:34.750 - 00:19:03.130, Speaker A: So what do our Lemmas tell us? Our Lemmas tell us that whatever the job J is, the size has to be bounded by the minimum possible makespan. So our benchmark, we also observe that the min make span can't be any less than the sum of all of the processing times divided by M. So it certainly can't be less than this prefix of the processing times divided by M.
00:19:03.130 - 00:19:16.430, Speaker A: So that's less than opt. This is using lima one, this is using lima two. So that gives us our competitive ratio of two.
00:19:18.000 - 00:19:18.750, Speaker B: Okay.
00:19:22.400 - 00:19:42.644, Speaker A: So that's a full proof of the theorem. Any questions about any of that makes sense, you might be wondering about this too. And is it tight? So it basically is tight in the worst case.
00:19:42.644 - 00:19:56.616, Speaker A: I'll ask you to think about that on exercise set number seven. If you make an assumption that the jobs aren't too big relative to sort of a lot of jobs, and they're relatively small, which is actually a pretty common case in practice, then you do do much better than two.
00:19:56.718 - 00:19:57.370, Speaker B: Okay.
00:19:59.420 - 00:20:08.120, Speaker A: Yeah. But again, even in the worst case, despite the fact that you're online, you still get within a reasonable constant factor. So that's definitely the happy case in online algorithms.
00:20:08.120 - 00:20:15.920, Speaker A: Doesn't always happen. Happens here. All right, so any more questions where I hide the proof?
00:20:17.940 - 00:20:18.690, Speaker B: Okay.
00:20:21.700 - 00:20:41.476, Speaker A: All right, so I want to do two more sort of case studies in the design and analysis of online algorithms. The first one is going to be about a problem called the Steiner Tree problem. We'll talk about that today, and then mostly on Thursday we'll talk about we'll revisit our old friend bipartite matching.
00:20:41.476 - 00:21:07.816, Speaker A: But this time we'll think about it from an online algorithm's perspective. And there we'll see a very nice sort of randomized, greedy algorithm, which does quite well, and I should say, just so you don't get the wrong idea. So in all three of these case studies this week, we're going to be using this super strong benchmark of the best thing you could have done in hindsight, or the best thing you could have done with full foreknowledge.
00:21:07.816 - 00:21:28.532, Speaker A: So that's kind of a brutal comparison to compare yourself when you only know what you've seen so far, to compare yourself to someone who knows everything. Obviously, if you get positive results, it's great so like for this scheduling algorithm, there are a lot of problems where all online algorithms have terrible competitive ratios just because it's too unfair a comparison. So sometimes you have to change the model.
00:21:28.532 - 00:21:46.312, Speaker A: That's one of the themes in my 264 beyond worst case analysis course. But for 261, what I'm going to do is I'm just going to cherry pick a few problems that are quite fundamental. Problems for which you actually can get good online algorithms with good worst case competitive ratios.
00:21:46.312 - 00:22:08.000, Speaker A: So there are some success stories and that's what I'm focusing on in these lectures. All right, so now I want to talk about a graph problem, the Steinertree problem. So the punchline here is going to be that a natural greedy algorithm turns out to be the optimal online algorithm.
00:22:08.000 - 00:22:27.050, Speaker A: And the analysis is pretty nice and we'll introduce some tricks that we'll use again later when we study the traveling salesman problem. Okay, so again it's an online problem. So I have to tell you in what sense the data arrives piece by piece and what decision does the algorithm make each times it gets a new piece.
00:22:27.050 - 00:22:56.000, Speaker A: So there's a bunch of stuff, you know, up front, so there's an undirected graphic. And also every edge has a cost non negative. Okay, so this is known quote unquote, offline I e at the beginning.
00:22:56.000 - 00:23:19.960, Speaker A: What arrives online are terminals. So arriving online one by one, terminals t one, t two, up to tk. These are all vertices, vertices of the graph.
00:23:19.960 - 00:23:32.428, Speaker A: And the way you think about a terminal is the terminals need to be connected to each other. Okay, that's sort of the responsibility of the algorithm at all times. Whatever terminals have showed up, they should be connected to each other.
00:23:32.428 - 00:23:40.450, Speaker A: Now there are these edge costs. Presumably we want to keep them connected as cheaply as possible. Okay, so that's the standard tree problem.
00:23:40.450 - 00:24:02.360, Speaker A: So goal maintain, in a perfect world you'd maintain the minimum cost. It's going to be approximate because it's online. Min cost subgraph spanning all terminals.
00:24:02.360 - 00:24:17.604, Speaker A: Spanning all terminals so far. Okay, so I say the min cost subgraph. But if think about it, edges are non negative.
00:24:17.604 - 00:24:22.824, Speaker A: So there's never a point in having a cycle. You may as well just delete an edge from a cycle. You get an only cheaper solution.
00:24:22.824 - 00:24:33.408, Speaker A: So really these are going to be trees. So it's called the Steiner tree problem. And so the terminals arrive online.
00:24:33.408 - 00:24:54.280, Speaker A: So in general maybe in the 11th iteration you have, these ten terminals have showed up, they're already connected to each other. The level one shows up, you have to somehow extend it to the existing infrastructure. So you might want to think about something like a cable company extending infrastructure to a new neighborhood to keep them, to connect them when a new market comes, emerges.
00:24:54.280 - 00:25:00.600, Speaker A: All right, so any questions about the problem definition?
00:25:01.340 - 00:25:05.300, Speaker B: Yeah, the goal itself is defined in an online manner.
00:25:05.460 - 00:25:14.556, Speaker A: Yeah, you're right. I'm being sort of sloppy about that. I mean so maybe a better way to say it would be I first just define the Steiner Tree problem in the way that I normally define problems offline problems.
00:25:14.556 - 00:25:27.244, Speaker A: Just say I give you all the terminals up front and then the goal is just to have the tree spanning them with the minimum possible cost. Here we have an online version. So again, we're going to be comparing ourselves to the best solution in hindsight.
00:25:27.244 - 00:25:40.710, Speaker A: So the best Steiner tree in hindsight. So in a perfect world we'd love it if at every single time step we currently have the minimum cost diner tree on the current points. Now that's not going to happen, but we hope to be close to that sort of gold standard.
00:25:41.400 - 00:25:42.150, Speaker B: Yeah.
00:25:44.760 - 00:26:08.740, Speaker A: So I'm sort of conflating together here the definition of just the normal Steiner Tree problem and then the online version that we're studying now. Yeah, it any other clarifications? It's a good question. Okay, so I'm going to prove a result for what looks like a very special case though it turns out to not be a special case at all.
00:26:08.740 - 00:26:12.560, Speaker A: And so it's a special case which is the.
