00:00:00.570 - 00:00:39.434, Speaker A: So this is going to be one of those lectures where I tie together a bunch of the themes that we've been discussing both to sort of reinforce the concepts but also expose you to the connections between them. So we're going to talk more about multiplicative weights and in particular, we're going to use it as a black box to derive various interesting algorithms and results. We're going to talk a little bit more about zero sum games and we're also going to talk about linear programming. Then next week we'll start kind of a traditional introduction to online algorithms. So recall multiple weights as an online algorithm. It solves the online decision making problem where the reward vectors come in online one by one. Other than that, we're actually not going to really focus on the online aspect today.
00:00:39.434 - 00:01:05.694, Speaker A: We're going to focus more on applications which are even interesting offline like linear programs. So last lecture we covered multiple weights. So it's a very simple algorithm. So we're not going to actually need the code of the algorithm today. So it's okay if you don't remember exactly what the update rule is, but I do want you to sort of remember that it was some super simple algorithm which was very lightweight and easy to implement. That much you should remember. So you can look it up if you need it.
00:01:05.694 - 00:01:37.402, Speaker A: The analysis was not that easy, but we went through it and now we're just going to use it as a black box. So let me tell you what, we remind you what we proved on Tuesday. So an adversary picks these reward vectors. So there's some set of actions that you're allowed to pick. Actually it should be minus eleven. And each day you commit to a distribution over actions using this multiple awaits algorithm. Then an adversary produces a reward vector with all the rewards between one and minus one.
00:01:37.402 - 00:02:15.474, Speaker A: You want to have the biggest sum of rewards that you can. And remember, the benchmark we use is the best fixed action in hindsight. So we look at the best actions summed over all time periods of what reward you could have got had you always played that action every single day. And the guarantee is that the expected so the expectation is over the coin flips in multiple weights, right? It's a randomized algorithm, so the expected reward earned by multiple weights is as good as this benchmark up to an error term that grows sublinearly in the time horizon. Capital t okay. So we proved that last time. For today's applications, I'm going to need some sort of very minor extensions of this guarantee.
00:02:15.474 - 00:03:26.990, Speaker A: So let me tell you two sort of very simple extensions. So first of all, if we think about this for a second, the multiple weights algorithm doesn't just have a guarantee with respect to every fixed action in hindsight. It also has exactly the same guarantee with respect to every mixed strategy over actions in hindsight. So right now I'm sort of doing the counterfactual. Well, what if I just always played action one? What if I just always played action two? What if I just always played action three? So I could give you the more general, seemingly more general counterfactual, where you could say, what if I always play 60% on one and 40% on two? What if I always play 20% on two and 80% on four, et cetera. Okay, so this guarantee immediately implies the guarantee for probability distributions. So note, for any distribution or mixed strategy P over the actions, it's actually true that the expected reward of multiplicative weights well, actually, let me just write it this way.
00:03:26.990 - 00:04:34.210, Speaker A: I mean, the upshot is, in hindsight, there's no reason to randomize. In hindsight, you may as well just play a best action. That's the best you can do. So why is that? Well, think about any fixed distribution and imagine every day you mixed, you chose an action according to this distribution P. Then on day T, what would be your expected reward? Well, we look at each of the actions, we look at the probability that you play that action, and we look at what you would have gotten on that day if you wound up randomly picking action A. So that's what you would have gotten had you played the mixed strategy P every day. So if we just summon the other direction, you start seeing what I mean, that in hindsight, there's no reason to do anything other than just play a fixed action, because for each term for each A, this is at most the best action.
00:04:34.210 - 00:05:09.230, Speaker A: So this is just saying for some action A, what would you get if you played it day over day after day after day after day? And by definition, that's at most the maximum over those same quantities. So we just upper bound this by the best fixed action. In hindsight, this is a probability distribution, so it sums to one. And so we're just left with no better than max over a sum T equal into T RTA.
00:05:10.050 - 00:05:10.800, Speaker B: Okay?
00:05:12.370 - 00:05:55.326, Speaker A: So again, I'm not saying anything deep here. I'm just saying that because in hindsight, fixed actions are the best things. If you want, you can interpret the multiplicative weights guarantee as also being good against any fixed mixed strategy, but it turns out that's a convenient form to plug into our applications. Is everyone clear on that? I hope that's straightforward. Okay, so let me also remind you then about a second way of stating the multiple weights guarantee. So sort of an interpretation, and this is the form in which we're going to apply it in today's lecture. So here we say, okay, well, how do we know how to think about this error term minus two square root t log n.
00:05:55.326 - 00:06:19.350, Speaker A: And so the suggestion is, well, let's think about it. Time averaged, meaning we divide through by capital T. So then that becomes root log n, over root T. So that's the sort of per step regret that we're suffering. And then we ask the question if we had some target per step regret epsilon, how many days would we have to play this game before the per time step regret had dropped to epsilon or below?
00:06:19.500 - 00:06:20.200, Speaker B: Okay?
00:06:20.890 - 00:06:59.678, Speaker A: And so phrase that way, this is equivalent to saying that once we've played a game at least four log N over epsilon squared timesteps, then the time average regret is at most epsilon. And so again, how do you get that? You just divide this thing by T and then you ask what does T have to be before two square root log N over T becomes less than epsilon and it's four log n over epsilon squared.
00:06:59.774 - 00:07:00.274, Speaker B: Okay?
00:07:00.392 - 00:07:41.230, Speaker A: And again, I mean, I want to emphasize this is like really good. We don't see a lot of stuff which is logarithmic in kind of the input size. But here the convergence sum of multiplicative weights, even though you have N actions is sort of logarithmic in the number of actions, okay? So just notice you will not have played very many actions by the time this algorithm stops, right, if you think of epsilon as being not too small. So you're just randomly sampling an action each day, you're going to terminate within a logarithmic number of days and you have N actions, so you have plated a vanishingly small fraction of the actions while you've done it. So it's really very fast. And that's one of the reasons why multiplied of weights has a lot of applications. It's because of that logarithmic only dependence on the number of actions.
00:07:41.230 - 00:08:46.770, Speaker A: Okay? So here's the other simple extension which we'll also use. So we've been making the assumption that the rewards are bounded between minus one and one, okay? And I said that's without loss of generality by scaling, which is true, but actually you have to think a little bit about what effect that scaling has on the convergence rate in our applications. We're going to want to allow, we're going to work with rewards which are not necessarily in minus eleven. So suppose the rewards are in minus M m, okay, for some possibly big number M. And now suppose we again wanted to get per time step regret at most epsilon. So how do we do it? Well, we do it just by reducing to the previous case. We're just going to scale the rewards, run our old algorithm, inherit the old guarantee, and then scale back up to the original setting.
00:08:46.770 - 00:09:44.760, Speaker A: So one thing we got to remember is that if we're going to scale down by capital M and after we scale back up, we want regret only epsilon. After we've scaled down, we need to make sure we have regret a most epsilon over M, so that when we scale back up it's a most epsilon. So to get a most epsilon, scale all rewards down by M. So now they lie in minus eleven. So we can just run our old algorithm on these scaled down rewards. We run multiplicative weights on the scaled costs scaled rewards, excuse me. Till the average regret is the most epsilon over m, and then you scale back up.
00:09:48.730 - 00:09:49.480, Speaker B: Okay.
00:09:53.470 - 00:10:04.874, Speaker A: So we're just going to how many, how many iterations does it take? It's just the number of iterations our old Molar weights took to get down to this per time step regret. So that just means we pick up an extra m squared in the number of iterations.
00:10:05.002 - 00:10:05.680, Speaker B: Okay.
00:10:09.970 - 00:10:18.562, Speaker A: So I'm just substituting epsilon over m for epsilon on the upper right board. So m squared over epsilon squared log.
00:10:18.616 - 00:10:19.220, Speaker B: N.
00:10:21.510 - 00:11:06.930, Speaker A: Alternatively, one way to think about the scaling is you could just kind of modify the multiplicative update weight update rule so that it scales inside. So alternatively, if you prefer, you can think about the scaled version of multiplicative weights where at every time step, remember, the weight of an action evolves from time T to t plus one, so it's its old weight times some multiplicative update. Yesterday we had one plus Ada, the reward of a at time t. So instead you use one plus Ada over m times the reward of time t. Okay, so if you like, you can just sort of work directly with the original rewards and just use this update step instead, and that's going to be equivalent.
00:11:07.350 - 00:11:08.100, Speaker B: Okay.
00:11:09.910 - 00:11:26.242, Speaker A: Any questions about those extensions? So we can compete with all fixed mixed strategy distributions and it's not a big deal if the rewards are big, we just have to be sensitive to the fact that our convergence time is quadratic and are bound on the rewards.
00:11:26.386 - 00:11:27.080, Speaker B: Okay.
00:11:29.550 - 00:11:31.260, Speaker A: All right, what was I going to do next?
00:11:32.030 - 00:11:34.460, Speaker B: Okay, so questions?
00:11:35.710 - 00:11:36.780, Speaker A: Everything clear?
00:11:37.710 - 00:11:41.318, Speaker B: Yes. Try to interpret this time average regret.
00:11:41.414 - 00:12:15.462, Speaker A: This does mean if we were running this for like identifying something or some sort of problem like that, that we're actually getting better at doing what we wanted to do over time on average. Yeah, there'll be some time steps where it didn't work out so well. And again, you should always remember we're comparing ourselves to the best fixed thing we could have done. But absolutely, I mean, if there's some fixed thing which is good, then we're converging closer and closer to that in average performance over time, which is great. What's the meaning of that?
00:12:15.516 - 00:12:17.640, Speaker B: M square over epsilon square.
00:12:20.250 - 00:12:33.790, Speaker A: This is the number of iterations. So if rewards are between minus m and m and you want to get down to a time average regret of at most epsilon, then it's going to be m squared over epsilon squared times login iterations. That'll be sufficient.
00:12:35.730 - 00:12:36.480, Speaker B: Okay.
00:12:39.490 - 00:12:45.120, Speaker A: Oh, yeah. Yes. Thank you. It's still the four.
00:12:45.890 - 00:12:46.800, Speaker B: Thank you.
00:12:48.290 - 00:12:49.280, Speaker A: Other questions?
00:12:51.170 - 00:12:51.920, Speaker B: Yes.
00:12:52.530 - 00:13:10.986, Speaker A: At the bottom. What on earth is that? So sorry. This is my Ada three. So previously the update rule didn't have the m. It was just one plus Ada. Times the reward. Remember, the reward can be positive or negative, so the weight could go up or down.
00:13:10.986 - 00:13:31.298, Speaker A: And now I'm just slowing down how rapidly the weights evolve. Okay, we're not going to ever need this in today's lecture, okay? So we're just going to be applying this guarantee as a black box greater.
00:13:31.314 - 00:13:34.054, Speaker B: Than four m squared of epsilon squared domain no.
00:13:34.092 - 00:14:00.706, Speaker A: So I want to know how many are sufficient. Oh, I see what you're saying. So I guess what I'm saying is within at most four epsilon squared over epsilon, four m squared over epsilon squared log, you'll get down to within epsilon. So it's guaranteed by that number. Maybe it happens faster, but you're right, your question is a good one. Maybe a more natural way of saying it is to guarantee at most epsilon run for that many or more iterations. So absolutely.
00:14:00.706 - 00:14:48.042, Speaker A: So it depends on your interpretation. Other questions? Okay, good. Then let's talk about some applications of this. So I want to say a little bit more about the Minimax theorem. So consider a zero sum game. Think of the payoffs as between one and minus one, although you could scale if you wanted. I showed you the Minimax theorem a week ago, which says that in a zero sum game, it doesn't matter which player goes first, as long as they both play optimally.
00:14:48.042 - 00:15:32.910, Speaker A: Then you get the same expected payoff either way, sort of very quickly. At the end of last lecture, I sort of sketched how you could derive this from the multi good weights algorithm. I'll put some details on that in exercise set six, which I'll post in a couple of days. So I want to talk about a different setup than last time. So last time we were thinking about both players independently playing multiple weights. Now we're going to have the players play in different ways, so they're still going to play for capital T time steps, which is what's needed to get down to epsilon. Regret, but only the row player will be using multiplayer weights.
00:15:32.910 - 00:16:15.798, Speaker A: So the row player picks a distribution over rows using a copy of multiple weights algorithm, right? So that so multiple weights outputs a distribution. So that totally makes sense. And then actually we're going to make this game more unfair for the row player than we did last time. So last time we envisioned the row and column player going simultaneously, each using their own private copy of multiple weights to pick an action at random. Today we're going to let the row player go. We're going to force the row player to go first. Okay, so the row player has to announce its mixed strategy over the rows that multiple weights recommends.
00:16:15.798 - 00:16:29.478, Speaker A: And then we allow the column player to do whatever it wants afterwards, knowing that. So in other words, we allow the column player to best respond to pick the column with the best expected payoff. Given the mixed strategy played by the row player.
00:16:29.594 - 00:16:30.260, Speaker B: Okay.
00:16:32.230 - 00:17:40.040, Speaker A: So the column player responds optimally. And we remind you something that we saw last lecture, or maybe the lecture before, which was that if you go second, there's no reason to randomize. So if you know what your opponent's doing first, you can just look at the expected payoff of each of your actions and play the best of them deterministically. So that's how we're going to think about the column player acting deterministically. Okay, now, I can't be done because this still isn't well defined, right? Multiple of weights is happy to recommend to you strategies, mixed strategies, but you need to feed it as input reward vectors. So the question is, the row player, what is it going to be feeding its copy of multiple weights at each time step? And here we use basically the same idea as last time, which is after the fact, after the play at Day T, the row player finds out what column the column player chose and then just looks at the induced reward vector. So what payoff the row player would have gotten in hindsight for each of the rows it could have played, given what the column player played at time T.
00:17:40.040 - 00:18:25.880, Speaker A: So, by the way, I'm going to call this deterministic column strategy Q sub T. Okay? So if QT consists of choosing column J, then we set, for the purposes of keeping multiple ways happy, the reward of row I, just as the payoff that the row player gets when the row player plays row I and the column player plays row J. Right? This is exactly the payoff the row player would have gotten at Time T had it played row, given what the column player did at Time T. Okay?
00:18:28.890 - 00:18:29.510, Speaker B: All right.
00:18:29.580 - 00:18:34.020, Speaker A: So is the setup clear? So again, it's the same kind of.
