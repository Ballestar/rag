00:00:00.410 - 00:00:12.442, Speaker A: All right, so let's jump into the formal proof. So first, just to remind you of the notation little M that denotes the number of machines. And then we have job length l one up to ln n is the number of jobs.
00:00:12.442 - 00:00:22.894, Speaker A: Let me introduce some notation for the two quantities that we really care about for this approximate correctness guarantee. So first of all, we care about the minimum possible make span. We're going to call that capital M star.
00:00:22.894 - 00:00:37.622, Speaker A: And of course we care about the make span of the schedule produced by Graham's algorithm. We're going to call that capital M. The whole point of this proof is to show that capital M is not too much bigger than capital M star, specifically bigger by at most a factor of quantity, two minus one over little M.
00:00:37.622 - 00:00:55.150, Speaker A: So it's pretty tricky to directly compare M and M star. So instead we're going to introduce two intermediate quantities. One is the maximum job length, the other is the average machine load, which will then be able to relate directly to both M and M star, giving us a relationship between the two.
00:00:55.150 - 00:01:08.430, Speaker A: So the first intermediate quantity, which is a lower bound on the minimum possible make span, it's easy enough. And actually we sort of already mentioned it. So remember that if some job has length twelve, any schedule has to put that job somewhere.
00:01:08.430 - 00:01:26.902, Speaker A: So there's going to be some machine that has load at least twelve, which means the make span is going to be at least twelve. So in other words, whatever the best possible makespan M star is, it can't be any smaller than the length of a job. So it's at least as big as every job, or if you like, at least as big as the longest job.
00:01:26.902 - 00:01:47.066, Speaker A: So what the second lower bound basically says is that the best case scenario for the makespan is for every machine load to be exactly the same, for everything to be perfectly balanced. And we saw that, for example, in the quiz where the optimal schedule balanced everything and had load five across all the machines. So that's as good as it gets, and that's what the second lower bound asserts.
00:01:47.066 - 00:02:14.642, Speaker A: So a little more precisely, what is the sum of all of the machine loads? In a schedule where you've got your end jobs, each job is assigned to exactly one machine. And then the job of length, say twelve, will then contribute exactly twelve to the load of the machine to which it is assigned. So in other words, the sum of all of the machine loads is exactly the same as the sum of all jobs sum over j equal one to n of l sub j, because every job has to go to exactly one machine.
00:02:14.642 - 00:02:32.310, Speaker A: So what does it mean for them to be perfectly balanced? It means that all of the machine loads are exactly the same, which means that each of the little M machines has an equal share of the total sum of the jobs. So one over little m times the sum of the job links, that's the best case scenario. Perfectly balanced schedule.
00:02:32.310 - 00:02:43.134, Speaker A: No schedule is better than the perfectly balanced one. So in particular m star, the minimum mixpan can't be any better than that idealized machine load for the perfectly balanced case. So this is good.
00:02:43.134 - 00:03:04.290, Speaker A: We've introduced two simple intermediate quantities and we've related both to the minimum possible make span. Specifically, each of these intermediate quantities can only be less than the minimum possible make span. So what remains to be done and what we'll do on the next slide is to relate the make span, capital M of the schedule produced by Graham's algorithm to these same two intermediate quantities.
00:03:04.290 - 00:03:14.934, Speaker A: So let me just sort of circle the key inequalities. These are going to be used crucially in the proof I show you on the next slide. So let's move on to the analysis of the schedule produced by Graham's algorithm.
00:03:14.934 - 00:03:33.866, Speaker A: And what we want to do here is formalize the second step of our intuition. Remember what we said then. We said, well, the difference there's going to be some imbalance maybe in the schedule, but the difference between the maximum machine load at the end of the gram algorithm versus the minimum machine load, it's going to differ by at most a job, because a single job toggled some machine status from the minimum to the maximum.
00:03:33.866 - 00:03:51.526, Speaker A: So let's make that precise, let's start with some notation. So first of all, by machine i, we're going to mean the machine that has the maximum load under the schedule produced by Graham's algorithm. So in other words, the machine whose load achieves the make span of the Graham algorithm so it has load capital m.
00:03:51.526 - 00:04:05.914, Speaker A: Meanwhile j, we're going to use that to denote the final job that the algorithm assigns to this most loaded machine i. This is going to be the one that toggles its status. So this job j, it may be the last one in the input or it may not.
00:04:05.914 - 00:04:24.510, Speaker A: So what I want you to think about doing is rewind Graham's algorithm to the iteration in which j is assigned. So after this iteration, no more jobs get assigned to machine i. Also let l hat sub I denote the load of machine I just before this job j was assigned to it.
00:04:24.510 - 00:04:39.538, Speaker A: So it's load just before it got its final job J. So over on the right, let me show you a cartoon of what this might look like. So suppose you have five machines, they have pretty balanced loads, but if you had to pick, it would be the first machine that at this moment in time had the lightest load.
00:04:39.538 - 00:04:53.174, Speaker A: And then job J comes and piles on and maybe job J was a pretty big job. Now maybe there's some more jobs to schedule and maybe Graham's algorithm goes ahead and schedules them. But by definition, it's not going to schedule anymore on the first machine.
00:04:53.174 - 00:05:18.258, Speaker A: That's by the definition of this job J being the last one assigned to this machine. But maybe a little more stuff sort of gets sprinkled on top the other four machines. So how big could capital M, how big could this make span possibly be? Well, let's first think about how big could l hat I possibly be? How much could the load of this machine have been before job J was assigned? And then let's think about what's the extra machine load contributed by job J.
00:05:18.258 - 00:05:38.834, Speaker A: So first of all, machine I's load at that time. So capital L hat I that can't have been any bigger than the average machine load at that time because remember, I had the minimum machine load at that time. So for example, if things were perfectly balanced at the time job J showed up, what would that mean? That would mean the sum of the job lengths of the previous jobs.
00:05:38.834 - 00:05:53.126, Speaker A: So the jobs one two up to J minus one, the sum of those job lengths would have to be spread exactly evenly over the M machines. So in a perfect schedule, all the loads would be one over little M times the sum of the first J minus one. Job lengths.
00:05:53.126 - 00:06:07.650, Speaker A: In general, in any schedule, there might be some machines that are smaller than that and then other machines that are bigger than that. But in any case, there's going to be machines that have at most this average load. And so I, as the one that had the minimum load, is one of those machines.
00:06:07.650 - 00:06:19.662, Speaker A: So the final makespan capital M. We know that's just the load L hat I before the last job J was assigned to I, plus the length of job J itself. So let's just add those two things together and then simplify.
00:06:19.662 - 00:06:34.294, Speaker A: We're in this last inequality. Just for fun, just for convenience, I've thrown in the lengths of the jobs that come after job J, if there are any. So I've just added in the lengths of the jobs J plus one up to job n to the sum.
00:06:34.294 - 00:07:02.606, Speaker A: Links are positive, so that only makes this quantity larger. Now let's just move a one over M fraction of job J's length into the sum. So now that that sum is going to be a sum over all of the job lengths, why did I do this? Because now we have an upper bound on the make span of the schedule produced by Graham's algorithm, which is in terms of the two intermediate quantities that we introduced on the previous slide, our two lower bounds on the minimum possible makespan those two circled inequalities.
00:07:02.606 - 00:07:18.038, Speaker A: So specifically, remember that in our first lower bound, we just said that every job has to go somewhere. So whatever the minimum makespan is there's no way that it's any less than the length of some job. So we can upper bound in particular, the length of job j by the optimal makespan capital M star.
00:07:18.038 - 00:07:41.566, Speaker A: Don't forget we also had a second intermediate quantity, another lower bound on the minimum possible makespan, where we just said that the best case scenario for minimizing the make span is that the machine loads are perfectly balanced, that they're all exactly the same. If they were all exactly the same, then each machine would have load one over little m times the sum of all the job lengths because they'd be sharing equally. And in general, any schedule can only have makes band worse than that.
00:07:41.566 - 00:07:59.622, Speaker A: So this quantity in our second term here the sum of the job length divided by the number of machines the perfectly balanced machine load that's also a lower bound on the minimum possible mix band capital M star. And now we are done. We can upper bound the first term by a one minus one over little m times capital M star.
00:07:59.622 - 00:08:09.622, Speaker A: We can bound the second term by capital M star. Add them together, there's that quantity two minus one over little m times the optimal makespan m star. So that is pretty cool.
00:08:09.622 - 00:08:43.458, Speaker A: That is an approximate correctness guarantee for Graham's fast heuristic algorithm, graham's greedy algorithm. And it's great to have an insurance policy like the one that we get from that, that no matter what happens, even in a doomsday scenario, the make span is never going to be worse than double the minimum possible. But as always, as algorithm designers, it's our duty to ask can we do better? Could there be another fast heuristic algorithm with an even better approximate correctness guarantee? An insurance policy with an even lower deductible? In fact, we can do better.
00:08:43.458 - 00:09:13.194, Speaker A: And all we need to do is make use of a very familiar for free primitive. One of those things which you can throw in as a pre processing step whenever you want, even if you don't know quite why you need it and which for free primitive. Well, you can sort of already get an idea from the contrived example we had in the quiz, a couple slides back where Graham's algorithm unfortunately considered all of the length one jobs first perfectly balanced them and then got stuck with this sort of really big job at the end.
00:09:13.194 - 00:09:32.958, Speaker A: So we'd like to somehow avoid sort of saddling Graham's algorithm with really big jobs at the end. How would we do that? Let's just sort the jobs by length at the beginning, starting with the biggest jobs and concluding with the smallest jobs. That is an algorithm known as the longest processing time algorithm or the LPT algorithm.
00:09:32.958 - 00:09:52.582, Speaker A: So that's the LPT algorithm, it's obviously, again quite simple and accordingly it can be implemented to run in a blazingly fast fashion with running time close to linear. I'm assuming here that you use a sorting subroutine that runs in near linear time. So for example, merge sort would take o of n log n time to sort n jobs.
00:09:52.582 - 00:10:17.242, Speaker A: And then for the second step, I'm assuming that we're implementing it using heaps, as we discussed back when we first talked about the running time of Graham's algorithm. So again, the question is not so much, is it a blazingly fast algorithm? Clearly it is. The question is, how good is it? What is the quality of the solution produced, the make span of the schedule output by the LPT algorithm? To get a feel for that, let's move on to another quiz.
00:10:17.242 - 00:10:31.030, Speaker A: So the input in this quiz is going to be a little more complicated than in the previous one. We're still going to have five machines, we're going to have eleven jobs, but they're going to have five different possible lengths. So three jobs with link five and two jobs with lengths each of six, seven, eight and nine.
00:10:31.030 - 00:10:57.652, Speaker A: And the question then, of course is what is the make span of the schedule output by the LPT algorithm? And how does that compare to the minimum possible make span, the best mixpan you could achieve, say, using exhaustive search? So the correct answer is D. And let's actually answer the questions in reverse order. Let's start with the minimum possible schedule of these eleven jobs.
00:10:57.652 - 00:11:11.600, Speaker A: As you've seen, the best case scenario is the perfectly balanced case. So we could ask the question, does there exist a schedule that makes all of the machine loads exactly the same? And if you stare at it, you're like, yeah, there is. We can get all five machines to have load exactly 15.
00:11:11.600 - 00:11:31.764, Speaker A: We pair up the sixes with the nines, the sevens with the eights, and then the three length five machines all have a dedicated machine. Meanwhile, what's going to happen with the LPT algorithm? Well, remember, the LPT algorithm processes jobs from biggest to smallest. So it's going to process the two length nine jobs, then the two length eight jobs, the two length seven jobs, and so on.
00:11:31.764 - 00:12:02.672, Speaker A: So for the first four iterations, things look pretty good. So the LPT algorithm schedules the two nine jobs on separate machines, then uses a third and fourth machine to schedule the two length eight jobs exactly, mimicking what's happening in the best possible schedule. So the difference happens in the fifth iteration, where in a perfect world, the LPT algorithm would put a job length of seven, sort of on top of one of the jobs of length eight, therefore keeping the fifth machine in reserve for the length five jobs that are eventually going to come.
00:12:02.672 - 00:12:11.360, Speaker A: That, of course, is not how the LPT algorithm works. It's going to schedule that first length seven job on the most lightly loaded machine. There's currently a machine with load zero.
00:12:11.360 - 00:12:23.364, Speaker A: So that's where it's going to put the first job of length seven. Now, when it gets to the second job of length seven, well, the least loaded machine is still machine number five. Now, with load seven, but still the best.
00:12:23.364 - 00:12:36.692, Speaker A: So that's where the second length seven, machine length seven job will go as well. The two length six jobs come next, and they'll be scheduled on the two machines that have current load eight. So that'll bring their loads also up to 14.
00:12:36.692 - 00:12:48.232, Speaker A: And then of course, the two length five jobs will be scheduled on the first two machines to join their length nine counterparts. At this stage, everything looks rosy. We've got perfect balance across all the machines.
00:12:48.232 - 00:12:58.144, Speaker A: Every one of them has load exactly 14. Unfortunately, here's the caboose one final third job of length five. And now we're out of options of where to put it.
00:12:58.144 - 00:13:04.928, Speaker A: Everything has load 14. We have to put it somewhere. Wherever we put it, it's going to boost the makespan up to 19.
00:13:04.928 - 00:13:20.632, Speaker A: So what's the point of this quiz? The point of this quiz is that the LPT algorithm is not always optimal. So it's a concrete instance where its makespan 19 is bigger than the minimum possible 15. And again, this isn't surprising, right? We know it's an empty hard problem.
00:13:20.632 - 00:13:39.964, Speaker A: We know this algorithm runs in polynomial time. Unless we're trying to refute the p n equal to NP conjecture, we're fully prepared for there to be examples where the LPT algorithm schedule is suboptimal. But again, we're kind of worried about how bad could this get? And in particular, you might be wondering, why did we bother to sort the jobs? For all we know, LPT is just as bad as Graham's algorithm.
00:13:39.964 - 00:13:51.080, Speaker A: It's not any worse. But is it really an improvement? In fact, the answer is yes. As I will prove to you next, the LPT algorithm does indeed have a better insurance policy than Graham's algorithm.
00:13:51.080 - 00:14:07.396, Speaker A: It does guarantee a schedule that has make span strictly less than twice the minimum possible. So in terms of the notation we were using before, remember capital M star denotes the minimum possible make span capital M. Before that was the make span achieved by Graham's algorithm.
00:14:07.396 - 00:14:14.484, Speaker A: Now it's going to be the make span achieved by the LPT algorithm. We're going to prove that it's no more than 1.5 times the minimum makespan.
00:14:14.484 - 00:14:34.592, Speaker A: Actually three halves minus one over two m, where little m is the number of machines. So the eagle eyed among you may have noticed that this doesn't actually quite answer the question of exactly how good is the LPT algorithm? So we had this bad example in the quiz and we saw that the LPT algorithm may blow up the make span by a factor of 19 over 15. So that's roughly 1.26
00:14:34.592 - 00:14:40.588, Speaker A: seven. So that's the blow up we saw from this heuristic algorithm in that example. That example had five machines.
00:14:40.588 - 00:14:54.896, Speaker A: If you plug in m equals five into this bound, you'll see that this claim only promises a blow up of at most 1.4, leaving open the possibility that there are worse examples than the ones the one we saw in the quiz. Well, I'm happy to report that if you're willing to work a little bit harder.
00:14:54.896 - 00:15:05.484, Speaker A: And I encourage you to do this in the privacy of your own home if you're interested. If you're willing to work a little bit harder. In fact, you can show that the bad example in the quiz is as bad as it gets.
00:15:05.484 - 00:15:37.204, Speaker A: This factor of three halves minus one over two m can actually be improved with sort of a little bit more difficult argument to a factor of four three minus one over three m. And if you plug in m equals five into that formula, you will get that there's a guaranteed blowup of only 19 five. And more generally for any number of machines, m, the natural generalization of the example in the quiz is as bad as it gets, and it approaches a factor of four thirds, so worse by 33% as the number of machines grows large.
00:15:37.204 - 00:16:05.132, Speaker A: So just like with our approximate correctness guarantee for a Graham's algorithm, you should view this as a kind of insurance policy about the doomsday scenario of the most contrived inputs that could possibly exist. And as with Graham's algorithm, if you run this on realistic inputs empirically, you will see that it usually over delivers and outputs schedules that are much closer to the minimum possible. Indeed, if you find yourself needing to solve the minimum makesband problem in one of your own applications, the LPT algorithm is probably the perfect place to start.
00:16:05.132 - 00:16:17.756, Speaker A: Maybe add some bells and whistles if you want, but LPT already will do very well for the basic makespan minimization problem. So let's move on to the proof. Proof is basically a refinement of the same argument we used for Graham's algorithm.
00:16:17.756 - 00:16:31.904, Speaker A: You might recall the three step intuition we had for that analysis. We're basically going to have a better version of the second step of that analysis. So in Graham's algorithm we said, oh, the damage that could be caused by a single job, sort of the difference between the maximum and the minimum loads.
00:16:31.904 - 00:16:54.700, Speaker A: A single job could cause damage no worse than the minimum possible make span. Here, because we've sorted the jobs from biggest to smallest and we're only going to be dealing with small jobs at the end of the algorithm, we can actually say that the damage is only going to be the minimum make span divided by two m star over two. And with a more refined argument, which again I'll leave to you, you can actually even improve the worst case damage to m star over three.
00:16:54.700 - 00:17:12.528, Speaker A: But let's just prove the M star over two bound. So precisely we're going to use the following variant of the first lower bound that we used in the analysis of Graham's algorithm. In Graham's algorithm, we just know whatever the job J is, the minimum make span has to be at least as big as the length of that job.
00:17:12.528 - 00:17:19.728, Speaker A: This version we're going to say. Now consider a job which is not one of the m longest jobs. So it's one of the other jobs.
00:17:19.728 - 00:17:33.160, Speaker A: So forget about the m longest jobs and just think about the rest. For one of those jobs, j actually, you can say that the minimum make span has to be at least twice as long as that job. Not one time as long as that job, but twice as long as that job.
00:17:33.160 - 00:18:02.080, Speaker A: So the reason this inequality holds is basically due to the pigeonhole principle, which is the intuitively obvious statement that if you stuff N plus one pigeons into N holes, there's going to be a hole with at least two pigeons. So for us, the machines are playing the role of the holes and we're looking at the longest m plus one jobs as playing the role of the pigeons. So by the pigeonhole principle, no matter how smart your schedule is, you have to put two of the longest m plus one jobs on the same machine.
00:18:02.080 - 00:18:30.936, Speaker A: So that means that machine has length at least as long as the sum of those two jobs, each of which is at least as long as the m plus one longest overall. So now that we have this refined version of the first lower bound, we can finish the proof of approximate correctness, basically following the exact same argument we did in Graham's algorithm. So, like in our previous proof, we're going to use I to denote the machine that winds up with the largest load in LPT's schedule.
00:18:30.936 - 00:18:46.860, Speaker A: And we're going to denote by J the final job that LPT assigns to that most loaded machine I. So one edge case, which is actually quite easy for us, is if actually LPT only assigns this one job J to this machine I. It never assigns any other job to I.
00:18:46.860 - 00:18:57.252, Speaker A: Why is that? You know, maybe this is like a huge job. It has length 173. Well, this is our most loaded machines, right? So this is going to be our make span 173.
00:18:57.252 - 00:19:17.320, Speaker A: But again, any other schedule also has to have a make span at least 173 because it has to put this job J somewhere. So we're actually optimal if it turns out that no other job is ever assigned to machine I. So suppose that the LPT algorithm does assign at least two jobs to machine I the last job J plus some previous job.
00:19:17.320 - 00:19:34.124, Speaker A: Now, if you think about it, this means that j this last job assigned to machine I, it actually cannot be one of the longest M jobs. And this is where we're using the fact that it's the LPT algorithm. What does LPT do? It sorts the jobs right up front, from the biggest jobs to the smallest jobs.
00:19:34.124 - 00:19:48.948, Speaker A: Now, the first m iterations of LPT are like super uninteresting. There's always some machine that's empty that has load zero, so it's just going to take the current job and assign it to some empty machine. It's only after the first m iterations of LPT that anything interesting happens.
00:19:48.948 - 00:20:02.036, Speaker A: So by virtue of this job j not being the first one assigned to this machine I, some other job was assigned to it beforehand. That means that J cannot be one of the first M jobs. It has to be outside of the first M jobs.
00:20:02.036 - 00:20:16.588, Speaker A: And because LPT sorts the jobs, that means it's not one of the longest M jobs. And that means our variant of lower bound number one applies to this job J. And in fact, the minimum makespan has to be at least double the length of this job J.
00:20:16.588 - 00:20:40.336, Speaker A: So that means if we just sort of reboot or restart our proof of the analysis of Graham's algorithm with that second to last inequality. So if you go back and check, you'll see that inequality said that the makespan capital M of our algorithm schedule, it's bounded above by two terms. First, the length of this last job on machine I times quantity one minus one over little M plus what would be the case if the schedule was perfectly balanced.
00:20:40.336 - 00:20:55.336, Speaker A: So one over M times the sum of all the jobs. Now, before in the analysis of Graham's algorithm, we bounded each of these two terms above by M star. And here, though, because J is not one of the M longest jobs, we can use the stronger version of lower bound number one.
00:20:55.336 - 00:21:06.510, Speaker A: And therefore, this LJ length of J is actually going to be at most M star over two. The second term, we're going to bound just like before using our second lower bound. So that's going to be at most M star.
00:21:06.510 - 00:21:36.052, Speaker A: So if you add these two things together and let the dust settle, you see you get this three halves minus one over two M insurance policy approximate correctness guarantee. So that wraps up our first case study of using the greedy algorithm design paradigm to design fast heuristic algorithms for an application to a scheduling problem. Let's move on to another application of greedy algorithms to another NP hard problem concerning assembling a team in the best possible way, something known as the maximum coverage problem.
00:21:36.052 - 00:21:37.680, Speaker A: I'll see you then. Bye.
