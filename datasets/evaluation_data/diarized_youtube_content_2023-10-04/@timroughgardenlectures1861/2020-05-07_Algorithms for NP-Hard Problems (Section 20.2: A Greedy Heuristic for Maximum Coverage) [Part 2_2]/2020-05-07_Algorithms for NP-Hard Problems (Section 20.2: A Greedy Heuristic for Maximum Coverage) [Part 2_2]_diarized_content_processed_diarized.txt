00:00:00.570 - 00:00:45.366, Speaker A: So, in other words, for every possible input of the maximum coverage problem, doesn't matter how big the universe is, doesn't matter how many subsets you have, if you have a budget of k, then the greedy coverage algorithm will guaranteed output. A solution might not be optimal, but its coverage at least will be a one minus quantity, one minus one over k to the k fraction of the best case scenario of the maximum coverage possible using k subsets. For example, if k equals two, this says the greedy algorithm is guaranteed to get you at least 75% of the maximum possible coverage. If k equals three, at least 70.4% of the maximum coverage. And no matter how big k is, at least 63.2% of the maximum coverage.
00:00:45.366 - 00:01:22.006, Speaker A: And as usual, this is just an insurance policy. This is just what you're guaranteed. Even in the most contrived doomsday scenario on realistic inputs, the greedy algorithm will generally get quite a bit closer to 100%. All right, so we will actually prove this theorem in full. But first, let's spend a little time developing your intuition for it and let's revisit the 81 element example that we had in that last quiz. So why doesn't the greedy algorithm just produce the optimal solution in this example? Well, in the first iteration, it had the option of picking any of the three sets in the optimal solution. So t one, t two, or t three.
00:01:22.006 - 00:02:13.830, Speaker A: Unfortunately, the algorithm was tricked by a fourth set, t four, the green set that was equally large. It had 27 elements in it, just like t one, t two, and t three. In the second iteration, the algorithm was tricked once again. It had the option of picking any of t one, t two, or t three, but it was tricked by t five, which covered just as many new elements, namely 18 new elements. But so in general, this kind of suggests that every miscue by the greedy algorithm can be attributed to a subset, the one it chose, that covers at least as many new elements as each of its k options from the optimal solution. But if greedy had the k options of the sets in the optimal solution, and it chose something even better than any of those, doesn't that mean it should be making a lot of progress in each iteration? And the answer to that question is yes. And that's going to be formalized in the following key lemma.
00:02:13.830 - 00:03:00.588, Speaker A: So what is this key lemma saying? Basically, it's saying that the greedy algorithm keeps successfully nibbling off pieces of the optimal solution each iteration, just like we saw in the examples. So it's going to be a guarantee that's going to hold for each of the k iterations. So maybe k equals ten. Maybe we want to focus on iteration number seven right now. But it doesn't matter. The progress in iteration seven is going to be measured according to the deficiency of the current solution that is the coverage of the six subsets chosen by greedy thus far. So imagine, for example, imagine that the optimal coverage, the maximum coverage you could ever achieve by choosing ten subsets.
00:03:00.588 - 00:03:40.268, Speaker A: Imagine that was 173. And imagine the greedy algorithm over the course of its first six iterations has managed to cover 133 elements. So that's a deficiency of 40 relative to the maximum possible. So what this lemma says is that in every iteration, the greedy algorithm captures at least a one over K fraction of its current deficiency. So in our example where the optimal solution covers 173, we're currently covering 133. So we have a deficiency of 40. If K equals ten, then this lemma is guaranteeing that in the next iteration you will be covering at least four new elements, if not more.
00:03:40.268 - 00:04:03.444, Speaker A: So that's the key lemma, and there are now two remaining orders of business. So first of all, we need to prove this key lemma. We need to see why this is true. The, the greedy algorithm does indeed make healthy progress in every iteration. Then we have to show that this lemma, that this healthy progress actually does result in a good approximate correctness guarantee. So we'll do each of those in turn the proof of the key lemma. This is kind of the conceptually most interesting part.
00:04:03.444 - 00:04:29.944, Speaker A: Once we know the greedy algorithm is always making progress intuitively, there has to be some kind of approximate correctness guarantee. And the question is just what is it exactly? So that's more or less going to be algebra. So let's dive in now to the proof of the key lemma. Then we'll see why it implies this one minus one over e type guarantee. So first a little bit of notation by capital k with a hat on top. That's just going to denote some subset of k indices. So k of the subsets.
00:04:29.944 - 00:04:54.340, Speaker A: So maybe it's subsets 3712 and 19. So khat is going to be our competition. We're aspiring to do almost as well as Khat. So you can think about Khat as being an optimal solution, one that maximizes the coverage, although that's actually not important for the proof. Khat can be whatever you want. Now let's zoom in on any iteration of the outer for loop of greedy coverage that you want. So again, for example, iteration number seven out of ten, say, fix.
00:04:54.340 - 00:05:33.490, Speaker A: Also whatever subsets the greedy algorithm may have chosen in the previous iteration. So in our example, the first six subsets that it's chosen. So that brings us to the most important inequality in the proof. And I want to explain this inequality using this cartoon on the right. So in this cartoon, I have a blue circle that's meant to represent all of the elements covered by some by our reference solution. So we have these indices in K hat that corresponds to little K subsets. They have some coverage called their coverage c hat.
00:05:33.490 - 00:06:30.922, Speaker A: And this blue circle then contains the c hat elements covered by this reference solution, like the 173 elements covered by all ten subsets chosen in Khat. Meanwhile, we have the Magenta circle, and that's meant to indicate the elements that have been covered so far by the greedy algorithm. So for example, the 133 elements that might have been covered by the first six subsets chosen by the greedy algorithm. Now I want to pay particular attention to the green region. The green region, what is that? That's all of the elements that are covered by this reference solution covered by the subsets in Khat, but are not at least not yet covered by the greedy solution. What we're going to do is relate the area of this green region, the number of elements in it. We'll relate that to the left and right hand sides of this inequality that I've written on the slot, we will show that the green region, on the one hand, its area is at least as big as the right hand side, and on the other hand, it's at most as big as the left hand side.
00:06:30.922 - 00:07:26.490, Speaker A: And that'll prove that the left hand side is indeed at least as large as the right hand side because you've got the area of the green region wedged right in between. So let's start with the right hand side, which I'm claiming is no bigger than the area of the green region. The green region should be at least as big as this right hand side. So what's the right hand side? This is exactly the deficiency of the greedy solution so far that we've been talking about, right? So c hat that be like the 173 elements covered by a reference solution, the number of elements already covered, that would be like the 133 elements covered by the first six subsets. So that would be a difference of 40. So the claim is this green region has to have at least 40 elements in it. So in fact, if the 133 elements covered by the greedy algorithm were all also covered by a reference solution, k hat that is, if the magenta circle lay entirely inside the light blue circle, then actually the green region would have exactly 40 elements.
00:07:26.490 - 00:08:24.038, Speaker A: More generally, if for example, the greedy solution so far, maybe it covered 123 of the elements in the optimal solution in the blue circle, but then it sticks out a little bit and there's ten elements that have been covered by the greedy solution that actually are never covered by the reference solution. K hat well, in that case, the green region is going to have an even bigger area, right? If 123 are inside the blue circle and ten are outside, that means the other 50 elements in the blue circle are going to belong to the green region. So that's why the green region's area is at least as big as the right hand side of this inequality of the current deficiency of the greedy solution. So now let's move on to the left hand side, which I want to argue has value at least as big as the green region. So now we're going to want to think about the K subsets that belong to the reference solution, to the subsets corresponding to capital K hat. So in the cartoon, I've drawn two of them, I've drawn a T one, I've drawn a t two. Both of those are part of the ten subsets, say, of the reference solution K hat.
00:08:24.038 - 00:09:08.454, Speaker A: You'll notice that they overlap a little bit, so that's the dark shaded region in brown is where those two subsets overlap. So the quantity on the left hand side is basically the sum of K different thought experiments. So we say, hey, the greedy algorithm, it had the option of including the first subset from the reference solution Khat. It could have done that if it wanted to. What would have been the coverage increase had the greedy algorithm done that? Then we asked the exact same question about the second subset in the reference solution, capital K hat. What if the greedy algorithm added that second subset right now? What would the increase in coverage be? So we ask that question, k times one for each subset in the reference set, and then we add up the results. That's the left hand side of this inequality.
00:09:08.454 - 00:09:49.622, Speaker A: So for example, going back to our cartoon, we're going to ask the thought experiment once using the set t two. You'll notice that t two is actually disjoint from all of the elements that greedy covers so far. So the increased coverage from choosing t two is just going to be the size of T two, the number of elements in it. On the other hand, we ask a separate thought experiment about adding T one. T one does overlap the greedy solution a little bit. So the increase in coverage is going to be the part of t one which is not already covered by the greedy solution, by the Magenta circle. So how do we relate the sum of these K thought experiments to the area of the green region? Well, here's how I'd like you to think about it.
00:09:49.622 - 00:10:51.818, Speaker A: Imagine that unlike our thought experiment, where we look at the increased coverage of adding just one of the ten subsets, say, in the reference solution, imagine we instead go crazy and we add all ten of those subsets all at once. So greedy had its six subsets, and we just say, here, take all ten that are in the reference solution, capital K hat. What's going to happen? What's the increased coverage of that? Well, we know those ten subsets, they cover the blue circle, right? They cover whatever the optimal solution covers. And so the increase in coverage, when you add them to the six greedy sets, that's just going to be whatever's in the green region. You're going to cover exactly the elements covered by the reference solution that are not yet covered by the greedy algorithm. So that would be exactly the green region. So what's the difference between adding all ten sets at once and looking at the increase in coverage and the sum of our ten thought experiments of adding just one subset at a time? Well, we're only going to get a larger number from the sum of the ten thought experiments involving a single set than the increased coverage from adding all ten at once.
00:10:51.818 - 00:11:38.726, Speaker A: To see why, look at the subsets t one and t two in the cartoon. So in the thought experiment involving t two, we get credit for every element in t two. So the increase in coverage is just its size. In the thought experiment involving t one, we get credit for every element in t one that is not already in the greedy solution. And here's the important point, is that the elements in the overlap in the brown shaded region, we get credit for those twice, once for the thought experiment involving t one and again for the thought experiment involving t two. Whereas if we think about the increase in coverage by adding t one and t two both, then we get credit for each element in the overlap in the shaded brown region only once. So that's why we get an only bigger number when we look at the sum of these k different thought experiments.
00:11:38.726 - 00:12:15.480, Speaker A: We're going to get double counting whenever you have an element which appears in more than one of the subsets of the reference solution. So that's why the left hand side is at least as big as the green region. It's basically the same as the green region, except with multiple counting of elements who lie in more than one of the subsets of capital K. Hat all right, you can breathe a sigh of relief. That was the most difficult part of the proof of the key. Lemma so next, let's observe that the left hand side of this inequality, it's the sum of k different terms. Now, one of those k terms must be at least as big as the average value of one of those k terms.
00:12:15.480 - 00:13:09.548, Speaker A: So, for example, if I tell you I have ten positive numbers and that they sum to 100, you know immediately one of those ten numbers has to be ten or larger, has to be at least the average value, right? Because if all of the numbers were less than ten, then the sum wouldn't be 100, it would be less than 100. So same thing going on here. We're looking at the left hand side of the inequality. We're saying, okay, that's the sum of the k terms, the average value of the k terms is just that sum divided by k. That's the average value across the k terms. And we're just saying one of those k terms has to be at least as big as that average. So with this new inequality, it says that one of the little k subsets in the reference set, capital k hat makes at least this much progress, one over little k times the deficiency of the current greedy solution.
00:13:09.548 - 00:13:41.156, Speaker A: Now, by the greedy criterion of the greedy coverage algorithm, it picks some subset which is at least as good. Maybe it doesn't pick one of these subsets in the reference solution, maybe it's tricked by something else. But whatever it picks does at least as well as all of its options in the reference set, capital k hats. And now we really are done. For all we know, the reference solution could be an optimal solution. So this capital c hat would then be the same as the capital c star. We have in the Lemma statement the maximum possible coverage of any collection of k subsets.
00:13:41.156 - 00:14:24.180, Speaker A: And then this is exactly the guarantee that we stated that each iteration of the greedy algorithm, it makes progress lower bounded by one over k times the deficiency of the current solution. So that finishes the first part out of the two parts in the approximate correctness guarantee for the greedy coverage algorithm. But that was the harder part. That's the trickier part. The second part, we have to show that now that we have this lower bound on the progress made by the algorithm in each iteration, we do indeed get an approximate correctness guarantee. And in fact, the factor is exactly that magical, one minus quantity, one minus one over k raised to the k, that's just going to be algebra. So a little notation, capital C star, that's going to mean the same thing that it's meant before.
00:14:24.180 - 00:14:49.528, Speaker A: That's going to be the maximum attainable coverage. So the maximum number of elements that you can cover using just k of the given subsets. And then we're also going to be tracking the coverage attained by the greedy coverage algorithm. So c sub j is going to be the number of elements that the greedy algorithm has covered in its first j iterations. We can succinctly recap the key Lemma in terms of this notation. So remember what that said. It said the increased coverage that the greedy algorithm enjoys.
00:14:49.528 - 00:15:27.976, Speaker A: So that's just going to be the difference between a capital C sub j and a capital C sub j minus one. That's the increased coverage that you get in iteration j of the greedy algorithm. And what's the lower bound? In the key lemma? It says you get at least a one over k fraction of the current deficiency. So in the jth iteration, the deficiency is going to be c star, the maximum coverage minus CJ minus one, which is the coverage of the first j minus one subsets. So all we're going to do is apply this key lemma over and over again, basically unrolling the progress made by the greedy algorithm. Then we'll see when the dust settles, we'll get exactly the approximate correctness guarantee that we wanted all along. So let's apply the key Lemma.
00:15:27.976 - 00:16:15.612, Speaker A: First of all, to the final iteration of the greedy algorithm. So that just means we're plugging in k, the last iteration in for j. So let me just rearrange terms a little bit so that this is stated as a lower bound than the coverage achieved by the greedy algorithm at the end of all k iterations in terms of a coverage, the coverage that it had already achieved in the previous iteration. So there's that term, one minus one over k we were sort of hoping would show up in the analysis. Let's now do exactly the same thing for the previous iteration. So in other words, apply the key lemma with j now equal to k minus one. So now we're in a position to combine the two inequalities.
00:16:15.612 - 00:17:23.416, Speaker A: So we're just going to take the first inequality and substitute for CK minus one, the lower bound that we have in the second inequality. So we're just going to substitute c star over k plus quantity one minus one over K-C-K minus two in for C k minus one in the first iteration in the first inequality. So let's do the same thing one more time. Apply the key lemma to the third to last iteration when j equals k minus two, and then we'll see the pattern. So all I've done here is apply the key lemma with j set equal to k minus two, and then taking the resulting lower bound on capital C k minus two, the coverage after k minus two iterations, I've just plugged in our lower bound on that into the previous inequality and then simplified terms. And you can now sort of see the pattern, right, so in the first term, the C star over k term, we keep getting an additional power of one minus one over k. And in the second term we're sort of rolling back.
00:17:23.416 - 00:18:08.656, Speaker A: So we look at the coverage one iteration back and then we're getting increasing powers there of one minus one over k. So you can imagine what this is like in the end once we get all the way down to j equals one. So we've applied the lemma k times. That's why in the first term, the C star over k term, we have the sum of k things, right, the powers of one minus one over k, starting from the zero th power ending in the k minus one th power. As usual, we have sort of the final residual term, but this time the residual term has c zero, which is the coverage of greedy after it's chosen zero subsets. And we certainly know what that is. That's going to be equal to zero.
00:18:08.656 - 00:18:41.116, Speaker A: All right, so we had this crazy expression and now it's gotten at least a little less crazy. That residual term has disappeared, but we're still left with this crazy thing in the parentheses, right, the sum of all of these powers of one minus one over k. But maybe you recognize that this is actually an old friend. This is actually a geometric series we've seen occasionally at other times. For example, in the Proof of the Master Method, way back in part one of this book series. So I don't expect you to have the closed form formula for a geometric series memorized. So let me just sort of remind you what it is.
00:18:41.116 - 00:19:08.276, Speaker A: So suppose you're summing up powers of R. Here, r can be any number you want, as long as it's not equal to one. And again, for us, it's going to be one minus one over K. So it's not going to be equal to one. So suppose you're summing up the powers of r one, the zero th power of r plus r plus r squared all the way up to, let's say, the l power of r. That sum is exactly equal to one. Minus R raised to the L plus first power divided by one minus R.
00:19:08.276 - 00:19:33.132, Speaker A: In case this looks weird, notice that it's super easy to check. Just clear denominators, multiply both sides by one minus R. On the left hand side, almost all of the terms are going to cancel out. Like you're going to get a minus R and a plus R. They'll drop out, you'll be left only with a plus one and a minus R to the L plus one. So that verifies the equation by just clearing the denominators. So this is exactly what we needed.
00:19:33.132 - 00:20:09.252, Speaker A: This is our night in shining armor. We have this crazy messy expression, but now we realize it's really just summing up powers of one minus one over K, where the last of those powers is equal to K minus one. So we're just going to plug in this crazy expression into the geometric series, setting R equal to one minus one over K, and setting L equal to K minus one. So plugging in on the top, we get one minus quantity one minus one over K raised to the KTH power. In the denominator, we get one minus quantity, one minus one over K. The denominator, you'll notice, is also known as just one over K. So this is the same as just forgetting about the denominator and multiplying the numerator.
00:20:09.252 - 00:20:39.812, Speaker A: By K. So remember, in sort of our main argument, this geometric series was multiplied by this leading coefficient of C star divided by K. That K in the denominator. Cancels out with this K we just got in the numerator, the geometric series, leaving us with exactly what we wanted, the coverage of the greedy algorithm after all K of its iterations. Well, it may not be as big as the maximum possible coverage, but it's not that much smaller. No smaller than 63.2% times the best you could do.
00:20:39.812 - 00:21:25.456, Speaker A: And again, precisely, it's going to be one minus quantity one minus one over K raised to the K power. So that concludes the proof of the approximate correctness guarantee of this fast greedy heuristic for the maximum coverage problem. Indeed, this shows that the family of examples that were suggested by that quiz, they actually are the worst possible examples. No matter what your budget k is, you're guaranteed to get at least a one minus quantity, one minus one over k raised to the k fraction of the maximum possible coverage. And again, that means you get at least 75% of the maximum coverage of k equals 270.4% for k equals three. And no matter how big k is, you're getting at least a one minus one over E, also known as roughly 63.2
00:21:25.456 - 00:21:42.950, Speaker A: fraction. That concludes what I wanted to tell you about the maximum coverage problem. Let's go on to an application in the analysis of social networks where we'll see that a generalization of this greedy algorithm gives is a fast heuristic solution for what's known as the influence maximization problem. See you then.
