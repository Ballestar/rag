00:00:00.250 - 00:00:09.710, Speaker A: Yet another flavor of exact recovery. Although, you know, a lot of the details will be different. I'll have a fairly different flavor technically.
00:00:09.710 - 00:00:29.302, Speaker A: So this is going to be back in the spirit of lectures six, seven, and eight, where we took NP hard problems, posited extra structure on them, and then showed we could have polymor time algorithms to get back the optimal solution. Those lectures focused on these different stability properties. So we had approximation stability for k median, perturbation stability for both k median and cut problems.
00:00:29.302 - 00:00:48.198, Speaker A: So here we're going to be going back to graph problems like we were studying in lecture eight, and we're going to be studying conditions that have the flavor of these stability conditions, I think in spirit, but technically they're going to be fairly different. They're going to be randomized models. So we're actually going to be looking at we're going to posit a generative distribution over graphs.
00:00:48.198 - 00:01:21.538, Speaker A: And we're going to argue we want to know, like, with high probability, can we have polynomial time algorithms that solve NP hard problems, like finding the maximum clique, finding the minimum bisection, that sort of thing. All right? So again, what we're trying to accomplish with these models is we want to have a different way of reasoning about graph algorithms especially. So recall we talked about maximum weight independence set if you don't make any conditions, if you just talk about approximation ratios, all algorithms have a terrible approximation ratio.
00:01:21.538 - 00:01:37.980, Speaker A: There's no way to distinguish between them. So now we're asking, can we have a different model of inputs which allow us to get more informative analyses of algorithms and maybe suggest using one over another? And so we're going to think about distributions over graphs. So average case analysis in a sense.
00:01:37.980 - 00:01:46.414, Speaker A: And if you want to talk about distributions of a graphs, you have to start with a erdos rengi model.
00:01:46.532 - 00:01:47.200, Speaker B: Okay?
00:01:53.250 - 00:02:13.954, Speaker A: So we're not going to actually wind up using these, but it's important to understand what they are and why they're not sufficient for our goals of differentiating between different algorithms. So when people talk about the model GNP, so there's two parameters, n and P. N is the number of vertices, p is the edge probability or expected edge density.
00:02:13.954 - 00:02:17.750, Speaker A: Okay? So there's N vertices that's deterministic.
00:02:20.170 - 00:02:20.486, Speaker B: And.
00:02:20.508 - 00:02:32.810, Speaker A: Of the N, choose two possible edges. For each of those edges, you flip an independent coin with probability P, it comes up heads and you stick in the edge. With probability one minus P, it comes up tails and you don't stick in the edge.
00:02:32.970 - 00:02:33.680, Speaker B: Okay?
00:02:36.690 - 00:03:00.130, Speaker A: And so you can go ahead and think about P equal one half as a canonical case if you like. So each edge present IID probability P. Okay? So if P equals one half, this is in fact the uniform distribution over all of the undirected graphs in the world, turns out.
00:03:00.200 - 00:03:00.820, Speaker B: Okay.
00:03:01.690 - 00:03:34.850, Speaker A: All right, so this is the first place to start. There's a beautiful theory of eritre schwannie random graphs. So how about using it for algorithms for some computational problems? You can say some interesting things about eroticrene graphs, but I'm going to pick a couple of problems where it's really not a very helpful model because it can't differentiate between different algorithms.
00:03:38.070 - 00:03:38.820, Speaker B: Okay?
00:03:41.030 - 00:03:53.720, Speaker A: And so this is true for I'm going to show you two examples, but it's true for many problems, many empty, hard problems. Let me show you a really extreme example. So let's talk about the Min bisection problem.
00:03:53.720 - 00:04:04.966, Speaker A: This means you want to find the minimum graph cut, except I'm also going to insist that the two sides of your cut have equal cardinality. Each one has exactly half of the nodes.
00:04:05.078 - 00:04:05.740, Speaker B: Okay?
00:04:07.230 - 00:04:26.850, Speaker A: So input, let's just focus on the unweighted case, even number of vertices. And so the output should be a bisection SS complement of equal sizes, minimizing the number of cut edges.
00:04:31.190 - 00:04:31.940, Speaker B: Okay?
00:04:33.750 - 00:04:59.740, Speaker A: All right. So why is this sort of not an interesting model to study algorithmically? So here's something, I'll sort of state it now and leave the details to the homework. It's a sort of straightforward exercise in probability, which is if you take a random graph from GNP, then with high probability, all of the bisections are pretty much exactly the same.
00:04:59.740 - 00:05:36.982, Speaker A: Okay, so with high probability, every bisection cuts roughly N squared over four p edges. Get the door. So why is this true? So this is basically the, you know, this is roughly the total number of edges, n squared over four.
00:05:36.982 - 00:05:49.498, Speaker A: Each one is sorry, fix a bisection n over two vertices on the left, n over two vertices on the right. This is the number of potentially crossing edges. You have a choice of one of N over two vertices on the left, one of N over two on the right.
00:05:49.498 - 00:06:05.722, Speaker A: Each of those edges is present with probability P. So for a fixed bisection, if you then flip the coins for the edges, this is the expected number of edges that get cut by that particular bisection. So in expectation, every bisection is the same and these random variables are well concentrated.
00:06:05.722 - 00:06:41.600, Speaker A: Okay, so let's say if P is a constant, and so this will be the exercise on the homework, which you can prove using so called churnoff bounds, that there's very little difference between all of these differ very little from their expectation. So suppose you had different heuristics for minimum bisection and you wanted to say which one was better? Well, even if you had an algorithm where your bright idea was to compute the maximum bisection, that would be as good as any of these other heuristics, because any bisection is as good as any other bisection. So really there's just no way to talk about different algorithms in this model.
00:06:41.600 - 00:06:51.838, Speaker A: Makes sense. Okay, so they're just sort of not very interesting for the respect of this problem. Here's another example.
00:06:51.838 - 00:07:04.850, Speaker A: It's a more interesting example. This is a famous problem, actually, whoops not planted getting ahead of myself. Klik max clique.
00:07:04.850 - 00:07:18.514, Speaker A: So you should all know this problem. It's just the complement of independent sets. So I give you an undirected graph.
00:07:18.514 - 00:07:25.740, Speaker A: A clique is just a bunch of nodes so that they're all connected to each other. And you want to just find the biggest clique in the graph. So that's just as hard as independent set.
00:07:25.740 - 00:07:42.480, Speaker A: What about in Erdis rennie graphs? So it is easier, it's an easier problem in eritreus rennie graphs, but somehow, again, it just doesn't force us to design interesting algorithms. We just don't learn anything illuminating by studying the problem in such graphs. So let me explain.
00:07:42.480 - 00:07:59.540, Speaker A: So, fact number one. So let's just take P equal one half. Okay? So each edge is equally there or not, and G n one half.
00:07:59.540 - 00:08:14.090, Speaker A: So what do we think the answer is going to be? So for Bisection, right, we thought the answer was going to be N squared over four times P. Here it's a little less obvious, but I'll show you a heuristic calculation. That the expected.
00:08:14.090 - 00:08:39.860, Speaker A: We expect the Mac cleek size to be roughly two log base, two of N. Okay? So here is stick proof. So let's do something different.
00:08:39.860 - 00:08:59.874, Speaker A: Let's actually, just for a given size of cliques k, let's just count how many cliques of size k are going to be in this graph in expectation. Let's just look at sort of like, what's the magic number of k for which we expect to be roughly like one k cleak. And we're going to heuristically interpret that as sort of the likely size of the biggest k cleak.
00:08:59.874 - 00:09:07.500, Speaker A: It doesn't prove that. Okay, it proves the correct answer, but it's not a proof of that fact. But this gives you an idea of where does this number come from.
00:09:07.500 - 00:09:13.260, Speaker A: All right, so for parameter k.
00:09:16.430 - 00:09:16.746, Speaker B: How.
00:09:16.768 - 00:09:36.098, Speaker A: Many k clicks are you going to see in a random sample from GNP? Well, how many opportunities for a k click do we have for the N vertices? There are N choose k ways of choosing a subset of k vertices. That's a cleak. If, and only if every single one of the edges amongst those k vertices is there.
00:09:36.098 - 00:09:48.394, Speaker A: There's k choose two edges that need to be there for it to be a cleak. Each is there with 50% probability. So for a given choice of k vertices, it is a clique with probability two raised to the minus k.
00:09:48.394 - 00:09:55.370, Speaker A: Choose two. Okay, so this is exact. So this is the expected number of k cleaks.
00:09:55.370 - 00:10:40.380, Speaker A: Let's just kind of do some sloppy estimation. Let's say this is like N to the k or so let's think of this as like k squared over two. Now the question is, so we want to know when is this one for what value of k is this equal to one? That's the same thing as saying for what value of this? For what value is this going to be? N to the k sorry, for what value of k is this term equal to the reciprocal of this term? And that's going to be when k is like two log base two of n.
00:10:40.380 - 00:10:44.522, Speaker A: Okay, you can plug this in and check.
00:10:44.656 - 00:10:45.420, Speaker B: All right.
00:10:48.350 - 00:11:01.054, Speaker A: So fine. In fact, I mean, just a little trivia fact. It turns out that, again, we didn't prove it, but it is true that it's very likely that the maximum clique is going to be around this number.
00:11:01.054 - 00:11:05.170, Speaker A: In fact, it's one of the most concentrated random variables you'll ever see.
00:11:05.320 - 00:11:05.906, Speaker B: All right?
00:11:06.008 - 00:11:12.498, Speaker A: So with probability tending to one, as n grows large, it's either the floor or the ceiling of that number.
00:11:12.584 - 00:11:13.218, Speaker B: Wow.
00:11:13.384 - 00:11:33.450, Speaker A: So it's one of two numbers with overwhelming probability, and you start seeing stuff like that. You're like, maybe this uniform distribution over graphs isn't actually giving me good information about graphs that I encounter in real life. It's a beautiful model, but there's not a lot of real world distributions over graphs for which it's a good first order approximation.
00:11:33.450 - 00:11:47.040, Speaker A: Okay, so that's fact one the target. So if we're running an algorithm for max clique on erdosreny graphs, this is sort of what we're shooting for. We'd like to get as close to this as possible.
00:11:47.040 - 00:12:04.990, Speaker A: Now, it turns out. So we don't have any algorithm that can get this, which is okay. I mean, it's not clear that we expect this problem to suddenly become easy, like polynomial time solvable and error training graphs.
00:12:04.990 - 00:12:39.518, Speaker A: But what's frustrating is, no matter how smart or stupid your polynomial time algorithm, everything gets exactly the same performance, which is you get one times log base two of N. Okay, so fact two all heuristics get stuck at log base two event, more or less. Including, for example and I'll ask you to analyze this on the homework, just the obvious greedy algorithm where you pick a node, you take an arbitrary node that's connected to it.
00:12:39.518 - 00:12:43.854, Speaker A: You make that a two click. You try to find a node that's connected to both of them. If you find one, that's your three click.
00:12:43.854 - 00:12:49.998, Speaker A: You try to find a node that's connected to all three of them, and so on. And you can just do that until you get stuck. That's going to get stuck at, like, log n, turns out.
00:12:49.998 - 00:13:02.454, Speaker A: And we don't really know how to do better, nor do we know how to prove this would also be super interesting. So it'd be super interesting to could do better than that. It would also be super interesting to say if you could do better than a two approximation, some other unlikely consequence would happen.
00:13:02.454 - 00:13:24.282, Speaker A: But because this is sort of an average case problem, we don't really know how to connect average case problems to kind of empty completeness. So there's no connections between making progress on this problem and making progress on, say, sat. So there's some really nice open problems there, but the moment the state of the art trying to come up with Clique algorithms on edge training graphs has not led us to any interesting new results.
00:13:24.282 - 00:13:37.262, Speaker A: Is that a product of our lack of imagination or is it something more fundamental? It's a good question. All right, so that's what I want to say about Erdos Rennie graphs.
00:13:37.326 - 00:13:37.602, Speaker B: Okay?
00:13:37.656 - 00:13:59.530, Speaker A: So if you're talking about random graph models and you're going to do something other than Erdos Rennie, you have to explain why. And hopefully now I've explained why it's not good enough. So how are we going to change the Erdos Rennie model to hopefully get more interesting algorithms? So the next thing we're going to do is talk about planted solutions.
00:13:59.530 - 00:14:22.558, Speaker A: So problem can't differentiate between different algorithms fix. We're going to plant a clearly optimal solution. Okay.
00:14:22.558 - 00:14:46.470, Speaker A: And we're going to ask upon a multimedia algorithm to recover it. So again, this is, I think, very much in the spirit of the lectures on stability properties, stability assumptions. So let me show you the planted version of both minimum bisection and Max Cleak.
00:14:46.470 - 00:15:04.640, Speaker A: They're both quite nice problems. So example planted bisection. And so this was studied all the way back in the early to mid 80s even.
00:15:04.640 - 00:15:11.418, Speaker A: So what we do, so this is how we generate an instance.
00:15:11.514 - 00:15:11.774, Speaker B: Okay?
00:15:11.812 - 00:15:29.720, Speaker A: So if you like, there's again going to be a distribution over graphs, but it's going to be non uniform. So here's how you sample from this distribution or generate a graph from this distribution. So you first partition V into s one and s two, where these have equal size.
00:15:29.720 - 00:15:38.854, Speaker A: So you can do this randomly or you can do it arbitrarily by symmetry. It's not going to matter. Okay, so there's some sort of target partition.
00:15:38.982 - 00:15:39.660, Speaker B: Okay.
00:15:41.390 - 00:16:09.106, Speaker A: Now how do we make this okay, so that's our plant. How do we make sure that it's clearly optimal? Well, rather than picking every edge with the exact same probability, we're going to bias the edges so that we're much more likely to see edges inside s one or s two than we are to see edges crossing between s one and s two. This is how we'll ensure that this bisection cuts fewer edges than anything else.
00:16:09.106 - 00:16:46.720, Speaker A: We're going to bias the distribution so that the bisection is sparse. All right, so edges inside s one, s two with probability P between s one, s two with some probability Q less than P. All right, so we have s one, we have s two, possibly pretty dense inside, but fairly sparse between them.
00:16:46.720 - 00:17:04.974, Speaker A: Now, needless to say, the algorithm isn't presented with the graph with a nice picture like this. These things are all scrambled together. And so the question is, can you figure out where even I tell you there's one of these sparse bisections, but there's a lot of bisections to try.
00:17:04.974 - 00:17:07.458, Speaker A: So can you quickly find that sparse bisection?
00:17:07.634 - 00:17:08.360, Speaker B: Okay.
00:17:13.210 - 00:17:34.970, Speaker A: So the theory question that's sort of really nice to think about then is something very clean. Okay? So hopefully it's clear that this problem is going to get easier and easier as the gap between P and Q grows. Okay, so this bisection will be more obviously there, the bigger the differential between the density of edges across the bisection and those inside the clusters.
00:17:34.970 - 00:18:05.910, Speaker A: So obviously, if P and Q are the same, we sort of argued it's not an interesting problem, and if they're super far apart, then presumably trivia algorithms work. So the question is, what is the gap between P and Q that's necessary and sufficient for exact recovery by a polynomial time algorithm? Elliot so what is the reason for creating these graphs that are very contrived and trying to find algorithms? That's an excellent question. So I thought about sort of talking about this at the beginning.
00:18:05.910 - 00:18:28.874, Speaker A: If we think about where do these so this is now, like, I think, the fifth lecture we've had basically on exact and approximate recovery. And I think you're right to point out that there's sort of a shift here. So we had these three on these stability notions where we sort of argued it's like, well, if you think these are interesting problems to solve, you're implicitly making these assumptions anyways.
00:18:28.874 - 00:18:57.614, Speaker A: And that's kind of where the story came from. Plus, I don't know, there's some kind of intuitive appeal to that, at least maybe approximate versions of these stability notions might hold for, say, clustering problems that we care about. Then we had last lecture where our sufficient condition was a little hard to get our head around these almost Euclidean subspaces, but at least we sort of proved that the ends justified the means, right? So basically it allowed us to solve the problem that we wanted, and it holds for lots of random matrices.
00:18:57.614 - 00:19:32.862, Speaker A: And I think the models we're talking about in this lecture don't really satisfy either of those criteria. These, I think, are more so sometimes when you're stuck on a problem, right? So when all of your current theories are failing, it can be a useful exercise to kind of invent a model which seems likely to force you to design new algorithms. Or again, even I'd say, maybe you even have a sense that one algorithm is better than another one, and you're just sort of looking for any rigorous justification of that fact to sort of separate the two of them.
00:19:32.862 - 00:19:39.410, Speaker A: And in that sense, the models I'm going to be describing today, I think have been successful. I think, for example, for the maximum.
