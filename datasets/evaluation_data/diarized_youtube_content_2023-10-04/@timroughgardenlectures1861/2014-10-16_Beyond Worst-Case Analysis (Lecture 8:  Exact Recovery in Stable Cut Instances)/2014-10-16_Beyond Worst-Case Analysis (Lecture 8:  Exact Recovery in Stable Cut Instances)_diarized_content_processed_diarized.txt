00:00:00.170 - 00:00:14.030, Speaker A: The topic for today is what I'm going to call exact recovery in cut problems. So let me tell you what I mean by exact recovery. This is a topic I actually surreptitiously already introduced you to on Monday.
00:00:14.030 - 00:00:47.126, Speaker A: So in exact recovery, the question you're trying to understand is for a given algorithm that you have in mind and figure out which inputs this algorithm works properly on. So solves the problem correctly. Okay, so for what inputs does a given algorithm a solve the problem exactly? Our main theorem on Monday was exactly this type of exact recovery guarantee.
00:00:47.126 - 00:00:54.842, Speaker A: We proved that single link plus plus exactly recovers optimal solutions in three stable K median instances.
00:00:54.906 - 00:00:55.134, Speaker B: Okay?
00:00:55.172 - 00:01:09.794, Speaker A: That was the main theorem on Monday. So I really want to sort of tease out that kind of property and give you some more examples of these kinds of exact recovery results now. Okay, so what's the motivation for these kinds of mean?
00:01:09.832 - 00:01:09.986, Speaker B: Here?
00:01:10.008 - 00:01:17.422, Speaker A: We're thinking of this algorithm A, as something which either we already have access to, so maybe like today it's going to be linear.
00:01:17.566 - 00:01:17.922, Speaker B: Okay?
00:01:17.976 - 00:01:27.318, Speaker A: So Stanford has a site license for CPLEX. So this is an algorithm we can just run for free. We just feed an input, see how it does, or it's an algorithm which should be very easy to code up.
00:01:27.318 - 00:01:47.150, Speaker A: So a simple algorithm like single link plus plus the idea is, and this is really often the case in practice, where designing an algorithm from scratch, especially a complicated algorithm, was sort of a last resort. Really. It's much preferable that something in your existing toolbox suffices for the problem at hand, at least for the inputs that you care about.
00:01:47.150 - 00:02:05.342, Speaker A: That's actually how most clients of algorithms actually use them. By contrast, not all, but a majority of theoretical research on algorithms kind of says, let's try to come up with new algorithms, which is a lot of fun and really interesting, come up with new algorithmic ideas, often general purpose. But here we're really going to sort of take this different approach.
00:02:05.342 - 00:02:20.598, Speaker A: We have our algorithms that we're allowed to use in hand. Let's understand what kinds of inputs they work on. All right, so the theory that we talked about Monday today and then at least one lecture next week, all is trying to capture sort of that use case, if you will, formally with formal theory.
00:02:20.598 - 00:02:42.910, Speaker A: Okay, good. So we're going to do another case study today of exact recovery, and it's going to be graph cut problems, which are quite similar in spirit to clustering problems. You're taking a graph and breaking it into pieces.
00:02:42.910 - 00:03:27.774, Speaker A: The difference is going to be on the kind of algorithm that we talk about, namely, I really want to talk about just linear programming. So when does linear programming exactly recover solutions to NP hard graph cut problems? So that's the topic today. So when our NP hard problems solved exactly by a linear program? Okay, so the answer of course is not going to be always, because if you don't know this already, I'll tell you right now, linear programming can be solved in polynomial time.
00:03:27.774 - 00:03:52.326, Speaker A: So if we're talking about an NP hard problem, we're not going to solve worst case instances of linear programming, but maybe we can solve a lot of instances that we care about. All right, we want theory to explain when it works. Well, so the segue that I'm sort of doing today is I'm keeping the kind of conclusions that we're going to be proving similar to last lecture, okay? So we're still going to be doing exact recovery for clustering style problems, but I'm sort of shifting the algorithm on you.
00:03:52.326 - 00:04:17.790, Speaker A: I really want to start talking about linear programming because it turns out a lot of the sort of greatest hits in exact recovery are for proving relaxations linear programming or convex programs proving them exact. So I want to push us toward the direction of understanding linear programs so that'll allow me to next week hopefully say a little bit about some hot topics like compressed sensing and stuff like that. All right, so similar guarantees but with linear programming algorithms.
00:04:17.790 - 00:04:38.102, Speaker A: So the first part of the lecture, I just want to give you a relatively gentle introduction to how this is going to work. So I'm going to talk for maybe the first half of the lecture or so about a problem. Some of you I'm sure, have seen the St Min cut problem.
00:04:38.102 - 00:04:58.346, Speaker A: So the dual problem to max flows, so this problem is actually solvable in polynomial time. So it's not really the type of problem where you are going to ask a question like this. But later on in this lecture, we'll look at an NP hard generalization of Min cut and then we'll extend the same techniques to get the exact recovery results that we want.
00:04:58.448 - 00:04:59.100, Speaker B: Okay?
00:05:00.530 - 00:05:17.134, Speaker A: All right. So for those of you that haven't seen St Mincut or need a reminder, so the input here is an undirected graph with costs on the edges. Sometimes they're called capacities.
00:05:17.134 - 00:05:18.862, Speaker A: In this context, I'm going to go with costs.
00:05:18.926 - 00:05:19.540, Speaker B: Okay?
00:05:20.470 - 00:05:43.718, Speaker A: So positive costs on the edges and there's a source vertex and there's a sync vertex. And the problem you're trying to solve is you're trying to identify the minimum St cut. So the St cut which cuts the minimum total cost of edges.
00:05:43.814 - 00:05:44.506, Speaker B: Okay?
00:05:44.688 - 00:06:00.414, Speaker A: So this is something you definitely spend some quality time with in CS 261, for example, for those of you that have taken that class. Okay? So first of all, remember what a cut is. A cut is just you take the vertex set and you partition it into two groups, A and B.
00:06:00.414 - 00:06:10.434, Speaker A: What makes a cut an St cut? S. This distinguished source vertex should be on one side, t, the sync vertex on the other side. Okay, so little S is in capital A, little T is in capital B.
00:06:10.434 - 00:06:30.298, Speaker A: That's an St cut and among all st cuts. We want to minimize the total cost of the cut edges, where of course, a cut edge is just one with one endpoint in each of the two sides, one in A and one in B.
00:06:30.384 - 00:06:31.020, Speaker B: Okay?
00:06:31.550 - 00:06:59.634, Speaker A: So for example, a picture you might want to have in your mind would be a graph like this. So here's s here's t costs. So stare at that graph for a second and try to figure out what the value of the best st cut is.
00:06:59.634 - 00:07:13.062, Speaker A: The minimum st. Cute. Seven.
00:07:13.062 - 00:07:33.360, Speaker A: Heard a six. I think a six. So if that's A and that's B, that separates S from T and it cuts six edges, and I don't think there's any cheaper one.
00:07:33.360 - 00:07:59.606, Speaker A: Okay, so if you want, you can solve the minimum cut problem using lots of pretty maximum flow algorithms, which you study in CS 261. That's not what I'm going to do here. Rather, I'm going to show you how we can just solve a linear program and the integer solution, the optimal solution is just going to pop out, okay, as the exact answer to this linear program.
00:07:59.708 - 00:08:00.566, Speaker B: All right?
00:08:00.748 - 00:08:05.254, Speaker A: And then we're going to generalize it shortly to MP hard generalizations of this cut problem.
00:08:05.372 - 00:08:06.040, Speaker B: Okay?
00:08:06.650 - 00:08:15.258, Speaker A: So I'm going to assume so we're given an input like this. I'm not going to assume any stability condition, anything like that. I don't need to.
00:08:15.258 - 00:08:18.010, Speaker A: I will assume that the optimal solution is unique.
00:08:18.910 - 00:08:19.660, Speaker B: Okay.
00:08:22.350 - 00:08:30.650, Speaker A: So that's like a super, super weak version of stability, okay? Only one optimal solution. It's not really that important, but it'll just make things simpler.
00:08:30.730 - 00:08:31.520, Speaker B: All right.
00:08:34.130 - 00:08:47.460, Speaker A: Actually, let me ask, so how many of you who really knows nothing about linear programming or feels like they know nothing about linear programming? Okay, a couple of you? That's fine. Good. We won't need too much.
00:08:47.460 - 00:09:00.698, Speaker A: What I'm going to do next is show you a linear program which is meant to capture this minimum cut optimization problem. Okay? So just the cartoon. I talked a tiny, tiny bit about linear programming in lecture number one.
00:09:00.698 - 00:09:12.326, Speaker A: And so remember, this is cartoon. The upshot is you want to maximize a linear function over linear constraints. So you have a feasible region, which is the intersection of half spaces.
00:09:12.326 - 00:09:19.806, Speaker A: So it just looks like a polygon in two dimensions. And then you have some linear objective function. That just means you want to push in some direction as far as possible.
00:09:19.806 - 00:09:26.618, Speaker A: And so linear programming is just meant to find the point in the feasible space, which is as far as possible in the desired direction.
00:09:26.714 - 00:09:26.974, Speaker B: Okay?
00:09:27.012 - 00:09:48.150, Speaker A: So that's the sort of geometric picture you should have in mind. Now let it so the way you specify a linear program is first you got to say what your variables are, your decision variables. Then you got to say what your objective function is, which should be linear in the variables, and you got to say what your constraints are, which should also be linear in the variables.
00:09:48.150 - 00:10:17.090, Speaker A: So let me show you such a linear program meant to capture minimum cut or at least a relaxation of minimum cut. So the variables so there's going to be a variable XE per edge, and the semantics of XE is that we want XE to be one if this edge winds up getting cut, and zero if this edge is not cut. That's the intended semantics.
00:10:17.090 - 00:10:35.382, Speaker A: We're also going to have a vertex for sorry, a variable for each vertex. So here the semantics are if DV is zero, that means we're thinking of v as being on the source side, the s side of the cut. If DV is one, then it's on the sync side of the cut.
00:10:35.516 - 00:10:35.910, Speaker B: Okay?
00:10:35.980 - 00:10:52.320, Speaker A: So those are the two variables, two sets of variables. All right, so what are the constraints? Well, certainly s better be on the s side of the cut. So we fix DS at zero, that's tethered there.
00:10:52.320 - 00:11:17.766, Speaker A: Similarly, t better be on the sync side of the cut so that's tethered at one, the other d values are allowed to range freely. And then we want to say something that we want to constraint, which says that if U and V are on different sides of the cut, if the vertices U and V are on different sides of the cut and there happens to be an edge between them, well, then that edge gets cut.
00:11:17.868 - 00:11:18.182, Speaker B: Okay?
00:11:18.236 - 00:11:40.830, Speaker A: So I'll show you the constraints that indicate that. So for all edges with endpoints U and v, we have a pair of constraints which says XE is at least du minus DV, that's an XE and XE is also at least DV minus du.
00:11:41.810 - 00:11:42.560, Speaker B: Okay?
00:11:44.050 - 00:11:54.750, Speaker A: So this is just saying XE is at least the absolute value of the difference between du and DV. Okay, I wrote it this way. So it's obviously linear in the decision variables.
00:11:54.750 - 00:12:05.182, Speaker A: The point here is that if one of the DS is zero and the other is one, this forces the x to be one. So if the nodes are on different sides of the cut, it forces the edge to be cut, which is what we want.
00:12:05.336 - 00:12:06.040, Speaker B: Okay?
00:12:07.930 - 00:12:21.130, Speaker A: All right. And then with the decision variables, I hope it's kind of quite clear what the objective function is going to be. So XE just is supposed to denote whether the edge gets cut or not.
00:12:21.130 - 00:12:33.102, Speaker A: The objective function of course, is to minimize the total cost of the cut edges. So the objective just sums over all of the edges. It looks at whether or not that edge got cut.
00:12:33.102 - 00:12:39.760, Speaker A: And if it does get cut, if this is a one, then you pick up its cost ce. Okay, and you want to minimize this.
00:12:41.570 - 00:12:42.480, Speaker B: All right.
00:12:45.990 - 00:13:18.790, Speaker A: So that's a linear program whose intent is to capture the minimum cut problem, and it's at least a partial success in the sense that if you show me an St cut that certainly naturally induces a feasible solution to the linear program. You show me the cut that sets A and B, I'll set all of the DS equal to zero for vertices in A, I'll set the DS equal to one for the vertices in B, and I'll set X's to be zero or one, depending on whether the edge is cut or is not cut or cut, respectively.
00:13:18.870 - 00:13:19.498, Speaker B: Okay?
00:13:19.664 - 00:13:36.942, Speaker A: Really, by definition, we've basically engineered this linear program so that cuts would give you feasible solutions. But now remember, the feasible solutions to a linear program, they're not just a bunch of discrete zero one points out in space. Feasible regions of linear program look like this.
00:13:36.942 - 00:14:14.314, Speaker A: They look like convex sets, okay? So they include other stuff than just the things that we had in mind, than just the sort of the zero one vectors for cuts. They have in particular fractional solutions, okay? And to a linear program, you optimize over the entire set of feasible solutions, the fractional ones and the integer ones, the ones we care about and ones we don't care about, and you get whatever you get. You get whatever's optimal, okay? So the concern always with when you try to formulate discrete problems like a cut problem as a linear program is that when you solve the linear program and you get the optimal solution, it's not meaningful, okay? It doesn't actually correspond to a discrete solution.
00:14:14.314 - 00:14:17.082, Speaker A: It corresponds to some fractional thing that you can't interpret.
00:14:17.226 - 00:14:30.610, Speaker C: Yeah, I mean, we know for linear programs that there's one optimal solution that is I don't know what the correct term is, but basically an extreme solution.
00:14:31.190 - 00:14:40.360, Speaker A: Okay, would be right. There's no guarantee that that's not fractional either. In general.
00:14:44.250 - 00:14:44.806, Speaker C: No.
00:14:44.908 - 00:14:51.098, Speaker A: In general, no. Well, maybe here I wouldn't expect you to necessarily know anything, but I'm going to prove something about it.
00:14:51.184 - 00:14:51.626, Speaker B: Okay?
00:14:51.728 - 00:15:38.870, Speaker A: So in general, when you formulate linear programming, you intend for them to be formulations, but frankly, they're relaxations, okay? They include stuff beyond the just integer solutions you're trying to capture. And so when you optimize over the linear program, you might conceivably get a fractional solution, which is not what you want, which really just doesn't correspond to a feasible solution of your original optimization problem, okay? However okay, well, first of all, so facts, and again, this is something you just, even if you don't know why this is true, all of you should know this is true and remember forever that this is true. So fact, linear programs can be solved efficiently, okay? That's true both in the practical sense with the appropriate commercial software, and it's also true in theoretical sense, in the sense of polynomial time solvability.
00:15:38.870 - 00:16:13.586, Speaker A: So you should know both of those things so can solve LP in polytime, okay? It's not essential that you know why. And in fact, frankly, most people who use linear programming, both in practice and in theory treat it as a black box and don't know a lot about the why okay? So if you're feeling very keen, by all means, learn more about it. Okay? There are people who are experts in this and it's useful skills, but a lot of people just use linear programming without actually understanding it.
00:16:13.586 - 00:16:25.654, Speaker A: And that's a totally acceptable and normal thing to do. And that's what we're going to do in this class. Needless to say, there's some other great classes at Stanford where you can peek under the hood and see how these things actually work.
00:16:25.654 - 00:16:37.814, Speaker A: And those are fun as well. Okay, but so here's the claim. So the claim is that the optimal solution to this linear program is not something that we can't interpret.
00:16:37.814 - 00:16:42.270, Speaker A: It's not fractional, in fact, is integral.
00:16:44.290 - 00:16:45.040, Speaker B: Okay?
00:16:45.410 - 00:16:54.110, Speaker A: So we just write down this LP, we just stick it into our favorite LP solver and out pops on a silver platter this unique optimal cut.
00:16:54.260 - 00:16:54.622, Speaker B: Okay?
00:16:54.676 - 00:17:02.098, Speaker A: So we just run this algorithm, which presumably someone up has coded up and engineered very well, and it just gives us exactly what we want. Okay, so that's a pretty sweet guarantee, actually.
00:17:02.184 - 00:17:02.820, Speaker B: Right.
00:17:06.090 - 00:17:50.606, Speaker A: Okay, so any questions before I start explaining why this is true? This isn't obvious and the proof is not that long, but it's not an obvious statement. It right. I meant to say just to illustrate sort of like what fractional solutions might look like on that graph over there, imagine in this graph you set all of the X subies equal to a fifth in this graph here, okay? So you could, for example, set D here to be a fifth, d here to be two fifths, the D value here to be three fifths, and the D value here to be four fifths.
00:17:50.606 - 00:17:53.918, Speaker A: And you could set all of the XW's equal to one fifth.
00:17:54.094 - 00:17:54.722, Speaker B: Okay.
00:17:54.856 - 00:17:58.802, Speaker A: That would be an example that satisfies all of these inequalities.
00:17:58.946 - 00:17:59.590, Speaker B: Okay?
00:17:59.740 - 00:18:08.054, Speaker A: And it doesn't correspond in some obvious way to a cut. If it feels like kind of a superposition of a few cuts, that's good intuition. It is.
00:18:08.054 - 00:18:11.750, Speaker A: But it's not obviously a cut to have all these fractions.
00:18:13.370 - 00:18:14.120, Speaker B: Okay.
00:18:17.370 - 00:18:22.780, Speaker A: So the proof of this claim game is pretty neat, actually. I mean, there's a bunch of ways to prove it.
