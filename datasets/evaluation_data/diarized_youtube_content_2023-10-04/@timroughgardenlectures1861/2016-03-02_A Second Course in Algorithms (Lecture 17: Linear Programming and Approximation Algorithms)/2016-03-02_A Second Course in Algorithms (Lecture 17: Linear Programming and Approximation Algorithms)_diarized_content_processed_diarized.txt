00:00:00.250 - 00:00:42.614, Speaker A: Um, all right, so I want to keep on talking about approximation algorithms. And again, one of the takeaways from approximation algorithms is that all of the design paradigms that you've studied over your education so far are super useful for approximation algorithms. And so this lecture is going to be about how one of the design tools that you just got this quarter, namely linear programming, how that gets used in approximation algorithm design. And in some ways, it's it's sort of a match made in heaven, right? Because, remember, a key challenge in the design and analysis of approximation algorithms is getting a handle on what the optimal value is. So the minimum cost of a tour, the minimum make span of a schedule, whatever. And now often it's hard to get a handle exactly on what the optimum is. After all, it's usually empty, complete to compute.
00:00:42.614 - 00:01:25.462, Speaker A: But so what you wind up doing is you prove bounds on how good the optimal solution could possibly be. So you identify quantities that are only better than opt. You use that as a yardstick to compare the solution of your own algorithm. And if you're within a factor alpha of something which is better than opt, then you're certainly within a factor of alpha of the optimal solution as well. So the question then is, where do these bounds, where do these things only better than opt come from? So last week, we saw a couple of ad hoc examples. So, like, for scheduling, we looked at the maximum job size, we looked at the average load for metric tsp, we looked at the minimum spanning tree. But if you think about it, when we talked about duality, the whole point of dual linear programs was to give you bounds on how good the optimal solution could be.
00:01:25.462 - 00:02:01.314, Speaker A: And indeed, dual feasible solutions turn out to be the key to a lot of kind of the state of the art approximation algorithms. So I'm going to give you a glimpse of that today. Actually, I want to start with a problem where we're initially not going to use linear programs. It's going to really be kind of a continuation of our lecture a week ago, which we concluded with set coverage. So I want to talk a little bit about a variant called the set cover problem. So the input is exactly the same as in set coverage. There's some ground set U.
00:02:01.314 - 00:02:23.946, Speaker A: I give you a bunch of subsets of it, each just a list of elements. There's no budget, though. There's no k. So while in set coverage, the hard constraint is that you're only allowed to pick k sets. Subject to that, you want to cover as many elements as possible. Those are reversed in set cover. The hard constraint is you have to cover everything subject to that.
00:02:23.946 - 00:02:42.720, Speaker A: You want to do it using as few of the subsets as possible. So pick as few subsets as possible subject to the union of the subsets you pick should be all of you.
00:02:43.970 - 00:02:44.720, Speaker B: Okay?
00:02:45.970 - 00:03:29.662, Speaker A: So for example, if this is your ground set and S One is the top half, s two is the top half, bottom half, and maybe there's a different way that it's partitioned s three, S four, and S five. Here, the optimal set cover is going to use two sets. You're just going to pick S one, and you're going to pick S two, s three, s four and S five would also be a set cover, but it uses three sets, so it's not as good. Okay, so that's the set cover problem clear. So we allow you to pick as many sets as you want, but you have to cover everything you again want to do it sort of as economically as possible. All right, so this is another MBRD problem. It's MP hard basically for the same reasons that set coverage as MP hard.
00:03:29.662 - 00:04:10.998, Speaker A: And again, the tension of which sets to pick is the same as before. On the one hand, you want like, big sets. The other hand, some sets largely just are redundant with other sets, whereas some are sort of uniquely useful. So you have to balance that tension between the size of a set and sort of the other possibilities of covering those same elements. Okay, so the greedy algorithm that we discussed for the set coverage problem, again, makes perfect sense for the set cover problem. So what was the greedy algorithm for set coverage? Well, we just kept picking the set that covered as many new elements as possible. We were only allowed to pick K sets, so we stopped after K iterations.
00:04:10.998 - 00:04:49.610, Speaker A: So we're again going to pick the set which covers as many new elements as possible. We're just going to run to completion. We're going to run until we've covered everything. Okay, so greedy algorithm. So C is the collection of subsets that we've chosen. So while C not a set cover, add to C the subset si covering the max number of new elements.
00:04:54.990 - 00:04:55.740, Speaker B: Okay?
00:04:56.510 - 00:05:21.538, Speaker A: So as usual, when you're thinking about picking a set, the elements which have already been covered by previous sets don't count, okay? You're only looking at the new progress you're making relative to the union of the previous sets that you've already chosen, and then you just return this collection. Okay, so everyone clear on the greedy algorithm. So again, just use the one from last week, but let it run until it's a cover.
00:05:21.624 - 00:05:22.258, Speaker B: Okay?
00:05:22.424 - 00:06:16.366, Speaker A: Obviously, if there is a set cover, it's going to find one. If nothing else, it'll just pick every set. So the only question is how good is the set cover in terms of how many sets it uses compared to the minimum possible, the minimum size of a set cover? That's sort of the key question. All right, so we're not going to get a factor as impressive as for set coverage, but we'll get something. So we're going to prove next is that greedy is a natural log of n approximation. Okay, so I should say we don't expect this to be optimal, certainly because this NB hard problem and it's obvious you could implement this in polynomial time. And actually if you want concrete examples where this won't output an optimal solution, you can just go back and look at exactly the same examples we discussed for set coverage.
00:06:16.366 - 00:07:03.106, Speaker A: So the first example will show that there might be a set cover of size two, but this greedy algorithm will output one of size three. The second example from last week will show that even when there's a set cover of size three, the greedy algorithm might output something of size five. And there are worse examples than these as well. Okay, so we expect it to be suboptimal by how much? Well, this is the upper bound we're going to prove, okay, so what's n, I should say, where n is the number of universe elements. If you stare at the proof closely, you'll realize we actually prove its natural log of the maximum size of a subset, which could be much smaller than the size of the universe. Okay, but let's just prove this version for simplicity. So this is the first result I want to talk about today.
00:07:03.106 - 00:07:45.970, Speaker A: We're not going to use linear programs actually, for this. We're going to be able to really nicely piggyback on our understanding of the set coverage problem. And so that'll allow us to really quite quickly prove this guaranteed. So let's remember what we proved for set coverage. We proved that no matter what the collection of subsets is, no matter what the budget k is, the greedy algorithm will output something that covers at least a one minus one over refraction of what the best possible collection of k sets could cover. So that's what we proved last time, a one minus one over e approximation. And now notice.
00:07:45.970 - 00:08:24.910, Speaker A: So think about our current greedy algorithm and just sort of zoom in on the first, say, like ten iterations of this greedy algorithm for set cover. Now think about if we ran the greedy algorithm for set coverage on the same set system with a budget of ten. Any difference between those two algorithms? Exactly the same every iteration, you just pick the set that covers the largest number of new elements. Okay, so iteration by iteration, these algorithms are the same for set cover and set coverage. The only difference is the stopping condition. Set coverage, you stop after k iterations. Set cover, you stop when you've got a set cover, when everything's covered.
00:08:24.910 - 00:09:15.390, Speaker A: So what's the point? Okay, so imagine that there exists a set cover that uses only ten sets, okay? And now imagine in our minds, imagine we ran that set coverage approximation algorithm with a budget of ten. Well, we know it's a one minus one of reappreximation. If there's something with size ten that covers everything with our ten sets, we're going to cover at least a one minus one over refraction of everything. Okay, so that's what happens with the first ten sets. We get at least one minus one over e covered. All right, so rather than ten, let me just say opt for the number of sets in the best set cover. So the first op subsets cover at least one minus one over e times U elements.
00:09:17.410 - 00:09:18.160, Speaker B: Okay?
00:09:19.570 - 00:09:26.690, Speaker A: Again, there exists a solution with opsets covering all of U and the greedy algorithms of one minus one over e approximation.
00:09:27.030 - 00:09:27.780, Speaker B: Okay?
00:09:30.070 - 00:10:03.562, Speaker A: Now this set cover algorithm is not going to stop after just picking op sets unless it's lucky enough to already cover everything. It's going to keep going. But the point is that within opt iterations, it has shrunk the number of elements it has to cover by a factor of e. One minus one over e at least got covered. At most a one over e fraction still needs to be covered. So opt iterations shrink the universe size by a factor of e. Okay, that's true.
00:10:03.562 - 00:10:48.234, Speaker A: Not just for the first batch of opt iterations, right? Sort of. You know, the second batch of opt iterations can just be thought of as a sort of residual set cover set coverage problem, and the same guarantee continues to hold. So every opt iterations, this algorithm reduces the number of uncovered elements by a factor of at least e. So you divide by e every time you do opt iterations of this greedy algorithm. How many times can you do this? Well, you're done. Once you have less than one uncovered element, you start from N. You're dividing by e every time.
00:10:48.234 - 00:11:09.250, Speaker A: Remember that the natural logarithm is log base e. So we're done. Meaning all elements covered within at most opt times the natural log I e log base e of n iterations.
00:11:11.670 - 00:11:12.420, Speaker B: Okay.
00:11:14.150 - 00:12:05.960, Speaker A: So opt is the smallest size of a set cover, and the greedy algorithm will get you that number times the natural log of N. Okay, so any questions about that? So we actually, actually sort of the point of this is we already, again, sort of we could reuse the tools from last week to just solve this problem quite easily, which is good. It's a basic problem, set cover. The motivations are very similar for set coverage, right? We mentioned a lot of applications of set coverage. It's just the difference is sort of which is the hard constraint and which is the objective. You could imagine, for example, if you're trying to locate fire engines in a city, maybe it's a hard constraint that you really have coverage to all of the neighborhoods and then the natural objective is just to minimize the cost or the number of fire stations. So it's going to depend on the context, which one's the hard constraint and which one you just want to do as well as you can.
00:12:05.960 - 00:13:06.614, Speaker A: Okay, but actually, when set cover arises in applications, often not all the sets are the same. So like some neighborhoods might be more expensive to build in than others. So in the usual set cover problem, which is what we're going to discuss next, we still have this input, we still have this collection of subsets, but also each set can have its own cost, possibly different for different sets. Okay, so cost CI for each si. The goal now of course, is to compute the set cover with minimum possible total cost. So the previous problem corresponds to all of the CIS being equal to one. Now in set coverage, there were no costs, right? So once we talk about set cover with costs, it's not clear how to piggyback on our set coverage analysis to very quickly prove something.
00:13:06.614 - 00:13:51.110, Speaker A: Okay, that's the bad news. So we're going to need some other idea. The good news is the idea that we need is really only in the analysis. As far as the algorithm, it's pretty easy to think about extending the greedy algorithm to when sets have costs. So like if one set costs twice another, maybe you're only tempted to pick it if it covers at least twice as many new elements. Okay, that's sort of the obvious extension of the greedy algorithm and so then that turn out is going to work well. Okay, so what is the greedy algorithm with costs? I'll just pick the subset si with the minimum.
00:13:51.110 - 00:14:23.140, Speaker A: I'm going to call it a ratio, which is the cost of the set divided by the number of new elements that the set covers. Okay, so in other words, it's the average cost per new element that you're covering. Okay, so if the set has cost ten and it's covering four new elements, it's going to be 2.5.
00:14:26.710 - 00:14:27.314, Speaker B: Okay?
00:14:27.432 - 00:15:16.980, Speaker A: Notice that if all of the costs are one minimizing, this is the same as maximizing the number of new elements covered, which is exactly our old algorithm. Okay, so this does indeed specialize to our previous greedy algorithm and all of the costs are the same. Is everyone clear on the new algorithm? You just use this different criterion in the main while loop. And so the good news is, even with costs, the greedy algorithm is still just as good as it was before. It's still basically log n approximation. And again, the n is really the maximum set size. So that's what I want to prove next.
00:15:17.430 - 00:15:18.180, Speaker B: Okay.
00:15:18.870 - 00:15:43.622, Speaker A: The greedy algorithm still works even when there are costs. Now here it's not clear how to prove it because again, we can't piggyback on our set coverage analysis. So what I'm going to do is I'm going to give you a direct argument. So we'll just see a proof and then we'll zoom out and we'll recognize that the proof that we just did was really using a dual feasible solution and weak duality. That's really what's going to be going on in this argument.
00:15:43.766 - 00:15:44.460, Speaker B: Okay?
00:15:49.390 - 00:16:26.230, Speaker A: All right, so what's our strategy for the analysis? How are we going to prove a bound on the, on the greedy algorithm. Well, as always, when you're proving a guarantee on a greedy algorithm, you better use the fact that it's greedy. Somewhere in your proof, you need to use what its greedy criterion is. So let's go ahead and get that out of the way. First to have a very simple lemma, which is where we use the definition of the greedy algorithm. So this will be familiar from a similar but harder lemma, slightly harder lemma that we had a week ago for set coverage. So suppose the current greedy solution.
00:16:26.230 - 00:17:08.342, Speaker A: All right, so imagine we've been running the greedy algorithm. We've run it for ten iterations, okay? We're going to look at some subset si. Any of these input subsets si. So after ten iterations, some of the elements of si will be covered and some will be uncovered. So call L, the number of elements covered by si at this time. What can we conclude? So a week ago we wanted to say that the greedy algorithm made healthy progress at every step. That's sort of what we were doing here.
00:17:08.342 - 00:17:15.650, Speaker A: We're going to say, well, we're going to prove an upper bound on how big this ratio could be of the next set chosen by the greedy algorithm.
00:17:15.730 - 00:17:16.360, Speaker B: Okay.
00:17:18.430 - 00:17:39.550, Speaker A: So next set chosen by greedy has ratio at least sorry, at most the cost of si divided by the number of elements of si not yet covered.
00:17:43.350 - 00:17:44.100, Speaker B: Okay.
00:17:45.990 - 00:18:22.266, Speaker A: So if you've understood everything I've said so far, this is like totally immediate. Okay, why? Well, the greedy algorithm of the next iteration has the option of picking the set si. If it picks the set si, this is the cost that will be incurred. This is the number of new elements that will be covered. Okay, just by the definition of L. Now the greedy algorithm picks the set with the smallest ratio, so it'll be at most the ratio it would attain if it achieved si. Okay, so that's it.
00:18:22.266 - 00:18:45.522, Speaker A: So that's the proof. So any questions about dilemma? So in set coverage, we said, oh, well, as long as the greedy solution is far from optimal, we're going to make healthy, we're going to cover a lot of elements. Now what we're saying is if there's some set where we haven't done a good job of covering much of it, it's because we're picking cheap sets or sets with small ratios, small average cost per element hundreds.
00:18:45.586 - 00:18:46.200, Speaker B: Okay.
00:18:52.410 - 00:19:22.430, Speaker A: Good. So a little notation for the analysis we're going to define for an element q sub will be the ratio of the first set or first subset.
00:19:24.450 - 00:19:24.814, Speaker B: To.
00:19:24.852 - 00:19:27.710, Speaker A: Cover e in the greedy algorithm.
00:19:27.790 - 00:19:28.370, Speaker B: Okay?
00:19:28.520 - 00:19:56.460, Speaker A: So think about it this way. We zoom in on the 17th element of the ground of the ground set. We watch greedy run. Okay, the greedy algorithm terminates with a set cover, so at some point it's got to pick a subset that covers this element number 17. We look at the first iteration where it gets covered, where it gets newly covered, we look at the ratio of the set that's chosen in that iteration. That is q sub e. Okay, so every element just says, oh, yeah, the ratio of this first set to cover me was this.
00:19:56.460 - 00:20:35.640, Speaker A: I'm using this notation, q sub e, to sort of perhaps remind you of the Q values we talked about when we analyzed online bipartite matching. As we'll see, something very similar is going on with set cover. Okay, so maybe just a quick example. So if you imagine that sort of a very simple set system, say this was the set system, let's say all the costs were one. So greedy is just going to pick the biggest set first if all the costs are the same. So each of these gets a Q value of what.
00:20:39.470 - 00:20:40.540, Speaker B: What was it?
00:20:42.750 - 00:21:04.506, Speaker A: One by three. Right. So when we pick this, the cost is one. The number of elements covered for the first time is three. So the set has a ratio of a third. Everything that it covers, newly gets exactly that as its Q value. Okay, so then what happens next? Well, so in the next iteration of Greedy, this covers two elements, newly.
00:21:04.506 - 00:21:24.854, Speaker A: This one only covers one element, newly. So the Greedy algorithm will pick this one next. And again, its ratio is one over two. So both of these get a Q value of one half. Now in the final iteration, greedy has no choice but to pick this set. It only covers one new element, so the ratio is one over one. And so this one gets a Q value of one.
00:21:25.052 - 00:21:25.800, Speaker B: Okay.
00:21:29.390 - 00:22:00.674, Speaker A: All right, good. Let's do the next step. So so what, how do we use this lemma? Well, this lemma gives us an upper bound on how big the Q values of the elements of a set can possibly be.
00:22:00.792 - 00:22:01.460, Speaker B: Okay.
00:22:06.090 - 00:22:13.990, Speaker A: So fix your favorite subset, si. Think about the jth element of si to be covered by Greedy.
00:22:16.570 - 00:22:17.320, Speaker B: Okay.
00:22:23.150 - 00:23:15.660, Speaker A: The Q value of this element e, is almost the cost of this set over the size of this set minus J minus one. Okay, how should you think about this? Okay, so here's the way to think about it. So imagine j minus one elements have already been covered. Okay, that's just like plugging in J minus one for L in the lemma. Okay, so if only j minus one are covered right now, at some point there'll be a set which covers this Jth element to be covered. And the lemma is in effect. So it will say that whatever set is used to cover this Jth element has ratio at most the cost of this set, si divided by the number of still uncovered elements in si, which is the size of si minus J plus one.
00:23:15.660 - 00:23:51.090, Speaker A: Okay, it may also help to look at this figure. So let's say for this biggest set, the set of size three. Now, one thing that's a little confusing maybe is, as you can see in this figure, the greedy algorithm doesn't exactly cover the elements sort of one by one. In general, it covers a batch of elements in the same iteration. So like for this set, it covers all three of them in the same iteration, so they all get a one third. But if you think about it, that's fine for this. What is this asserting for a set of size three? It's saying that the first element to be covered has a Q value of almost a third.
00:23:51.090 - 00:24:12.270, Speaker A: If the cost is one, the second element to be covered has a cost at most a half. And the third element has sorry, not a cost, a Q value of at most a half. The third element to be covered has a Q value of at most one. Here it's only more true, right? It's one third, one third, one third. Similarly, for a set of size two, this corollary says the first one to be covered should be at most one half. We're doing even better than that. We have a third.
00:24:12.270 - 00:24:54.634, Speaker A: The second one to be covered has cost the most one. And indeed, that is in fact, the Q value of the second one to be covered. Okay, so the way to think about the proof of this is just first think about the special case where you have this set si, and literally each element gets picked off in a different iteration. Really? In the first iteration, the first one gets chosen. In the second iteration, a set containing the second one gets chosen and so on. Then it's easy to see why this holds, right? It's really just you substitute J minus one, the number of predecessors of the Jth element for L in this lemma, and then you realize, oh, if they get covered in batches, actually the Q values are going to be even smaller. Because that means at the time I was covered, there were even more uncovered elements, even fewer covered elements than there was in the case where they get picked off one by one.
00:24:54.634 - 00:24:57.498, Speaker A: And so it's going to be only more true just like in the figure.
00:24:57.664 - 00:24:58.380, Speaker B: Okay.
00:25:00.210 - 00:26:06.450, Speaker A: All right, so any questions about that? All right, so that's how we use the greedy algorithm. We use the greedy algorithm to say, to basically get these upper bounds on how big the Q values of elements of a set could be. So let's just sort of compile this information. So for all subsets si, how big could the sum of the Q values of elements in the set S be? Well, there's the first one to be covered. So that's taking J equal one in the corollary. Sorry, taking J equal to one in the corollary. There's the second one to be covered, that's taking J equal two in the corollary.
00:26:06.450 - 00:26:23.030, Speaker A: The second to last one will have a Q value upper band of CI over two, and then the last one to be covered CI over one. Okay, so it's just applying that corollary for each of the elements in the set.
00:26:23.100 - 00:26:23.720, Speaker B: S.
00:26:25.930 - 00:27:04.720, Speaker A: So if you look at this, this is approximately so bring the CI out, we're left with this harmonic sum, one plus a half, plus a third, all the way up to one over the set size. That's basically the natural log of the set size being slightly sloppy. There's an extra additive term known as Euler's constant here, but that's less than one, so I'm just going to ignore it. So this sum is bounded above by basically the set cost times log of the set size, which of course is at most, in the worst case, CI times the natural log of the ground set size.
00:27:05.050 - 00:27:05.800, Speaker B: Okay?
00:27:06.330 - 00:27:17.540, Speaker A: So that's the first thing to notice. Greedy gives us lemma gives us control over how big Q values can be. So therefore, of course, there's some kind of bound on the sum of the Q values of a set.
