00:00:00.570 - 00:00:14.506, Speaker A: In the last video, we sketched what a proof of stake longest chain consensus protocol might look like. I hope it seemed pretty natural, right? As usual, we're just trying to reduce permissionless consensus to permissioned consensus. We already know everything we want to know about permissioned longest chain consensus.
00:00:14.506 - 00:00:21.210, Speaker A: That was back in lecture number eight. We already know tons of techniques for proof of stake random sampling. That was part two of this lecture.
00:00:21.210 - 00:00:37.334, Speaker A: And why not stitch those two together? In particular, we're looking at a VRF based approach to proof of stake random sampling. Sadly, in blockchain protocol design, it's rarely this simple. Rarely can you just solve two problems completely independently and stick together the solutions and call it a day.
00:00:37.334 - 00:01:15.086, Speaker A: Usually there are some additional complications that arise on the edges. And we, I think, saw this as we talked through the protocol in the last video we saw, there were spots where sort of in some sense the output of our proof of stake random sampling based on VRFs was not exactly the input that permission the longest chain consensus was expecting. So in this video, I want to talk about how to resolve some of those mismatches as we saw back in our part two videos on VRF based random sampling, as we saw in the first half of part three when we sort of talked about BFT type consensus protocols, one big pain about VRF based sampling.
00:01:15.086 - 00:01:41.062, Speaker A: So one of the real costs of getting that secrecy property is you're stuck with a variable number of winners. So you can tune a difficulty parameter to target some number of winners of a VRF sampling lottery. But by virtue of being in effect independent random experiments across different private keys with which you evaluate the VRF, you're going to have some variance around that expectation.
00:01:41.062 - 00:02:07.582, Speaker A: So you just have to learn to live with a variable number of winners coming out of any VRF based selection procedure. So two of the three issues I want to highlight on the left half of this slide concerning this variable number of winners, quirk of VRF based sampling for issue number one, let's start with what's, hopefully the simpler one, which is sometimes you're not going to get any winner at all. Sometimes nobody's credential is going to be small enough, no one's credential will be smaller than the threshold.
00:02:07.582 - 00:02:15.458, Speaker A: And so we'll have a time step with no leaders whatsoever. We're obviously not happy about these kinds of time slots. They're intuitively just kind of wasted.
00:02:15.458 - 00:02:25.338, Speaker A: We'd rather have one leader in a time step than zero at the same time. It kind of just doesn't seem like that big a deal. Like presumably when you have a time step like this, you just kind of skip on to the next one.
00:02:25.338 - 00:02:46.370, Speaker A: So obviously not good for latency, meaning the amount of time you have to wait before transactions get confirmed, the more slots there are nothing happening, the longer that's going to take it's also worse for throughput. So for example, the number of transactions that are getting processed per second. If you have skip slots where nothing happens, that's obviously going to decrease the rate at which you're processing transactions.
00:02:46.370 - 00:02:59.202, Speaker A: But these time slots are certainly not an existential threat. They don't have any effect on the basic consistency and liveness guarantees of longest chain consensus. Now, I'm not going to propose any fix to this issue.
00:02:59.202 - 00:03:18.886, Speaker A: Actually that'll be true for issues two and three as well. This is just kind of a drawback of this approach that we're going to have to absorb this degradation in latency and throughput due to skipped slots, due to VRF based sampling, not giving us any winners at a given time step. Let's now focus on time steps where we have one or more leaders.
00:03:18.886 - 00:03:46.894, Speaker A: And obviously one thing we have to worry about is we might have at least one Byzantine leader and they can do lots of random stuff, right? For example, they're free to propose any number of blocks that they want, extending any parts of the blockchain that they want. So for example, if we currently have an entry consisting of four blocks, nothing is stopping a Byzantine leader from extending literally all four of these blocks with its own newly created four blocks. So that might seem kind of scary.
00:03:46.894 - 00:04:04.938, Speaker A: It might seem Byzantine nodes by virtue of proposing all these different blocks, extending all these different parts of the blockchain, they might be able to confuse the honest nodes. But actually, if you remember the lessons we learned back in lecture number eight when we talked about permissioned longest chain consensus, we actually were allowing Byzantine nodes to do exactly this. This was part of our model back in lecture number eight.
00:04:04.938 - 00:04:24.430, Speaker A: And in particular we proved that longest chain consensus remains consistent and live under the usual assumptions. So you need 51% honest participation, you need to be in the synchronous model and so on. But under our usual assumptions for longest chain consensus it remains consistent and live even if Byzantine nodes can do these multiple block proposals.
00:04:24.430 - 00:04:46.674, Speaker A: In lecture number eight we gave a full proof of the consistency and liveness properties of longest chain consensus. In the proof of work case, in the Nakamoto consensus case where you have the stronger assumption, a four prime, that a Byzantine node only makes one block proposal when it's selected as a leader. In that case we were able to factor the analysis into two parts.
00:04:46.674 - 00:05:06.430, Speaker A: We had this one part saying that a randomly generated leader sequence is with high probability going to be balanced meaning you're going to have proportional representation of honest leaders in any sufficiently long window. And then secondly, we showed that as long as you have a balanced leader sequence then you're good to go. You get the common prefix property from which you can prove consistency, liveness, et cetera.
00:05:06.430 - 00:05:29.510, Speaker A: Unfortunately, if you do allow a Byzantine leader to make multiple block proposals. If you only have the weaker assumption, a four, then the balancedness assumption we used is no longer sufficient to deduce consistency on liveness, but random leader sequences actually satisfy a stronger property with higher probability. balancedness on steroids is what I called it back in lecture number eight.
00:05:29.510 - 00:05:50.380, Speaker A: And it turns out that stronger property now is sufficient to derive exactly the same consistency and liveness guarantees that we prove for Nakamoto Consensus. So definitely harder to analyze the proof of stake version of longest chain consensus than the proof of work version. But whatever, at the end of the day, the good news is exactly the same consistency and liveness guarantees hold.
00:05:50.380 - 00:06:12.260, Speaker A: So this property of proof of stake longest chain consensus, with leaders potentially trying to extend lots of parts of the blockchain at the same time, that's related to a phrase you might come across sometimes called nothing at stake. I'm going to mostly avoid this phrase, even though you do hear it from time to time. One reason is I just don't think it's super descriptive of what it's trying to explain.
00:06:12.260 - 00:06:34.426, Speaker A: The other reason is that different people use it to mean pretty different things sometimes. Nothing at stake is used to mean the property that we're seeing here of proof of stake longest chain consensus that a leader can effectively reuse its stake, in effect mining on multiple different chains at the same time. And that's different than how Nakamoto Consensus works.
00:06:34.426 - 00:06:50.560, Speaker A: So Nakamoto Consensus, you can only mine on a single block at a time. Your lottery ticket comes embedded in it with the block that you are extending. That is part of the input that you are passing to the cryptographic hash function, trying to get an output that's sufficiently close to zero.
00:06:50.560 - 00:07:17.042, Speaker A: Other times, nothing at stake refers to a much broader collection of attacks on proof of stake blockchain protocols not really specific to longest chain, which are just based on the idea of costless simulation. The fact that unlike in proof of work where if you want to fabricate an alternative history, for example, going back to the Genesis block, you have to solve all the proof of work crypto puzzles along the way. In proof of stake, it's typically very cheap to fabricate long alternative histories.
00:07:17.042 - 00:07:30.470, Speaker A: So that gives you a lot of different types of attacks and sometimes nothing at stake is used to refer to that issue. We'll talk about that broader class of attacks in part four. Specifically, we'll drill down on the important class of long range attacks.
00:07:30.470 - 00:07:58.386, Speaker A: But for now, I want to focus on this narrower meaning of the term, the fact that in proof of stake longest chain, a liter of a time step can in effect mine all of the chains simultaneously. Now, one of the original concerns with this nothing at stake problem in the narrower sense actually didn't really have to do with Byzantine nodes. It was more about what would be honest nodes would do if there was a deviation that was somehow obviously better for them to see the issue.
00:07:58.386 - 00:08:09.074, Speaker A: Imagine that you have a fork that creates a tie in the longest chain, right? So you imagine you had this long chain. It was like 100 blocks long. But then unfortunately, perhaps inadvertently, a fork was created.
00:08:09.074 - 00:08:34.400, Speaker A: Two different nodes proposed blocks at height 101, both extending what was once the tip of the longest chain. Now, if you're a leader at the next time step and you're honest, what you're supposed to do is propose exactly one block, and you're supposed to extend one of the two longest chains. So some block at height 102, extending one of the height 101 blocks, you're supposed to break the tie in effect, and you have discretion about how you want to do that.
00:08:34.400 - 00:08:43.426, Speaker A: Now, in Nakamoto Consensus, you have no choice. You really have to pick sides. You cannot reuse hash rate to try to extend both branches at the same time.
00:08:43.426 - 00:09:04.234, Speaker A: But in proof of stake longest chain consensus, you can, you actually don't need to take sides. You can equivocate, you can propose two height 102 blocks, extending the two height 101 blocks. Now, why would you do that? Well, let's imagine it's a longest chain protocol that uses block rewards the same way that bitcoin does.
00:09:04.234 - 00:09:29.422, Speaker A: So you're going to get an economically meaningful reward whenever you propose a block that winds up getting finalized. Whenever it winds up getting sufficiently deep on the longest chain. If you only extend one of the two branches, right? If you only extend, say, the first branch with a height 102 block, there is some risk that what winds up being the longest chain at the end of the day extends the other branch, the branch that you're not on, in which case your block is going to get orphaned.
00:09:29.422 - 00:09:45.282, Speaker A: You're not going to get your block reward. But if you extend both branches of the fork, you propose two height 102 blocks, then if the longest chain doesn't matter which of the branches the longest chain winds up extending, your block is going to be on it. You're going to be collecting your block reward.
00:09:45.282 - 00:10:11.210, Speaker A: So that's the reason why even if you're mostly honest, you might say, well, let me do this little deviation just to kind of hedge my bets about what the longest chain is going to wind up being just to guarantee my block reward for myself. Problem being is that even if there's no Byzantine nodes, just if every node does exactly that little deviation from longest chain consensus, well, now you're going to have two height 103 blocks. You can have two height 104 blocks, and this fork will be perpetuated forever.
00:10:11.210 - 00:10:24.942, Speaker A: So that's kind of the doomsday scenario people were worried about when they first started thinking hard about nothing at stake. So that's certainly a problem. In principle, with proof of stake longest chain protocols, I should say this hasn't really been observed in practice.
00:10:24.942 - 00:10:41.218, Speaker A: So for the proof of stake longest chain protocols that are out there, the nodes running those protocols don't seem to be doing this. That of course does not mean that they're not going to start deviating at some point in the future. If this did become a problem in practice, you could imagine various ways a protocol could respond.
00:10:41.218 - 00:10:51.930, Speaker A: Maybe the most obvious would be to add Slashing. So that's something we talked about in a video way back in part one when we talked about advantages of proof of stake civil resistance. We'll have a whole video about it in part four.
00:10:51.930 - 00:11:38.330, Speaker A: But just again, the high level idea is that if there's sort of a provable deviation from honest behavior in your protocol, like voting on two different conflicting blocks in the tenderman protocol or here proposing two different blocks with the same winning lottery ticket two different blocks at the same time step that you're a leader. If the protocol receives evidence of that form, then they exploit the fact that you have stake locked up in the Staking contract controlled by the blockchain protocol and they take away part or all of your stake that you have locked up. So Slashing has its own set of risks as we'll discuss in part four, but presumably implemented correctly, that would be a pretty strong deterrent to anyone trying to exploit this nothing at stake property of proof of stake longest chain protocols.
00:11:38.330 - 00:11:59.380, Speaker A: All right, so that's enough about nothing at stake. Let's go back to talking about this complication of VRF based sampling that you get a variable number of winners. So issue number one, already talked through why the case that you get no leaders at all in a time step, why? That's not a big deal, but we still have to deal with the fact that we might get many leaders, we might have as many as like, I don't know, seven leaders all in the same time step.
00:11:59.380 - 00:12:30.918, Speaker A: So we had to deal with this issue earlier in part three when we talked about BFT type protocols there. It was actually pretty simple, right? So there if an honest node is just trying to figure out what block number nine is, it's just going to ignore block proposals for block number nine, except for the proposal that has the smallest adjusted Credential, everything else it just ignores. Now in longest chain consensus it's more complicated because block proposals are not going to be merely for some specific location like block number nine, right? They're going to be extending different parts of the blockchain.
00:12:30.918 - 00:12:47.194, Speaker A: So you might hear about one block proposal that on the one hand has a smaller Credential, which is good, but extends sort of a smaller chain, which is bad. And you also hear about a block with a worse Credential, but extending a longer chain. It's not really clear which of those you should be in a position to discard.
00:12:47.194 - 00:13:01.718, Speaker A: So in longest chain consensus when you have these multiple block proposals going on at the same time step, you kind of got to listen to and respect all of them. You really don't know which of those is going to wind up being on the eventual longest chain. So like with the first two issues, we're not going to fix this.
00:13:01.718 - 00:13:22.790, Speaker A: We're just going to talk through why this actually isn't that a big deal at the end of the day. Specifically, it's not a big deal if you set the difficulty parameter Mu sufficiently small. Now, what was mu again, you might be asking? Mu was one of the two parameters that showed up in our formula for our thresholds.
00:13:22.790 - 00:13:34.442, Speaker A: So when we asked the question, how small does a credential have to be before it qualifies as a leader of a time step? The threshold was a function of the stake amount Q subi. Right. With higher stake amounts, you want higher thresholds.
00:13:34.442 - 00:13:44.770, Speaker A: So it's easier for your VRF output to be below that threshold. But then also on this difficulty parameter mu. Right? And all of our approaches to civil resistance have had some kind of difficulty parameter.
00:13:44.770 - 00:14:03.298, Speaker A: So back in Nakamoto Consensus, we tuned a difficulty parameter to target a rate of growth in the longest chain. Here we're using the difficulty parameter Mu to tune how many liters we expect to have in a given round. And in the last video, I said for simplicity, just imagine we set Mu as a target of one liter per round.
00:14:03.298 - 00:14:10.490, Speaker A: But now I'm actually going to change that a little bit. Let's assume mu is a little bit smaller than that. So we're targeting well less than one liter in any given round.
00:14:10.490 - 00:14:30.580, Speaker A: To be more concrete, maybe we tune mu so that we expect 80% of the time slots to have no leaders and the other 20% to have one or more liters. The benefit you get of choosing mu that small is that it's actually pretty rare you're going to have a time step where you have two or more liters. That's just only going to happen a few percent of the time.
00:14:30.580 - 00:14:48.600, Speaker A: So let's talk through the intuition about why, if it's infrequent that you have more than one leader of a time step, probably everything's going to work out fine. It's kind of two things to talk about byzantine leaders and honest leaders. So if you have more than one Byzantine leader all active at the same time step, it's totally possible.
00:14:48.600 - 00:15:13.790, Speaker A: Intuitively, doesn't seem like that should help the adversary all that much, right? So if an adversary controls even one leader at a time step, it already has the power to propose whatever blocks it wants, extending whatever parts of the blockchain that it wants. So how does it help to have more than one node that can do that? Right. So whatever your multiple leaders might have done, why not just simulate that with a single leader? Now, I'm being a little glib here.
00:15:13.790 - 00:15:33.010, Speaker A: I'm sort of ignoring the possibility that the protocol might be using slashing, in which case you might want to use sybils and have multiple leaders so you can deviate in ways that are undetectable. You might want to use sybils to try to manipulate pseudorandom seeds. Remember, at the moment we're thinking about an ideal randomness beacon, so we're not worried about manipulation of pseudorandonists.
00:15:33.010 - 00:15:48.220, Speaker A: That's another reason why maybe multiple leaders would sort of be helpful to an adversary. But for the most part this is a correct statement. Basically, having multiple Byzantine leaders all active at the same time slot is no more helpful to the adversary than only having one.
00:15:48.220 - 00:16:23.974, Speaker A: The slightly bigger concern then is having multiple honest leaders all active at the same time step, because then some of their work might go wasted, right? If you have two honest leaders, they might both independently propose conflicting blocks, both extending the current end of the longest chain. One of those blocks at least is going to get orphaned wasting the contribution of that honest leader. If you watch the video from lecture number nine about extending our guarantees for Nakamoto Consensus from the supersynchronous model to the standard Synchronous model, we saw something very similar back then.
00:16:23.974 - 00:16:37.498, Speaker A: So back then we were thinking about message delays. And so we realized all of a sudden you could have inadvertent honest forks among different honest nodes because they didn't hear about each other's blocks in time. Here we're getting something similar inadvertent honest forks, but for a different reason.
00:16:37.498 - 00:17:01.006, Speaker A: Not because of message delays, but rather because of the variability inherent in VRF based leader selection. Now what we have going for us is that we're setting the difficulty parameter mu to be pretty small, which means it's going to be quite infrequent that we have a time slot with more than one honest leader. So it just won't be that often that any honest node's contribution winds up getting cancelled out by some other honest node.
00:17:01.006 - 00:17:19.378, Speaker A: So that means there will be some cancellation, there will be some wasted work. But it's basically the same as sort of a small, modest reduction in the fraction of stake that is controlled by honest nodes. So maybe we assume that, okay, maybe at least 52% of the stake in the staking contract is controlled by honest nodes.
00:17:19.378 - 00:17:50.034, Speaker A: A little bit of their contributions get wasted, but it's still as if we had a 51 49 split honest versus Byzantine stake, which is good enough for the consistency and liveness guarantees of longest chain consensus to go through. So those are the issues I wanted to highlight that come up either because of this so called nothing at stake problem or because of VRF based random sampling and its variable number of winners. That actually ties back, if you think about it, to the discussion we had when we were talking about VRF based sampling for the first time in part two.
00:17:50.034 - 00:18:03.010, Speaker A: Back then, we sort of pointed out that any approach to proof of stake random sampling has to make a pretty tough choice between three different options. Option number one is you can forget about secrecy. You could use something like weighted round robin.
00:18:03.010 - 00:18:28.446, Speaker A: You could just accept the risk that block proposers might are subject to, for example, denial of service attacks if you're not comfortable making that risk. If you actually do care about secrecy, then what are your other options? Well, there is Ssle single secret leader election where we have sort of in principle constructions that guarantee exactly one winner of the election. But that's really an experimental technology.
00:18:28.446 - 00:18:45.486, Speaker A: We don't really have practical deployments yet. We may have them soon, but we don't have them as of early 2023. So with that unavailable and wanting secrecy, you're kind of stuck with this VRF based sampling which seems to inevitably have this drawback of a variable number of winners.
00:18:45.486 - 00:19:10.374, Speaker A: And we talked about way back in part two that if you use that approach, you're going to have to probably wind up modifying whatever consensus protocol you're building on to accommodate the fact that there might be a variable number of winners. And indeed, when we talked about BFT type protocols, we modified tendermint in a couple different ways to accommodate the VRF based sampling that we were using. Right people ignored block proposals that weren't from the node with the smallest adjusted credential.
00:19:10.374 - 00:19:23.306, Speaker A: We used sort of weighted votes instead of unweighted votes. Here with longest Shank consensus, same thing. Changes are more modest, but still some changes compared to the lecture number eight version of permission longest chain.
00:19:23.306 - 00:19:33.940, Speaker A: So first of all, we have this variable number of winners. You might have multiple nodes with the right to make block proposals at each time step. And then secondly, we need to just make sure this parameter mu is set sufficiently small.
00:19:33.940 - 00:19:48.438, Speaker A: So let me now move on to making a few comments. These comments are optional, by which I mean they're sort of standalone. So if you skip the rest of this video, it's not going to affect your understanding of the rest of lecture number twelve at all.
00:19:48.438 - 00:20:19.620, Speaker A: But for those of you that are sort of following a good chunk of this lecture series rather than just sort of a handful of videos, I did want to take the time to point out some interesting connections between this discussion of proof of stake longest chain protocols and other things we've talked about in the past, like impossibility results for consensus that we talked about early in the lecture series and then also various nuances of Nakamoto Consensus that we talked about in lecture nine. So the first thing I want to talk about is what I've been calling the Pslflm impossibility theorem. This was the subject of lecture number three.
00:20:19.620 - 00:20:49.770, Speaker A: So let me briefly remind you the context. So in the lecture prior to lecture three and lecture two, we covered the dole of strong protocol and the point there was to show that we could get consensus even with 99% Byzantine nodes, which is kind of amazing if we're willing to assume that we're working in the synchronous model. So there are bounded message delays and also we're willing to make the public key infrastructure assumption, which again, PKI assumption means that sort of the protocol is born knowing public keys of all of the nodes that are going to be running that protocol.
00:20:49.770 - 00:21:13.714, Speaker A: So with that positive result under our belt, we proceeded to start probing to what extent the assumptions were necessary to achieve the guarantees of the Dolos Strong protocol. And in lecture number three, we're probing the PKI assumption. So we were asking what if we continue to work in the synchronous model, but a protocol does not have operiori knowledge of public keys of all of the nodes that are running it.
00:21:13.714 - 00:21:26.370, Speaker A: And here we proved an impossibility result. You might remember the hexagon argument that's what the proof sort of revolved around. So we had an impossibility result saying that you could not get the guarantees of the Doloph Strong protocol without the PKI assumption.
00:21:26.370 - 00:21:47.070, Speaker A: In fact, if you don't have a PKI assumption, there's no way to achieve consensus. If a third or more of the nodes can be Byzantine, there is also a matching positive result that we didn't cover, and that's less important for us. So if you have strictly more than two thirds of the nodes being honest and you're in the synchronous model, you actually can achieve consensus even without the PKI assumption.
00:21:47.070 - 00:22:09.606, Speaker A: Now, here we're talking about longest chain protocols and whenever we've been talking about longest chain protocols, the threshold has always been 50%. It's always been the case that if you have more than 50% honest participation, then you get guaranteed consistency and guaranteed liveness in the synchronous model. And obviously 51%, that's a weaker assumption than 67%.
00:22:09.606 - 00:22:46.030, Speaker A: So always with longest chain we find ourselves asking, wait a minute, why doesn't this positive result actually contradict the negative result, the impossibility result that we showed in lecture number three? Now, in the first version of longest chain consensus, we considered the permissioned version back in lecture number eight, this mystery was not hard to resolve because we very explicitly made the PKI assumption. And that was important, so that nodes couldn't basically fabricate block proposals in rounds where they're not actually the leader. And something I asked you to think about back then, and it's still a good thing to think through, is why the lecture eight analysis would break down if we didn't have that PKI assumption.
00:22:46.030 - 00:23:03.398, Speaker A: So the impossibility result in lecture three is when there's no PKI lecture eight, we had PKI, so clearly no contradiction. What was confusing really was in Nakamoto Consensus that we studied in lecture nine, because there really is no PKI assumption. The protocol has no idea sort of who's running it at any given moment.
00:23:03.398 - 00:23:30.746, Speaker A: Just once in a while, some block shows up that was found by some minor who you'd never heard about before, so very much not in the PKI setting. On the other hand, as we proved, you do get guaranteed consistency and liveness with 51% honest hash rate. So why was that not a contradiction? We resolved that mystery by noticing that proof of work as practiced in Nakamoto Consensus in effect really restricts the power of byzantine nodes.
00:23:30.746 - 00:23:48.866, Speaker A: It really prevents a Byzantine node from making more than one block proposal in a given round. And that's because rounds are defined by winning lottery tickets and every lottery ticket, right? Every input to the cryptographic hash function has encoded within it the choice of block. And so this then resolved the mystery, right? So we realized, oh, okay.
00:23:48.866 - 00:24:09.930, Speaker A: So if Byzantine nodes are restricted, if they're less powerful than they usually are when we talk about consensus, maybe it makes sense that you could actually get away with tolerating more of those more restricted types of Byzantine nodes. And that is in fact the case with Nakamoto Consensus. We also talked through why the proof, why the hexagon argument for the PSL FLM impossibility result.
00:24:09.930 - 00:24:30.174, Speaker A: We also talked through where it breaks down in the proof of work case. And the idea was this costly simulation. So if you remember the way that hexagon argument worked, basically a Byzantine node had to be capable of simulating simultaneously the behavior of four different honest nodes.
00:24:30.174 - 00:24:40.774, Speaker A: Now, if you're thinking about computation as being like free, then that's no big deal. Just a Byzantine node. It's basically just running kind of on four different threads, sort of simulating four fictitious honest nodes in its head.
00:24:40.774 - 00:25:09.200, Speaker A: But if each of the honest nodes is actually sort of maxing out its hash rate trying to solve proof of work crypto puzzles, then all of a sudden as the adversary, you have a problem, right? So if you have some hash rate, you can't magically fabricate four times that much hash rate just because you want to be able to simulate four different honest nodes simultaneously. So proof of work makes simulation costly. And so that's why byzantine nodes can't actually enact the strategy that we needed to get that proof to go through.
00:25:09.200 - 00:25:45.770, Speaker A: Now, in proof of stake longest chain Consentus, the whole point is that nodes should not have to be able to work hard to run the protocol, right? That was one of the motivations for going to proof of stake in the first place. So that means simulation is no longer costly, right? So one machine really could just sort of simulate all of the computations done running the protocol by four other machines. So what's going to save us in the proof of stake setting is the exact same thing that saved us back in the permission setting in lecture number eight, which is that basically in a proof of stake setting, we, for the purposes of this discussion, basically do have the PKI assumption, right? At any given time.
00:25:45.770 - 00:26:01.562, Speaker A: The public keys of all of the participants, all of the validators. They're publicly visible in some staking contract that's stored on the blockchain, which means nodes can sign their messages like sign their block proposals. And those signatures can be verified by everybody else who knows the public keys of all of the validators.
00:26:01.562 - 00:26:22.390, Speaker A: And so that then actually makes simulation costly once again. So it's true that in a proof of stake setting, one node can sort of simulate all of the computations done by four honest nodes. What it can't do is forge the signatures that those honest nodes would be attaching to their messages, right? Because this Byzantine node doesn't know the private key that those four honest nodes have.
00:26:22.390 - 00:26:56.870, Speaker A: So the hexagon argument breaks down in the proof of stake setting for the exact same reasons that it breaks down in the permissioned plus PKI setting. So while this sort of costless simulation doesn't actually trip us up here because we're still saved by sort of the unforgivability of secured digital signatures, we will see later that costless simulation does provide some problems for proof of stake chain, specifically in the long range attacks discussion that are going to be coming up in part four. The second comment concerns sort of a rare thing which is actually simpler with proof of stake blockchains and proof of work blockchains.
00:26:56.870 - 00:27:16.150, Speaker A: And actually this comment is actually not specific to longest chains. This will apply also to the proof of stake BFT type protocols that we discussed earlier in part three. So specifically, one somewhat messy component of Nakamoto Consensus, namely a difficulty adjustment mechanism is actually not necessary for proof of stake blockchain protocols.
00:27:16.150 - 00:27:32.666, Speaker A: So in Nakamoto Consensus, the whole difficulty was the unobservability of hash rate. So you always have sort of the difficulty parameter. In that context we're calling a tau that was tuned so that given the historical recent hash rates, you'd be targeting a block rate of once per ten minutes.
00:27:32.666 - 00:27:53.806, Speaker A: If the hash rate changes, you're going to then notice that by blocks being added to the longest chain either too quickly or too slowly, which will cause you to then modify the protocol to modify the difficulty threshold accordingly. Now, here we still definitely care about the total amount of stake in the staking contract. That's important, just like the total amount of hash rate is important in Nakamoto Consensus.
00:27:53.806 - 00:28:06.760, Speaker A: But in proof of stake, that number, that total stake that is observable, that's just the sum of the QIS that are in the staking contract. That can change. But the protocol is well aware of when it changes, the instant that it changes.
00:28:06.760 - 00:28:37.886, Speaker A: So for that reason there's no need to sort of monitor kind of the speed of block production, you just tune your difficulty parameter in response to whatever the current total amount of stake is in the staking contract. So a consequence simplification then is that with proof of stake longest chain, longest chain can really just mean longest chain really just the chain with the largest number of blocks. If you remember back when we talked about Nakamoto Consensus in lecture number nine, for a while we talked about it as if the longest chain should just be the one with the largest number of blocks.
00:28:37.886 - 00:29:02.490, Speaker A: But then once we talked through difficulty adjustment and we allowed the possibility of sort of very sudden changes in hash rate, we realized actually it's important that you redefine longest chain as the chain with the largest amount of work that's been put into it. And so because we don't have difficulty adjustment, or rather we have kind of automatic sort of instantaneous in protocol difficulty adjustment, if you like. So because of that we can actually just work with the sort of unweighted longest chain.
00:29:02.490 - 00:29:40.790, Speaker A: The final thing I want to do is just compare and contrast kind of the approach to leader selection in proof of work versus in proof of stake, right? So in proof of work and noncommittal consensus, the leader of a round is determined by having the output of a cryptographic hash function on a particular input be sufficiently close to zero. Whereas here in the proof of stake version we're defining it as someone's Credential, which is the output of a VRF on some input being sufficiently close to zero. So let's drill down on the differences between those two and in particular the difference in what fields get passed in as input to either the cryptographic hash function, as in the case of proof of work, or the VRF, as in the case of proof of stake.
00:29:40.790 - 00:29:50.760, Speaker A: So again, in proof of work, you're the leader if the output of a cryptographic hash function is sufficiently small. So let's let little h denote that hash function. For example, shot 256.
00:29:50.760 - 00:30:03.974, Speaker A: Now the hash of what so what is the input to this hash function? I'm just going to copy down exactly how we described it back in lecture number nine. So there's going to be four fields. There's going to be the block that's being proposed.
00:30:03.974 - 00:30:16.618, Speaker A: There's the sort of explicit predecessor pointer. So there's a naming of a previous block that this current block is extending. There's the public key, presumably of the block proposer, and finally there is a nonce.
00:30:16.618 - 00:30:30.070, Speaker A: As usual, I'm going to ignore the implementation details about how these fields are actually represented. So like, what does it mean to pass a block to a hash function? Maybe you're passing the block's metadata instead. Maybe you're passing a sort of cryptographic commitment to the block, whatever.
00:30:30.070 - 00:30:43.420, Speaker A: Just in some form you're passing the block to the hash function. Similarly, in some form you're passing sort of a naming of a previous block in that second field, the fourth field knots. Remember, that means number used once.
00:30:43.420 - 00:31:06.410, Speaker A: So if you fix a value for these four fields, so the block and the predecessor and the public key and the nonce for a fixed value of those four fields that is a lottery ticket. By varying the values of those fields, you get different sort of outputs of the cryptographic hash function. If we're willing to make the random Oracle assumption about this cryptographic hash function, those are basically independent lottery tickets.
00:31:06.410 - 00:31:26.326, Speaker A: So if you're trying to propose blocks like you want to earn your block reward, for example, you're motivated to generate as many lottery tickets as you can, generate as many different inputs as you can to try to find one that's winning, one that has a sufficiently small hash. That of course, is the entire point of proof of work. That is why everyone running it is so busy evaluating shot 256.
00:31:26.326 - 00:31:45.162, Speaker A: They're just trying all these different as many possibilities as they can for these four fields. Given that that grinding is happening by design, why not? The point of this nonce is just to make the grinding as straightforward as possible. Let's give people enough degrees of freedom that they can really just keep the block, the predecessor and the public key fixed.
00:31:45.162 - 00:32:00.418, Speaker A: Don't bother to grind on those and just grind over nonces until you find one that gives you a sufficiently small hash. Now, in proof of stake, we do not want validators to be grinding over lots of different inputs to their VRF. That would basically get us back into the proof of work world.
00:32:00.418 - 00:32:18.386, Speaker A: We're trying to build a proof of stake protocol, one of the benefits of which is going to be much smaller energy consumption, much less work for the validators than you have in Nakamoto Consensus. So viewed in that light, let's just remember how leaders are selected in the current proposal for a proof of stake blockchain protocol. Basically you're a leader.
00:32:18.386 - 00:32:36.270, Speaker A: If your Credential is sufficiently close to zero, what's your Credential? That's the output of a verifiable random function. If you're the owner of some public key, you should be evaluating that VRF with a corresponding private key on some input. And the input we're currently thinking about is just the time step combined with a random or pseudorandom seed.
00:32:36.270 - 00:32:55.442, Speaker A: And so if in proof of work the goal is to sort of just allow participants to grind away to their heart's content, in proof of stake, our goal is the opposite. We would like ideally no grinding whatsoever. So what kind of grinding attacks might we be worried about? Well, we might be worried about grinding on the private key.
00:32:55.442 - 00:33:05.250, Speaker A: Right. Obviously it's basically costless to generate as many public key private key pairs that you want. You could imagine someone trying to generate a whole bunch of them looking for a private key that gives them favorable VRF outputs.
00:33:05.250 - 00:33:35.502, Speaker A: And again, as usual, if we assume there's some sort of suitable warm up period so that basically everybody must commit to their public key private key pairs before they're aware of the VRF inputs that they're going to be evaluating, like before they're aware of the seeds, the R sub T's, then you can't productively grind on key pairs at all. So what about the input to the VRF? Well, this is actually sort of going to depend. So remember, we're currently, for simplicity, thinking about the R sub T's as being the outputs of some ideal random beacon.
00:33:35.502 - 00:33:49.174, Speaker A: So literally just randomness that falls from the sky at each time step that nobody can predict. Under that assumption, then nobody can manipulate the input to the VRF at all. Right? The time just is whatever time it is, r sub T just is.
00:33:49.174 - 00:34:18.926, Speaker A: Whatever the ideal randomness beacon happened to give you, you have no choice. All right? So that's under the sort of unrealistically strong assumption of access to an ideal randomness beacon, in practice that RCT is usually going to be generated pseudo randomly in some way, perhaps, for example, based on aspects of the current blockchain state. And as we've seen, when you have pseudorandum seeds like that, then potentially validators are going to be able to manipulate them via manipulation of the blockchain state.
00:34:18.926 - 00:34:43.942, Speaker A: So obviously, to the extent that validators can manipulate R sub T, they're able to manipulate the input to the VRF. So in the next video, our last part three video, the last video on proof of stake longest chain protocols, we're going to talk all about all the things that kind of go haywire in a concrete implementation where you need to define R sub T in some sort of pseudorandom way. So I'm not going to talk no more about it now.
00:34:43.942 - 00:34:56.586, Speaker A: Lots more on that in the next video. Now, one can't help but notice that the Credential in the proof of stake design depends on many fewer things than the cryptographic hash function output. In the proof of work design.
00:34:56.586 - 00:35:19.298, Speaker A: In particular, the Credential is independent of whatever block is being proposed, independent of whatever block is named as a predecessor, and independent of any kind of nonce. And again, the reason of course being that if the Credential did depend on any of these, these would be opportunities for grinding, which would get us more or less back to the proof of work world. All right, a few comments on these.
00:35:19.298 - 00:35:30.046, Speaker A: So first of all, obviously we're not going to have a nonce in the proof of stake design. That would be stupid. The only role that the nonce played in the proof of work design was to make it very straightforward for people to grind.
00:35:30.046 - 00:35:56.830, Speaker A: If we don't want people to grind, we're obviously not going to have a nonce. Let's move on to the more interesting case of the block B, which again is part of the input in the proof of work design and not part of the input in the proof of stake design. Put differently, that means in the proof of work design and Nakamoto Consensus, each lottery ticket comes with it a specific block, whereas each lottery ticket in the proof of stake design does not mention a block in the proof of work design.
00:35:56.830 - 00:36:08.398, Speaker A: If your lottery ticket is a winner, well, then you're already committed to what block you're proposing. It's just whatever block is part of the lottery ticket. In the proof of stake design, you are not committed to any given block.
00:36:08.398 - 00:36:26.626, Speaker A: You get to wait and see whether or not your lottery ticket wins, whether or not your credential is sufficiently small. And then at that point, knowing that you're the winner, you then get to choose whatever block you want, whatever transactions you want. So that's a quite subtle, but also quite important distinction.
00:36:26.626 - 00:36:48.158, Speaker A: And while it's hard to imagine why you'd ever want a nonce in a proof of stake design, you could imagine maybe wanting the block. In a proof of stake design, maybe you would want lottery tickets to include as part of them a commitment to a single block like we're used to for proof of work. Of course we can't do that because then validators would be incentivized to grind over the different choices of the block, getting us back to proof of work.
00:36:48.158 - 00:37:05.266, Speaker A: So avoiding grinding forces us to remove the block from the input when we're defining credentials in the proof of stake design. And the fact that we've removed the block creates two downstream complications. The first complication we've sort of seen many times.
00:37:05.266 - 00:37:21.330, Speaker A: So in Nakamoto Consensus, because the winning lottery ticket includes a commitment to a block, that means in particular a Byzantine node. When they win the lottery, when they're leader of a round, they can only make one block proposal whichever one is embedded in their winning lottery ticket. In proof of stake.
00:37:21.330 - 00:37:47.650, Speaker A: Meanwhile, because you win the lottery first and then get to choose whatever block you want, that frees you up to in particular, propose multiple blocks all based on the same one winning lottery ticket. Propose multiple blocks and send, for example, conflicting blocks to different honest nodes. So the fact that Byzantine nodes are more powerful in the proof of stake design than in the proof of work design, that's exactly stemming from the fact that we've been forced to remove the block from the input.
00:37:47.650 - 00:38:09.890, Speaker A: The second complication is going to concern interactions with the seed R sub T when that seed is chosen pseudo randomly. So we'll talk more about that in the next video. The story is similar for the remaining field, the predecessor field, right? In the proof of work design, a lottery ticket includes encoded in it a commitment to a choice of a predecessor.
00:38:09.890 - 00:38:29.914, Speaker A: So if you win the lottery, there's no more decisions to be made. You know not just what block you're proposing, but you also know which of the previous blocks you're going to be extending proof of stake because predecessor is not part of the input. Why is it not part of the input? Because then validators would be incentivized to grind over all possible choices of the predecessor because the predecessor is not part of the input.
00:38:29.914 - 00:38:51.542, Speaker A: That means that you get to choose your predecessor after you find out that you've won the lottery. And this is exactly why we have issue number two that's listed on the left part of the slide and why we can have pictures like the orange and light blue picture at the bottom of the slide. Because a leader gets to choose its predecessor block after the fact.
00:38:51.542 - 00:39:24.298, Speaker A: After it's won, it can go ahead and just broadcast block proposals extending all the different blocks in the current blockchain. Now, one super subtle point that we'll take up in the next video is that while it would seem that this credential is independent of a choice of predecessor, it would seem to depend only on the private key. The time step and the seed r sub t we'll see in the next video that actually when you're generating r sub t pseudorandomly r sub t itself can kind of depend on your choice of predecessor.
00:39:24.298 - 00:39:40.838, Speaker A: So we will be getting implicit dependence on predecessor choices once we make Rsubt pseudorandum in the next video. So the final sort of subtle detail, fortunately, is very easy to fix. So think about what a block proposal is going to look like in this proof of stake design.
00:39:40.838 - 00:40:01.914, Speaker A: Well, presumably you're going to be broadcasting your credential, right? That's what sort of lets other people know that you are in fact a leader of the current timestep. You do have the privilege of proposing a block. And then because your credential doesn't make reference to any block or to any predecessor, you have to separately, in addition to their credential, tell you the block you have, tell them the block you have in mind and tell them the predecessor you have in mind.
00:40:01.914 - 00:40:30.790, Speaker A: And the one thing you just want to be careful for is it shouldn't be possible for other Validators to be able to hijack your winning lottery ticket. They shouldn't be able to just sort of replay your credential and attach to it their own choice of a block and of a predecessor. So you don't just broadcast your credential to everybody, you don't just broadcast a block and a predecessor to everybody, but also you sign your proposed block and predecessor with, of course, the same private key that you used to evaluate your verifiable random function and prove that you are indeed a leader.
00:40:30.790 - 00:40:53.510, Speaker A: Conversely, if you're listening to block proposals being made to you by other Validators, there's a few checks that you need to make before you take those proposals seriously and regard them as legitimate. So first of all, you need to check people's credentials, right? So when someone makes a block proposal, they say, here's my credential, look how small it is. So you have to make sure they computed that correctly.
00:40:53.510 - 00:41:13.022, Speaker A: You should know their public key, they should tell you their public key, they should tell you the alleged VRF output. And just by the efficient verifiability property of verifiable random functions, you can check that Credential was indeed computed correctly. Secondly, whether or not this validator actually is a leader of the current timestep depends not just on the Credential, but on the threshold.
00:41:13.022 - 00:41:21.558, Speaker A: What you're comparing the Credential to, and that threshold depends on the difficulty parameter mu. Everybody knows what that is. That's just maintained by the protocol at all times.
00:41:21.558 - 00:41:30.294, Speaker A: And you also have to know the stake amount for the proposing validator. But again, stake amounts are associated with public keys. They're right there in the Staking contract for everybody to see.
00:41:30.294 - 00:41:42.220, Speaker A: So you can also compute the threshold and then if the Credential is less than that threshold, you're like, AHA, okay, cool. This person actually is a leader of this time step. I should be looking for a block proposal from them.
00:41:42.220 - 00:41:59.060, Speaker A: And finally, along with a Credential, you should be expecting a signed block and a signed predecessor. And you obviously want to check that those signatures are correct. So you're going to be verifying that with the exact same public key you use to, for example, verify the output of the verifiable random function.
00:41:59.060 - 00:42:31.374, Speaker A: So those are the three issues and the three comments around our proposed proof of stake longest chain protocol that I wanted to cover on this slide in this video. We still have one more thing to do. It is actually a fairly difficult thing to do, but important, which is to examine the additional complications that come up specifically really in longest chain protocols when you actually discard the unreasonable assumption that the R sub T's are generated by an ideal randomness beacon and assume instead that RTS are generated pseudorandomy based in some way blockchain state.
00:42:31.374 - 00:42:36.410, Speaker A: So that's coming up next, the last video of part three. I will see you there. Bye.
