00:00:00.490 - 00:00:11.598, Speaker A: Hi everyone and welcome to this video that accompanies section 22.3 of the book Algorithms Illuminated, part Four. This is a section about the big picture of what chapter 22 is going to look like.
00:00:11.598 - 00:00:24.206, Speaker A: So, as we've mentioned, there's going to be 19 problems that we're discussing. All NP hard, all proved NP hard via reductions from the three Sat problem. So there's 19 problems and 18 reductions to keep track of.
00:00:24.206 - 00:00:36.882, Speaker A: Let's get organized. First, a quick sanity check. As algorithm designers, we're accustomed to designing reductions for the honorable purpose of spreading computational tractability.
00:00:36.882 - 00:00:50.070, Speaker A: Whereas at nt hardness we're using reductions to spread computational intractability, which goes in the opposite direction. And it's really easy to mix up those directions. So let's have a quiz to sort know, hone our understanding.
00:00:50.070 - 00:01:31.024, Speaker A: If you watch the video about mixed integer programming or MIP solvers, we showed that the Napsack problem is easily formulated as a MIP. So in other words, the Napsack problem reduces to the mixed integer programming problem. So what does that then imply? Okay, so the correct answer is are the second and third ones B and.
00:01:31.062 - 00:01:33.664, Speaker B: C. The easiest way to see why.
00:01:33.702 - 00:01:50.804, Speaker A: Is just to remember the cartoon we had about a problem A reducing to problem B and the directions that tractability and intractability spread. Suppose a problem A reduces to a problem B. If we're talking about spreading tractability, then remember that goes in the opposite direction of the reduction.
00:01:50.804 - 00:02:03.676, Speaker A: So if problem A reduces to B, then tractability of B implies tractability for A, right? Given a polynomial time algorithm for problem B, you would just compose that with your reduction from A to B to get yourself a polynomial time algorithm for.
00:02:03.698 - 00:02:06.856, Speaker B: A, showing that A is tractable, whereas.
00:02:06.968 - 00:02:27.940, Speaker A: Intractability spreads in the opposite direction of tractability that is in the same direction of the reduction. Remember, the two step recipe for proving a problem B is NP hard is you take an NP hard problem A and then reduce A to B and that spreads the intractability from A to B. So for us in this quiz, we have the Napsack problem reducing to the MIP problem.
00:02:27.940 - 00:02:43.876, Speaker A: So the Napsack problem is our problem A, the MIP problem is our problem B. So computational intractability goes in the same direction from Napsack to MIP. And that is exactly what answer B says, whereas answer A has the spread of intractability in the wrong direction.
00:02:43.876 - 00:03:09.040, Speaker A: So B is correct because the intractability will spread from the problem A to the problem B MIP. And then for the same reason, answer C is also correct because computational tractability spreads in the opposite direction. So given a reasonably good algorithm for MIP, you get a reasonably good algorithm for Napsack just by composing the reduction from Napsack to MIP with the assumed reasonably good algorithm for solving MIPS.
00:03:09.040 - 00:03:18.900, Speaker A: With that sanity check on your understanding out of the way, we can proceed to talking about the 18 reductions by which we will generate 18 new MP hard problems.
00:03:18.970 - 00:03:21.350, Speaker B: In addition to the three set problem.
00:03:22.760 - 00:03:43.832, Speaker A: In this cartoon, we're going to have 19 Blobs corresponding to 19 computational problems, including all of the ones that we've discussed thus far in this video playlist. We're going to have an arrow from one problem to the other if there's a reduction in that same direction. So an arrow from A problem A to A problem B if A reduces to B, and intractability is going to spread in that exact same direction.
00:03:43.832 - 00:03:47.308, Speaker A: So the mother of all NB hard problems, the Cook Levin Theorem, we're going.
00:03:47.314 - 00:03:49.180, Speaker B: To start from threesat.
00:03:49.920 - 00:04:05.116, Speaker A: Now we're going to have a tree directed outward from threesat. So the NB hardness will spread from threesat through this directed out tree to the other 18 problems. Let me now just go ahead and draw the rest of this directed out tree.
00:04:05.116 - 00:04:58.318, Speaker A: Then we'll discuss which of these reductions we've already seen or are easy, and then which reductions are going to be the hard ones occupying the next four videos. That's a bit overwhelming, probably. It's a lot of problems.
00:04:58.318 - 00:05:10.600, Speaker A: You will notice almost all of these problems we have discussed in the past. I'll tell you about the other ones in a second. It's also a lot of reductions, 18 reductions, but actually some of those we've seen in the past also, as we'll discuss.
00:05:10.600 - 00:05:26.246, Speaker A: Let's start with the one truly trivial reduction. You'll see, there's an arrow from the three sat problem to the sat problem saying the three sat reduces to sat. And that's completely trivial, right? Because three sat is literally a special case of sat.
00:05:26.246 - 00:05:40.510, Speaker A: It's the special case of the instances where each constraint happens to be a disjunction of at most three literals. So certainly any subroutine for sat automatically solves threesat. One of these reductions, while not trivial, is something we've actually already seen.
00:05:40.510 - 00:05:56.142, Speaker A: We saw it back in the opening sequence of videos corresponding to Chapter 19. And that's the reduction from the directed Hamiltonian path problem to the cycle free shortest paths problem when you can have edges with negative lengths. So back then we were talking about using reductions to spread intractability.
00:05:56.142 - 00:06:16.310, Speaker A: We introduced the two step recipe and I just wanted to give you a quick glimpse of what that two step recipe looks like in action. And the particular case study we used way back in Chapter 19 is we took on faith that the directed Hamiltonian path problem is NP hard, something we'll prove in this chapter. And then we showed that that problem reduces reasonably easily to the problem of computing cycle free shortest paths.
00:06:16.310 - 00:06:39.902, Speaker A: And the reason that was interesting is it sort of explained the limitations that we had on the shortest path algorithms that we were discussing in an earlier book of this series. So like the Bellman Ford algorithm and the Floyd Warshall algorithm, they're guaranteed to compute correct shortest path distances only in the special case of graphs that have no negative cycle. And then finally, from this reduction from directed Hamiltonian path, we finally understood why.
00:06:39.902 - 00:06:46.386, Speaker A: Because if the Bellman Ford or Floyd Warshall algorithms were correct, more generally, even when graphs did have negative cycles, that.
00:06:46.408 - 00:06:49.170, Speaker B: Would actually refute the p zero equal to NP conjecture.
00:06:49.590 - 00:07:06.058, Speaker A: So that leaves us with 16 of the 18 reductions to understand. Now, of those, 1611 of them are actually quite easy and all show up in the book as end of the chapter exercises. So let me just enumerate those eleven relatively easy reductions now and I'll give you some hints along the way of.
00:07:06.064 - 00:07:08.060, Speaker B: What those reductions actually look like.
00:07:08.430 - 00:07:18.378, Speaker A: For example, one pretty easy one is the reduction from the independent set problem to the clique problem. So let me remind you what these two problems are. They're both problems about undirected graphs.
00:07:18.378 - 00:07:33.554, Speaker A: So an independent set of a graph, that's a subset of vertices that are all non adjacent. So there's no edge that has both endpoints in that subset and a clique is exactly the opposite, that's a subset of mutually adjacent vertices. Every edge with both endpoints in that.
00:07:33.592 - 00:07:35.854, Speaker B: Set is in fact present in the graph.
00:07:35.982 - 00:07:53.526, Speaker A: In the independent set problem, we're looking for the maximum size of an independent set. In the maximum clique problem, we're looking for a clique of maximum possible size. If someone hands you an efficient subroutine for the clique problem on a silver platter, it's very easy to extract from that an efficient algorithm for the independent set problem.
00:07:53.526 - 00:08:08.678, Speaker A: If I give you an instance of independent set, I say find me the biggest independent set you can find. What do you do? You toggle which edges are in and which edges are out of the graph. So you take the complement edge set of the graph that I gave you that turns all the independent sets into cliques.
00:08:08.678 - 00:08:16.926, Speaker A: Now you run the assumed subroutine for computing a maximum size clique, it hands it back to you and now you just toggle those edges back and you get a maximum size independent set of.
00:08:16.948 - 00:08:18.400, Speaker B: The graph that you started with.
00:08:18.770 - 00:08:39.266, Speaker A: Something very similar is going on in the reduction from the independent set problem to the vertex cover problem. So what's the vertex cover problem? So now you want to find a vertex cover, which means you want to find a subset of vertices that includes at least one endpoint of each edge. So you want it to be the case that there's no edge both of whose endpoints are omitted from your subset.
00:08:39.266 - 00:09:01.402, Speaker A: And then of course all the vertices would be a vertex cover that would cover both endpoints of all the edges. So you want a minimum size vertex cover, the smallest subset of vertices, such that every edge has at least one endpoint covered. And so here the trick is to realize that in any graph, if you take any independent set, the complement of the independent set is a vertex cover and vice versa.
00:09:01.402 - 00:09:17.730, Speaker A: So this means that if you want to find the maximum size independent set of a graph, and I give you a subroutine for computing the minimum size vertex cover, all you need to do is invoke the subroutine for finding the smallest vertex cover, you take the complement. So you take all the vertices that are not in the smallest vertex cover.
00:09:17.800 - 00:09:20.900, Speaker B: And that's going to be a largest possible independent set.
00:09:21.350 - 00:09:34.850, Speaker A: Let's go ahead and follow the rest of the arrows down that directed path. So next we have that the vertex cover problem reduces to the set cover problem. What's the set cover problem? Well, the input looks a lot like it did in the maximum coverage problem, so there's a ground set of elements.
00:09:34.850 - 00:09:42.854, Speaker A: I give you a collection of subsets of that ground set. But rather than having a budget on how many subsets you can pick, I give you the requirements that your subsets.
00:09:42.902 - 00:09:44.954, Speaker B: Have to cover everything, have to cover.
00:09:44.992 - 00:09:51.150, Speaker A: The whole ground set. And then your job is to cover the whole ground set with as few of the subsets as possible.
00:09:51.300 - 00:09:53.006, Speaker B: That's the set cover problem.
00:09:53.188 - 00:10:18.786, Speaker A: Turns out the vertex cover problem is simply a special case of the set cover problem. So given an efficient subroutine for set cover, it immediately gives you an efficient subroutine for vertex cover. Why is vertex cover a special case of set cover? Well, just think of the ground set as being the edges of the graph and you're going to have one subset for each vertex of the graph, and the elements of that subset are going to be the edges incident to that vertex.
00:10:18.786 - 00:10:28.566, Speaker A: So in effect, it's almost like subsets correspond to stars in the graph that you're given. And if you think about a little bit you'll realize that set covers of that set system are exactly correspond to.
00:10:28.588 - 00:10:31.350, Speaker B: The vertex covers of the original graph.
00:10:31.850 - 00:10:42.622, Speaker A: Next, the set cover problem reduces to the maximum coverage problems. Remember, these two problems have almost the same input. The difference is that in maximum coverage I give you a budget on how many subsets you can pick and you want to cover as much stuff as possible.
00:10:42.622 - 00:10:45.374, Speaker A: In set cover I give you a requirement that you have to cover everything.
00:10:45.412 - 00:10:47.520, Speaker B: And you want to pick as few sets as possible.
00:10:47.890 - 00:11:05.414, Speaker A: So if you were given a subroutine for the maximum coverage problem, how would you use it to solve the set cover problem? Well, you'd take your maximum coverage subroutine and you would ask it for the maximum amount that can be covered with one set. Then you'd ask it again for the maximum amount that could be covered by two sets and then again by three.
00:11:05.452 - 00:11:06.678, Speaker B: Sets, and so on.
00:11:06.764 - 00:11:29.230, Speaker A: And it would be handing you back the maximum coverage that could be achieved with that number of subsets. At some point, once your budget k is high enough, then the maximum coverage subroutine will actually be able to cover everything. And so the first value of k, like if k equals 17, was the very first time that the maximum coverage algorithm was able to cover the entire ground set, then you know that that's the answer for your set cover problem.
00:11:29.230 - 00:11:33.594, Speaker A: Those 17 subsets are a minimum size set cover in the set cover instance.
00:11:33.642 - 00:11:35.120, Speaker B: That you started with.
00:11:35.570 - 00:11:53.502, Speaker A: Finally along this path, and this is something we mentioned back when we were discussing influence maximization, is that the maximum coverage problem is basically a special case of influence maximization. That's why it was so cool. We were getting just as good an approximate correctness guarantee for the more general problem, that one minus quantity, one minus one over k raised to the K.
00:11:53.656 - 00:11:54.726, Speaker B: Let me give you kind of a.
00:11:54.748 - 00:12:08.002, Speaker A: Quick hint of why, how to view maximum coverage problem as a special case of influence maximization. I'll leave it to you to think through the details. So suppose we're given an instance of maximum coverage, right? So we have our ground set and we have our subsets.
00:12:08.002 - 00:12:11.170, Speaker A: We need to somehow view this as an instance of influence maximization.
00:12:11.250 - 00:12:13.350, Speaker B: So in particular we somehow need a graph.
00:12:13.730 - 00:12:29.454, Speaker A: So what we're going to do is we're going to have two rows of vertices. In the top row we'll have one vertex for each of the subsets that we were given in the maximum coverage instance. In the bottom row, we're going to have one vertex for each of the elements of the ground set that we were given in this maximum coverage instance.
00:12:29.454 - 00:12:41.550, Speaker A: Those are going to be the vertices for the edges. You're going to have a directed edge from each vertex corresponding to a subset to each of the vertices corresponding to the elements that it contains.
00:12:41.710 - 00:12:43.250, Speaker B: That's going to be the graph.
00:12:43.410 - 00:13:01.738, Speaker A: The last thing the influence maximization subroutine is going to be expecting is an activation probability. We're just going to set that equal to one. Think this through and what you'll find is that the influence maximizing subsets in this graph, they're going to be subsets of K vertices in the top row that activate as many vertices in the bottom row as possible.
00:13:01.738 - 00:13:15.566, Speaker A: And that's going to correspond exactly to the coverage of those corresponding K subsets. Moving on to a different part of the graph, let me just talk briefly about the traveling salesman problem. Let's not worry right now about why the traveling salesman problem is NP hard.
00:13:15.566 - 00:13:24.546, Speaker A: Let's worry about the two edges going out of the Tsp. So the reductions from Tsp to two other problems. So let's start with the traveling salesman path problem.
00:13:24.546 - 00:13:42.646, Speaker A: So this is just the slight variant of the Tsp where instead of a tour that loops back on itself, you want a path, but still a path that's cycle free and visiting every vertex. So this is going to be a path where if there's N vertices, there'll be exactly N minus one edges visiting each of the vertices exactly once and in the path problem, you want a.
00:13:42.668 - 00:13:44.966, Speaker B: Minimum cost such path.
00:13:45.158 - 00:14:16.306, Speaker A: A reduction from the Tsp to this traveling sales and path problem is then just a way of using an efficient subroutine for computing the best path and extracting from that subroutine the best tor in the original instance. And this is not hard to do, I'll leave it to you to think through the details. But basically it's possible to do a little bit of minor surgery on the TSB instance that you start with, so that if you feed this sort of slightly modified version of the graph to a subroutine for computing a traveling salesman path, you're actually going to trick that subroutine into computing for you.
00:14:16.306 - 00:14:30.626, Speaker A: A tour in the original graph that you started with, Again, I'll leave that as an exercise for you to think through. The exact same idea works for the minimum cost Kpath problem that we discussed at length. So the problem of given a graph with edge costs, compute the K path.
00:14:30.626 - 00:14:37.226, Speaker A: So a path with k minus one edges, k distinct vertices, no cycles, the minimum cost K path exact same idea.
00:14:37.328 - 00:14:39.820, Speaker B: Will prove that that problem is NP hard as well.
00:14:40.190 - 00:14:56.110, Speaker A: Next, let's talk about the Hamiltonian path problem. So because some of the graph problems we care about are most naturally directed graphs like suckle free shortest paths, whereas for others we're focusing on undirected graphs. Like in the Tsp, it's useful to have two versions of the Hamiltonian path problem.
00:14:56.110 - 00:15:25.814, Speaker A: One for directed graphs, again, where you're looking for a simple path that visits every single vertex exactly once, or the same version, the same problem in undirected graphs. And as you'd hope these two problems are almost the same, there are very easy reductions in both directions. So for example, if you start from an undirected graph and I give you a subroutine for the directed case, you can transform your undirected graph into a directed one just by replacing each undirected edge with a directed edge going in either direction.
00:15:25.814 - 00:15:46.998, Speaker A: If you think about it, if you compute a directed Hamiltonian path after you've bi directed all the edges, it's easy to extract from that an undirected Hamiltonian path from the graph that you started with. On the other hand, if you start from a directed Hamiltonian path instance so a directed graph, there is a way, it's a little more complicated, but not much. There's a way to turn a directed.
00:15:47.034 - 00:15:50.146, Speaker B: Graph into an undirected graph so that.
00:15:50.248 - 00:16:14.710, Speaker A: After you compute an undirected Hamiltonian path in the undirected graph that you've produced, you can extract from it a directed Hamiltonian path in the graph that you started with. So the upshot is, again, just using a very minor surgery on the graph, there are reductions either direction between the undirected or directed Hamiltonian path problems. Let's now zip over to the far left of the slide and talk about the two problems that we discuss.
00:16:14.710 - 00:16:34.142, Speaker A: Semi reliable solvers for the sat problem and mixed integer programs. So when you have a sort of feasibility problem like graph coloring or whatever, usually Sat is the most natural encoding of it. But whenever you have an encoding using Satisfiability, if you wanted, you could have an encoding using mixed integer programming as well.
00:16:34.142 - 00:16:51.842, Speaker A: Now, mixed integer programming is about optimization, so you would expect there to be an objective function. But if all you're trying to do is encode an instance of Sat, you don't need an objective function, you just have sort of a placeholder objective function like maximize zero. Then all you need to do is translate the logical constraints in the given Sat instance.
00:16:51.842 - 00:17:16.486, Speaker A: So a disjunction of literals into arithmetic constraints into inequalities. And as you'd probably expect in your integer program, you're going to have one random one decision variable for each Boolean variable in the Sat instance where the value one corresponds to true and the value zero corresponds to false. And then if you think about it, each of the disjunctions of literals is quite easily translated into an inequality.
00:17:16.486 - 00:17:20.094, Speaker A: Saying something like x one plus x two plus x three is at least.
00:17:20.132 - 00:17:21.950, Speaker B: Equal to some constant.
00:17:22.450 - 00:17:36.430, Speaker A: There's other ways you could also argue that mixed integer programming is NP hard. So for example, as we saw it includes Napsack as a special case and eventually we'll be proving that Napsack is NP hard. It's very easy to encode the independent set problem as a mixed integer program, et cetera.
00:17:36.430 - 00:17:41.350, Speaker A: But I chose one particular proof of NP hardness of mixed integer programming here.
00:17:41.420 - 00:17:43.560, Speaker B: Going via the Satisfiability problem.
00:17:44.090 - 00:18:03.306, Speaker A: The last of the easy reductions concern a problem we haven't discussed yet, the subset sum problem. So in the subset sum problem I give you n positive integers, so 17,230, 419 million, blah blah blah blah. I give you n positive integers and I also give you a target 21,325,173.
00:18:03.306 - 00:18:18.174, Speaker A: And the question is, is there a subset of the n numbers whose sum is exactly equal to that target, yes or no? So it's not an optimization problem, it's just feasibility. Or is there not a subset with.
00:18:18.212 - 00:18:20.430, Speaker B: Exactly the target sum?
00:18:20.770 - 00:18:35.922, Speaker A: We will be proving that the subset sum problem is NP hard. But here I just want to record the fact that if subset sum is NP hard, it will immediately imply NP hardness of two problems. We have discussed at length the Napsack problem and make spin minimization.
00:18:35.922 - 00:18:56.750, Speaker A: It turns out subset sum is basically a special case of both of these two problems. So if subset sum is NP hard, certainly so are the more general problems, which we actually care quite a bit about. Subset sum is basically the special case of knapsack where the item sizes are the same as the item values and it roughly corresponds to the subset of makespan minimization when you have only two machines.
00:18:56.750 - 00:19:08.110, Speaker A: So those are the eleven kind of easy exercise reductions that I wanted to mention. That leaves us with five reductions. Four of those we will be covering in the next four videos.
00:19:08.110 - 00:19:14.498, Speaker A: The fifth one, even though it's not that easy, I'm still going to leave it as an exercise. And that is the proof that the.
00:19:14.504 - 00:19:16.580, Speaker B: Graph coloring problem is NP hard.
00:19:17.110 - 00:19:33.590, Speaker A: The difficulty of that reduction is roughly on par with the reductions that we're going to see in the forthcoming videos. I could have had another video with this reduction, but by the time we finish these four reductions, you're going to be sick of them. So you're going to be glad that I just sort of deferred this as an exercise for those of you interested.
00:19:33.660 - 00:19:35.770, Speaker B: To do in the privacy of your own homes.
00:19:36.270 - 00:19:59.120, Speaker A: That leaves us with four harder reductions. So we're going to start in the next video by reducing the three sat problem to the independent set problem. Once we have that reduction from three sat to independent set, at least assuming the Cook Levin theorem, assuming three sat is NP hard, at that point the hardness will flow all the way to the end of that path, including to the maximum coverage and influence maximization problems.
00:19:59.120 - 00:20:10.178, Speaker A: The next order of business will be to establish that the traveling salesman problem is NP heart. That's really the canonical NP heart problem. So I certainly want to show you why it is in fact NP heart.
00:20:10.178 - 00:20:20.950, Speaker A: So we have two steps to do there. The first one is going to be another reduction from three sat to a graph problem. This time not to independent set, but rather to the directed Hamiltonian path problem.
00:20:20.950 - 00:20:34.298, Speaker A: So that'll be in the second of the forthcoming videos. That reduction will establish NP hardness of the directed Hamiltonian path problem. Again, assuming the Cook Levin theorem, as we discussed, the directed and undirected Hamiltonian path problems are not that different.
00:20:34.298 - 00:20:47.342, Speaker A: So once we know the directed version is NP hard, we also know the undirected version is NP hard. And then there's actually a pretty easy reduction from the undirected Hamiltonian path problem to the traveling salesman problem. So we'll do that in the third of the forthcoming videos.
00:20:47.342 - 00:21:06.278, Speaker A: That'll be the easiest of the four. And that'll finally fulfill this promise that the traveling salesman problem is indeed an NP hard problem. That leaves us with one last reduction, which will be in the fourth of the forthcoming videos a reduction from the independent set problem, which again, will be proving NP hard in the next video to this subset sum problem.
00:21:06.278 - 00:21:24.986, Speaker A: Which again by virtue of it being a special case of the napsack and makespane minimization problems that will immediately establish that those two problems are NP hard. And what's interesting here is that napsack and the subset sum special case, those are problems. They don't involve these complicated objects like graphs or something like that.
00:21:24.986 - 00:21:42.850, Speaker A: The input to those problems is just a bunch of numbers, just N numbers. And in particular, we know we can solve the knapsack problem and therefore, the subset some special case in pseudopolynomial time using an algorithm that runs in time proportional to the magnitude of the input numbers. Now, that's not what we want.
00:21:42.850 - 00:22:04.940, Speaker A: We really want to run in time polynomial in the input size and the number of digits needed to represent the input numbers. And this NP hardness result is interesting because it explains why we can't get what we want. It explains why for the Napsack problem, we're really stuck with this running time that depends on magnitudes of the input numbers rather than on the number of digits in those numbers.
00:22:04.940 - 00:22:27.610, Speaker A: The next four videos will be filling in these gaps and presenting those proofs of those final four reductions. Before we do that, I kind of feel like I should level with you because NP hardness reductions, including some of the ones we're about to see, they can be kind of painfully messy. And to be honest, pretty much nobody remembers the details of any NP hardness proofs.
00:22:27.610 - 00:22:49.862, Speaker A: All that said, there's still some pretty good reasons for seeing a few of them. So let me just be explicit about what the goals are for going through these empty hardness reductions over the next four videos. Goal number one is to fulfill a bunch of promises that I owe you from previous videos in this playlist from earlier in the book, right? So problem after problem that we tackled, I kept saying, oh, bummer, this is MP hard.
00:22:49.862 - 00:22:56.490, Speaker A: Oh, man, this one's empty hard, too. We got to compromise on correctness or compromise on running time. We can't get what we want.
00:22:56.490 - 00:23:10.398, Speaker A: And I feel a little bit fraudulent if I didn't actually explain why those problems are MP hard and why we needed all those compromises that we went through in the videos corresponding to chapters 20 and 21. So that's the first reason we're going.
00:23:10.404 - 00:23:11.870, Speaker B: To go through these reductions.
00:23:12.530 - 00:23:33.190, Speaker A: The second goal is to give you some ammunition for carrying out the first step of the simple two step recipe for proving problems that are NP hard. Step one is you have to choose a known NP hard problem. So going through these reductions, that'll really solidify sort of our knowledge of a bunch of NP hard problems that you can then use in your own NP hardness reductions.
00:23:33.190 - 00:23:51.306, Speaker A: If 19 problems aren't enough for you, well, let me refer you to the classic book by Michael Gary and David Johnson, computers and Intractability a Guide to the Theory of NP Completeness. This book is all the way back in 1979. And don't forget the Cook Levin theorems around 1971.
00:23:51.306 - 00:23:59.054, Speaker A: And Carp demonstrated the power of NP hardness around 1972. So this is just later that same decade. So it's still sort of early days.
00:23:59.054 - 00:24:19.860, Speaker A: But already in 1979, there were hundreds of NP hard problems known. And that book includes a compendium of over 300 NP hard problems, which is a great place for brainstorming your own NP hardness reductions. Indeed, it's very hard to think of any computer science books from 1979 that remain as useful as Gary and Johnson to this day.
00:24:19.860 - 00:24:50.970, Speaker A: Finally, I don't expect you to really remember the details of any of these NP hardness reductions, say, a year from now, or, frankly, even like a week from now. But still, this exercise should be empowering to you, right? So if at some point in the future, your boss hands you a problem and says your raise for the year depends on whether you can prove that this is NP hard, by next week, you should feel like that is something you could do if you really needed to, right? They're messy, they're problem specific. But look, you're just about to watch a few 20 minutes videos with examples of reductions.
00:24:50.970 - 00:24:53.486, Speaker A: You're going to understand them. You're going to be like, yes, if.
00:24:53.508 - 00:24:55.680, Speaker B: I really had to do this, I could.
00:24:56.130 - 00:25:03.694, Speaker A: That's enough about the big picture of chapter 22. Let's dive into the reductions, starting with the independent set problem.
00:25:03.812 - 00:25:05.770, Speaker B: See you in the next video. Bye.
