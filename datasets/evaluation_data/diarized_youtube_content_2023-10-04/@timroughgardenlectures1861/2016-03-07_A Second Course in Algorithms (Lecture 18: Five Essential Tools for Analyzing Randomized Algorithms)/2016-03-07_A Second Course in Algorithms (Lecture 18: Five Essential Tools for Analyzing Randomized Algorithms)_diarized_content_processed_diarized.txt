00:00:00.490 - 00:00:21.038, Speaker A: Okay, so the purpose of today's lecture is to talk a bit about randomized algorithms. Now in your previous studies, specifically 109 and 161, you already learned some of the tricks of the trade of analyzing randomized algorithms. You probably saw applications to the running time of Quicksorts and perhaps some analysis of hashing as well.
00:00:21.038 - 00:00:30.822, Speaker A: Now there's also this class, 265, which is entirely about randomized algorithms. So sort of like more than you ever wanted to know about randomized algorithms. It's an awesome class, so I encourage you to take it.
00:00:30.822 - 00:00:51.002, Speaker A: But so in 261 we kind of have this one lecture which is sort of a bridge between the introductory stuff in 161 and 265. And specifically there's some tools which you don't learn in lower level classes, most specifically what we'll call the churnoff bounds, which get used all the time. And in particular, any future algorithms course that you'll take, you'll need them.
00:00:51.002 - 00:01:07.922, Speaker A: And also a lot of machine learning classes, you wind up needing to know some of these probabilistic tools as well. You've seen probability now in many previous courses, so just to kind of like remind you what the standard setup is. But again, hopefully your intuition is reasonably well developed for this stuff.
00:01:07.922 - 00:01:23.474, Speaker A: So we have a state space. So for us we're thinking like there's a randomized algorithm, it flips some coins, and a point in the state space is just the sequence of heads and tails that the algorithm happened to flip. Okay? So omega sort of captures the randomness, which you don't know opera a random variable.
00:01:23.474 - 00:01:33.770, Speaker A: That's just a real valued function defined on the state space. So again, for us we usually think about an algorithm. So say like for a fixed problem instance, we might be interested in the function of the running time.
00:01:33.770 - 00:02:11.830, Speaker A: So as a function of the coin flips made by the algorithm, what is the running time of the algorithm? Or as a function of the coin flips, what is the objective function value of the solution produced by the algorithm? Those are the kind of random variables that we care about in the analysis of algorithms. And of course, the first thing you kind of might want to know about a random variable is what is it on average? So a random variable will take on different values at different points in the state space, but you might want to say on average over everything that might happen, what's the average value? So that's just the expectation. So you just average the possible values that the random variable could take on weighted by whatever probability distribution you're assuming on the state space.
00:02:11.900 - 00:02:12.086, Speaker B: Okay?
00:02:12.108 - 00:02:30.714, Speaker A: So for example, if it's just uniform random coin flips, this is all just going to be with respect to the uniform distribution. And then finally, just to remind you about this key concept of independence, an event, remember, is just a subset of Omega. So an event just tells you whether sort of something happened or didn't happen.
00:02:30.714 - 00:02:51.454, Speaker A: So if you look at the probability of the intersection or conjunction of two events, if they're independent events, by definition they factor. This is the definition of what it means for two events to be independent probabilities factor. And then if you have two random variables which are independent, meaning that the events of them taking on any particular pair of values are independent events, then expectations factor.
00:02:51.454 - 00:03:03.046, Speaker A: So this is true for independent random variables. Notice that neither of these is true if they're not independent random variables. So like imagine you had an event e one and then e two was the complementary event.
00:03:03.046 - 00:03:13.126, Speaker A: So exactly one of e one or e two happens, but not both. Well then on this side you'd have a zero because they're complementary events. But on the right hand side you'd have a positive number times a positive number.
00:03:13.126 - 00:03:27.018, Speaker A: So the equality would fail. Similarly here, if you had x and y were the indicator random variables of complementary events, this side would be zero and this side would not would be positive. So it's certainly plenty of examples of things which are not independent.
00:03:27.018 - 00:03:51.826, Speaker A: We'll see examples today of both independent and non independent random variables. And so using sort of approximation algorithms as kind of the framing device, we're going to talk about five kind of really essential tools for the analysis of randomized algorithms. A couple of these you've seen before, so I'll treat them very quickly, but maybe at least one or two you haven't.
00:03:51.826 - 00:03:58.642, Speaker A: And so I'll spend more time on those. So here's number one, the first of our five essential tools. Linearity of expectations.
00:03:58.642 - 00:04:01.110, Speaker A: Super simple but super useful.
00:04:01.190 - 00:04:01.482, Speaker B: Okay?
00:04:01.536 - 00:04:24.734, Speaker A: So you've seen this before, let me remind you what it is. And then we'll put it to use in approximation algorithms in a second. So suppose you have n random variables all on the same state space and linear expectation just says you can take sums and expectations and interchange them to your heart's content.
00:04:24.852 - 00:04:25.134, Speaker B: Okay?
00:04:25.172 - 00:04:51.130, Speaker A: You can just like swap them back and forth and it's the same number. And so the point here is that the reason it's so powerful is there's no independence assumption, okay? Not necessarily independent. What's the statement? So the statement is the expected sum is equal to the sum of the expectations.
00:04:55.310 - 00:04:56.060, Speaker B: Okay?
00:04:57.630 - 00:05:15.054, Speaker A: So like when I teach TS 161, I give 20 lectures, I put a box around exactly one thing in the entire quarter and this is what I put a box around, okay? Because sort of like the bang per buck, it's sort of how easy it is versus how useful it is, is sort of off the charts. I'm not going to prove it. The proof is sort of trivial.
00:05:15.054 - 00:05:19.298, Speaker A: You just expand the expectations into sums, you reverse the order of the double summation. This is what you get.
00:05:19.384 - 00:05:20.020, Speaker B: Okay.
00:05:26.330 - 00:05:39.490, Speaker A: What else? Okay, right. So that's the key thing to remember about linearity of expectations. They need not be independent.
00:05:39.490 - 00:06:17.538, Speaker A: Now why should you care? What can linear of independence, linear excuse me, linearity of expectation do for you? Well, here's sort of the canonical use case in a lot of things, but in particular in algorithms. Imagine you have some random variable that you really want to understand, like the number of comparisons that Quicksort makes, for example, or the number of iterations that a randomized min cut algorithm takes to terminate something like this. Often you can take a complicated random variable that you care about, like the number of comparisons used by Quicksort, and write it as a sum of much simpler random variables, often in fact just zero one or indicator random variables.
00:06:17.634 - 00:06:18.374, Speaker B: Okay?
00:06:18.572 - 00:06:34.218, Speaker A: So linear expectation then says if you want to understand the expectation of this complicated random variable, that task reduces to just understanding the expectation of each of the simple constituent random variables. Then you just add it up and you get the answer that you wanted.
00:06:34.304 - 00:06:34.554, Speaker B: Okay?
00:06:34.592 - 00:06:50.106, Speaker A: So if you like, think of this as a reduction from complex random variables that are really sums to just analyzing the sums independently. This is all kind of very elementary, but this already gives us actually some pretty interesting results. Again, you've seen some in previous classes.
00:06:50.106 - 00:07:05.990, Speaker A: But let me give you an application in approximation algorithms where this is the only tool we need. So it's the max three sat problem. So you know about sat or three sat? That's a yes, no problem.
00:07:05.990 - 00:07:24.570, Speaker A: This is an optimization problem. The input is the same. So I give you a three sat or three CNF formula m clauses, each with three distinct literals.
00:07:24.570 - 00:07:35.230, Speaker A: So remember how this works. In a Sad formula you have variables boolean, variables x one through XN. A literal is either a variable or it's negation.
00:07:35.230 - 00:07:43.422, Speaker A: And a clause is a disjunction of three literals. So it's going to be something like x three or not x seven or not x ten.
00:07:43.556 - 00:07:44.046, Speaker B: Okay?
00:07:44.148 - 00:07:54.178, Speaker A: So that's a particular clause. This triple this disjunction of three literals and then the CNF formula is a conjunction. So ideally you want to satisfy all of those disjunctions if you can.
00:07:54.178 - 00:08:08.146, Speaker A: So that's the sat problem. Can you or can you not satisfy all clauses simultaneously? The Maxat problem says, well, if you can't satisfy them all simultaneously, at least satisfy as many as you can. Do the best you can given a possibly unsatisfiable formula.
00:08:08.146 - 00:08:12.090, Speaker A: All right, so that's the max three sat problem, just number of satisfied clauses.
00:08:13.790 - 00:08:14.540, Speaker B: Okay?
00:08:15.710 - 00:08:42.260, Speaker A: So output truth assignments are you just going to set each variable to either true or false and you want to maximize numbers satisfy clauses. This includes the sat problem as a special case, right? That's just checking whether or not the optimal solution is m. So certainly this is an NP hard problem, no doubt about that.
00:08:42.260 - 00:09:10.566, Speaker A: But it turns out it's actually and linear expectations will tell us this it's actually sort of embarrassingly easy to get a kind of pretty good approximation guarantee. So here's the claim. Imagine you took a random assignment, meaning for each variable, each of the N variables, you assigned it independently, uniformly, at random to either true or false.
00:09:10.598 - 00:09:10.746, Speaker B: Okay?
00:09:10.768 - 00:09:22.358, Speaker A: So among all two to the N possible truth assignments, you just pick one uniformly. So up here, our state space for this claim is just omega is all possible truth assignments. The distribution is just the uniform distribution.
00:09:22.358 - 00:09:38.820, Speaker A: If you do that and you look at the expected number of clauses that you satisfy, you're going to satisfy at least seven eight of the clauses or 87.5%.
00:09:39.510 - 00:09:40.260, Speaker B: Okay?
00:09:42.790 - 00:09:59.290, Speaker A: Now Op can't possibly be any bigger than M. So by being within seven 8th of M, you're certainly within seven 8th of opt. So in other words, picking a random assignment is a seven 8th approximation algorithm, at least with respect to the expectation.
00:09:59.970 - 00:10:00.720, Speaker B: Okay?
00:10:02.850 - 00:10:18.614, Speaker A: So this has a sort of existential consequence which in my experience many people find very counterintuitive. Every single three Sat formula, no matter how deviously you pick all those clauses, admits a truth assignment. So that seven 8th of the clauses are satisfied.
00:10:18.614 - 00:10:33.750, Speaker A: You can always satisfy seven 8th. Why? Well, a random assignment satisfies on average seven 8th the clauses, so there's got to be some truth assignment that does at least as well as the average. Okay, so you can always satisfy seven 8th the clauses in three Sat formula.
00:10:33.750 - 00:10:41.754, Speaker A: So any questions about that before we do the very short proof, is there.
00:10:41.792 - 00:10:43.820, Speaker B: Any efficient algorithm to find this?
00:10:46.190 - 00:10:54.190, Speaker A: Well, define efficient. I mean, just picking one at random is pretty efficient. You want a deterministic algorithm?
00:10:56.130 - 00:10:59.338, Speaker B: I'm just wondering because we can only prove the existence.
00:10:59.514 - 00:11:13.358, Speaker A: No, so the existence is a consequence of the claim. The claim says flip one fair coin for each variable and on average you'll satisfy seven 8th of them. So the expected number of clauses is indeed a seven eighths approximation algorithm.
00:11:13.358 - 00:11:24.102, Speaker A: You could ask about a deterministic algorithm. Is that what you're asking about? Yeah. So you can basically we'll talk about something called Markov's Inequality in a second.
00:11:24.102 - 00:11:41.290, Speaker A: And I won't go through it in detail, but just to tie the things together, a Markov inequality type argument will say, you have at least, say, a 1% chance of getting 87.4%, and then you can just run it a bunch of times and it's going to succeed once. Now it's with very high probability.
00:11:41.290 - 00:11:54.900, Speaker A: There's other ways to just completely derandomize it, which is also not very hard, but it's outside the scope of this lecture. So you can absolutely get a deterministic seven 8th approximation. Other questions? It's a good question.
00:11:54.900 - 00:12:10.778, Speaker A: All right, so proof? Well, we're going to use exactly the same template that I told you about. So what is the random variable that we care about? Well, obviously the random variable that we care about is how many the number of clauses which are satisfied.
00:12:10.894 - 00:12:11.462, Speaker B: Okay?
00:12:11.596 - 00:12:30.490, Speaker A: So we're going to decompose that into a sum of very simple random variables, analyze each of those separately and then add it up and we're done by linearity of expectation. So proof. So define random variable XJ for each clause.
00:12:30.490 - 00:12:43.706, Speaker A: So let XJ be one whenever clause J is satisfied or zero otherwise. Okay, so this is a random variable. Again, our state space is just truth assignments.
00:12:43.706 - 00:12:57.510, Speaker A: So omega is the two to the N truth assignments. Given a truth assignment, you certainly know whether or not clause J is satisfied, and it's just equal to one or zero accordingly. So in other words, it's the indicator random variable for the event of clause J being satisfied.
00:12:57.510 - 00:13:35.890, Speaker A: Now these are our simple random variables, so we're just going to analyze their expectation directly and then we'll sum them up. So what's the expected value of XJ? Well, for indicator random variables, the expectation is just the probability that the event actually happens, right? So just expanding um, right, so the J satisfied plus zero times the probability that J is not satisfied.
00:13:37.430 - 00:13:38.180, Speaker B: Okay?
00:13:38.630 - 00:13:51.126, Speaker A: So that's just expanding the definition of expectation. Obviously we don't care about the zero and the one we can ignore. So the expected value, and this is true for any zero one random variable, the expected value is just the probability that it's equal to one I e.
00:13:51.126 - 00:14:02.140, Speaker A: That the event occurs. So let's zoom in on this clause, the Jth clause. For simplicity, maybe it's the clause x one or x two or x three.
00:14:02.140 - 00:14:24.522, Speaker A: What's the probability that this clause gets satisfied? Well, how could it not be satisfied x one or x two or x three? The only way you could screw it up is if you manage to pick x one false, x two false, and x three false. Any of the other seven out of eight possibilities satisfies the clause. We're picking a truth assignment uniformly at random.
00:14:24.522 - 00:14:38.118, Speaker A: So the probability that we get one of the seven that work is seven out of eight. And of course there's nothing special about x one or x two or x three. Any clause, only one out of the eight possible assignments to the three variables contained in the clause could fail to satisfy it.
00:14:38.284 - 00:14:39.000, Speaker B: Okay.
00:14:41.930 - 00:14:44.038, Speaker A: So that's where the seven 8th comes in.
00:14:44.204 - 00:14:44.920, Speaker B: Okay.
00:14:46.250 - 00:14:56.090, Speaker A: This is also where I'm using the assumption that the three literals are distinct in each of the clauses, and I'm using that they're independently chosen.
00:14:56.590 - 00:14:57.340, Speaker B: Okay?
00:14:57.710 - 00:15:07.790, Speaker A: And now we're done. So sum of XJS from J equal to M. This is the number of satisfied clauses by definition.
00:15:07.790 - 00:15:27.670, Speaker A: By linearity of expectation, we can reverse the summation and the expectation. We just argued that this is seven 8th exactly for every single clause J. So we're just adding up seven eight M times it, and that's proof.
00:15:27.670 - 00:15:58.640, Speaker A: Other questions? Yes. To imagine clause was not x one or not x two or not x three. The only way you could screw it up is by picking true true any of the other seven are going to work.
00:15:58.640 - 00:16:09.940, Speaker A: So you really have a one in two chance to get each of the variables right. The clause is satisfied. If any of the three variables are right, there's only a one in eight chance in screwing it up, there's a seven and eight chance of getting it right.
00:16:09.940 - 00:16:12.820, Speaker A: Other questions?
00:16:18.490 - 00:16:19.240, Speaker B: Okay.
00:16:20.510 - 00:16:53.538, Speaker A: All right, so it's remarkable if maybe a little depressing, that if P is not equal to NP, you actually cannot beat seven eight for the max three set problem. In the worst case, there's no polynomial time algorithm with approximation ratio better than seven over eight unless P equals NP, which is a little kind of kind of hard to know how to interpret this because clearly this is not the algorithm you're going to be using in practice. Notice this algorithm never even looks at what the clauses are and it gets a seven eight and there's no way you can do better in the worst case.
00:16:53.538 - 00:17:07.094, Speaker A: So obviously there's a lot of things you could do in practice to try to do better, but in theory, in the worst case, we can't do better. Okay, so some announcements. So, problems at four.
00:17:07.094 - 00:17:18.314, Speaker A: The last one I'm sure you're happy about, that is due Tuesday. Exercise number nine, I will post shortly, either today or tomorrow. I'm not sure whether or not there'll be an exercise set number ten.
00:17:18.432 - 00:17:21.510, Speaker B: Yeah, why did you state the claim.
00:17:21.590 - 00:17:29.802, Speaker A: Greater than or equal to? Sorry, let me finish the announcements and then I'll take the clothes question. So I don't know yet whether or not there'll be an exercise set number ten. If there is, it'll be optional.
00:17:29.802 - 00:17:45.234, Speaker A: Okay, so exercise number ten, if it exists, will not be important for the final. Speaking of the final, it's at the standard time, so that's going to be Monday March time slot. The room got assigned, it's 300 301.
00:17:45.234 - 00:17:52.930, Speaker A: Thing you should know. So it's closed book, except I'm happy for you to bring in two sheets of notes. That is four pages.
00:17:52.930 - 00:18:00.022, Speaker A: So two sheets double sided. You can have Arbitrarily, small font sized, but you have to prepare them yourself.
00:18:00.156 - 00:18:00.502, Speaker B: Okay.
00:18:00.556 - 00:18:07.910, Speaker A: They should really be your own work. And a little bit about the final. So it's a three hour exam, but I expect many of you will finish earlier.
00:18:07.910 - 00:18:31.822, Speaker A: It's intended to be, frankly, kind of easy if you've been actually coming to lecture and sort of following the class. So unlike the problem sets, which are really pushing you to sort of expand your knowledge of algorithms, the final is just meant to test like, your basic understanding of the most important concepts in 261 as covered in lecture. And so I promise you that at least half the problems will be literally identical to stuff that's on the exercise sets.
00:18:31.822 - 00:18:38.182, Speaker A: The other half of the problems will be at roughly the same level, so much less substantial than the problem set.
00:18:38.236 - 00:18:38.840, Speaker B: Questions?
00:18:39.290 - 00:18:45.480, Speaker A: So any questions about the final or other logistics before I go back to this technical problem.
00:18:46.810 - 00:18:49.622, Speaker B: Cool. Okay.
00:18:49.676 - 00:18:51.786, Speaker A: So you were saying why did you.
00:18:51.808 - 00:18:52.918, Speaker B: State it as greater than or equal.
00:18:52.934 - 00:19:03.686, Speaker A: To actually just prove equality? Yeah, so it is equal. You're right. I mean, claim is still true, but you're right, it's a good observation that it's exactly seven eight m somehow.
00:19:03.686 - 00:19:12.160, Speaker A: Like as far as our interpretation from an approximation algorithm standpoint, this is the direction of the equality that we're interested in. But you're right, it's no better than that. Absolutely correct.
00:19:12.160 - 00:19:46.102, Speaker A: All right, so linear expectation is great if you're happy just analyzing the expectation of a random variable, it sort of does what you need in many cases. But if you want to make a different statement, if you want to not talk about the average value of something, but you want sort of a higher probability guarantee. So if you want to say some number is big or some number is small almost all the time with high probability, that requires different tools.
00:19:46.102 - 00:20:06.334, Speaker A: And so these tools are called tail inequalities. And this is what we're going to be talking about for the rest of the lecture. Okay, so the point of a tail inequality is to say that a random variable is very likely to be quite close to its expected value.
00:20:06.334 - 00:20:16.174, Speaker A: So there's not much fluctuation. People often talk about this being proving the concentration of a random variable. So if you think of like the density function, there should sort of be this big spike.
00:20:16.174 - 00:20:35.126, Speaker A: So think about like a gaussian that's kind of very skinny. Okay, so you want sort of sharp concentration around the expectation. Now, the way it works with tail inequalities and just randomized algorithm analysis sort of no surprise, the stronger the assumptions you can make about the random variable you're talking about, the sharper the concentration you can prove.
00:20:35.238 - 00:20:35.562, Speaker B: Okay?
00:20:35.616 - 00:20:52.842, Speaker A: And so it depends on sort of the context how much you have going for you and depending on how strong your properties are, you're going to be able to prove better concentration bounds. So my plan next is to just show you the three most frequently used points on this trade off curve. We're going to start with one where you assume like nothing and what you prove is very weak.
00:20:52.842 - 00:21:05.060, Speaker A: And then we'll culminate with turnoff bounds where you assume much more, but you prove really quite sharp concentration. Okay, but all three of these are useful in many, many different contexts. But so just keep that trade off kind of in mind as we do them.
00:21:05.060 - 00:21:18.082, Speaker A: And my plan is just to make the trade offs between these tail inequalities as clear as possible. I'm going to back off from approximation algorithms a little bit. I'm going to talk about a simpler setting, namely just hashing.
00:21:18.082 - 00:21:22.630, Speaker A: But then at the end of the lecture, I'll bring it back and show you some ramifications for approximation algorithms.
00:21:22.710 - 00:21:23.340, Speaker B: Okay?
00:21:24.990 - 00:21:42.590, Speaker A: So as we discuss these, it's going to be three I want to use as a running example sort of the load of buckets in a hash table, say a hash table with chaining. You can think about it. So just to remind you, the setup for hashing.
00:21:42.590 - 00:21:56.466, Speaker A: So let script h be a family of hash functions. Family just meaning set, hash function just meaning a function. H taking a big universe.
00:21:56.466 - 00:22:16.634, Speaker A: Think of this as like 32 bit or 64 bit values and mapping it down into some small number N of buckets. Okay? So here's your huge U, here's your very modest size array of N buckets and your hash function is mapping everything to U.
00:22:16.752 - 00:22:17.420, Speaker B: Okay.
00:22:19.790 - 00:22:37.786, Speaker A: Good. So what I want to study is I want to study so we're going to be thinking about picking a hash function at random, little H from big H. And we want to know how sort of crowded can these buckets be in the hash table.
00:22:37.786 - 00:23:00.074, Speaker A: So if you like, if you think about chaining, how long can these linked lists in different buckets be? And what we're going to see is as we assume more and more about the family of hash functions, we'll be able to prove better and better concentration of how loaded the most loaded bin is bucket is. So for starters, we're just going to assume something very weak. We'll strengthen the assumptions as we go along.
00:23:00.074 - 00:23:19.950, Speaker A: So again, it's assumption on the hash functions. So for now, let's just assume that for any particular universe element, like any particular 64 bit value, it's equally likely to map to any of the N buckets. So if you just look at one element, it's going to be uniform across the N buckets.
00:23:19.950 - 00:23:31.650, Speaker A: Importantly, I'm not assuming that where different elements get mapped are independent events. Once you look at two of them, all bets are off. But if you just focus on one, you see it going uniformly at random between the N buckets.
00:23:31.650 - 00:23:53.500, Speaker A: So for all universe elements, for all buckets, the probability, okay? And so the state space here is just the hash functions. We're picking a hash function uniformly at random. So H drawn from H, that H of x equal I equals one over N.
00:23:53.500 - 00:24:09.850, Speaker A: Okay? It's kind of a pretty minimal assumption for hash functions. To be reasonable, any given element should be mapped basically uniformly. All right? So this property is already enough to say some interesting things about expectations.
00:24:09.850 - 00:24:32.610, Speaker A: So suppose we hash some subset of buckets just for simplicity. Let's say we also hash N things, okay? I don't care which N things, just some N things from this huge universe u.
00:24:32.760 - 00:24:33.458, Speaker B: Okay?
00:24:33.624 - 00:24:34.400, Speaker A: So certainly on.
