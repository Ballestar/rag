00:00:00.730 - 00:00:42.426, Speaker A: So as far as what are we doing today? So we're going to have one more lecture which is focused squarely on parameterized analysis. I'll remind you what that is in a second. So this will be the last lecture that's purely about it, but it'll be a recurring theme throughout the course as we'll see. And so I want to talk about another case study today for another fundamental problem, that of computing large independent sets in graphs. And then time permitting, we'll do a little tour Dorizon of kind of parameterized analysis and the various parameters maybe you should know about that you'll see on the homework to some extent, but otherwise I won't have enough time to really discuss in class. Remember the point of parameterized analysis. You really want to have some numerical measure of how easy or how difficult a particular input is.
00:00:42.426 - 00:00:46.714, Speaker A: And then ideally you want to parameterize the performance of an algorithm in terms.
00:00:46.752 - 00:00:50.394, Speaker B: Of this parameter, in terms of the easiness of the input. So an algorithm is going to do.
00:00:50.432 - 00:01:16.814, Speaker A: Better and better in some sense as the instances get easier and easier. We've seen some examples. The first one was 2D maxima in lecture number two. So then we actually saw multiple ways to parameterize the running time of Kirkpatrick sidell. First n log n purely in terms of the input size, n log h in terms of the input and output size. And then we had this complicated parameter which was sort of necessary for the instance optimality result that we were shooting. For last lecture, we did a parameterized.
00:01:16.862 - 00:01:18.606, Speaker B: Analysis of the LRU algorithm.
00:01:18.718 - 00:01:24.866, Speaker A: So there the inputs were page request sequences. We weren't thinking about running time, we were thinking about the page fault rates.
00:01:24.898 - 00:01:29.014, Speaker B: It was a different performance measure but didn't matter. Parameterized analysis was still a good idea.
00:01:29.132 - 00:01:36.826, Speaker A: We parameterized sequences in terms of the amount of locality and we showed that LRU sort of improves with the locality and in fact it improves faster than.
00:01:36.848 - 00:01:39.094, Speaker B: Competitors like the FIFO algorithm.
00:01:39.222 - 00:01:47.178, Speaker A: And so today we're going to see one more example and there's a lot let me just also review the reasons.
00:01:47.194 - 00:01:49.262, Speaker B: That we're seeing about why this is a good idea.
00:01:49.396 - 00:02:07.746, Speaker A: So the reasons why actually answer all three of the goals that I set forth for mathematical analyses of algorithms. So if you remember them, there was the explanation goal, or equivalently the prediction goal, where you actually want sort of guarantees for algorithms you can interpret literally that are meaningful. Then even if you don't do that, you might want the comparison goal.
00:02:07.778 - 00:02:14.226, Speaker B: So you want a good ordinal ranking. So the relative performance of algorithms you want to at least get right. So you want good advice about what's an optimal algorithm.
00:02:14.338 - 00:02:26.298, Speaker A: The third goal that we haven't talked about yet, but we'll start talking a little bit about today is the design goal. So having sort of a yardstick to measure algorithm performance naturally leads clever and creative people to design new algorithms that.
00:02:26.304 - 00:02:28.326, Speaker B: Are better with respect to that yardstick.
00:02:28.438 - 00:02:40.874, Speaker A: And parameterized analysis is useful for all of these reasons. So first, if nothing else, doing parameterized analysis, you just get more information. It's just a strictly stronger mathematical statement.
00:02:40.922 - 00:02:43.626, Speaker B: Than general worst case analysis parameterized by the input.
00:02:43.658 - 00:02:50.994, Speaker A: And just remember 2D maxima n log n oh, it seemed like it was tight, but then when we looked a little closer, we sort of zoomed in a little bit. We realized we could have a more.
00:02:51.032 - 00:02:55.522, Speaker B: Fine grained performance guarantee about the algorithm's performance. So that's the first reason.
00:02:55.576 - 00:03:07.046, Speaker A: So fine grained information, same deal with Paging. So it seemed like if we didn't do parameterized analysis, LRU and FIFO were the same. Again, we sort of zoomed in the microscope using this framework and we were.
00:03:07.068 - 00:03:11.514, Speaker B: Able to distinguish the two. It was more informative and allowed us to differentiate things that we couldn't before.
00:03:11.632 - 00:03:13.606, Speaker A: So that's progress on that comparison goal.
00:03:13.638 - 00:03:16.700, Speaker B: Comparing different algorithms that we couldn't make without it.
00:03:17.230 - 00:03:23.434, Speaker A: Another reason to do it is it gives you advice about when to use an algorithm, by which I mean on.
00:03:23.472 - 00:03:25.786, Speaker B: What types of inputs and what kinds of domains.
00:03:25.818 - 00:03:31.354, Speaker A: Okay, so imagine you're not really looking to build an algorithm from scratch. You just want to be an educated.
00:03:31.402 - 00:03:33.226, Speaker B: Client of the algorithms that exist.
00:03:33.338 - 00:03:38.346, Speaker A: Okay, well, then part of sort of what you'd like to be on the instructions on the back of the algorithm.
00:03:38.378 - 00:03:40.146, Speaker B: If you will, is advice about what.
00:03:40.168 - 00:03:41.298, Speaker A: Kinds of inputs it's good for and.
00:03:41.304 - 00:03:42.722, Speaker B: What kinds of inputs it's bad for.
00:03:42.776 - 00:03:42.994, Speaker C: Okay?
00:03:43.032 - 00:03:45.266, Speaker B: And that's something parameterized analysis can give to you.
00:03:45.368 - 00:03:49.366, Speaker A: So whatever instant inputs are easy with respect to an algorithm, that's where you.
00:03:49.388 - 00:03:50.102, Speaker B: Want to use it.
00:03:50.156 - 00:04:08.378, Speaker A: Okay, so when to use an algorithm, and again, this is often in theoretical research, things aren't necessarily phrased this way, although we did talk about the example about algorithms that are good for sparse graphs versus dense graphs. You see it sometimes, but in general this isn't so much the emphasis in the theoretical guarantees, but it really is.
00:04:08.464 - 00:04:10.938, Speaker B: A big part of how practitioners think about algorithms.
00:04:11.034 - 00:04:37.614, Speaker A: So that's another reason to do this. I mentioned briefly last time how it offers a two step approach toward explaining why an algorithm which is bad in the worst case is actually good empirically. So the two step approach is, first of all, you do this parameterized analysis so you identify easy inputs on which the algorithm performs well, much better than the worst case. And then secondly, you make some argument.
00:04:37.662 - 00:04:39.874, Speaker B: About why real inputs are easy.
00:04:39.992 - 00:04:46.566, Speaker A: Maybe it's a parameter you can compute explicitly out on the standard benchmarks. Maybe you propose a generative model and you prove that with high probability on.
00:04:46.588 - 00:04:47.606, Speaker B: Samples from that model.
00:04:47.708 - 00:04:52.394, Speaker A: The instances are easy. In fact, that's exactly the philosophy of smooth analysis when we get to that.
00:04:52.432 - 00:04:53.500, Speaker B: Later in the course.
00:04:54.670 - 00:05:10.334, Speaker A: So that's sort of progress on the explanation goal. And then again, we haven't seen this yet, but this will be one of the points of this lecture is that indeed, once you have a novel way of parameterizing performance you naturally the Pavlovian response of any theoretical computer scientist worth their salt is to start thinking about.
00:05:10.372 - 00:05:14.478, Speaker B: How can we do better than the state of the art? And we'll see exactly an example of that today.
00:05:14.644 - 00:05:15.360, Speaker C: Okay?
00:05:16.130 - 00:05:20.174, Speaker A: All right, so these are some of just we'll do these concrete examples, but just big picture.
00:05:20.222 - 00:05:24.980, Speaker B: These are some of the things I want you to remember about the meaning of all this stuff. What's the point?
00:05:26.090 - 00:05:29.602, Speaker A: Okay, so any questions before we descend.
00:05:29.666 - 00:05:36.566, Speaker B: From these sort of lofty statements and actually talk about a real problem, actual problem? Questions? No questions?
00:05:36.748 - 00:05:41.114, Speaker A: Okay, so here's a problem I'll bet you've at least heard of at one point in your life.
00:05:41.152 - 00:05:43.100, Speaker B: I'll certainly remind you what it is.
00:05:44.030 - 00:05:51.006, Speaker A: We want to find the maximum weight independent set of a graph, a well.
00:05:51.028 - 00:05:52.480, Speaker B: Known MP hard problem.
00:05:56.290 - 00:05:59.710, Speaker A: So the input is a graph and the weights are on the vertices.
00:06:01.890 - 00:06:02.206, Speaker B: So.
00:06:02.228 - 00:06:15.098, Speaker A: The graph is undirected and the weights are non negative. And we want to find the independent.
00:06:15.134 - 00:06:18.710, Speaker B: Set no prizes with the maximum weight.
00:06:19.130 - 00:06:20.454, Speaker A: So what's an independent set?
00:06:20.492 - 00:06:21.640, Speaker B: Got to remind you that.
00:06:23.850 - 00:06:28.630, Speaker A: Independent set is a bunch of vertices, so that no pair of them are adjacent.
00:06:28.710 - 00:06:29.002, Speaker C: Okay?
00:06:29.056 - 00:06:39.434, Speaker B: So no edge has both endpoints in this set. So if x y are both in.
00:06:39.472 - 00:06:48.158, Speaker A: S, then it should be the case that there's not an edge between them maximizing and by max the weight is just the sum of the weights of.
00:06:48.164 - 00:06:56.340, Speaker B: The vertices in the set. So maximizing the sum over V and s w sub V. Okay?
00:06:58.470 - 00:07:13.480, Speaker A: So one sort of very canonical case of this is just the unweighted special case. That's where every weight of a vertex is one. So then you're just trying to basically pack in as many vertices as possible without.
00:07:16.330 - 00:07:18.982, Speaker B: Subject to being an independent set, without capturing any edges.
00:07:19.126 - 00:07:27.420, Speaker A: So like in that red graph there, what's the maximum size, maximum cardinality of an independent set?
00:07:31.170 - 00:07:31.920, Speaker B: Two.
00:07:33.650 - 00:07:35.920, Speaker A: Anyone vote for a higher number?
00:07:41.810 - 00:07:42.560, Speaker B: Yeah.
00:07:43.090 - 00:07:50.418, Speaker A: Can only get two because it's pretty clear that the only way to get three would be to have one in.
00:07:50.424 - 00:07:51.650, Speaker B: Each layer, if you like.
00:07:51.720 - 00:07:52.974, Speaker A: But if you take the last vertex.
00:07:53.022 - 00:07:55.278, Speaker B: It precludes picking both of the ones in the middle layer.
00:07:55.374 - 00:07:55.982, Speaker C: Okay?
00:07:56.136 - 00:07:57.158, Speaker A: So you can only get two of.
00:07:57.164 - 00:07:59.720, Speaker B: Those vertices without capturing an edge as well.
00:08:00.970 - 00:08:11.626, Speaker A: Okay? So that's the independent set problem. Any questions about that? Definition is clear. All right, so when you first study MP completeness, you sort of learn that.
00:08:11.648 - 00:08:13.580, Speaker B: All MP complete problems are the same.
00:08:14.110 - 00:08:19.706, Speaker A: Well, yes and no. Okay. They are sort of all the same with respect to whether or not they.
00:08:19.728 - 00:08:23.930, Speaker B: Can be solved exactly in the worst case in polynomial time. In that sense, yes, they're all the same.
00:08:24.000 - 00:08:25.930, Speaker A: If you trim almost any other angle.
00:08:26.010 - 00:08:30.206, Speaker B: They start looking like a very diverse group of problems. And one sense in which they're very.
00:08:30.228 - 00:08:31.886, Speaker A: Diverse is to what extent they can.
00:08:31.908 - 00:08:36.526, Speaker B: Be approximated in the worst case by heuristics by polynomial time algorithms.
00:08:36.638 - 00:08:41.442, Speaker A: And as NP complete problems go, independent sets a very, very hard problem, even.
00:08:41.496 - 00:08:43.300, Speaker B: Amongst NP complete problems.
00:08:45.190 - 00:08:49.798, Speaker A: So here's a fact. I'll tell you I'll sort of write this in standard shorthand, but I'll tell.
00:08:49.804 - 00:08:50.680, Speaker B: You what it means.
00:08:52.010 - 00:08:55.878, Speaker A: Basically, what it says is you can't have any approximation algorithm that runs in.
00:08:55.884 - 00:09:01.020, Speaker B: Polynomial time and gives you a nontrivial guarantee in the worst case, that's basically what it says, okay?
00:09:15.650 - 00:09:24.158, Speaker A: All right, so think of epsilon. Epsilon here can be as small as you want, but think of it at, say, as zero one for concreteness. So by approximate the problem, what I.
00:09:24.164 - 00:09:24.826, Speaker B: Mean is the following.
00:09:24.858 - 00:09:34.666, Speaker A: So a C approximation algorithm for a maximization problem like this one, by definition, what it does is for every single input, opt is whatever it is. And the promise is that the algorithm.
00:09:34.698 - 00:09:36.086, Speaker B: Will return to you an independent set.
00:09:36.108 - 00:09:38.262, Speaker A: Whose total weight is at least C.
00:09:38.316 - 00:09:40.534, Speaker B: Times whatever the maximum possible is.
00:09:40.572 - 00:09:40.726, Speaker C: Okay?
00:09:40.748 - 00:09:42.262, Speaker B: So if C is like zero five.
00:09:42.316 - 00:09:43.334, Speaker A: Then it says you're getting at least.
00:09:43.372 - 00:09:44.146, Speaker B: Half of the optimum.
00:09:44.178 - 00:09:44.326, Speaker C: Okay?
00:09:44.348 - 00:09:48.522, Speaker B: If it was zero one, it would sound kind of pathetic. You're getting 1% of the optimum and so on.
00:09:48.576 - 00:09:52.300, Speaker A: Okay, so I guess I should really.
00:09:52.670 - 00:09:54.858, Speaker B: Do one over this, the way I just described that.
00:09:54.944 - 00:10:01.966, Speaker A: Okay, now suppose you just wanted like a one over n fraction of the.
00:10:01.988 - 00:10:09.870, Speaker B: Maximum possible independent set. Let's start with the unweighted case. That's a pretty easy problem. Take a vertex.
00:10:10.290 - 00:10:12.266, Speaker A: So a vertex is definitely an independent.
00:10:12.298 - 00:10:14.240, Speaker B: Set, so that gets you one.
00:10:14.850 - 00:10:16.066, Speaker A: How many could opt get?
00:10:16.088 - 00:10:21.218, Speaker B: Well, there's only n vertices, right? So n approximation is not so impressive, right?
00:10:21.384 - 00:10:22.658, Speaker A: If there are weights, you can just.
00:10:22.664 - 00:10:29.074, Speaker B: Pick the max weight vertex that will be an approximation. Okay, so not so impressive.
00:10:29.202 - 00:10:31.462, Speaker A: This is almost saying there is no.
00:10:31.516 - 00:10:34.898, Speaker B: Polynomial time algorithm with a better approximation.
00:10:34.994 - 00:10:35.206, Speaker C: Okay?
00:10:35.228 - 00:10:38.726, Speaker A: It's not saying there's nothing better than n, but it says n raised to.
00:10:38.748 - 00:10:43.946, Speaker B: The point 99 you can't get, n raised to the .99 you can't get, and so on.
00:10:44.048 - 00:10:44.410, Speaker C: Okay?
00:10:44.480 - 00:10:47.818, Speaker A: So unless P is equal to NP, in which case of course, you may.
00:10:47.824 - 00:10:53.558, Speaker B: As well solve the problem. Exactly. If P is different than NP, you can't get anything nontrivial.
00:10:53.654 - 00:10:54.298, Speaker C: Okay?
00:10:54.464 - 00:10:58.814, Speaker A: So for worst case analysis, the problem is essentially closed. I mean, you can say, well, maybe.
00:10:58.852 - 00:11:06.386, Speaker B: I can get root log n over n, maybe I can get log n over n, and there actually are algorithms that beat one over n a little bit, but that's all you can do.
00:11:06.488 - 00:11:09.086, Speaker A: Okay, so for worst case analysis, polynomial.
00:11:09.118 - 00:11:12.306, Speaker B: Time algorithms totally stuck with this problem. It's really not a lot you can.
00:11:12.328 - 00:11:19.346, Speaker A: Say, all right, so there's some parallel to when you're talking about competitive ratios.
00:11:19.378 - 00:11:20.882, Speaker B: For online Paging algorithms.
00:11:21.026 - 00:11:31.890, Speaker A: I do want to point out that the reason that we're doing so badly is because of a different handicap. So with Paging algorithms, the reason we were so far from optimal, it wasn't that we had to run fast, it's.
00:11:31.890 - 00:11:34.374, Speaker B: Just that we didn't know the future. It was a lack of information.
00:11:34.572 - 00:11:36.246, Speaker A: Here. The reason we're doing so poorly is.
00:11:36.268 - 00:11:38.086, Speaker B: Because we don't have enough time on our hands, right?
00:11:38.108 - 00:11:41.566, Speaker A: We have all of the input and if we exponential time, we could of course solve it.
00:11:41.588 - 00:11:42.346, Speaker B: Exactly, but we don't.
00:11:42.378 - 00:11:44.686, Speaker A: So our handicap is polynomial time, but.
00:11:44.708 - 00:11:48.480, Speaker B: Under P, not equal to NP, we have a very similarly damning negative result.
00:11:51.170 - 00:11:59.022, Speaker A: But again, this doesn't say that one shouldn't attempt to reason about various algorithms about independent set. It just says we need to kind of change our viewpoint.
00:11:59.086 - 00:12:00.622, Speaker B: We need some new way of comparing.
00:12:00.686 - 00:12:02.802, Speaker A: Algorithms if we want to say, design.
00:12:02.856 - 00:12:05.640, Speaker B: New algorithms, interesting algorithms, or analyze old ones.
00:12:06.010 - 00:12:20.182, Speaker A: So let's go through that exercise. So idea number one is to say, okay, well, let's parameterize the inputs in some way and have a heuristic, which does better and better, hopefully much better.
00:12:20.236 - 00:12:24.022, Speaker B: Than this terrible lower bound on relatively easy inputs.
00:12:24.086 - 00:12:24.698, Speaker C: Okay?
00:12:24.864 - 00:12:27.366, Speaker A: So there's a lot of ways you can parameterize graphs.
00:12:27.398 - 00:12:28.346, Speaker B: Many, many ways of doing it.
00:12:28.368 - 00:12:32.010, Speaker A: You're certainly used to vertices, you're certainly used to edges. What we're going to use right now.
00:12:32.080 - 00:12:38.126, Speaker B: Is the maximum degree, the maximum number of edges, incident, on a vertex. There's many others, you've probably seen many others.
00:12:38.228 - 00:12:47.346, Speaker A: So idea number one, parameterize by the.
00:12:47.368 - 00:12:50.340, Speaker B: Max degree, delta, okay?
00:12:51.350 - 00:13:00.946, Speaker A: And the intuition that we're hoping, we're hoping that somehow as delta is smaller, this problem gets easier. I got to say the narrative for.
00:13:00.968 - 00:13:04.822, Speaker B: This isn't quite as strong as it is with locality and the LRU algorithm or something like that.
00:13:04.876 - 00:13:06.450, Speaker A: I mean, in some sense it's not going to get any harder.
00:13:06.530 - 00:13:08.514, Speaker B: There's only fewer and fewer graphs you have to handle.
00:13:08.562 - 00:13:17.658, Speaker A: As delta gets smaller, the vague intuition is that, well, independent set, that means once you take a vertex, you can't take any neighbors. Okay? So if you're using a heuristic, you're.
00:13:17.664 - 00:13:18.634, Speaker B: Going to screw up sometimes.
00:13:18.752 - 00:13:20.518, Speaker A: And somehow, like the cost of screwing.
00:13:20.534 - 00:13:24.294, Speaker B: Up by erroneously adding a vertex is now you've blocked all of its neighbors.
00:13:24.422 - 00:13:28.026, Speaker A: So if the degree is small, that means sort of anyone mistake won't penalize.
00:13:28.058 - 00:13:31.070, Speaker B: You that much because there aren't that many neighbors incident to it.
00:13:31.220 - 00:13:32.666, Speaker A: So that's kind of the best intuition.
00:13:32.698 - 00:13:33.998, Speaker B: I can give you about until we.
00:13:34.004 - 00:13:35.226, Speaker A: Talk about once you see an algorithm.
00:13:35.258 - 00:13:38.526, Speaker B: You'Ll see why it matters. But without an algorithm, that's kind of the best I can say.
00:13:38.548 - 00:13:39.466, Speaker A: So it's intuitively.
00:13:39.498 - 00:13:41.498, Speaker B: Mistakes seem less costly if the degrees.
00:13:41.514 - 00:13:43.678, Speaker A: Are small, but we'll see.
00:13:43.764 - 00:13:45.200, Speaker B: This is in fact the case.
00:13:46.010 - 00:13:48.500, Speaker A: All right, so let's talk about an algorithm then.
