00:00:00.250 - 00:01:04.356, Speaker A: So the point of this video is to relax an unreasonable assumption that we've been making over the past few videos. Namely up to this point, we've been assuming that a blockchain protocol has access to an ideal randomness beacon. And so this kind of beacon, right, this is just perfect randomness falling from the sky. So the assumption is that this beacon periodically, maybe it's once every second, maybe it's once every 10 seconds, whatever, periodically emits independent uniform randomness. So here the output of the beacon is from some known set, so maybe the set of all 256 bit strings. So we've seen throughout part two of this lecture of lecture twelve why access to such a beacon is useful to a blockchain protocol. Specifically, proof of stake protocol that wants to sample one public key out of all of the public keys registered in the Staking contract with probability proportional to the stake.
00:01:04.356 - 00:01:48.500, Speaker A: Actually, four videos ago when we talked about weighted round robin, we didn't actually need such a beacon. But three videos ago we introduced this assumption and we showed how a protocol with access to such a beacon can actually, in a quite straightforward way, sample a public key with probability proportional to stake. And so, if you remember the approach, it was very simple. We just took the unit interval zero comma one. We sort of partitioned it into buckets one bucket for each public key in the Staking contract with the width of a bucket proportional to stake. And then we just interpreted the output R sub T with the random beacon as a uniformly random sample from the interval zero comma one. And then whichever bucket the uniformly random sample lies in, that's going to be the public key that gets sampled.
00:01:48.500 - 00:02:48.360, Speaker A: We also talked about the main weakness in that solution, which is that everybody finds out which public key gets sampled at exactly the same time. As soon as randomness falls from the sky, everybody knows which public key is the one getting sampled. And you might in part motivated by how proof of work works, want more of a secrecy property where only kind of the winner of the lottery finds out. And then it's really kind of they can notify other people at their leisure that they did in fact win the lottery. And so the last two videos were about more sophisticated approaches to proof of stake random sampling, where you do get that secrecy property specifically using verifiable random functions or VRFs, which again, you can think of as not so far from, say, a secure digital signature scheme. So in addition to making use of some cryptography, the VRF based solution we outlined over the last two videos also was using the ideal randomness beacon. The output R sub t of the beacon is what everybody was evaluating their VRF on to check whether or not.
00:02:48.510 - 00:02:49.960, Speaker B: They won the lottery.
00:02:50.720 - 00:03:23.700, Speaker A: So those are some pretty nice ideas, but we still have this lingering issue that where does this perfect randomness from the sky come from, right? Presumably that doesn't exist. So in any concrete implementation of, for example, VRF based random sampling, we need to say what is r sub t? If it's not going to be the output of some ideal randomness beacon, what is it? Now, the easy way out would be to just outsource the production of r sub t to some third party. So basically entrust some third party to periodically report what is allegedly perfect randomness.
00:03:24.040 - 00:03:25.860, Speaker B: To the blockchain protocol.
00:03:26.680 - 00:03:58.716, Speaker A: But that of course requires a significant amount of trust in that third party. So, first of all, just for liveness so just that the person actually shows up on time and does report something. R sub t but maybe more worrying, you have to trust that person that they're actually sampling. R sub t according to the agreement that they actually are doing it uniformly at random, as opposed to perhaps, say, reporting some R sub t, which is in the interests of that third party. So you actually do see this approach sometimes. It's called using an oracle to export randomness from the outside. You do sometimes see that used at the application layer.
00:03:58.716 - 00:04:38.780, Speaker A: So especially for sort of lower value projects, especially early in their lifetimes, maybe to sort of get the ball going, you would just sort of trust some third party to supply randomness that would then be used in your application. If you're talking about a major blockchain protocol, however, that kind of trust assumption is just a non starter. And indeed, none of the major deployed blockchain protocols export randomness from the outside. So one nuance to that point is that proof of work blockchain protocols, like ones based on Nakamoto Consensus, there's a sense in which they are actually exporting randomness from the outside, but there's no trust assumptions, at least certainly no trust assumptions in a third party. Maybe you're trusting like that the random oracle assumption holds for your cryptographic hash function.
00:04:38.930 - 00:04:41.084, Speaker B: But that's about it, right?
00:04:41.122 - 00:05:24.584, Speaker A: Because under the random oracle assumption, really, there's nothing miners can do than just sort of repeated guesses. And so then the probability that you're going to be hearing from somebody is just proportional to how much of the hash rate belongs to them. Proof of stake blockchain protocols. Meanwhile, there's no obvious source of untrusted external randomness. So the way proof of stake blockchain protocols work is they generate randomness or really pseudo randomness from the only stuff they know about, really just from their own hermetically sealed environment. And that's a fundamental challenge of proof of stake random sampling, which we didn't have to worry about back when we were doing proof of work. In this video, I'm going to show you an approach to approximating an ideal randomness beacon with some kind of pseudorandomness beacon.
00:05:24.584 - 00:05:32.380, Speaker A: And what we'll talk about on this slide in this video is actually pretty common practice in today's proof of stake blockchain protocols.
00:05:32.880 - 00:05:35.176, Speaker B: So this approach, while popular, does have.
00:05:35.218 - 00:06:17.740, Speaker A: Some issues which we will be discussing. And so that's why after this video, we're going to have two more videos on a more sophisticated approach that in effect crowdsources randomness as an approximation to an ideal randomness beacon. And the key tool there that we're going to discuss is Verifiable delay functions, or VDFS. And that's an approach which is just starting to make its way into practice, kind of. Right now it's still, I'd say, pretty experimental, but in the next generation of proof of stake blockchain protocols, the approach in the next two videos may well become best practices we'll have to see. But I do want to sort of show you how what I think is going to come next. What are the key ideas in that VDF based approach.
00:06:17.740 - 00:06:27.600, Speaker A: But let's not get ahead of ourselves. Let's just proceed to understanding a relatively straightforward way of having a pseudorandonous beacon in a proof of stake protocol.
00:06:28.580 - 00:06:30.112, Speaker B: So at a high level, the idea.
00:06:30.166 - 00:07:51.640, Speaker A: Is really quite straightforward, right? So we need some random bits or bits that sort of, for all practical purposes, look as if they're random. Where are those going to come from? Well, we've had this like, random oracle assumption, right? We've mentioned over and over and over again, which asserts that for some cryptographic hash function like, say, shot 256, for all practical purposes, the output is indistinguishable from literally just a gnome in a box flipping a fair coin 256 times. So why not just pick your favorite cryptographic hash function for which you're comfortable making the random oracle assumption and interpret its output as random bits? There is the question, of course, of like, what are you hashing? What's the input to the hash function? And as we've discussed, that has to come from the hermetically sealed environment. That has to come from something the blockchain protocol knows about at the time it's evaluating the hash function. So what's known to the protocol at time T? Well, maybe most simply the time step. If it's time step 17, the protocol is well aware that it's time step 17. So an initial idea, which I'm going to be honest is a bad idea, is to just hash whatever the current time step is.
00:07:51.640 - 00:08:20.078, Speaker A: So this is certainly something the protocol could do if it wanted. And actually, believe it or not, a couple of the early proof of stake blockchain protocols actually used exactly this approach. You don't see this in the last couple of generations of proof of stake blockchain protocols, but it's kind of like a first thing you might try. Now, the good news is that the RTS are completely unmanipulable by anybody, right? So the choice of the cryptographic hash function, little h that's going to be baked into the protocol and the time step just is the time step and.
00:08:20.084 - 00:08:21.920, Speaker B: There'S nothing anybody can do about it.
00:08:23.730 - 00:09:01.986, Speaker A: The obvious issue is that at the time of the protocol deployment, a timestep t equals zero already, then everybody knows what R sub T is going to be for the rest of time. All of the RTS are completely predictable in advance. We've mentioned several times issues that come up if you have predictability. So for example, if you're using the R sub T's to select block proposers of the different time steps, good chance that if you know the R sub T's, you also know who the block proposers are going to be. As we've discussed, that's a window to sort of attack that block proposer. For example, denial of service to force.
00:09:02.018 - 00:09:04.714, Speaker B: Them to skip their turn with this.
00:09:04.752 - 00:09:58.102, Speaker A: Extreme form of predictability, with everything known at the beginning of time. Actually some other big issues come up as well. For example, right. So toward the beginning of our VRF discussion, we talked about the importance of a warm up period where basically people have to commit to a public private key pair in the Staking contract before they know what are going to be the messages that they're going to be signing, or the messages that they're going to be evaluating their VRF on. And that's to avoid people sort of grinding or trying multiple public private key pairs in order to register with one that's going to serve them well in the future. So you could escape that with a warm up period back then, but if all the RTS are known for the beginning of time, no warm up period is going to save you. So that is generally not how the pseudorandom seed RCT is defined in modern proof of stake blockchain protocols.
00:09:58.102 - 00:10:00.330, Speaker A: So let's talk about a potentially better.
00:10:00.400 - 00:10:04.278, Speaker B: Idea, which is to use an input.
00:10:04.294 - 00:10:12.160, Speaker A: To the hash function which depends not only on the current time step, but also in some nontrivial way on the current blockchain states.
00:10:16.110 - 00:10:17.434, Speaker B: All right, so let me just talk.
00:10:17.472 - 00:10:57.204, Speaker A: You through the symbols in this blue expression I just wrote down. So first of all, by the two parallel lines there, I just mean concatenation. So the input is going to be the time step concatenated with stuff derived from the current blockchain state. Sigma T minus one, that's just the blockchain state. The blockchain state at the conclusion of round or time step, t minus one. Now sigma T minus one, right? The full blockchain state, that's potentially terabytes of data. Safe to assume you're not going to be feeding all of that into the hash function at each time step.
00:10:57.204 - 00:11:40.636, Speaker A: So presumably you're just going to extract something from the current blockchain state and use that as part of the hash function input. And this function Little F that I've written down here and we'll talk much more about what Little F might mean. But basically little F just takes the whole blockchain state, extracts a little bit and then that's what gets fed into the cryptographic hash function. So on efficiency grounds, the need for the function little F should be intuitively clear actually, that's not the only reason. As we'll see, little F needs to be chosen carefully to avoid grinding attacks. When we discuss concrete choices for little F that people use in practice, we'll see that often the output of Little F is going to be basically kind of as good as random already. In which case you can omit.
00:11:40.636 - 00:11:43.090, Speaker A: That cryptographic hash function little H.
00:11:51.050 - 00:11:51.366, Speaker B: So.
00:11:51.388 - 00:12:02.122, Speaker A: In our original blue expression with the H, we're including the timestep little t as part of the input because we want R sub t to keep changing even if the state sigma is staying the same.
00:12:02.176 - 00:12:02.346, Speaker B: Right?
00:12:02.368 - 00:12:17.086, Speaker A: So like, maybe you have a Byzantine block proposer who refuses to change the state. You'd still like to have a different pseudorandum seed at the next time step. So if you do this trick where you sort of omit h and you just used F, well, then you still should have some dependence on the current timestep, little t in the input that's.
00:12:17.118 - 00:12:21.326, Speaker B: Being fed into little f. So comparing.
00:12:21.438 - 00:12:32.630, Speaker A: This idea to the previous one, when you just look at the hash of the timestep, on the one hand, it does a lot better as far as unpredictability, and it does slightly worse in terms of unmanipulatability.
00:12:33.610 - 00:12:35.606, Speaker B: So on the positive side, compared to.
00:12:35.628 - 00:12:51.500, Speaker A: The first idea where all of the RTS were known arbitrarily far in advance here, you're not going to know r sub T till sort of the last minute. So basically you don't know r sub T until you know what the blockchain state sigma sub T minus one is going to be at the conclusion of the previous time step.
00:12:52.830 - 00:12:54.314, Speaker B: Now, to be clear, this still does.
00:12:54.352 - 00:13:47.520, Speaker A: Leave a little bit of predictability, right? So like, if you're the block proposer chosen at timestep T minus one, you then know what block you're going to propose. So assuming everything goes as planned, you actually know what the state of the blockchain is going to be at the end of this time step. So you'll know it a little bit before everybody else. But clearly there's much less predictability than we had with the first idea. On the flip side, in that first idea, it was clear that nodes had no ability to manipulate the r sub T's at all, right? They were just sort of deterministically determined by the protocol's, choice of the hash function and future time steps. Whereas here, now that R SubT actually depends nontrivially on the blockchain state, you have to ask yourself like, okay, who determines the blockchain state that is not determined purely by the protocol, right? That is really determined by nodes. They're the ones who are proposing blocks, finalizing blocks, and therefore affecting the current blockchain state sigma T minus one.
00:13:47.520 - 00:14:29.954, Speaker A: Think again about the node that happens to be chosen as the block proposer at the previous round. In round T minus one, that node's choice of a block. Again, assuming that everything sort of goes as expected and that block gets finalized. That node's choice of a block directly determines what sigma T minus one is. So that node is certainly in a position to manipulate the blockchain state sigma T minus one. At the end of that time step by virtue of being in a position to manipulate the input sigma T minus one to the function little f the node potentially has the ability to manipulate the output of the function little F and therefore the choice of the pseudorandom.
00:14:30.002 - 00:14:33.058, Speaker B: Seed R sub T. Now, the extent.
00:14:33.074 - 00:15:15.602, Speaker A: To which the node is going to be able to do that is going to depend on exactly which function little F you use. Right? At one extreme, you could imagine a function little F that's independent of sigma, independent of the blockchain state. Like it just always outputs the empty string. If you think about it, that special case is kind of exactly what our initial first idea was. So in that case, manipulating the input to little F doesn't help you manipulate the output, the output is just going to be fixed. At the other extreme, suppose little F depends nontrivially on the set of transactions that have been included in the most recent finalized block. Well then guess what? The block proposer at timestep T minus one is exactly who typically chooses those transactions so they can directly manipulate the output of little F.
00:15:15.602 - 00:15:18.418, Speaker A: If it depends on the transactions in the most recently.
00:15:18.434 - 00:15:21.910, Speaker B: Finalized block, you could imagine intermediate points.
00:15:21.980 - 00:15:58.946, Speaker A: Also, like you could have little F depending on the block that was ten blocks back rather than the most recent block. Then the block proposer at timestep T minus one is not in a position to manipulate RT. But then of course the block proposer at timestep T minus ten is in a position to manipulate R sub T. So that problem hasn't really gone away. So this discussion gives us guidance about exactly how we want to choose the function little F. Exactly which aspects of the blockchain state do we want to have determining these pseudorandom seeds? On the one hand, we want to make the good news as good as possible, so we'd like to be as unpredictable as possible. On the other hand, we'd like to make the bad news the least bad as possible.
00:15:58.946 - 00:16:24.996, Speaker A: So even if we can't completely eliminate node's ability to manipulate these R sub T's, we'd like to sort of give them as few avenues as possible for doing so. So, for example, one of the options that we mentioned in passing, right? Having little F depend nontrivially on the transactions in the most finalized block, that would be a bad choice of the.
00:16:25.018 - 00:16:32.776, Speaker B: Function little F. The reason this is.
00:16:32.798 - 00:16:51.052, Speaker A: Such a bad idea is that it just provides an astronomical number of options to a node that wants to manipulate R sub T. So think again about whichever node winds up being the block proposer in the previous round around T minus one. And the point is, there's an astronomical number of different blocks that that node can propose if it wants.
00:16:51.106 - 00:16:51.276, Speaker B: Right?
00:16:51.298 - 00:17:29.396, Speaker A: If there's like a bunch of pending transactions, it can choose any subset of those transactions it wants to include. It can order those transactions in any way. So there's just zillions of possibilities for what that node could propose. And so what that means is that this block proposer is going to be able to pick from zillions of possibilities for what the blockchain states sigma T minus one is going to be. Which means they get to pick from among zillions of possibilities for the pseudorandomous r sub t at the next round at round t. So the node might be then incentivized to try zillions of different possibilities until it gets a pseudorandum seed r sub t. That's advantageous, for example, a choice of r sub t so that it winds up being the block.
00:17:29.396 - 00:17:43.712, Speaker A: Proposer again at round t in addition to round T minus one. And then of course, there's no reason not to just keep doing it. So if you have enough options and a note is fast enough, it might literally be able to just guarantee that it's selected as the block proposer every single time step.
00:17:43.766 - 00:17:47.616, Speaker B: That's obviously a bad thing if you think about it.
00:17:47.638 - 00:18:59.770, Speaker A: This also kind of turns this proof of stake protocol into basically a proof of work protocol, right? Which was really not the point. So proof of work, of course you're incentivized to grind through lots of different nonsense trying to find a crypto puzzle solution here. You're incentivized implicitly to grind through lots of different choices of blocks again to find something with a suitable hash, something that leads to a pseudorandum seed r sub T for the next round that you're happy with. So if one of our goals of implementing a proof of stake protocol was to make sure nodes don't have to do that much work and don't have to consume ridiculous amounts of electricity to sort of get lots of computing power, well, with this choice of little F, we have completely screwed up that objective. The problem persists if you think about the variant we mentioned briefly, where you have little F depend not on the last block, but on the block ten blocks back. So now a block proposer, okay, it doesn't have an incentive sort of to grind away at different blocks to manipulate the pseudorandum seed for the next time step, but it does have that exact same incentive to grind through different blocks to get a good pseudorandum seed for ten timesteps from now. So it might be able to guarantee that it's elected the leader selected as the leader again ten times steps from the current one.
00:19:00.460 - 00:19:01.832, Speaker B: All right, so if that's a bad.
00:19:01.886 - 00:19:32.380, Speaker A: Choice of little F because it's way too manipulable, what would be a better, much less manipulable choice of little F? Well, the answer to that question depends a little bit on other design decisions that we've talked about along the way. But think for example, about a proofitech protocol that uses VRF based sampling. So in those kinds of protocols to earn the privilege of proposing a block generally your VRF output has to be sufficiently close to zero.
00:19:33.070 - 00:19:34.966, Speaker B: So in those types of protocols, whenever.
00:19:34.998 - 00:20:16.644, Speaker A: You make a block proposal, you have to also sort of prove that you're eligible for proposing that block generally by exhibiting the fact that your VRF output is sufficiently close to zero. So the idea here then is going to be to have little F be independent of the transactions in the previous, the most recently finalized block, but instead to depend on the credential. So this close to zero VRF output that the proposer of the previous block used to justify that block proposal. So why is this choice so much better than the last one? Well, fundamentally it gives much fewer degrees of freedom for a node to try to manipulate the output of the function.
00:20:16.762 - 00:20:19.796, Speaker B: Little F. Think again about the node.
00:20:19.828 - 00:20:58.496, Speaker A: Who'S chosen to propose a block in the previous round round t minus one. Now, with our first idea where little F depended on the transactions in the block they put together, that meant they had zillions of different outputs of the function little F amongst which they could choose. Now, with this definition of little F, it actually seems like they have no degrees of freedom at all. We've defined little F to be literally independent of what they put in the block. Little F does not care what transactions are there or what they're ordering it is all it cares is about the VRF outputs of that node. Now, that VRF output is going to depend on two things. So first of all, the pseudorandum seed for that time step that's the VRF input.
00:20:58.496 - 00:21:50.230, Speaker A: And then secondly on the private key that this chosen node uses to evaluate the VRF. So the hope then would be that the node can't manipulate the input to the VRF. That's just the result of some other node's behavior a while ago. And moreover, it can't manipulate its choice of private key because assuming we have a cooldown period, those must be locked up, those must be fixed, chosen and fixed well in advance. So the intuition there is correct in the sense that this choice of the function little F is far less manipulable than our first attempt due to this independence of the output of F from the transactions that are in the blockchain state. The intuition is also a little bit incorrect in the sense that it's still the case that nodes may be able to manipulate the output of the function little F nowhere near as much as before, but they still can manipulate it.
00:21:50.760 - 00:21:52.704, Speaker B: The details there are a little subtle.
00:21:52.752 - 00:23:06.750, Speaker A: And we'll describe them at length in part three when we talk about the various complications that arise when you try to sort of combine a BFT type consensus protocol with proof of stake random sampling. But the rough idea is if a node has a bunch of sybils all registered far in advance, but so it has all these public private key pairs registered far in advance. There is this case where more than one of its sybils has a winning lottery ticket. Remember in VRF based sampling you could sort of have potentially multiple winners. And in the event that the node does have multiple winning lottery tickets, then it can choose amongst them which of them it wants to broadcast along with its Brock proposal in around T minus one. So even if this description didn't make 100% sense, I hope it sounds like that's sort of like way more of an edge case than in our first attempt, where basically any reordering of the transactions would give you sort of another possible output of little f to experiment with. So in practice, several proof of stake blockchain protocols have basically made peace with this limited amount of manipulability that you get from this kind of approach.
00:23:06.750 - 00:24:04.582, Speaker A: For example, the approach to sampling leader, as I've sketched here, it's basically how it works in the Algorand blockchain. In proof of stake ethereum, it's pretty similar, a little more complicated. Rather than just looking at the credentials submitted by the most recent block proposer, you actually look at a batch of signatures by previous block proposers and then you aggregate all of those signatures and then you treat the aggregated signature as basically being some kind of pseudorandum seed. So both of those examples, algorand and ethereum, use more or less BFT type consensus. And in fact, if you try to pair proof of stake civil resistance with longest chain consensus, which is not as popular but can be done, and if you do that, actually some additional difficulties arise. And the issue is how to define the blockchain state sigma T minus one. If there's a fork, you could in principle sort of try to define it as like all the blocks that are known to everybody, whatever branches of various forks that they're on.
00:24:04.582 - 00:24:47.230, Speaker A: That's probably impractical. It's certainly not how longest chain protocols typically work. Generally, the blockchain state with respect to a block is just viewed as kind of everything that happened before it. But what that means now is if you're a leader of a round and longest chain consensus, not only do you have the usual freedom that you can sort of put together whatever block that you want, but you also get to pick the predecessor, you also get to pick which branch of a fork you might try to extend. And so if the blockchain state depends on the branch, then the randomness, the RSPT will also depend on the branch. So this is an additional kind of thing one can grind over in proof of stake longest chain protocols, you can basically choose all these different potential predecessors for your block and pick whichever kind of sequence of historical R sub T's.
00:24:47.310 - 00:24:49.010, Speaker B: You find most favorable.
00:24:49.590 - 00:25:22.590, Speaker A: So we'll talk more about these issues in part three. Again, part three of Lecture Twelve is where we really kind of talk through the additional challenges that come up when you try to pair these part two techniques. So really just sort of proof of stake random sampling, picking one public key out of the public keys registered in the Staking contract, how to actually combine that idea with the consensus protocols that we've been talking about this whole time, lectures one through nine. So, as discussed, we'll have some lectures there on pairing proof of stake consensus with BST type consensus. But then we'll also have some about the challenges like this one that come up when you try to pair it with longest chain consensus.
00:25:23.650 - 00:25:25.146, Speaker B: In any case, as you can hopefully.
00:25:25.178 - 00:26:12.190, Speaker A: Tell from this discussion, we're now really kind of at the state of the art, like a number of the sort of biggest proof of stake blockchain protocols out there work more or less along the lines that I've described in this slide. We're not quite done with part two because I'm not content to merely tell you about the present. I also want to give you a glimpse of what I think is the likely future. So the last two videos of part Two, they're going to focus on a sort of more experimental technique. We are seeing it sort of in deployments to some extent, but I think we'll see it a lot more in the years to come, which is a way of doing proof of stake random sampling, which actually is completely unmanipulable, which really you can't bias at all. And this is going to be based on a cryptographic primitive known as a verifiable delay function or a VDF. So that's what you have to look forward to in the next couple of videos.
00:26:12.190 - 00:26:14.650, Speaker A: I will see you there. Bye.
