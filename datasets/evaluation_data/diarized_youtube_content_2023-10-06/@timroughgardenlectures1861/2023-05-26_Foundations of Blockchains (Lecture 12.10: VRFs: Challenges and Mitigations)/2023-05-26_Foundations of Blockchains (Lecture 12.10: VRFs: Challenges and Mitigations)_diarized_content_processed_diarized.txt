00:00:00.250 - 00:00:16.298, Speaker A: All right, so we've made some really good progress toward having a version of proof of stake random sampling that has the additional secrecy property that we want, that it's only the owner of the public key that gets sampled who finds out that they got sampled. On the other hand, if they want to convince everybody else they got sampled.
00:00:16.394 - 00:00:20.698, Speaker B: That'S easy to do as well. And so the big ingredient that we're.
00:00:20.714 - 00:00:53.854, Speaker A: Using in this current approach is a verifiable random function or VRF. So remember, given a VRF our approach to sampling a public key, each owner of each public key evaluates the VRF using their own private key and then we define that public key is being sampled if and only if the VRF output again with the corresponding private key is sufficiently close to zero. So note that each owner of each public key is going to be getting a different VRF output because each owner of each public key is using a different private key when they evaluate the.
00:00:53.892 - 00:00:57.562, Speaker B: VRF, the defining properties of a VRF.
00:00:57.626 - 00:01:51.358, Speaker A: Then give us the properties we wanted of a proof of stake sampling scheme, right? So first of all, we wanted it that the owner of a public key should be able to just check whether or not it has been sampled. And it can do that because property one of a VRF is that it's easy to evaluate if you know the appropriate private key. Secondly, we wanted to make sure that if someone sampled, they can convince the rest of the world of that fact. And so that follows from property two of VRFs, which is if someone knows your public key and you tell them the alleged VRF output with your private key, then they can easily check that is in fact the case. So you can check if you got sampled, if you did get sampled, you can tell everyone else about that in a verifiable way. The third defining property of a VRF is what gives us the secrecy property that we wanted. We wanted to be the case that nobody could tell if you've been sampled, if you're the leader, unless you tell them so.
00:01:51.358 - 00:02:26.650, Speaker A: And that's exactly what the third property says. It says someone who does not know the private key and also is not told the output of the VRF cannot themselves figure out what the output of that VRF is, so they can't figure out whether you've been sampled or not. And finally, in order to get the details right around what sufficiently small means, we're going to want the output of VRFs to be, for all practical purposes, indistinguishable from uniformly at random. So in particular, if you're feeding in random inputs like the output of a randomness beacon to a VRF, the outputs you're going to see are going to be basically as good as uniformly at random.
00:02:27.230 - 00:02:28.810, Speaker B: We also talked in the last video.
00:02:28.880 - 00:03:01.382, Speaker A: About a canonical construction of a VRF, which is really just to use a digital signature scheme with signatures kind of playing the role of VRF outputs. You do want to use a signature scheme that has unique signatures. So for a fixed message and a fixed secret key, there's only a single signature, which would cause anybody who knowing the public key to accept the signature. But if you don't have that property, then you can grind over multiple, possibly correct signatures which would manipulate the randomness. You don't want that. So you really want unique signatures. If you feel like the signatures you're getting from your signature scheme are basically uniformly random already, then you're done.
00:03:01.382 - 00:03:09.670, Speaker A: If not, then you should feel free to pass those signatures through a cryptographic hash function for which you're comfortable making the random oracle assumption.
00:03:10.170 - 00:03:12.194, Speaker B: So that summarizes the really healthy progress.
00:03:12.242 - 00:03:40.290, Speaker A: We made in the last video. But we are not done. We already mentioned sort of issues one, two, and three in the last video. That's what led us to the definition of a VRF. But there's two more things we have to discuss. One is a major complication that arises in proof of stake sampling using this VRF approach, since we need to talk about what that is and what we can do about it. And then I also want to spend some time filling in the details of how to define sufficiently small in a way that gives you sybil resistance.
00:03:43.090 - 00:03:44.926, Speaker B: So issue number four is actually a.
00:03:44.948 - 00:04:36.378, Speaker A: Pretty major complication of using this VRF approach to do proof of stake sampling. And I'll bet a bunch of you have already kind of noticed this issue. To get the secrecy property that we're after, right, we're having each of the public key owners independently evaluate the VRF. So each owner of a public key might or might not be selected with some probability, but they're basically like independent coin flips, which means there's a chance that, first of all, nobody gets sampled, right? Maybe all of the owners of the public keys evaluate their VRFs. None of those VRF outputs are sufficiently small. Similarly, it's possible that two or more of those VRF outputs qualify as sufficiently small. So when we're talking about sort of sampling from the list of public keys, obviously we always had in mind, like, you sample one and exactly one public key out of PK, one through PKN.
00:04:36.378 - 00:04:57.960, Speaker A: That is not actually what we're getting out of this VRF approach. We are getting a variable number of samples from the list. Now, we can tune that difficulty parameter tau so that we expect to get, say, exactly one public key sampled. But there's going to be variance around that, right? Because these are basically independent coin flips. We might get zero, we might get more than one.
00:04:58.570 - 00:05:00.386, Speaker B: We can also fiddle with a difficulty.
00:05:00.418 - 00:05:35.060, Speaker A: Threshold tau to trade off between those two bad cases, right? So if we tune tau so that we expect to get, I don't know, like five samples at a given time step, well, then it's very unlikely we're going to have zero. But of course it's very likely we'll have two or more. Alternatively, if we tuned the parameter tau so that we were expecting to get in an average time step, one 10th of a sample, well, now all of a sudden it becomes not impossible but unlikely that we'll get two or more. On the other hand, a lot of the time we won't get any. So you can kind of trade off these two bad cases of having no samples or having multiple samples, but you can't really get rid of either of them.
00:05:35.670 - 00:05:37.538, Speaker B: So that's actually a major issue.
00:05:37.624 - 00:05:39.442, Speaker A: So let's talk through sort of three.
00:05:39.496 - 00:05:41.940, Speaker B: Options you have for dealing with it.
00:05:42.710 - 00:06:05.450, Speaker A: So option number one is just to punt. Frankly, you could say, you know what, in the abstract, this sort of secrecy property seemed like something that would be nice to have. But now that I see the costs that I would have to pay using this VRF approach where I might have to deal with like seven different public keys sampled in the same round, like, what do I do with that? You could say, yeah, you know what, it would have been nice to have, but it's just not worth having to deal with. So let's just give up on secrecy.
00:06:06.510 - 00:06:08.294, Speaker B: So what this means is getting comfortable.
00:06:08.342 - 00:06:32.920, Speaker A: With the fact that everybody's going to find out who the leader is at exactly the same time. And as we discussed, there are some problems with that, right? So like if the point of a leader is to assemble a block, well, then presumably you got to give the leader a little bit of time to assemble their block. And the time that you give them is then a window by which anyone else can try to attack that leader, for example, by a denial of service attack, thereby forcing that leader to skip its turn.
00:06:33.530 - 00:06:34.978, Speaker B: So I think everybody can agree that's.
00:06:34.994 - 00:06:52.250, Speaker A: A bad thing, but I also think it's a reasonable opinion that it's not bad enough to deal with all of the additional complications that come in from VRF sampling. And indeed, some of the major proof of stake blockchains do take exactly this approach. Like Solana and Cosmos would be two examples.
00:06:53.390 - 00:06:54.666, Speaker B: On the other hand, there are some.
00:06:54.688 - 00:07:09.060, Speaker A: Major proof of stake blockchain protocols that actually do this VRF based sampling do actually really care about the secrecy property, algorand being an example on the BFT type consensus side, cardano being an example on the longest chain consensus side.
00:07:09.430 - 00:07:11.266, Speaker B: So those types of projects really have.
00:07:11.288 - 00:08:18.402, Speaker A: To do some additional work just so that the consensus protocols that they're using accommodate this quirk of VRF based sampling, that it might produce zero leaders, it might produce multiple leaders. Because if you think back to when we were talking about BFT type consensus, like tendermint or longest chain consensus, there were rounds and there was always one unique leader for a round. That was by definition, we only considered all of those consensus protocols with one liter per round. So now if you're going to use VRF based sampling, you have to go back and say, oh boy, what if there's actually like seven legitimate leaders in a given round because of the quirks of VRF sampling? Like, what is my consensus protocol then supposed to do? Now, the first of the two bad cases. So the case where actually you get no samples, nobody's VRF output is sufficiently small intuitively, that should feel like it's maybe not that big a deal, because all the consensus protocols we've been looking at, they already have to be robust to things like a Byzantine node being chosen as a leader and then just remaining silent. Which is kind of the same thing as if there was no leader chosen at all. So obviously it's not good if nobody's chosen because it means it's like a time slot with guaranteed no progress.
00:08:18.402 - 00:08:26.890, Speaker A: So that's going to sort of slow everything down. But you might not expect that bad case to interfere with consistency or aliveness, and indeed it doesn't.
00:08:27.630 - 00:08:28.938, Speaker B: So the tricky case is going to.
00:08:28.944 - 00:09:15.770, Speaker A: Be that second bad case where in a single round we get, let's call it sort of seven legitimate leaders. So that's really going to require some modifications to the consensus protocol. And the modifications you want to make are going to be somewhat different for a BFD tech consensus protocol versus a longest chain protocol. So we'll get into all those details, but not right now. So in part three, those videos are going to be all about sort of the various issues that come up, extra details you have to handle when you try to couple proof of stake random sampling using one of the methods we're discussing here in part two with a consensus protocol like a tendermint or a longest chain protocol. So that's we're going to talk about sort of details on the interface between those two. And that's the natural place to discuss how you modify a consensus protocol to deal with the multiple samples coming out of this VRF based approach.
00:09:15.770 - 00:09:34.430, Speaker A: The third option is the most ambitious one, which is to say I don't want to make either of these two compromises. I want secrecy and I want a guarantee that exactly one public key is going to be sampled. So that problem now has a name which is Single Secret Leader Election or Ssle.
00:09:41.690 - 00:09:43.478, Speaker B: So this problem was first given a.
00:09:43.484 - 00:10:14.626, Speaker A: Name and formally defined in a 2020 paper by Bonet, Eskindarian, Hanslich and Greco. There's also been some follow up papers. I'll I'll put citations in the lecture notes for this lecture. So at the state of the art at the moment, speaking in early 2023, we have in principle solutions to the Ssel problem. There are constructions that in principle give us what we want. None of those have been deployed. So the practicality of any of these approaches is still sort of an open question.
00:10:14.626 - 00:10:41.980, Speaker A: So I would say this is on the very, very cutting edge of proof of stake blockchain design and the next generation of blockchain protocols, proof of stake ones coming out 2023, 24, 25. I wouldn't be surprised if we start seeing some practical implementations of those Ssle ideas. But speaking as of now, none of the deployed proof of stake blockchain protocols do use this approach. It's a little too experimental at this moment in time.
00:10:42.750 - 00:10:44.042, Speaker B: So this is one of two points.
00:10:44.096 - 00:11:04.814, Speaker A: Here in lecture number twelve where we're really going to bump up against literally the very cutting edge of what we currently understand, right? So single secret leader election and whether or not that can be done in a practical way, right? So that's sort of the first open question. And then the other sort of quite experimental tool we'll start discussing soon is verifiable delay functions or VDFS.
00:11:04.862 - 00:11:06.420, Speaker B: That's something to look forward to.
00:11:07.110 - 00:12:03.164, Speaker A: The final thing with respect to this VRF approach to proof of stake random sampling that I want to discuss is just a little more details on what does sufficiently small mean. So let's remember where sufficiently small comes up. So let's go to the previous slide. You want to be looking at the upper right part of the slide, and in particular that blue inequality. And remember, the whole approach to VRF based sampling is that a public key is considered sampled if and only if that blue inequality is true. This, of course, is largely inspired by Nakamoto Consensus and proof of work where solutions to crypto puzzles are defined to be valid solutions if, and only if they lead to a hash that's sufficiently close to zero. Where there, in Nakamoto Consensus, sufficiently close is specified by a difficulty threshold that's adjusted by the difficulty adjustment component of the protocol.
00:12:03.164 - 00:12:49.810, Speaker A: Here we're defining a public P as being sampled if and only if the corresponding VRF output evaluated on the current randomness r sub t if the VRF output is sufficiently close to zero. And so here again, sufficiently close is parameterized. We actually have two parameters, not just a threshold tau, but that also gets multiplied by some function of the amount of stake Q sub I that's been posted in escrow by the owner of the public key PK subi. So you should have a solid feel for the left hand side of that inequality. That's literally just the VRF evaluated with the person's private key on the current day's randomness. So we know what the left hand side is, but what is the right hand side in particular? What is tau? And maybe even more you might be wondering what is this function f?
00:12:55.260 - 00:12:57.224, Speaker B: All right, so let's just copy down.
00:12:57.342 - 00:13:01.400, Speaker A: That blue inequality here on this slide using our VRF notation.
00:13:09.680 - 00:13:11.008, Speaker B: So the first thing to remember is.
00:13:11.014 - 00:13:35.624, Speaker A: That we really do need some function little f of the stake Q sub I on the right hand side of this inequality because what are we trying to do? We're trying to sample public keys with probability proportional to the corresponding stake, probability proportional to Q sub I and the left hand side, if you look at it, the left hand side of this inequality is completely independent of the amount of stake associated with the public key.
00:13:35.662 - 00:13:35.816, Speaker B: Okay?
00:13:35.838 - 00:14:23.290, Speaker A: It depends only on the public key and the corresponding private key and it depends on today's randomness r sub t. But independent of what the corresponding q sub I is, that left hand side is always going to be the same thing. So if we want this inequality to be true more frequently with higher amounts of stake, so this public key gets sampled more frequently the higher the stake is. Well, then we need that right hand side to be increasing in the stake cusubi the more stake they have, the higher the right hand side, the more likely it is. The left hand side will be at most or strictly less than the right hand side. All right, so we definitely need the function little F. But then the question is, what should little F be? And the answer to that is really going to be dictated by the sybil resistance property that we want.
00:14:23.290 - 00:14:51.170, Speaker A: We want someone to be unable to manipulate the probability with which it's sampled, no matter how it might spread at stake over multiple identities. So I said to first order, you should think about little F as just being the identity function. So that the right hand side just scales linearly with Q sub i. That's approximately correct, especially for small stake values, q sub i, but it's not quite sybil resistant. So in the rest of this video we're going to derive using sybil resistance, what that function little F actually needs to be.
00:14:52.500 - 00:14:54.828, Speaker B: So let's now fill in some of those details.
00:14:54.924 - 00:15:25.388, Speaker A: I should say the rest of this video, it's important, but it is a little bit specialized, a little bit technical. So I understand if you just sort of want to skip the rest of this video and proceed to the next topic of pseudorandomness beacons. That said, my guess is a few of you are wondering about exactly how this sampling works. So I want to show you some of those details. Furthermore, we actually will be reusing this work in part three when we talk about how to stitch together proof of stake sampling with either a BFT type or a longest chain consensus protocol. So we will see it again a.
00:15:25.394 - 00:15:29.308, Speaker B: Little bit later in lecture twelve. The good news is that we'll be.
00:15:29.314 - 00:15:53.510, Speaker A: Able to fill in these details in a quite systematic way using a two step process. Step one, we're going to think only about some public key that has associated with it the minimum amount of stake. So just one coin if you like, that's going to lead us to a method of choosing the parameter tau, and then the choice of the function little f is going to be forced by silver resistance. So let's start with step one.
00:15:55.720 - 00:15:57.056, Speaker B: So let's assume that all the stake.
00:15:57.088 - 00:16:18.300, Speaker A: Amounts, all the Q sub I's, are integers, which would mean that the smallest possible stake amount would be qi equal to one. That's basically assuming that sort of stake is denominated in the smallest possible unit of the native currency that you're using. So for example, the Bitcoin currency, there is some smallest possible fraction of a Bitcoin that's ever considered by the protocol known as the satoshi.
00:16:24.510 - 00:16:26.346, Speaker B: So in this case, obviously, we only.
00:16:26.368 - 00:17:04.978, Speaker A: Care about the value of little f at one point, evaluated at one, and without loss of generality, we can just set F of one to be equal to one. That's just a normalization. So in that case, the right hand side of the blue inequality now just becomes tau, the difficulty threshold, the left hand side. Meanwhile, the VRF output, we're interpreting that as a uniformly random number between zero and one. So the probability that that's less than tau, well, that's just going to be equal to tau, right? If tau equals one over 100, then the probability that a uniformly random number from zero one, which is how we're thinking of the left hand side. 1% of the time it's going to be less than one. Over 199% of the time it's going.
00:17:04.984 - 00:17:06.380, Speaker B: To be more more than that.
00:17:07.630 - 00:17:56.230, Speaker A: So in this case, there's only one thing to decide, which is the parameter tau, which is equivalently the probability that any given public key, which by assumption has the minimum possible stake amount, one tau, is the probability that any given public key winds up getting sampled. So we just have to figure out what we want tau to be. Now, the probability of selection is presumably going to depend on how many public keys are registered in the contract, right? So if we have like 100 public keys in the contract, all with stake amount one, well, then maybe tau should be something like one over 101% chance that each person gets sampled. There's 100 of them. So that gives us an expected value of one public key being sampled. If there's 1000 public keys in the contract, then presumably we want tau to be smaller, maybe something like one over 1000, so that we again have in expectation one public key getting sampled.
00:18:03.440 - 00:18:05.208, Speaker B: So as a first cut tau equal.
00:18:05.224 - 00:18:40.270, Speaker A: To one over the total stake amount targeting an expectation one sample, that's a natural place to start, as discussed in the past. Maybe you want to fiddle with that a little bit, multiply it by two, divide it by two, et cetera, in order to trade off the frequency of the first bad case of having nobody sampled with the frequency of the second bad case of having more than one person sampled. So we've figured out what we're doing in kind of a ridiculously simple case where for some reason, every single public key in the Staking contract has posted only the minimum possible amount of stake. What are we going to do in general, for arbitrary Q sub eyes?
00:18:41.760 - 00:18:43.660, Speaker B: So that brings us to the second.
00:18:43.730 - 00:19:25.770, Speaker A: Step, which is really the clever bit. So, for example, imagine that there's some public key that has twelve coins associated with it. So you think about the person that owns those twelve coins and you say an option that person has available to them is to launch a civil attack. So they could generate eleven more public key private key pairs, and they could register under twelve different public keys with one coin each. That's certainly an option. That's something they could do. So if we're going to be civil resistant, we need to make sure that we treat that person exactly the same, whether or not they register one public key with twelve coins, or whether they register twelve public keys with one coin each.
00:19:25.770 - 00:20:08.228, Speaker A: But that, if you think about it, means we can actually reduce the general case of arbitrary Q sub eyes to the seemingly trivial case that we've already solved where all the QIS are equal to one. Like, imagine we're dealing with three public keys, right? So one that has twelve coins, another one that has seven coins, and another one that has 13 coins. Well, we just said we need to treat the first person as if they're twelve sybils with one coin each. We need to treat the second person as if they're seven sybils with one coin each, and the third one as if they're 13 sybils with one coin each. If we do all of those things, we're really just working with a set of 32, in effect, public keys with one coin each. And we already know what we want the sampling probabilities to be from step one. For example, maybe it's one over 32.
00:20:08.314 - 00:20:10.660, Speaker B: For each of those 32 sybils.
00:20:11.740 - 00:20:51.990, Speaker A: So that's going to be the solution. So the detail we need to fill in is like, what do I mean when I say treat a public key with twelve coins as if it was twelve public keys with one coin each? What exactly does that mean? That's actually a pretty tricky question to answer, and there's multiple answers that make sense. So to see what I mean, put yourself in the shoes of a proud owner of twelve coins who's going to register in the Staking contract and is debating whether to do it under a single public key with its full stake with Qi equal to twelve, or alternatively, whether to register with twelve identities. So twelve different public keys with one coin each.
00:20:53.000 - 00:20:54.516, Speaker B: So in the first scenario, if you.
00:20:54.538 - 00:21:08.280, Speaker A: Register under only one public key, obviously either that public key gets sampled or it doesn't. So presumably you're kind of bummed if you don't get sampled, you don't get to do anything, and you're sort of happy if you do get sampled. Because maybe you get to, for example.
00:21:08.350 - 00:21:11.936, Speaker B: Propose a block in the second scenario.
00:21:11.988 - 00:21:30.240, Speaker A: Meanwhile, where you have twelve public keys with one coin each, it's really not a binary outcome. Right now you have basically twelve different versions of the blue inequality, each in its own, presumably much less likely to be satisfied than before because the stake amount is one rather than twelve. But there are twelve different blue inequalities.
00:21:31.540 - 00:21:33.424, Speaker B: So totally possible that none of them.
00:21:33.462 - 00:22:15.200, Speaker A: Hold, meaning none of the twelve sybils wind up getting sampled. Totally possible that one of them holds means exactly one sybil gets sampled. Totally possible that more than one of the blue inequalities is true, in which case two or more of the twelve sybils would be sampled. So a key question to understand in getting the details of step two right is whether or not it matters if two or more of the twelve sybils get sampled. So in other words, if I'm contemplating a civil attack and spreading things over twelve different public keys, do I only care about at least one of those twelve sybils getting sampled? Or do I more generally care exactly about how many? Like do I get more and more value as more and more of my twelve sybils get sampled?
00:22:16.020 - 00:22:17.616, Speaker B: So the first version of step two.
00:22:17.638 - 00:22:39.140, Speaker A: That we're going to look at is going to assume that additional sybils getting selected beyond the first one offer no additional value. So all someone cares about is whether at least one of their sybils gets sampled. As we'll see, that is exactly the right version of step two to consider when we talk in part three about coupling proof of stake sybil resistance with longest chain consensus.
00:22:45.600 - 00:22:47.004, Speaker B: So that if you think about it.
00:22:47.042 - 00:23:06.880, Speaker A: Actually uniquely pins down the function little f. So for example, if you want to figure out what f of twelve is, well, that should be set so that the probability of sampling a public key with twelve coins is exactly the same as the probability of selecting at least one out of twelve sybils all with exactly one coin.
00:23:08.180 - 00:23:09.884, Speaker B: So that's how this two step approach.
00:23:09.932 - 00:23:40.888, Speaker A: Toward choosing the sampling thresholds works. Okay, so first you just figure out what you want tau to be tau again, the probability that you'd sample a public key with the minimum possible stake amount stake amount of one. Again, that's generally a function of the total stake. Maybe something like one over capital Q or capital Q is the total stake. And then you can generalize to all QIS just if you have some arbitrary stake amount q sub I. I'm just conceptually going to flip Q sub I different independent coin flips each coming up heads with probability tau. That's like trying to sample each of your would be q sub I sybils.
00:23:40.888 - 00:24:16.148, Speaker A: And then I'll sample you with probability that's exactly the same just by definition. I'll define the sampling probability as the probability of one of those q sub I biased tau coins coming up heads. And so what's cool is you can see without doing any computations, without any formulas. Why this approach must give you sybil resistant random sampling. Why, sort of a node cannot affect its sampling probability. Why, it does not matter. How it spreads at stake over various accounts.
00:24:16.148 - 00:24:34.210, Speaker A: Because at the end of the day, however many accounts it uses, and whatever the balance of each of those accounts, ultimately its Qi total coins are going to be treated as qi sybils in total. So it literally just doesn't matter how you split across accounts, your sampling probability at the end of the day is going to be the same no matter what.
00:24:35.220 - 00:24:37.216, Speaker B: So very cool how much understanding we.
00:24:37.238 - 00:25:16.256, Speaker A: Can have of this approach and why it's civil resistant without writing down any formulas. Of course, if you were going to code this up, you actually would need those formulas. You need to know exactly what is the formula for the right hand side of the blue inequality. And it turns out there's actually a very slick implementation of that. So let me just write that down here. So specifically this idea is equivalent to setting the right hand side of the blue inequality to the expression one minus e. Here e is the base of the natural logarithm 2.7
00:25:16.256 - 00:25:59.840, Speaker A: 118 dot, dot, dot, one minus e raised to the minus stake amount q sub I times a parameter mu. Mu here, that's just a reparameterization of tau. Specifically, mu is the natural log of quantity one over one minus tau. So let me just talk through this slick implementation a little bit to relate this to things we've said in the past. So first of all, remember what tau is, right? So tau is the probability that you're going to be sampling a public key with the minimum possible stake amount, like a stake amount of one. So tau is generally going to be a number that's quite close, just barely bigger than zero, right? Like if your total stake amount is 10,000, you might expect tau to be something like one over 10,000. Now let's look at Mu, which is this sort of function of tau.
00:25:59.840 - 00:26:35.468, Speaker A: Well, because tau is so close to zero, right, that means one minus tau is going to be barely less than one. One over one minus tau will be barely bigger than one. You take the log of that, you get something that's barely bigger than zero. So mu will itself be barely bigger than zero and in fact it'll be very close to tau when tau is small. So not a big difference between mu and tau. So moving on to the right hand side of the blue inequality, right, that has the form one minus e to the minus x, where here x is going to be the parameter mu, which again is basically the same as the parameter tau times scaled linearly by the stake amount q. Sub i.
00:26:35.468 - 00:27:08.340, Speaker A: And one way to understand this, the right hand side of this blue inequality, is to remember that e to the minus x is very close to one minus x when x is close to zero one minus x e to the minus x. If you just plot those two functions right around x, you'll see that they're very close to each other. And so that means e to the minus Q I times mu, that's roughly the same as one minus Q I times mu when Q I times mu is small, which actually means this right hand side is basically qi times mu, which is then basically qi times tau.
00:27:14.180 - 00:27:15.968, Speaker B: So you might remember back when I.
00:27:15.974 - 00:27:36.650, Speaker A: First introduced this function little f, I said, don't worry about it for now, just think of it as the identity function. And now you can see why I told you to think of it as the identity function, which is as long as tau times the stake amount q sub I is small, then in fact the right hand side is quite well approximated by just tau times the stake amount. So that would be like having the function little f equal to the identity function.
00:27:41.550 - 00:27:43.066, Speaker B: I think it's also instructive to just.
00:27:43.088 - 00:28:00.740, Speaker A: See this formula in action a little bit to connect it to sybil resistance. So as a thought experiment, let's think about someone who owns Q coins, like say twelve coins, and is contemplating representing itself with two public keys with stake amounts, q one and Q two, which sum to Q. So like five coins in one, seven coins in the other.
00:28:05.080 - 00:28:06.756, Speaker B: So if this person actually does put.
00:28:06.778 - 00:28:19.210, Speaker A: All twelve coins into a single account, well, then either it's sampled or it's not. And the probability with which it's sampled is just equal to the right hand side of this blue inequality. So one minus e to the minus Q times mu, like twelve times mu.
00:28:20.620 - 00:28:22.104, Speaker B: On the other hand, suppose the person.
00:28:22.142 - 00:28:55.780, Speaker A: Uses two public keys, like seven coins in one and five coins in the other. And remember our assumption that this person only cares about whether or not at least one of their sybils is sampled. It gets no additional benefit if both of its sybils get sampled. So let's think through that probability. In the second scenario, right, the probability that at least one of the two sybils, either the one with seven coins or the ones with five coins that at least one of those gets sampled, is one minus the complementary probability. The complementary probability being the probability that neither of the two sybils gets sampled.
00:28:56.600 - 00:28:58.084, Speaker B: So this expression is going to be.
00:28:58.122 - 00:29:32.956, Speaker A: One minus something, where for us something is the probability that neither of the two sybils, neither the one with seven coins nor the one with five coins, gets sampled. Those two events are independent, so we can look at the product of their probabilities. So the probability that the first public key does not get sampled times the probability that the second key does not get sampled. So think about the first one. What's the probability that the sybil with seven coins, say, doesn't get sampled? Again, one minus the complementary probability, one minus the probability that it is sampled. We know what that is. That's just the right hand side of the blue inequality.
00:29:32.956 - 00:29:52.356, Speaker A: That's just going to be one minus e to the minus q one times mu. Or in our running example, e to the minus seven mu. So for the first symbol probability of sampling, one minus e to the minus q I times mu, the complementary probability, the probability of not being sampled is just one minus that which is then just going to be e to the.
00:29:52.378 - 00:29:54.090, Speaker B: Minus q one times mu.
00:29:55.340 - 00:30:12.936, Speaker A: And of course, same thing for the second symbol probability that it is sampled, one minus e to the minus q two times mu. The probability that it's not sampled just e to the minus q two times mu. So the probability that neither is sampled is just the product of those two probabilities. Again, e to the minus q one times u times e to the minus.
00:30:13.128 - 00:30:17.708, Speaker B: Q two times mu. Okay, so of course those two exponents.
00:30:17.724 - 00:30:49.530, Speaker A: On the right hand side add. And so because q one plus q two is equal to q, actually these two expressions are exactly the same thing. So this formula confirms the civil resistance property. We are expecting that it really doesn't matter if you split your stake or not, the probability that at least one public key under your control gets sampled is going to be the same no matter what you do. We of course were expecting that given the conceptual discussion, given the strategy of just always treating someone with twelve coins, say as if they were twelve sybils with one coin each.
00:30:51.100 - 00:30:52.516, Speaker B: So that's the right way to define.
00:30:52.548 - 00:31:37.784, Speaker A: Sufficiently small in scenarios where there's no benefit to anybody from having more than one of its sybils selected. Which again, as we'll see in part three, when you couple proof of stake sybil resistance with longest chain consensus, this is generally the version you want to use. As we'll also see in part three of this video, if you couple proof of stake with BFT type protocols, you usually want something different, conceptually similar, but the details are going to be different, specifically in step two. Now let's think about the case where if someone's doing a sybil attack, for example, using twelve sybils with one coin each, let's assume that it actually matters to them. Exactly how many of those twelve sybils get sampled, as opposed to previously when we were just thinking about the binary predicate of whether or not at least.
00:31:37.822 - 00:31:40.360, Speaker B: One of the twelve gets sampled.
00:31:40.940 - 00:32:19.232, Speaker A: So we again want to be guided by an equivalence. We're looking for between two scenarios. Scenario one, where someone stakes, say, twelve coins under a public key. Scenario two, where they stake one coin each and twelve different IDs. But now let's think about the case where in that second scenario with the twelve sybils, the person actually cares about exactly how many sybils get selected. It cares not merely about whether at least one gets selected, but it really prefers two to one, it prefers three to two, et cetera. Well, if we want it to be treated the same in these two scenarios, now there's 13 different outcomes that could happen in the second scenario.
00:32:19.232 - 00:32:45.710, Speaker A: Anywhere from zero to twelve of its symbols could be selected. So I guess that means we need to modify what we're doing in scenario one so that there's also 13 different outcomes. So if we have someone staking twelve coins, it is no longer going to be as simple as just we either sample them or not. We're actually going to be sampling their public key with multiplicity. So we will sample them some number of times between zero and twelve.
00:32:47.440 - 00:32:48.736, Speaker B: Now that of course means we need.
00:32:48.758 - 00:33:19.252, Speaker A: To specify a probability distribution that these multiplicities are drawn from, right? So if someone stakes twelve coins, what's the probability that they're chosen zero times, what's the probability they're chosen one time, two times, et cetera. But again, we can just be guided by the fact we want them to be treated identically to as if they had twelve different identities with one coin each. Because in that case that corresponds to step one. We know what's going on. Each of those twelve identities, it's like an independent coin flip that comes up heads with probability tau. Remember, that's the definition of the parameter.
00:33:19.316 - 00:33:19.844, Speaker B: Tau.
00:33:19.972 - 00:34:18.844, Speaker A: So we just should say the multiplicity should be sampled from the exact same distribution you would get were there twelve sybils with one coin each. And actually that's a famous distribution. That's just the number of heads that come up when you flip Q coins each with bias tau, something known as a binomial random variable. Okay, so bin of Q comma tau, that's literally just the number of heads you get when you flip Q independent random coins, each of which comes up heads with probability tau. So symbol resistance, I hope, is kind of obvious without any formulas. Just from this conceptual discussion, it doesn't matter whether you stake twelve coins in one public key or whether you use two public keys to stake seven and five. Either way that's all going to wind up, it's going to translate to twelve independent random coin flips on your behalf and you will just be credited with the total number of those coins that come up with heads.
00:34:18.844 - 00:34:44.620, Speaker A: Doesn't matter if all twelve coins are flipped in a batch or if seven are flipped in one batch, five in a different batch, and the results added. Either way, it's going to be exactly the same thing. A little more formally, we're using the fact that when Q equals Q one plus Q two, the binomial distribution for Q coin flips with a bias tau that's exactly the same as the sum of two random variables with q one and q two coin flips respectively, and that same bias tau.
00:34:51.620 - 00:34:53.276, Speaker B: So in the first version of Defining.
00:34:53.308 - 00:35:43.176, Speaker A: Sufficiently Small right, we had sort of the conceptual discussion from which sybil resistance was conceptually clear. Then I gave you kind of the slick implementation which would be relatively straightforward to implement. So here for the variance where you're sampling with multiplicities, we've talked through the conceptual approach and again, why it should be sort of obvious that civil resistance is going to hold. Once again, there is a more slick implementation where you really just sort of evaluate a VRF and then look at sort of which interval it lies in to figure out what the multiplicity should be. I'll leave the details of that slick implementation as an exercise to the reader. So that finally wraps up everything I wanted to tell you about VRF based random sampling. And this really brings us quite close to the state of the art, the sort of the cutting edge of how modern proof of stake blockchain protocols are designed and implemented.
00:35:43.176 - 00:36:09.300, Speaker A: There's still one obviously quite unsatisfying thing about the discussion so far to this point. Assuming that R sub t is sort of perfect randomness that falls from the sky, we're assuming access to an ideal randomness beacon which probably doesn't even exist. So there's still the question in a concrete implementation, how are you going to define that randomness or pseudonyte randomness r sub t at each time step? And that is exactly the issue we're going to start addressing in the next video.
00:36:09.370 - 00:36:11.200, Speaker B: I'll see you there. Bye.
