00:00:01.290 - 00:00:52.842, Speaker A: Hi, everyone, and thanks for taking the time to check out my talk, a Theory of DeFi, here at the Acmccs Workshop on DeFi and Security. And let me open just by thanking the workshop organizers, so Philip and dawn and Roger and Arthur both for putting together such a nice workshop on a very timely topic. And also I'm very grateful for the opportunity to speak. So thanks very much to all four of them for the invitation. This talk is not going to be so focused on my own research, though at times I will allude to ongoing work in progress with a PhD student of mine at Columbia, Jason Milionis. So a common refrain you hear in DeFi is, it's still early, usually referring perhaps to investment opportunities or perhaps just to adoption. So the number of users thus far.
00:00:52.842 - 00:01:30.280, Speaker A: And while I agree with both of those two sentiments, I mean, here I mean something different. So here I'm really speaking as a theoretician. I do theoretical computer science for a living. And from a theoretician's perspective, boy, is it early in DeFi. And indeed, one really tricky thing to figure out as a theoretician is at what point is some technology kind of stable and advanced enough that it's worth your time to sort of work hard to build a really nice theory around it? On the one hand, you want to be sort of very forward looking. You want to produce theory that will actually help things that haven't been built yet. On the other hand, you don't want to work hard to produce some beautiful theory that's just obsolete in 18 months.
00:01:30.280 - 00:02:34.970, Speaker A: So I think it's an interesting open question whether DFI is sort of matured enough that it makes sense to start building a theory around it. I think reasonable people could disagree on that question. For the purposes of this talk, I'm just going to, for the sake of argument, assume that it's worth starting to build a theory around DeFi and postulate what that theory might look like. And to explore that idea, I want to spend just a few minutes kind of looking at the recurring patterns that we see over the last several decades of computer science research about how sort of useful theory develops alongside new technologies. So what's theory good for? What does theory bring to the table? Well, to be honest, the thing that sort of grabs the most headlines, like the lion's share of the spotlight tends to be quite deep and difficult theorems. And indeed, this is a fantastic contribution of theory to broader computer science. It also kind of feeds into that romantic ideal people have of Andrew Wiles working away secretly in his attic for eight years trying to solve proof Fermat's last theorem.
00:02:34.970 - 00:03:17.386, Speaker A: So on the rest of this slide, I'm just going to sort of flash briefly a bunch of examples of sort of what I mean, difficult theorems in different branches of theoretical computer science. It's not important that you read this or know any of these, my guess is one or two at least will be familiar to you. I'm sure some will probably not be familiar to you, and that's fine. The point here is just that theory has many, many branches. It's touched on many parts of computer science, many types of technologies, and often, once a theory matures, you do in the end get some quite deep, quite difficult mathematical results. Now, in DeFi, it's not really obvious we're ever going to have any interesting difficult theorems at all. We may definitely a possibility, I would argue that we don't right now.
00:03:17.386 - 00:03:59.986, Speaker A: But the good news is that even while the difficult theorems kind of grab most of the headlines, some really, really impactful stuff happens en route to those difficult theorems as prerequisite for a mature theory. So the first prerequisite of getting to the hard theorems is you got to prove the easy theorems. You got to prove the stuff that we teach to undergraduates and beginning graduates in our courses. But I don't want to give easy theorems short shrifts. It's not like their purpose in life is merely to assist us in proving heart theorems. These are often super, super interesting and clarifying in their own right. Indeed, our mental models for different parts of computer science are often shaped fundamentally by easy theorems.
00:03:59.986 - 00:04:46.994, Speaker A: You can prove, I'll argue later in this talk, that in DeFi we actually are starting to see some easy theorems which are indeed quite clarifying about how to think about the space. So easy. But clarifying theorems, I already think is sort of a big victory when you're building a theory around some kind of technology. But in fact, there's even a prerequisite to these, which is you need the language to even sort of phrase your theorems. So the pre, prerequisite that it comes out, usually comes out of a theoretical study is just the basic mathematical definition. So it's literally just naming, identifying and naming the key objects of study and introducing the words, the vocabulary to talk about the properties of those objects. And once again, in theoretical computer science, there are many, many examples.
00:04:46.994 - 00:05:24.878, Speaker A: In fact, I really think you could make a strong case that the biggest export from theoretical computer science to the rest of computer science and the rest of sciences and engineering are in fact our definitions right, NP completeness being one obvious example, but there's many, many others. And in DeFi, I would argue this is actually exactly where we are. So not only do we not have sort of an academically rigorous textbook on DFI, obviously way too early for that, but if we even think about what that book might look like, presumably we don't know what theorem 1.1 is. We don't know the first result. I don't even think we know what definition 1.1 is in that textbook.
00:05:24.878 - 00:05:56.522, Speaker A: I'm not even sure all of the words that are going to be used in definition 1.1 have been invented yet. So this is kind of what we're fumbling around with. And this is the usual kind of honorable, if humble beginnings of any theory. What are the sort of most fundamental objects, what are the most fundamental properties so that you can get to those easy theorems and start developing your mental model for how the area works. So that's how I would sort of expect a theory of DeFi to unfold. We're currently struggling with the vocabulary and the basic definitions.
00:05:56.522 - 00:06:39.306, Speaker A: Hopefully those start settling in the upcoming years. Hopefully we'll have some good collection of easy theorems. Maybe those are sort of en route to more difficult theorems. Maybe already our definitions and easy theorems will give us a super useful theory, which in itself would be a tremendous victory. So what do I mean by a helpful theory? What are we hoping to get out of this whole endeavor? Because often when you talk about theory, you're thinking very kind of blue sky. You're thinking kind of, let's understand the computational mysteries of the universe, right? The physicists are trying to figure out the Big Bang, and the computer science theorists are trying to figure out the P versus MP question, as they should be doing. That is absolutely a huge part of what computer science theory should be working on.
00:06:39.306 - 00:08:00.694, Speaker A: Now, for DeFi, this is probably not the main point, although I don't want to dismiss this possibility, right? So if you look at all of the sort of action going on with validity roll ups and various sort of scaling solutions for Ethereum and other smart contracts platforms, I mean, honestly, that absolutely crucial. Technology has its roots in work from the 1980s in theoretical computer science about understanding the mysteries of the universe. So this can definitely feed back into useful technology. But arguably for DFI, at least here in the early stages, this is not what we're trying to do so much. One thing we might be trying to do is look at the existing solutions that are out there. So existing protocols in DeFi and ask, can we do some kind of theoretical analysis of the properties? And maybe we sort of identify some weaknesses or maybe in the best case, we actually justify the existence of existing popular protocols by proving some kind of optimality theorem or characterization result or anything else which kind of says this is the right design in some sense of the word. But beyond just analyzing and perhaps justifying existing designs, theory holds the promise of doing better, of actually clarifying the design space and the unexplored parts of the design space wherein there may hopefully lie solutions even better than the ones we have now.
00:08:00.694 - 00:09:02.694, Speaker A: And again, this is something we've seen play out over and over again in different parts of theoretical computer science over the years. And at least on the second and third points, I would argue there are huge opportunities here for any theoreticians interested in engaging with DeFi. So on the second point, certainly there's plenty of protocols out there that we know and love, you know, the uniswaps of the world, et cetera. And frankly, most of them have very little formal analysis. And so I think that's a very impactful way to sort of start getting involved, ask exactly what mathematical properties do the different existing designs have? But then also, I don't think anybody feels like we've come up with the best way of doing trades or best way of doing lending. I think everyone's expecting there to be sort of more interesting mechanisms in the years to come in theory, actually really might be able to help guide us toward better designs, again by sort of articulating the design space, particularly the parts of the design space that we haven't really explored yet. All right, so let me start getting a little bit more concrete.
00:09:02.694 - 00:09:59.370, Speaker A: So suppose you wanted to contribute along the lines of the second or third point on the previous slide to either justify existing designs or sort of guide us toward new and interesting designs. How would you go about doing it? Definitely more than one answer to that question. But let me here advocate sort of one concrete approach which has served theoretical computer science very, very well over and over again over the past half century, which is to take an optimization approach. So in other words, you need to identify a design space. So what is the range of possible solutions you're willing to consider? You need to identify an objective function or maybe multiple objective functions that you'd like to optimize. And then the goal is to identify optimal or near optimal designs with respect to your objectives. So first step of the optimization approach, which again, we're still sort of, I think, grappling with very much in DeFi, is you need to articulate the design space, what is allowed.
00:09:59.370 - 00:10:37.238, Speaker A: There's no question that a big reason for the success of, say, approximation algorithms is that everyone agrees on the design space, polynomial time algorithms. So for different DeFi problems, what's the design space? That's a good question. I'll show you one part of DeFi where we sort of have an answer to that question in a couple slides. The second step is to articulate what you want. So what objective function you would like to maximize or minimize? Or maybe you have more than one objective function you'd like to study trade offs between them. That's fine too. So in most areas of theoretical computer science, in hindsight, the objective function looks super obvious.
00:10:37.238 - 00:11:46.346, Speaker A: Like, you want to minimize the running time, you want to maximize the fault tolerance of a consensus protocol. Whatever. I'd say in DeFi, at least right now, I don't know that we have a very good sense of what objective functions we should be looking at, though I think that is one of the things that's likely to lock into place over the next year or two, which should then really sort of enable research to accelerate. So I already mentioned how parts of theoretical computer science succeed because they have an agreed upon design space, but they really start to succeed and flourish when there's also an agreed upon objective function like minimizing the running time, minimizing the approximation ratio, et cetera. Do not underestimate the ingenuity that comes out from super smart people when you set up an agreed upon leaderboard and you can have your name at the top of the leaderboard as long as you come up with a design which is better on whatever agreed upon objective than all of the previous ones. That's been the source of an unbelievable amount of really cool results in theoretical computer science. All right, so in DeFi, I really think there's a lot of work to do focusing on what objective function should we be looking at once we've settled on one or a couple? Then again, we'd like to prove guarantees.
00:11:46.346 - 00:12:28.154, Speaker A: We'd like to prove that designs are sort of as good or almost as good as possible with respect to whatever metrics we've adopted. And in the DeFi space, I actually think it's super plausible that this three step framework will be carried out successfully in lots of different parts of the space. I think that's really a realistic goal for the next couple of years. Honestly, at the moment, we're basically in step one. We're just trying to figure out what does it mean to talk about all automated market makers or all lending platforms? What does that actually mean mathematically? Okay, so how might a theory of DeFi develop? Well, I would be very surprised if it develops top down. I very much think it's inevitably going to develop bottom up. Right.
00:12:28.154 - 00:13:08.620, Speaker A: We were just talking about the difficulty of articulating the design space. And if you start at the top level of sort of a logical tree like all of DeFi, I don't even think it makes sense to talk about articulating the design space of all of DeFi. Right, because DeFi includes many different things. You can focus on borrowing and lending platforms, you can focus on decentralized exchanges, you can focus on yield aggregations, yield aggregation protocols. You shouldn't expect to sort of have a unified theory of all of those. Each probably demands its own bespoke analysis. So I think what you want to do is you want to descend into this tree one or more levels until you get something bite sized that you feel like you have a handle on.
00:13:08.620 - 00:14:00.538, Speaker A: And so in the rest of this talk, we're going to look into decentralized exchanges and they actually come in many in quite different flavors. So for example, you've got order book designs that mirror, say, the New York Stock Exchange, and then you've got automated market makers like, for example, Uniswap. And so I think especially just to bootstrap the theory initially, I think it's totally fine to just zoom into subsets of what, you know, the true design space to be just again, to know at least a detailed understanding of a bite sized piece of the space. And that's what we're going to do here. We're going to actually ignore order book formats. We're going to focus squarely on automated market makers. I do want to give a brief shout out to a paper that showed up a few weeks ago by Chitra and Garris and Evans, which actually does try to compare the two types of designs, which I think is actually a great research question.
00:14:00.538 - 00:14:45.946, Speaker A: Formalized senses in which order books are better than AMMS or vice versa. Very early on it was noted that from a gas efficiency perspective, AMMS are superior. Order books generally take much more gas to run than an automated market maker. And Chitra at all also observed an advantage of AMMS in that they can reequilibrate after a loss of liveness with only o of one transactions, whereas order book designs tend to need more than a constant number of transactions to reequilibrate if they go offline for some period of time. Okay, but that's pretty much all I want to say about order books. The rest of the talk, I really want to sort of delve into the automated market makers. Now, even within AMMS, there's different kinds of designs you can look at.
00:14:45.946 - 00:15:23.702, Speaker A: And indeed, I'm going to zoom in even further to a subclass of AMMS known as constant function market makers. Now, the good news is this actually does capture kind of all of the popular examples you see in DeFi. So it's not clear we're losing very much from this restriction, but it will give us a more well defined theory problem. So ultimately, the rest of this talk, this is where we're going to be working constant function market makers. And this is what I mean by developing a theory of D. Five bottom up, specialize, specialize, specialize. Go down, down in the tree until you get a bite sized piece where you actually feel like you have a shot at articulating the design space.
00:15:23.702 - 00:15:56.126, Speaker A: Maybe you've been optimizing over that design space. Then once you've sort of figured out some of the leaves of this tree, then start advancing the frontier of knowledge upward. That realistically, I think, is how you get a reasonably general theory. So let's dive in and start talking about automated market makers. Maybe just one slide to just pause for some history. Just to point out that it's not like AMMS were invented for DFI. AMMS already existed even at the end of the 20th century.
00:15:56.126 - 00:16:41.370, Speaker A: And the original motivation was actually prediction markets. So prediction markets are systems where anybody can basically bet on the outcome of some event, which is uncertain but will eventually be verifiable. The canonical example here would be like a presidential election. So right now you could go to the IO electronic markets and place a bet on whether you think a Democrat or Republican is going to be the winner of the 2024 US presidential election. And the way you set up the betting in a prediction market is you invent some fictitious securities, aerodebrew securities, where each security corresponds to a possible outcome. That security pays off a dollar. If that outcome eventually occurs and it pays off zero otherwise, then you just sort of open it up and let people trade.
00:16:41.370 - 00:17:46.366, Speaker A: And basically, whatever the market prices are for the various securities, you can interpret as the market's belief in the form of a probability distribution over the various outcomes that could happen. So back a couple of years ago, if like, biden stocks were trading at trump, stocks were trading at $0.30, if you want, you could interpret that that the market believes with 70% probability, biden's going to be the winner. So, like with any exchange, there's a lot of different designs you could consider. And the initial prediction markets, beginning with the IO electronic markets, they just sort of took their inspiration from the New York Stock Exchange and they just implemented a sort of standard central, limit order book type design. And for something like the US presidential election, that's sort of fine, because you wind up having lots of liquidity, sort of low bid ass spreads, plenty of trading. But then once they expanded to having sort of lots of random congressional races, they encountered a problem you can have with order book designs, which is a lack of liquidity, big bid ass spreads, and just not much trading.
00:17:46.366 - 00:18:54.540, Speaker A: So not much ability to actually match, buy and sell orders. And so motivated by this lack of liquidity at a time when blockchains were not even a gleam in anyone's eyes, that's when automated market makers came along. So one early example which actually still exists is the Hollywood Stock Exchange, where you can use automated market makers to buy or sell securities corresponding to who's going to win the Oscar, how much sort of revenue is some film going to generate on opening weekend, et cetera. So what's the key difference between order book designs and AMMS automated market makers? Well, with an order book, right, to trade, you need a counterparty. If you want to buy a certain number of shares of some security, there needs to be some seller out there willing to sell you those shares at the price that you're willing to pay. And conversely, whereas with an automated market maker, the platform itself will always be willing to serve as the counterparty, as long as you're willing to buy or sell at whatever price it is that the AMM is courting to you. Now, by virtue of being willing to take either side of any trade at a given price, an AMM has to be willing to lose money, right? Because only one side of that trade is going to wind up in the money at the end of the day.
00:18:54.540 - 00:19:36.406, Speaker A: So AMMS, or at least the liquidity providers for AMMS have to be ready to lose money as trades occur. That's kind of the price of enabling trades at all times. So there's actually a super nice theory of automated market makers for prediction markets. I sometimes teach parts of this theory in my algorithm that game theory courses. And there's one AMM that sort of stands out as by far the most well known and most widely used in a prediction market context, which is known as LMSR. That stands for Logarithmic Market Scoring Rule developed by Robin Hansen at the beginning of this century. But my only point here is that automated market makers have been around for a while.
00:19:36.406 - 00:20:15.762, Speaker A: They actually have already shown that you can build a really nice theory of them, at least in the prediction markets context. And then AMMS sort of in a DeFi context are really tech transfer from this earlier work that's been done on prediction markets. So moving on to our primary focus, which is DEXs or decentralized exchanges. An exchange of course is a platform where you can buy, sell or trade various tokens. And of course in the blockchain world there's many, many tokens that people are interested in trading. And of course there's various ways you can implement an exchange. So for example, you can consider order book models and indeed centralized know coinbase and the like.
00:20:15.762 - 00:21:14.520, Speaker A: They largely do follow order book models and that's because they don't have to worry about gas costs, they're just doing all computation, kind of on their own servers. If you wanted to implement a decentralized exchange, for example, one that ran on top of Ethereum, then you really have to worry about gas costs. And order book designs are probably prohibitively costly. But interestingly, actually in the blockchain world there's a second reason other than gas costs to seriously consider automated market makers, which is the same reason that they gained popularity in the prediction markets world, which is a lack of liquidity. If you're trading kind of ETH versus USDC probably have tons of liquidity. But as we know there's a lot of long tail tokens out there and you can't expect to have a large amount of liquidity for all possible token pairs. So in the same way that you might want to have a platform which is sort of always ready for business, always ready to buy or sell tokens at a given price, even if it's a very long tail token, that suggests considering an automated market maker design.
00:21:14.520 - 00:22:01.220, Speaker A: AMM started being discussed for being built on Ethereum around maybe 2016, 2017. The exact order of operations here is not sort of completely clear to me, but it's clear that some of the early people discussing this idea were vitalik. Butyrin of the Ethereum Foundation and Martin Coppelman and Alan Liu from Gnosis and their proposal, I really want to break it down into sort of two different ideas, both of which I think are really interesting ideas. The first idea is really just a tech transfer idea. And the second idea is really a specific design choice, a novel design decision. So idea number one is just to sort of recognize one, we want exchanges, we want crypto exchanges. Two, we want to have alternatives to centralized exchanges.
00:22:01.220 - 00:22:43.420, Speaker A: Three, if we build it on top of a blockchain like Ethereum, you're going to have to do something other than order books. And they were aware of the popularity of LMSR in a prediction markets context of automated market makers. And so big idea number one is let's use AMMS on top of Ethereum to implement a decentralized exchange. So that's already kind of a really great idea, sort of tech transfer idea. The second idea actually, as far as I know, does not have an analog over in the prediction markets world. So they actually proposed an AMM that looks fundamentally different than the ones that had been used in prediction markets. So the specific design they suggested, the x times y equals K curve.
00:22:43.420 - 00:23:30.396, Speaker A: Nowadays we might refer to it as a special case of a constant function market maker or CfMM. And again, this is a really nice idea and as far as I know, as in 2016, this was a completely novel idea to do it this way. So what's the idea? The idea is you're going to encode what trades a trader is allowed to carry out through an invariant function, something I'm going to call little f is a function of the quantities of remaining tokens of each of the token types supported by the pool in this talk. Just think of K equal two. Just imagine we have a pool that has a tokens, it has B tokens. So the function is going to be a function of x the number of A tokens and y the number of b tokens. So when a trader does a trade, one of them is going to go up, like x will go up and y will go down or vice versa.
00:23:30.396 - 00:24:36.260, Speaker A: And we're going to call a trade allowable if and only if the increase in one is completely canceled out by the decrease in the other with respect to the function f. So you can trade from x comma y to x prime y prime if and only if f of x y is equal to f of x prime y prime. You'll notice there is no explicit price in this description. However, it does imply a spot price of, say, of token A in terms of token b according to the ratio of the partial derivatives of f with respect to the various token types. So for example, uniswap corresponds to the example of the invariant function f of x y equals x times y, the product of x and y. And here the implied price of a tokens denominated in b tokens is going to be y divided by x, which makes sense because if you have very few a tokens left, x is very small this is saying the spot price is going to be very high. As you have fewer and fewer of a tokens left, you're going to have to pay more and more for them in terms of the number of B tokens.
00:24:36.260 - 00:25:17.330, Speaker A: All right, so these two sort of big ideas reported at basically the same time really kicked off kind of AMMS in the DeFi space use AMMS to implement decentralized exchanges. And by the way, actually think maybe about this particular type of AMM as CfMM as your design. Most of the most popular DEXes out there are in fact special cases of Cfmms. They differ only in the choice of what invariant function little F. They use uniswap. And speaking here about uniswap v one and v two, the X times y equals K curves. As we mentioned, that's the special case of a CfMM in which the invariant function little f is the product of the two token quantities the product of x and y, with the implied spock price of y over x.
00:25:17.330 - 00:25:51.692, Speaker A: Balancer is a generalization of uniswap where there's an additional parameter, little w which controls how much sort of the value of the pool you want to be devoted to. Token A versus token B. So here uniswap would correspond to the special case at balancer, where W equals a half. A very different kind of CfMM would use the sum. So the total number of tokens of both types combined should stay the same. Unlike the other examples on this slide, this cannot support trading at all different prices. The implied price is always equal to one.
00:25:51.692 - 00:26:38.236, Speaker A: So you're doing one for one trades between token A and token B if you're using this invariant function. And then curve here again, speaking about v one for simplicity, this you can express as a weighted average of the first and the third examples. So of a uniswap X times y curve and also a sort of constant sum x plus y curve. And moreover, the coefficients in that weighted average actually depend on the ratio between y and X. If y and X are sort of roughly the same, close to the same quantities, then the X plus y term dominates. But once X or y gets once the ratio X over y gets close to zero or close to infinity, at that point, the uniswap type curve x times y is the one that dominates. Okay, but so the point is, these are some DEXes out there you may have heard of.
00:26:38.236 - 00:27:37.784, Speaker A: And Cfmms, it's a really nice abstraction that captures all of these examples as well as others. All right? So I hope that sort of, at this point in the talk, you maybe have the feeling that we've descended deep enough into that tree, mapping out the DeFi space, that now we have something bite sized enough, we might conceivably be able to prove some cool theorems. And in particular, let's imagine we try to carry out that three step. Optimization approach I mentioned earlier, where we articulate a design space, propose some objectives and identify some optimal or near optimal designs. How would that work if you wanted to apply that specifically to constant function market makers? Well, as usual, step one is just like, what's the design space? But here we're actually kind of halfway there, right? It's sort of clear that the design space is defined by this choice of the invariant function little f. Now, we're not done because presumably some F's make more sense than others. Presumably we don't want to think about the function F, which is zero on the rationals and one on the irrationals.
00:27:37.784 - 00:28:55.220, Speaker A: So we do want to ask the question like, what mathematical properties should we restrict F to have so that we get sort of a design space of sensible invariant functions? The other question we want to ask is, is the invariant function really the best way or the most sort of illuminating way to parameterize the design space? Or is there some alternative parameterization which might be equally or even more useful? So those are the things we want to be on the lookout for. Then question two is, can we prove guarantees with respect to some objective or some mathematical property for the existing designs? So, for example, if you believe that uniswap is the answer, what's the question that it's answering? Can we sort of have a post hoc sort of justification of, let's say, uniswap as the unique AMM that has certain properties? That's the second thing we want to be on the lookout for. And ideally, we'd want to have even a theory of optimal Cfmms, whatever that means. So with respect to some natural objective function, we would like to say like, oh, if this is what you care about, this is the little F you really should be using. This is the best CFM to use. That would be the best case. So let me address those questions one at a time, beginning with the first question about articulating and parameterizing the design space of constant function market makers.
00:28:55.220 - 00:29:45.320, Speaker A: And on this slide, I'm going to be following quite closely a model that's been explored in a very nice sequence of papers over the last year, plus by Guillerme and Garris, Tarun Chitra and Alex Evans. And in particular, they answered the question, what sensible restrictions should we impose on the trading function little f, so that it makes sense in the context of AMS. So let me explain first the mathematical property that they pointed out we should impose on little F. For simplicity, let's consider the case of just two tokens. I mean, the model and theory is sort of more general, but just think there's a tokens, there's b tokens quantities x and Y. Let's go ahead and draw the sort of uniswap curve, the X times y equals K curve. That's the one you see that sort of starts up high and then bends to the right as you go down.
00:29:45.320 - 00:30:31.770, Speaker A: And what I want you to notice is that if you look at everything that's to the northeast of that x times y equals K curve, that level set of the function x times y, if you look at everything to the northeast of that curve, that's a convex set, okay? And remember, a set is convex if it's completely filled in. So you take any two points of the set, the entire line segment between the two points should also belong to the set that's a convex set. So that's true certainly of the x times y equals k curve, as we see in this picture. But Angarisodal really noted that kind of whatever function F you want to think about, you should really think about functions that have this property. It doesn't have to be this specific curve x times y equals k. But it should be the case that the stuff to the northeast of the curve is in fact a convex set. So we'll be going with that also in this talk.
00:30:31.770 - 00:31:12.940, Speaker A: All right. So that's the first thing I wanted to mention that they pointed out really the sensible restriction on trading functions little F that we should look at. The other thing I want to highlight from that sequence of papers is an alternative parameterization of the design space, a way to parameterize Cfmms, not by the trading function little F, but by a different value function capital V, which I'll tell you about. Now, this is maybe my favorite result from the whole sequence of papers. It's from the paper called Replicating Market Makers. Okay? So what's the idea behind this alternative parameterization? What's the idea behind this function capital V? Well, the intuition is that capital V is going to be the value of a portfolio. And I say value here in terms of some numeraire.
00:31:12.940 - 00:31:40.448, Speaker A: Like think, for example, us. Dollars. So the value of a portfolio, it obviously depends on a couple things. So first of know how many tokens of type A there are in the portfolio, how many tokens of type B there are. Furthermore, it depends on sort of the external market prices. So the price on the open market for token A and token B, the more tokens you have, the bigger your value is going to be. The higher the going prices for those tokens, the bigger the value the portfolio is going to be.
00:31:40.448 - 00:32:38.268, Speaker A: But V is going to depend not just on the token quantities and not just on the external market prices. It will also depend on what AMM you are using. So what trading invariant function little F is available to arbitras because mind you, at equilibrium arbitrage, if they can make money by trading against your portfolio, they're going to do it using the AMM in which this portfolio rests. So the final definition then of the function capital V, it's just going to be the USD value of the portfolio after optimal arbitrage. After arbitraris have gone in and made whatever allowable trades that they want, meaning any trade that leaves the trading function little f invariant. If you think about it, there's sort of a zero sum game being played between the arbitrars and the portfolio owner. So equivalently, this is kind of like the worst case USD value of the portfolio after any allowable trade, any trade that leaves F invariant.
00:32:38.268 - 00:33:40.570, Speaker A: Okay? So that's capital v, the value of a portfolio, the notice depends on the choice of the trading function little f because that controls what it is arbitrariors are able to do. And so the big idea in this replicating market makers paper is then to parameterize the design space of all constant function market makers, not by the training function little f, but rather by the value function capital v. All right? So to call this a parameterization, a repramorization, we need to talk about sort of how v's and f's correspond to each other. And one thing that's very easy to notice, any value function capital v you get. So no matter what trading function little f you look at, you're going to get a value function that is one homogeneous. That sounds fancy, but all that means is if you double the market prices of both tokens, you're going to double the in USD terms, you will also double the USD value of the portfolio, which sounds obvious and is kind of obvious. And furthermore, you will also get a value function that is concave in the market prices for any fixed portfolio x comma y.
00:33:40.570 - 00:34:21.872, Speaker A: This concavity is related to the impermanence or divergence loss that liquidity providers experience in AMMS. Basically, as sort of prices go further and further apart from each other, it's worse and worse for you. So that's kind of the concavity of capital v. That's the intuitive way to think about it. So those are properties that are pretty easy to establish, right? You start from some trading function little f you pass to the value function, who knows what you get, but you certainly get something that's both concave and one homogeneous. And then the really cool theorem from this paper says that actually the converse is true as well. So if you show me any capital V which satisfies these two properties concavity and one homogeneity okay? And technically non negativity.
00:34:21.872 - 00:35:15.908, Speaker A: Also, then in fact, there exists a corresponding trading function little F. So in other words, if you want, instead of specifying little F, just specify for me a capital V. That's one homogeneous and concave in the market prices and I will be able to reverse engineer from your value function the trading invariant function little F you should use in your AMN. So this in my opinion is a quite cool result. It gives us really a fundamentally different and really non obvious way to think about the design space of Cfmms, instead via these capital V's, instead of the more obvious way through the little F's. Now, you might remember at the beginning of the talk I said, theory produces hard theorems sometimes, but to get there, you need easy theorems, and before that, you need definitions. And so this I definitely think qualifies as an easy theorem in the best sense of the word.
00:35:15.908 - 00:36:24.252, Speaker A: So a theorem which on the one hand one was able to formulate and prove in the very early days of DeFi. On the other hand, it might plausibly lead to sort of a deeper and more general theory in the years to come. So when I said we were starting to have some easy theorems in the DeFi space, this is one of the things I had in mind. So on the next slide I want to tell you about yet another reparameterization of the design space of constant function market makers that my student Jason Milones and I have been sort of exploring. Before I do that, maybe I just should comment on why do we want to understand this design space in multiple different ways? Why all these different parameterizations? But notice, I mean, that's something you see over and over again in mathematics and theoretical computer science, right? Like when you first learn the regular languages, you learn about regular languages, they're characterized by regular expressions, they're characterized by DFAs, they're characterized by NFAs. Why do you want so many different ways to represent the same object? Well, it's because some representations are more convenient for some results, other representations for other results. If you want to prove like closed under complementation, that's going to be much easier under some of the representations of regular languages than others.
00:36:24.252 - 00:37:31.056, Speaker A: So, so too, the hope would be that having all these different ways of understanding Cfmms will really help us build out a kind of very satisfying theory. So the alternative parameterization we've been looking at is via the spot price implied by the trading function which we think is a pretty natural thing to focus on. So by spot price, just to be clear, what I mean is the price of a marginal unit of token a denominated in token B. So this is what for uniswap, as we mentioned earlier, is y over x, where y is the number of B tokens and x is the number of a tokens. So the fewer a tokens that you have, the more expensive it's going to be for you to buy some more. For example, interestingly if you go back to Alan Liu's post introducing the x times y equals K curve, he actually starts by proposing the spot price and then backs out the now famous x times y equals K curve rather than the other way around. As far as I know, the other examples of Cfmms we've been looking at did not start from the spot price, didn't necessarily even focus on the spot price all that much.
00:37:31.056 - 00:38:32.496, Speaker A: There's a very nice geometric interpretation of focusing on the spot price as a function of the ratio of Y and x. Really, it's just kind of the slopes of that bounding curve of the convex set, right? So that we said, what are the properties that we want of the function little f? We want that if you look at everything to the northeast of its graph, that should be a Condex set. And this spot price is really just the slopes along that bounding curve of that region. And in our opinion, this parameterization may be at least if not more natural and useful than the two that I mentioned so far, rather than looking explicitly at the trading function little f or the value function capital V. So for example, again, it's in the eye of the beholder a little bit. But curve is usually stated in terms of its trading function as this sort of weighted average of the x times y equals k and constant sum trading functions. We think actually it's quite a bit easier to interpret curve.
00:38:32.496 - 00:39:31.876, Speaker A: I'm talking about V one here in terms of the spot price, right? So the uniswap spot price would be this y over x. And then you can see very cleanly how it gets modulated by the second term here. Alpha and beta are suitable constants. The numbers aren't really important for us here, but the point is this two x plus y over x plus two y, that's the sort of very simple way of modulating what would be the normal uniswap spot price. And now that we have this kind of alternative understanding of curve through its spot price, we feel better positioned to perhaps answer the question is curve optimal? In some sense, curve's goal is to sort of have a lot of liquidity around Parity while giving up stuff sort of at the extreme points. So we get asked does it sort of optimally implement that goal or could there be something better? And we suspect that this kind of spot price viewpoint is going to be crucial for answering those questions. So that's what I wanted to say about the first question, articulating the design space, parameterizing it.
00:39:31.876 - 00:40:24.900, Speaker A: And now we've seen three different ways. The second question was can we sort of justify existing designs in any way? So if you believe that uniswap is somehow the answer to something, what exactly is the question for which it's the answer? And here I want to mention some easy observations. I suspect these are known to experts, but I've never really seen them written explicitly like this. And I think if you haven't thought about this before, I do think it's helpful. So let's start with the x times y equals k curve. So the uniswap curve, here's something you can prove, right, which is that no matter what the market prices are so I was calling these PX and PY earlier post arbitrage. So again, assuming that Arbitragers will trade along the allowable curve, the x times y equals k curve, to maximize their revenue and minimize the portfolio value post arbitrage.
00:40:24.900 - 00:41:16.404, Speaker A: Both sides of a uniswap pool are guaranteed to have equal USD value. So as the prices change, so too with the quantities on each side of the pool at equilibrium. But in fact the cash value will always be equal the cash value of the A tokens in the pool same as the cash value of the B tokens in the pool. So that's the easy to deduce property about uniswap x times y equals K and in fact you can prove sort of a representation result saying this is the only way to do it. Okay, not quite. You can take some invertible function of x times y but those are the only trading functions, little F that have this property of guaranteeing at equilibrium equal cash values of both sides of the pool. Not so difficult to generalize this to balancer pools both in the sense of there being multiple tokens and in the sense of there being non uniform weights.
00:41:16.404 - 00:42:06.068, Speaker A: My sense is that actually maybe the designers of balancers a balancer were actually thinking in these terms when they proposed their AMM. But in any case, if you have a balancer pool with weights 0.7 and zero three, it is the case that no matter what the external market price is at equilibrium there will be a 70% 30% cash value split across the two tokens in a balancer pool. Finally, let me touch on the last of the three questions, maybe the most ambitious one. Could you have optimality results for Cfmms? This is totally wide open. Honestly, we don't even really, I think, understand what are the right objective functions with which to discuss optimality. That said, I think it's totally reasonable to expect nice research progress on this question over the next year or two and I'm eagerly hoping to see work along those lines.
00:42:06.068 - 00:42:57.956, Speaker A: A couple of specific things I'm wondering about. So first back to this question of to what problem is uniswap the solution? And there's this original intuition behind uniswap and the x times y equals K curve that somehow it ensures you have lots of stuff to trade with even as the prices move in a very volatile fashion. So is there some sense in which, like worst case, over all possible price movements uniswap does sort of the least bad sort of minimizes the slippage that you're going to have to suffer? That I think is part of the intuition that went into it. I've never seen that formalized as an actual guarantee. I mentioned curve on the previous slide. Its raise on detriment is to make sure you have very little slippage near Parity, near a price of one. But on the other hand, you make these compromises to make sure you actually can support trades at all prices.
00:42:57.956 - 00:43:31.844, Speaker A: So you could again ask the question is curve somehow optimal with respect to that goal? Or is the seemingly ad hoc proposed curve trading function could that be improved in some way. Also, I think a very interesting question. Intuitively, there should be like some kind of conservation of slippage property. So, like, curve, by virtue of kind of having less slippage around, parity should be suffering more elsewhere. Again, that's a statement. I don't know how to formalize that. You kind of have to trade off which part of the price range you want to do well in.
00:43:31.844 - 00:44:18.124, Speaker A: That seems intuitively true, but I don't know the right way to phrase that. And then really ambitiously, but I think not hopeless would be to have some very generic optimality result, really some kind of theorem that acts as a compiler where you feed into it your assumptions about what market prices are going to look like and then it feeds back to you. The trading curve, little F or the value function capital v or the spot price, whatever. It feeds back to you, the AMM, which is optimal with respect to your beliefs about future price movements. And really a role model here would be something from auction theory. Myerson's optimal auction theory. And that's a theorem which basically takes as input assumptions about the distribution of what people are willing to pay and gives you back as output the expected revenue maximizing auction with respect to that distribution.
00:44:18.124 - 00:44:42.604, Speaker A: So I think it's conceivable you could have something similar here. So I would not say we have a theory of DeFi. Yet I do think we could have a theory of DeFi emerging over the next several years. I would love to see some of the people who have watched this talk contribute. If you do, please keep me posted, send me your papers. I look forward to following progress on all of these topics. So thanks very much.
00:44:42.604 - 00:44:43.450, Speaker A: That's it for me.
