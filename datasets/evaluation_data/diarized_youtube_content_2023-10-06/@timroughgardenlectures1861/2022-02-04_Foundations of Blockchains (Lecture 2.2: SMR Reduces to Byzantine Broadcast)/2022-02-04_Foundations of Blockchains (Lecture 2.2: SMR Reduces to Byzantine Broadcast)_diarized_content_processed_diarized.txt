00:00:00.650 - 00:00:44.570, Speaker A: Okay, so now let's see how we would solve the SMR problem. Meaning have a protocol which satisfies both consistency and liveness solve the problem under the four assumptions we gave in the last video. So the permissioned assumption, the set of nodes is sort of fixed and known to everybody up front. The PKI assumption that everybody knows, everybody's public keys, the synchronous assumption that people have a global shared clock and that messages sent in a given time step arrived by the beginning of the next time step. And then also this really strong all honest assumption that literally no node ever deviates intentionally or unintentionally from the prescribed protocol. So achieving liveness and consistency, it's going to be pretty easy. But let me just point out that the absolute simplest thing you could do won't work.
00:00:44.570 - 00:01:20.946, Speaker A: The absolute simplest thing you could do is I'm a node. I never bothered talking to anybody else. Just when some client contacts me with a transaction I immediately add it to my local history to the end of my appendonly data structure. And this would be fine if a client set a copy of its transaction to all of the nodes at exactly the same time. But in general maybe a client only contacts sort of a single node and says hey, here's my transaction. And in that event different nodes might hear about different transactions in the same time step and so they would wind up committing then different transactions to their local histories, which of course would be a violation of consistency. The machines would be out of sync.
00:01:20.946 - 00:02:04.108, Speaker A: So at the very least we need some kind of coordination between the nodes so that they're all aware of the same set of transactions. And so the way we're going to do that is through the use of rotating leaders. Now notice that under our assumptions this is not hard to do, right? All the nodes are aware of the entire node set and their names. Everybody has a global shared clock. So we can just agree that in time step t node number t mod n is going to be this round's leader. So just a straightforward round robin over the nodes. They're totally in a position to implement that and it's going to be the leader's responsibility to coordinate the nodes during that time step.
00:02:04.108 - 00:02:38.244, Speaker A: So if I'm node number seven, it's a time step where I'm the leader. Here's what I'm going to do. I'm going to take all the transactions that I know about, that clients have told me about and that haven't yet been added to my local history. So just the outstanding transactions that I'm aware of. I am then going to sequence all of those transactions in some way. So I'm going to have an ordered list of the pending transactions I'm aware of and I will just send my ordered list to everybody else in this time step. And if you like, you can very much think of this ordered sequence of transactions that I send to everybody else as a block.
00:02:38.244 - 00:03:00.770, Speaker A: In the sense of a blockchain, it's really sort of the same thing ordered list of transactions. It's going to get added in a batch. Now, mind you, totally possible this list is empty, right? Maybe. I haven't heard about a transaction recently. And so I just sort of send all the other nodes the empty list. If I do know about a whole bunch of transactions, I can sequence them any way I want. For example, one natural thing to do would be to sequence them in the order in which I first heard about them.
00:03:00.770 - 00:03:52.926, Speaker A: In any event, I assemble this ordered list of transactions, I send it to everybody else. And remember, because we're assuming we're in the synchronous model, if I send all these messages to all the other nodes in timestep T, they will receive those messages at the beginning of the next time step, time step T plus one. So what does a node do when it receives an ordered list from the leader of the previous time step? It just appends it to its appendonly data structure. So it just tacks on that ordered list of the ordered sequence of transactions, tax it onto the local history it had up to that point. And so that's it. That's the whole protocol, right? So the code that a node is running, it's literally just doing two things. It's listening for the most recently broadcast ordered list of transactions by the previous time steps leader.
00:03:52.926 - 00:04:25.002, Speaker A: And when it receives it, it tacks it onto the end of its local history. And then secondly, it's keeping track of whether or not it's currently the leader. And for the time steps where it is the leader, it just assembles this order sequence of transactions in some way of the ones that it knows about and broadcasts that to everybody else. And under our assumptions, this very simple sort of rotating leader protocol does satisfy everything we want. It satisfies both consistency and liveness. So let's just talk through those two properties one by one. So first of all, consistency.
00:04:25.002 - 00:05:00.230, Speaker A: We'll notice that in this protocol, all the nodes will operate completely in lock step. Every time step, time step T, everybody's going to be hearing about the exact same ordered list that was sent by last timestep's leader, the leader of timestep T minus one. All the nodes hear that exactly the same list at the beginning of time step t. So they're all going to be adding exactly the same ordered list of transactions to the end of their local histories. So by induction, they all start with a common empty history. Every time step, they add exactly the same thing. So forevermore they will always have identical local histories.
00:05:00.230 - 00:05:38.950, Speaker A: How about liveness? So suppose a client submits a transaction to, let's just say, one of the nodes, node number seven. Well, it might take a while to get added to everybody's local histories. But at some point it will be node number seven's turn to be the leader. And at that point that transaction will be one of the outstanding ones it knows about. It'll include that transaction in its ordered list and then in the next time step, all of the nodes will add that ordered list to their local histories. So in other words, within N timesteps, long enough so that the node you sent the transaction to is guaranteed to become a leader. Within N timesteps, a submitted transaction will in fact be added to everybody's local histories and that's liveness.
00:05:38.950 - 00:06:23.642, Speaker A: So in that last sentence, the argument for liveness, that's the point where we really needed there to be a rotating leader, okay? Because maybe not all nodes have heard about a transaction. So we need to sort of take turns so that the node that knows about a transaction has the opportunity to add it to everybody's local histories. You'll note it's not like we needed to do round robin. That's a very natural way to do it. But for example, if you just pick a random leader, uniformly a random every time step, again every node is going to become a leader infinitely often. And so we'll eventually get the opportunity to add all the transactions that it knows about. I mentioned that because sort of random leader selection becomes actually quite natural once we talk about permissionless protocols and different civil resistance mechanisms.
00:06:23.642 - 00:07:12.590, Speaker A: For now, if it's sort of simplest in your mind, just think of them as doing this round robin. So that's a solution to the SMR problem in the sort of very relatively simple scenario where all four of those assumptions hold. So we want to now push toward relaxing them. We're going to start with the last one, okay? The all honest assumption. That's not even a reasonable assumption in the best of all worlds because again, remember, honesty is not about intention, it's really about behavior. So like if you're in the synchronous model, honest behavior means your messages arrive in the next time step. So if you're a node participating in a protocol supposedly in the synchronous model and you wind up having your message delayed more than say, 10 seconds, right, because of some network outage, you will not count as an honest node because you will be deviating from the protocol unintentionally.
00:07:12.590 - 00:08:02.126, Speaker A: But it's nevertheless a deviation. We can't count it as an honest node. And the general term we'll use for nodes that whether by intent or whether by accident wind up deviating from the intended protocol, they will be referred to as faulty nodes. The question then is what should be our model for a node which is not honest, our model of a faulty node? In other words, what deviations from the protocol should we be willing to consider and protect against? And so this question has been studied really in great depth in the distributed computing literature. There's a lot of different models of faulty nodes that have been considered. Just to give you some of the broader context, I'm going to just mention three sort of common ones on the slide, starting from the strongest assumption to the weakest assumption about faulty nodes. Now, to be honest, I'm going to tell you three because I want you to know some of the history here.
00:08:02.126 - 00:08:49.610, Speaker A: But really it's going to be the third of these three, which is going to be the only one that's relevant in our discussion of blockchains, where we really can make very few assumptions about behavior on account of the permissionless and very high value nature of those systems. So the first type of fault we're going to discuss is a very natural one and it's a crash fault. So you're a machine running a protocol and literally someone just pulls out the plug and the machine goes down. And so this means exactly what you'd expect. It means you start at time zero, and this is a machine that will just behave honestly up till some failure time t star, and then after t star, it doesn't participate in the protocol at all. It doesn't receive any messages, it doesn't send any messages, it doesn't do anything. And you can imagine why researchers thinking about like database replication back in the 1980s would have been interested in crash faults, right? So IBM running at sort of seven different machines.
00:08:49.610 - 00:09:39.978, Speaker A: If the primary concern was hardware failures, which kind of in the early days of computer science, it actually was more than software failures. That's what you really want to protect against. So you're only worried about machines going down, as opposed to you're less worried maybe about software bugs, maybe not worried at all about malicious attacks on these seven servers living inside IBM. Now notice that if we only had to worry about this most benign type of fault, these crash faults, actually the protocol from the previous slide with the rotating leaders, that would actually basically work. So there's sort of two things that would go a little bit awry if some of the machines went down. So the first thing is that if a client submitted a transaction only to machines that wind up going down, then obviously we can't promise liveness for those particular transactions. So we have to only focus on transactions that are known to some node that's up and running, some honest node.
00:09:39.978 - 00:10:06.662, Speaker A: Okay, so we tweak liveness a little bit in that way. And then if you think about it, like consistency is totally fine, right? You're still going to have these time steps. Some time steps, there will be no leader or the leader will have crashed. So everyone will just sort of add nothing to their local histories. And then whenever you have a sort of honest leader again, they're going to send out this list of all the transactions they've heard of and everybody will add those. Everyone will continue to operate in lockstep. So with crash failures, you might expect to not make as much progress progress as quickly.
00:10:06.662 - 00:10:53.698, Speaker A: You might expect to lose transactions that were sent only to the crash nodes, but otherwise everything's fine. In particular, you never have violations of consistency in the presence of crash faults in that simple rotating leader protocol. So that point leads me naturally into the second type of fault I want to mention, which is an omission fault. So with an omission fault, the faulty node may fail to deliver some of the messages that it's supposed to send out. In other words, the faulty node will never make up a message that it's not supposed to send under honest behavior for this protocol. But it may, however, not send a message it is supposed to send had it been following the protocol honestly. Now, clearly one reason you could suffer from an omission fault is you may have a node operating in bad faith that's actually trying to mess up the protocol.
00:10:53.698 - 00:11:36.806, Speaker A: But notice, even with no bad intent, you might actually be suffering from omission faults. Honest behavior in a synchronous protocol usually requires that your messages arrive somewhere by a certain time step. So if through no fault of your own your messages wind up getting delayed and they don't make it to their recipients by that time step, you would actually count as a dishonest node. You would be then an omission fault. Due to the network delays, notice that an omission fault is at least as severe as a crash fault because obviously one thing this faulty node would be allowed to do would be to just withhold all future messages and never send anything again. Then it would be exactly the same as a crash fault omission fault. It can send some of them, but then also not send all of them.
00:11:36.806 - 00:12:23.374, Speaker A: And actually that slightly more powerful model of a faulty node actually messes up. If you think about it, the rotating leader protocol that we had on the previous slide, there's no issue with liveness. Good things still happen when an Odyssey node winds up being the leader. But you can violate consistency if you have a faulty node as the current leader, right? Because with an omission fault the leader may send a non empty list of transactions to some of the other nodes, but then it would withhold those messages from the other part of the nodes. So half the nodes would be getting a non empty set of transactions, half the nodes would be getting nothing. And they would then commit different things to the ends of their local histories. Half of them would add nothing to their local histories, half of them would add a non empty set of transactions.
00:12:23.374 - 00:13:04.478, Speaker A: And that's an immediate violation of consistency. The histories are not the same at that point, so emission faults are already enough to mess up that simple protocol from the previous slide. So that brings me to the final type of faulty node I want to discuss the one most relevant for blockchains, which is the idea of a Byzantine fault. So with Byzantine faults, when some of our nodes are Byzantine, we actually don't make any assumptions about their behavior at all. Okay, well, we still assume that cryptography exists and that they can't break cryptography. But as far as how they participate in this consensus protocol, we make literally no assumptions about what they do. So Byzantine faults, I mean, these were not invented to analyze blockchains.
00:13:04.478 - 00:14:05.458, Speaker A: They were invented way back there in the 1980s. Because back then, even for those old school sort of applications, the thinking was that if a machine running, the protocol goes down, if there's a power outage that you know how to model, the machine just goes away, packet drops, maybe you know how to model. Right? That's sort of the emission fault. But what if you have, like, software bugs? So what if the protocol is actually someone downloaded a buggy version of the protocol they're supposed to be running? Well, no one was very comfortable in sort of making any specific assumptions about how buggy software would behave. So as theoreticians, they said, well, let's make no assumptions and see what we can do and what we can't do. And now, again, one of these amazing twists of fate, right? Fast forward 30, 40 years. Now that we're talking about blockchain protocols securing billions of dollars, actually, this sort of worst case adversarial model of Byzantine nodes is literally exactly what you want to optimize for, right? So once these protocols capture enough value, you need to really be actively assuming that there are people that want to bring down your protocol.
00:14:05.458 - 00:14:49.358, Speaker A: So it becomes very much like cryptography at that point, where really a sort of worst case model of sort of what you're up against is the appropriate one. You may be wondering about the terminology here. Like why is sort of a sort of arbitrary fault known as a Byzantine fault. So a very long time ago, the city of Istanbul, even before it was known as Constantinople, was known as Byzantium. And so as a result, kind of colloquially, byzantine has come to be a stand in for things that are potentially devious. And so this idea of a Byzantine fault was introduced in a very famous paper by Lamport, Chassis and Peace. Lamport is actually a Turing Award winner to the Nobel Prize of Computer Science for his work in distributed computing.
00:14:49.358 - 00:15:37.182, Speaker A: And so in that paper, they introduced some of the problems we're talking about through an allegory of a bunch of generals of the Byzantine army trying to come to consensus of whether to attack a city at dawn, even though they knew that there were at least a few traitors amongst them. And so the name has sort of stuck ever since then. So a Byzantine node means a node that can do absolutely anything, really, that wants nothing more than to mess up your protocol. Now you see this and you're like, okay, behave arbitrarily like I guess, but still you might want to have some mental model of what a Byzantine node is probably trying to do to mess up your protocol. Again, we're assuming that they can't do things like forge digital signatures. So they must somehow be trying to manipulate the protocol through the messages that it sends and the canonical ploy for Byzantine nodes. And we'll see examples of this over the next several lectures.
00:15:37.182 - 00:16:03.340, Speaker A: The canonical thing that they do is they behave in an inconsistent way. So they'll basically tell some of the nodes one thing and some of the other nodes something else, which is conflicting information. With an omission fault. We already see how things can go wrong with inconsistency. There are, some of the nodes were told nothing, some of the nodes were told something. And so with a Byzantine node can go even further than that and tell all nodes something. But the information sent to one of them are in direct conflict with the information sent to the rest.
00:16:03.340 - 00:16:40.134, Speaker A: So that's going to be our model of dishonest nodes. We're going to allow them to behave totally arbitrarily, we're going to allow them to be Byzantine. Now of course, if all the nodes running a protocol are Byzantine, right, they can just throw out the protocol and do whatever they want. So there's nothing you can say about that situation. So what's really interesting for us is when some of the nodes are honest and are running the protocol in good faith and some of the nodes are faulty and either intentionally or unintentionally deviating from that protocol. And what we'd like to say is that the honest nodes still achieve the desired functionality even in the presence of the faulty nodes. So the new version of assumption four, remember the old version was that all the nodes are honest.
00:16:40.134 - 00:17:02.506, Speaker A: And the newer version of assumption four is going to be that at least a certain number of the nodes are honest. So there's going to be a parameter, little f. This is an integer between zero and little n. And f is the largest number of faults that we want to protect ourselves against. If there's more than faults, all bets are off. We make no promises. So our new assumption is that there's going to be at most f faulty nodes.
00:17:02.506 - 00:17:48.602, Speaker A: And again for us this will mean Byzantine faults and then at least n minus f honest nodes. So the previous version of assumption four corresponded to F equals zero. Now we're going to allow F's that are nonzero, okay? So that's a weaker assumption. It's making our life more difficult. We've already seen that the simple rotating leader protocol does not satisfy consistency. So now the question is, can we have a sort of new, maybe more complicated protocol that solves the SMR problem even in the presence of up to F Byzantine faults. Now one of the aspects that makes this question difficult is that while we know upfront a bound on the number of Byzantine nodes, maybe there's the most ten of them or whatever, we are not going to know in advance which ten nodes are the Byzantine ones.
00:17:48.602 - 00:18:50.000, Speaker A: If we did, everything would be easy, right? All of the honest nodes could literally just ignore any of the messages involving the Byzantine nodes, pretend they don't exist and just run the protocol amongst themselves as if they were all honest. So what makes it tricky is like, you know there's some Byzantine nodes out there and you don't know exactly who they are. We will be assuming a static set of Byzantine nodes. So basically before the protocol ever begins, some subset of it, most F nodes are designated as Byzantine and the protocol doesn't know which ones it is, but that set will remain the same throughout the entire execution of the protocol. Okay, so now it seems like it's back to the drawing board, right? We did solve the SMR problem in the case of all honest nodes, which in our new terminology, f equals zero. But we also saw that once F equals one, that protocol no longer works. So now we have to go back and say in the presence of faults with a nonzero little F, is there a protocol that satisfies both consistency and liveness? As we'll see, the answer is yes.
00:18:50.000 - 00:19:21.990, Speaker A: Now, at a high level we're going to stick with very much the same approach that we had in our simple protocol. Namely, we're going to use rotating leaders. So for concreteness, go ahead and think about the leaders taking turns round robin. So time step one, node number one is the leader. Time step two, node two is the leader and so on. Now, presumably in time steps where the leader happens to be an honest node, that's presumably not the hard case. We're really worried about the time steps in which the leader winds up being dishonest, winds up being Byzantine.
00:19:21.990 - 00:20:02.982, Speaker A: And in particular, like in the example, we're worried about the honest nodes getting tricked into inconsistency because of conflicting messages sent out by a Byzantine leader. So that's the hard case that we have to deal with. So we're going to do next is we're going to define a subroutine, the Byzantine broadcast problem, which is a famous single shot consensus problem in its own right. We're going to spell out what we want from this subroutine and then we're going to show that when we embed this subroutine in the round robin leader approach, we'll be fine. We'll get a protocol for SMR that satisfies both consistency and liveness. All right? So the setup is we're going to have the same four assumptions as before. We're going to be permissioned, we're going to have PKI, we're going to be in the synchronous model.
00:20:02.982 - 00:20:31.562, Speaker A: We'll have some bound little F on the number of Byzantine nodes. And on top of that, one of the N nodes is going to be designated as a sender. If we're thinking about the rotating leaders, the sender is going to be the leader for that time step. But in this subroutine there's end nodes. One of them is the sender and everybody knows who it is. At the start of the protocol, everyone knows node number 17 is the sender. Now this sender has something that they want to communicate to all of the other nodes.
00:20:31.562 - 00:21:21.454, Speaker A: So in the context of state machine replication, this would be like the ordered list of transactions that the leader of a timestep wants to communicate to all the other nodes. Precisely. We're going to say that the sender has in its possession a private input. So here by private what I mean is this information is known operate only to the sender and the other N minus one nodes have no idea what it is. I'm going to use little V Star to denote this value that the sender wants to communicate and that's drawn from some sort of set of all possible messages they might want to send capital V. So again, in our SMR context, V Star here is going to be an ordered list of transactions that the sender wants to communicate to everybody else. Capital V would be like all conceivable ordered lists of transactions a sender might be wanting to send out.
00:21:21.454 - 00:22:20.382, Speaker A: So our intention for this Byzantine broadcast subroutine is to play the role of one of the steps in our simple rotating leader protocol. So back when we were assuming all the nodes were honest, when it was a node's turn to be the leader, they would just sort of directly send their ordered list to everybody else and everybody would just believe them because by assumption everybody was honest. So here again, when it's a particular node's turn to sort of try to send out their ordered list of transactions, they again want to communicate that to all of the other nodes, specifically the other honest nodes. But now we want to achieve this broadcast functionality even in the presence of faults, even in the presence of at most f Byzantine nodes. So how do we make that precise that a subroutine actually accomplishes that? Again, just like with the SMR problem, we need to be a little bit careful about what we mean by correctness. Again, we're going to insist on one safety property and one liveness property. Let me also throw in one other preliminary requirements which I'll call termination.
00:22:20.382 - 00:23:14.758, Speaker A: So any honest node running a protocol should eventually terminate and we insist that it terminates with sort of a guess of what it thinks the sender was trying to communicate to it. Our next goal is going to be our safety property and here it's going to be called agreement. So agreement is a safety property. It's asserting that some bad thing never happens. And for us the bad thing we want to always avoid is we want to avoid the honest nodes getting confused and outputting inconsistent guesses about what it is that the sender was trying to send them. Now importantly, we insist that agreement is satisfied whether or not the sender is honest or Byzantine. Remember, safety Properties says the bad thing never happens no matter what.
00:23:14.758 - 00:23:56.702, Speaker A: So in particular, disagreement better not happen even with a Byzantine sender. Finally, we have our liveness type condition which here is going to be called validity. So here we're going to say in the event that the sender happens to be an honest node, we will require that its private input is indeed successfully communicated to all of the other honest nodes. So by agreement we already know that the honest nodes output the same thing if the sender was honest. We additionally require that that common value that they all output is the private input of that honest sender. Two quick comments. So first of all, all of these properties we require only of the honest nodes.
00:23:56.702 - 00:24:36.430, Speaker A: Byzantine nodes we have no control over, right? We can't force them to halt. They might run forever, nothing we can do about it. We can't force them to agree with anybody else, they can sort of guess whatever they want and we also can't force them to be convinced by an honest sender. So in all three cases, the property we want, it should hold for the honest nodes in validity. Additionally, we only require those properties if the sender happens to be honest. The second comment is it's really the tandem of all of these properties and in particular both properties two and three at the same time, which is nontrivial. If all we wanted to do was to satisfy termination and agreement, well, we could just have all the honest nodes always output some canonical value.
00:24:36.430 - 00:25:14.614, Speaker A: Like they could always just output the empty list of transactions, right? They would never disagree because it's hard coded what their output is going to be. If all we wanted to do was satisfy termination and validity, that would also be easy. Basically all of the nodes could just trust that the sender was honest and just output whatever they told them. Again, you're going to get a violation of agreement in the case that you have a dishonest sender. But this trivial protocol where you just sort of believe the sender does satisfy both termination and validity. And so that's the Byzantine broadcast problem. This is how we talk about achieving broadcast functionality.
00:25:14.614 - 00:25:47.750, Speaker A: Even in the presence of Byzantine faults. It should be the case that the honest nodes always come to agreement. And moreover, with an honest sender, their private input is accurately communicated to all of the honest nodes. So this I claim, is the subroutine we want to plug into our rotating leader idea to get consistency and liveness for state machine replication. On the next slide, let's see that that really is the case. So in other words, let's really prove formally that the state. Machine replication problem reduces to the Byzantine broadcast problem.
00:25:47.750 - 00:26:29.646, Speaker A: We use the same assumptions in both cases. Remember, we're assuming the permission setting, we're assuming PKI, we're assuming the synchronous model and we're assuming that there's at most f Byzantine nodes. We want to show that if you hand me a protocol for Byzantine broadcast that under those four assumptions satisfies both agreement and validity and termination, I can build from the Byzantine broadcast subroutine you handed me, I can build an SMR, a protocol for the SMR problem that satisfies consistency and liveness. And here's how I'm going to do it. So we're going to use rotating leaders. So the first step will be the exact same first step that we had a couple of slides ago. The nodes will rotate.
00:26:29.646 - 00:27:28.386, Speaker A: They will take turns as acting as a leader. Now it's the second step where we replace our previous naive broadcast, which was tailored for the Allana setting with our assumed subroutine for Byzantine broadcast. So with a given choice of a leader at a given time step, we're going to run Byzantine Broadcast. Where the sender in the Byzantine Broadcast subroutine is the leader the current leader at the current time step. In our SMR protocol, the private input that the leader is supposed to communicate in the Byzantine Broadcast subroutine is the ordered list of transactions that that machine is aware of. So the Byzantine broadcast subprotocol concludes with all nodes having a guess of what they think the sender was trying to communicate to them, which is going to be some ordered list of transactions, possibly the emptied list. That's fine, but everyone's going to conclude step two having some ordered list of transactions that they believe is what the sender was trying to communicate.
00:27:28.386 - 00:28:08.098, Speaker A: And so now step three is just like what it was before. Now that each node locally knows an ordered sequence of transactions, it thinks it's supposed to add to its local history, it go ahead and adds it and that is the entire reduction. So you give me a subroutine for Byzantine broadcast. I'll just use rotating leaders. Each time step I'll invoke Byzantine broadcast subroutine with the current leader as the sender, all of the nodes participating in the broadcast protocol will come to some kind of agreement on what transactions need to get added. And then all of the nodes just independently add those transactions to their local histories. So I hope you agree that is in fact some kind of protocol for the SMR problem.
00:28:08.098 - 00:28:39.134, Speaker A: We still should really prove that it's a correct protocol. I e. That the reduction works. Well, again, for SMR, what correctness means is satisfying consistency and liveness. And this is going to work just as you'd hope, where the safety property for Byzantine broadcast, namely agreement, is going to imply the safety property for SMR, namely consistency. Similarly, the liveness type validity guarantee for broadcast will imply liveness for the constructed SMR protocol. So let's take those one by one.
00:28:39.134 - 00:29:24.182, Speaker A: Why does the safety property for Byzantine broadcast imply the one for SMR? Okay, well, so this broadcast subroutine satisfies agreements. And so remember what that says. It says no matter what, doesn't matter if the sender is Byzantine or not, no matter what, the honest nodes all agree on the same value at the end of the day. So, looking at our SMR protocol, that means at the end of step two, whether or not the current leader is Byzantine, it will be the case that all of the honest nodes agree on exactly the same ordered list. Of transactions capital l. Because they agree on capital l. In step three, all of the honest nodes will be adding the exact same thing capital l, to their local histories.
00:29:24.182 - 00:30:12.140, Speaker A: So, just like in our naive protocol, we have consistency because the nodes will operate completely in lockstep, every single time step, they'll be adding exactly the same new list of transactions to their append only data structure so far. So again, by induction, they all start with the empty history. Every time step they have the same thing. So their histories are always exactly the same. And that's SMR consistency. So let's see the other statement that validity of the Byzantine broadcast subroutine implies liveness for the constructed SMR protocol. So again, because we have faulty nodes, we're again using sort of that slightly modified version of liveness, where as long as a transaction is known to some honest node, then at some point it gets added to all the honest nodes local histories, right? If only Byzantine nodes know about a transaction, they can all just pretend like they've never heard about it.
00:30:12.140 - 00:31:13.818, Speaker A: So consider a transaction that gets submitted to some honest node. Now, because we're using rotating leaders, eventually that honest node will become the leader in our SMR protocol. In that time step, we will be invoking the Byzantine broadcast subroutine with this honest leader as the sender. The private inputs of this honest sender is an ordered sequence of all the transactions that it's aware of that are still pending. Now, by the validity requirement for Byzantine broadcast, because we're in a time step with an honest sender, it will be the case that its private value, meaning this ordered sequence of transactions, including all the pending transactions it knows about, that ordered sequence is going to be the sequence that gets communicated to all of the honest nodes. That is the sequence that will be added to all the honest nodes local histories in this time step. So in particular, that transaction which was known to this honest node will get added the very first time that that node winds up being the leader.
00:31:13.818 - 00:31:57.366, Speaker A: All right, so this is actually really cool, right? So we took the SMR problem, which is really multi shot consensus. It's coming to consensus over and over and over again. And we've reduced that to a seemingly simpler single shot consensus problem, namely the Byzantine broadcast problem. So I've shown you that if you give me a solution to the latter satisfying validity and agreement and termination, we get a solution to the former satisfying consistency and liveness. Now, this reduction, of course, would be no use at all if we could fulfill the promise if we couldn't actually implement a fault tolerant Byzantine broadcast subroutine. So that's the next order of business that's going to bring us to the dole of strong protocol. But before we get to the final solution, I want to build up your intuition for what's hard about designing fault tolerant protocols.
00:31:57.366 - 00:31:59.480, Speaker A: So we'll do that in the next video. I'll see you there.
