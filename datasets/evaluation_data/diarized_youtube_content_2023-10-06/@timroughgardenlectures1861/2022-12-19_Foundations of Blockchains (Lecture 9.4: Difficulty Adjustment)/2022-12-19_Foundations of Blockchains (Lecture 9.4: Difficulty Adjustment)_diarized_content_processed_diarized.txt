00:00:00.330 - 00:00:58.430, Speaker A: All right, so in this video I want to talk about difficulty adjustment, a question that's probably been on your mind, right? We introduced the hard puzzles that are sort of central to proof of work, where you have a cryptographic hash function, little h, and there's some tunable difficulty parameter tau. And solving a puzzle means finding an input x to the hash function of the format that we talked about, such that h of x, such that x hashes to a number that's at most tau. The bigger tau is, the easier the puzzle. The lower tau is, the harder the puzzle. And so your natural question is exactly how hard should we make these puzzles? Exactly how should we set this tunable parameter tau? And if you think about it, it's actually been kind of weird, right? We've done all this analysis of longest chain consensus with proof of work, civil resistance, and actually tau has never come up in our analysis so far. So that should worry you, that should seem like a red flag. So let's think this through then.
00:00:58.430 - 00:01:04.990, Speaker A: What intuitively should be the pros and cons of setting tau to be larger or smaller.
00:01:05.970 - 00:01:07.458, Speaker B: So for example, let's think about the.
00:01:07.464 - 00:01:34.490, Speaker A: Case where we try to set sort of tau to be pretty big. We try to make the puzzles pretty easy. So this of course means that nodes, keeping the hash rates fixed, the nodes are just going to be finding puzzles faster than they would have found them with a smaller value of tau. So rounds will have a shorter duration and in a given period of time you will have more rounds, you will have more blocks created. So in other words, bigger tau means faster rates of block production.
00:01:35.470 - 00:01:37.174, Speaker B: And I think probably your first instinct.
00:01:37.222 - 00:02:20.230, Speaker A: Might be that this is just like purely a good thing. Like the faster blocks get produced, the better kind of for two reasons. One reason is, remember that in longest chain consensus we have the security parameter k. So to get all the nice guarantees, you have to consider a block finalized only after it's been extended on the longest chain by some number of K times and so holding everything else fixed. So in particular holding the security parameter K fixed, the faster blocks are produced, it would seem the better, right? So if they're being produced twice as fast, then it's going to take half the amount of time to extend a block on the longest chain by k times. In other words, it'll have the time to block finalization. So that's the first reason.
00:02:20.230 - 00:02:48.580, Speaker A: Time to finalization, or latency if you like. The second reason is throughput. So, so far we haven't really been worrying about any kind of capacity constraints in the blocks. But in practical blockchains, blocks can only be so big. And so then the rate of block production is sort of directly controlling how many transactions you can actually process per second. So again, holding everything else fixed, like the block size fixed. It seems like a rate of block production should just be a pure win, right? You're just actually processing more transactions in a given period of time.
00:02:53.020 - 00:02:54.408, Speaker B: But here's the reason.
00:02:54.574 - 00:03:28.020, Speaker A: Both of those arguments are a little bit flawed. Both of those arguments assume that all of the blocks that are getting produced wind up on the longest chain. So if blocks are being added to the longest chain more quickly, then that means both faster time to finality and it means more transactions processed per second. But the issue is that if you make these sort of hard puzzles too easy, if the rate of block production becomes too rapid, all of a sudden it increases the likelihood that blocks will be created and will wind up orphaned off of the longest chain.
00:03:33.000 - 00:03:34.596, Speaker B: So remember, there's really kind of two.
00:03:34.618 - 00:04:16.316, Speaker A: Different reasons that forks show up in longest chain consensus. Reason number one is shenanigans by Byzantine nodes. So they are not necessarily following the protocol correctly. They need not be proposing blocks that extend the end of the longest chain. They can choose a predecessor which deliberately creates a fork in the blockchain. But let's actually sort of set that aside, right? Let's actually assume that all of the nodes are honest, that there are no Byzantine nodes at all. You're sort of hoping happens, you're hoping that honest nodes are going to sort of successively find sort of puzzle solutions, kind of, sort of in sequence, right? So some honest node solves a puzzle, its solution includes sort of a new block which is going to be the new end of the longest chain.
00:04:16.316 - 00:04:40.600, Speaker A: It'll point back to the previous end of the longest chain. That new block is going to get propagated to everybody, including all of the honest nodes. They're now going to start solving puzzles, looking for solutions that encode a block, extending the new end of the longest chain and so on. So the ideal case would be an honest node finds a block, all the other honest nodes hear about that block, and then the other honest nodes know to extend this recently announced block.
00:04:41.340 - 00:04:42.824, Speaker B: So the worry would be that instead.
00:04:42.862 - 00:04:59.296, Speaker A: Of getting this nice chain of sort of three successive blocks that you see on the left part of this picture, the worry is that actually those sort of three new blocks all get created at roughly the same time. Solutions to the same puzzle all specifying the same predecessor. So that's really pretty bad news actually.
00:04:59.398 - 00:04:59.664, Speaker B: Right?
00:04:59.702 - 00:05:28.890, Speaker A: So only blocks that wind up on the longest chain wind up getting finalized. All the rest of them get orphaned. Because these three new nodes all have the same predecessor, no pair of them can appear on the same chain. So the longest chain is going to include at most one out of these three blocks, at least two of these three blocks. It's going to be just totally wasted work. So how does this discussion tie back into the pros and cons of setting the difficulty threshold tau to be pretty big? I e. Making the puzzles pretty easy.
00:05:28.890 - 00:05:47.950, Speaker A: Well, if tau is really small, if it's really, really hard to solve those puzzles, then puzzles are not being solved very frequently at all, which makes it quite unlikely you're going to have two honest nodes solve the same puzzle very close to each other. So if small is tiny and the puzzles are really hard, you expect to see pictures like the nice chain on the left.
00:05:49.120 - 00:05:50.624, Speaker B: At the other extreme, if you set.
00:05:50.662 - 00:07:05.080, Speaker A: Tau to be very large, so you make the puzles really easy, well, then honest nodes are going to be just finding blocks so quickly, there's no way they can all keep each other informed in time about their puzzle solutions. And you'll wind up with pictures like that on the right. So that's the sort of fundamental tension in figuring out exactly how to set the difficulty threshold tau, right. You're sort of trading off, on the one hand, rate of block production, which if those blocks are winding up on the longest chain, means lower, latency and higher throughput versus sort of lots of blocks getting wasted just because of this inadvertent forking. So kind of a rule of thumb would be roughly, you might want to take tau sort of as big as possible, so make block production as high as possible subject to some kind of threshold on what fraction of blocks wind up getting orphaned, right? So Setao as big as possible subject to the blockchain typically looking like the picture on the left rather than the picture on the right. Now, you'd be right to complain that actually we've been talking about longest chain consensus for a long time now, and actually, we've never seen a picture like the one on the right. I mean, we see the picture like the one on the right if you have Byzantine nodes creating these forks on purpose, but at no point have we ever acknowledged the possibility of inadvertent ties between honest nodes.
00:07:05.080 - 00:07:38.016, Speaker A: So here's what's been going on. We have this assumption, a five, that we're in this sort of instant communication model, that delta equals zero. We've had that all the way up to this point. So anytime an honest node knows anything, boom. Automatically, as if by clairvoyance, all of the other honest nodes know as well. And so that means it really doesn't matter how big you take tau, it doesn't matter how fast blocks are being created, as long as they're at distinct moments in time, no matter how close together. Because we've been assuming that delta equals zero, you never wind up with the picture on the right.
00:07:38.016 - 00:08:15.340, Speaker A: You always have the case that every honest node knows about all of the previously created honest blocks. So it knows the appropriate end of the longest chain, it should be extending itself. So it's true that tau the choice of tau has not shown up in the analysis so far. We see here there should be some very intuitive kind of sweet spot, right, where it's kind of big enough you get a healthy rate of block production, but sort of small enough so that most of the blocks created do wind up on the longest chain. But to capture that trade off, to say more about this quantitatively, we need to relax assumption a five. We need to assume there actually is nonzero message delay, possibly between the different honest nodes.
00:08:16.240 - 00:08:17.344, Speaker B: So we're not going to actually say.
00:08:17.382 - 00:09:23.540, Speaker A: Anything more about that right now. We'll sort of pick this up again in the fifth video. That's where we actually focus on sort of relaxing assumption a five, moving beyond the unrealistic supersynchronous model with delta equals zero to the somewhat more realistic synchronous model with some non zero value of delta. The main punchline of the next video in the general synchronous model will be basically what you would hope, right? It would say, okay, I guess if there's a non zero delta, there's now some chance the two honest nodes will solve a puzzle at roughly the same time. But as long as we make the puzzles appropriately difficult, so that the typical length and time between two consecutive puzzle solutions, that is the typical length of a round. As long as that's pretty big, like, let's say at least ten x or even 100 x as big as the maximum message delay delta, then in fact, you will see this picture on the right once in a while, but it's only going to be sort of very infrequent. So most of the time you look like the picture on the left, and so you really are getting the latency and throughput that you were expecting with almost all of the blocks winding up on the longest chain.
00:09:36.000 - 00:09:37.916, Speaker B: And so that ultimately gives us the.
00:09:37.938 - 00:09:55.060, Speaker A: Answer of how should we set tau? We should set the difficulty threshold tau in order to target a particular rate of block production. A rate of block production that depends on what value of capital delta we're comfortable with and exactly what fraction of the created blocks we want to wind up in the longest chain.
00:09:56.360 - 00:09:57.796, Speaker B: All right, so to make this all.
00:09:57.818 - 00:10:18.730, Speaker A: A little bit more concrete, let's see how this works exactly in the most famous proof of work longest chain protocol of them all, namely bitcoin. So in bitcoin famously, difficulty threshold is tuned to target the production of, on average, one block per ten minutes.
00:10:20.300 - 00:10:21.396, Speaker B: So if you think about it, that.
00:10:21.438 - 00:10:50.950, Speaker A: Means six blocks an hour, and therefore 144 blocks per day. If you're thinking that sounds like not that many, I'd kind of agree with you. Nakamoto anticipated that the overall hash rates devoted to running the bitcoin protocol may vary over time. One, just because the popularity of the protocol could vary over time, but know because of technological advancements, moore's law et cetera, that would also be a force causing hash rates to increase as time goes on.
00:10:51.640 - 00:10:53.636, Speaker B: And hopefully it's intuitively clear that if.
00:10:53.658 - 00:11:08.810, Speaker A: You want to keep the rate of sort of block production, in other words, the sort of frequency with which puzzles get solved, if you want that to remain roughly constant even as the hash rate fluctuates, well, you better make sure that the threshold tau fluctuates in exactly the same way.
00:11:09.500 - 00:11:11.292, Speaker B: So the bitcoin protocol does, in fact.
00:11:11.346 - 00:11:54.360, Speaker A: Automatically adjust the difficulty threshold tau as a function of sort of how quickly blocks seem to be producing. And if blocks are being produced too quickly, then tau gets smaller, so the puzzles get harder. If blocks seem to be getting produced too slowly, like slower than once every ten minutes, then tau would increase, meaning that puzzles would become easier. And in fact, these resets happen once every 2016 blocks. 2016 blocks on the longest chain. Why 2016? Well, that's 144 times 14. So if you really were producing blocks once per ten minutes, then over the course of a fortnight, you would see 2016 blocks.
00:11:54.360 - 00:12:28.084, Speaker A: So after 2016 blocks have been added, new blocks have been added to the longest chain. Basically, it looks at how long it took to do that. And if it took less than 14 days to produce those blocks, that means blocks are being produced too quickly. That means tau should be decreased. If it took more than 14 days to produce the last 2016 blocks, that means the puzzles appear to be too hard. And so tau should be increased to make the puzzles easier. So precisely if it took beta times 14 days to produce the last 2016 blocks, here, beta could be more or less than one.
00:12:28.084 - 00:13:03.292, Speaker A: So if it took beta times 14 days for the last 2016 blocks on the longest chain, then the threshold tau gets updated as beta times its previous value. So tau becomes beta times the old value of tau. So, for example, if it took 28 days to produce the last 2016 blocks, that's beta equals two. That means you really want the puzzles to be sort of twice as easy. So the blocks get produced twice as quickly. And so you accomplish that by doubling the threshold tau. Similarly, if it took only seven days to produce the last 2016 blocks, that would be beta equal to half.
00:13:03.292 - 00:13:12.220, Speaker A: That means the puzzles are too easy. You want to decrease tau. And so this is going to decrease it, it's going to cut it in half if it took seven days for the last 2016 blocks.
00:13:12.680 - 00:13:14.096, Speaker B: So why this formula?
00:13:14.128 - 00:13:36.010, Speaker A: Well, this formula basically says that if the hash rate stays exactly the same as it was in the last epic, by epic, I mean a block of, I mean sort of a period of 2016 blocks on the longest chain. This formula ensures that if the hash rate in the next epic is exactly the same as in the previous one, then in fact, this is the appropriate threshold to generate a block on average every ten minutes.
00:13:36.540 - 00:13:37.832, Speaker B: So you can sort of think about.
00:13:37.886 - 00:14:20.680, Speaker A: The current value of tau. It's kind of like a lagging indicator of what's going on with the hash rate, the total hash rate devoted to the blockchain. You could look at bitcoin, for example. You could look at the total hash rate devoted to bitcoin over time, and it's not like a perfectly monotone curve. But still, I mean, the long term trend is quite obvious, where the total hash rate devoted to running bitcoin nodes has just been going up over time. And so as a result, if you sort of look at the various epics in bitcoin, more often than not, the average rate of block production in a given epic will be actually less than ten minutes, because the difficulty threshold tau was sort of set kind of retrospectively to be the right one for the previous epic. But then in the meantime, the hash rate has gone up even a little bit further.
00:14:20.680 - 00:15:32.380, Speaker A: Now, if you've been paying really close attention, you should be bothered by the description I just gave you of how bitcoin sort of automatically adjusts the tunable difficulty threshold tau. Because remember, in the last video, I said one of the remarkable things about Nakamoto Consensus, so longest chain consensus with proof of work, civil resistance. One of its remarkable properties is that you don't actually need a global shared clock for that consensus protocol to have proofable guarantees, right? Remember, with proof of work, it's a purely event driven consensus protocol, with each round just corresponding to whatever time it is that some node happens to solve the next puzzle. Meanwhile, in this description of the bitcoin protocol, I talk about the protocol kind of measuring the time it took to add the most recent block of a batch of 2016 blocks to the longest chain. So your question should be like, but bitcoin, I thought bitcoin doesn't even know what time it is, right? How does it measure how long it took to generate those 2016 blocks? Right? Kind of all there is is this hermetically sealed environment without any notion of time, where you just have this sort of intrigue of blocks, the longest chain of which are the blocks that are considered finalized.
00:15:33.200 - 00:15:34.944, Speaker B: So that would be a really good question.
00:15:35.062 - 00:15:49.060, Speaker A: And in fact, and we'll probably talk about this more in the bitcoin deep dive a few lectures from now in bitcoin, actually, nodes that produce blocks are responsible for including a timestamp at which they created the block.
00:15:49.960 - 00:15:51.716, Speaker B: And the main reason they're required to.
00:15:51.738 - 00:16:11.860, Speaker A: Do that is exactly so that this difficulty adjustment algorithm is well defined. So recorded on the bitcoin blockchain, not only are there these 2016 blocks along with all their predecessors, but there's also a timestamp with each of the blocks. So there's at least reported amount of time that elapsed in between sort of the first of those 2016 blocks and.
00:16:11.870 - 00:16:12.990, Speaker B: The last of them.
00:16:13.680 - 00:16:41.110, Speaker A: So for the purposes of the current discussion, right, I don't want to get too far out into the bitcoin weeds. So just like, think of these timestamps with the blocks as just being completely accurate. So imagine that every sort of block producer, including the Byzantine ones, just actually includes an accurate timestamp in each of the blocks. And then if that were true, it would actually be evident from the blockchain itself the duration it took to generate those 2016 blocks. You would just look at the difference between the timestamp of the last one and the timestamp of the first one.
00:16:42.120 - 00:16:43.348, Speaker B: Now, some of you, I'm sure, are.
00:16:43.354 - 00:17:31.940, Speaker A: Wondering about like, why on earth can we get away assuming that the timestamps are accurate? So, again, that'll be a good comment. And the bitcoin protocol, for example, does have various rules that timestamps must obey that are meant to keep the timestamps to be at least roughly accurate, accurate, let's say, to within like an hour or two. And that level of accuracy is sort of sufficient. Given that an epic is typically two weeks long, being off by an hour or two doesn't make a very big difference as far as this difficulty adjustment algorithm. So maybe we'll get to those sort of details in the bitcoin protocol around timestamps in our deep dive on bitcoin, maybe five, six lectures from now, maybe we won't. Honestly, it's probably like the one inelegant part of the bitcoin protocol, in my opinion. So we'll see if we talk about it once we get to that deep dive.
00:17:31.940 - 00:17:54.730, Speaker A: So that's the basic approach to difficulty adjustment, and it's really something you tend to need in any kind of blockchain that uses proof of work civil resistance. You have to be ready for the hash rate to change over time. You can't let the sort of block production schedule just be completely uncontrolled. So you really have to expect to use some kind of adjustment algorithm like this.
00:17:55.420 - 00:17:57.144, Speaker B: So while it may well be kind.
00:17:57.182 - 00:18:13.310, Speaker A: Of unavoidable, that doesn't mean it's not annoying. And in fact, the difficulty adjustment algorithm does introduce some unfortunate complications. One example actually be in the next lecture, in lecture number ten, when we talk about selfish mining attacks. Those are really enabled by difficulty adjustment algorithms, as we'll see.
00:18:14.080 - 00:18:15.148, Speaker B: But I want to talk about a.
00:18:15.154 - 00:18:26.610, Speaker A: Different issue here, which is that actually when you have sort of variable difficulty over time, you actually need to no longer use the straightforward longest chain rule that we've been talking about thus far.
00:18:27.620 - 00:18:28.512, Speaker B: So why not?
00:18:28.566 - 00:19:17.970, Speaker A: What's the problem with simply counting the number of blocks on a chain when you're computing the longest chain? Well, the worry is that some adversary that actually doesn't really have necessarily all that much hash rate could create a sort of competing very long chain and even take over as the longest chain by creating a bunch of blocks just by itself, all of which have really easy puzzles. Now, doing this would sort of require timestamp manipulation, which we were kind of assuming away on the last slide. But this is a genuine concern that an adversary can kind of concoct the timestamps in a way that can get away with just creating a really long sequence of blocks, all of which had really easy puzzles. And because they're easy puzzles, the adversary, even with not that much hash rate, could discover solutions to those puzzles super quickly. So it could generate in a short period of time a sort of really long chain that appeared to have been created over a long period of time.
00:19:28.010 - 00:19:29.734, Speaker B: So that would obviously be really bad.
00:19:29.772 - 00:19:47.600, Speaker A: And in particular would violate finality it would cause a rollback of a bunch of blocks, right? If you had the honest nodes just sort of chugging along, doing everything the way they're supposed to, one block every ten minutes, and then this adversary with not actually that much hash rate could just suddenly generate an even longer chain just by making sure that the puzzle difficulties stayed really low.
00:19:48.610 - 00:19:49.614, Speaker B: For example, right?
00:19:49.652 - 00:20:53.086, Speaker A: So like, imagine you had sort of a Byzantine node and you had honest nodes, and actually the honest nodes had ten times as much hash rate as the Byzantine node. But the Byzantine node winds up just focusing on creating blocks that have puzzles that are 100 x easier than the ones that the honest nodes are trying to solve, right? So then the Byzantine node would be able to produce blocks ten times as rapidly as the honest nodes. Again assuming that it takes care to keep the puzzle difficulty easy that entire time, in which case the adversary's chain might eventually overtake the longest chain that had been constructed by honest nodes. Now, the fix for this, I think, is pretty obvious, right? So if you have two chains that have equal length, meaning they both consist of 117 blocks, one of them all the blocks have really hard puzzles, the other one, all of the blocks have really easy puzzles. Well, somehow it feels like we should be building, we should be extending the first of those chains, the ones with the hard puzzles, rather than the one with the easy puzzles. Right. Because so much more work has gone on, gone in to constructing that first chain of 117 blocks.
00:20:53.086 - 00:20:56.180, Speaker A: Clearly that's the one we want honest nodes to be building on top of.
00:21:03.140 - 00:21:04.704, Speaker B: So in a perfect world, we'd know.
00:21:04.742 - 00:21:24.820, Speaker A: Exactly the total amount of hash rate that went into the construction of each chain. So the total number of sort of guesses of little x for various crypto puzzles that contributed to the creation of the blocks on that chain. We don't know literally how much hash rate went into building a chain, but we do know a really good estimate for it through the difficulty parameter.
00:21:25.560 - 00:21:27.416, Speaker B: So for any given block there's some.
00:21:27.438 - 00:21:42.540, Speaker A: Sort of difficulty threshold that goes along with that block. So we can define the work of the block as the number of guesses number of different hashes. We'd sort of expect to be required to produce a solution to a puzzle with that particular difficulty.
00:21:50.530 - 00:21:52.666, Speaker B: So in other words, the block includes.
00:21:52.698 - 00:22:29.210, Speaker A: As part of its description the threshold tau that basically tells us how big the bullseye is on the dartboard. And then the work is just this expected number of darts you need to throw before you finally hit the bullseye. So like for example, with those parameters, if you think about the shot 256 hash function, 256 bits of output, we're thinking about tau equal to two raised to the 176th. So that's a two to the -80 chance of hitting the bullseye on any given dart throw, which means you expect fact to have to throw a dart two to the 80 times before hitting the bullseye, before actually getting a solution to that particular difficulty threshold.
00:22:34.880 - 00:22:36.316, Speaker B: So that's how we define the work.
00:22:36.338 - 00:22:48.640, Speaker A: That went into the construction of a block. Notice this is just a formula we can read straight off of the difficulty threshold associated with that block. And then of course, we just define the overall work along a chain to be the sum of the work of each of the blocks in that chain.
00:22:49.780 - 00:22:50.976, Speaker B: And so now we can sort of.
00:22:50.998 - 00:23:14.520, Speaker A: Just redefine longest chain consensus using this sort of redefinition of a longest chain. So the expectation is for honest nodes to always try to extend the chain that they know about that has the greatest amount of work involved in it, so not the number of blocks per se. Although of course, all else being equal, more blocks means more work, but rather the chain that has the most work that went into building it according to this definition.
00:23:15.420 - 00:23:17.016, Speaker B: So, over the last few videos we've.
00:23:17.048 - 00:24:15.886, Speaker A: Proved some pretty cool things about longest chain consensus coupled with proof of work, civil resistance, right? So probabilistic finality, liveness, et cetera. Everything we've done thus far has been in the setting where you have sort of a fixed amount of hash rate and a fixed puzzle difficulty. But with this sort of redefinition of longest chain with honest nodes extending the chains that have the most amount of work in them as opposed to the largest number of blocks, we can more or less sort of extend all of those same finality liveness, et cetera. Guarantees to the setting where you have a variable amount of hash rate and accordingly, via the difficulty adjustment algorithm, a varying puzzle difficulty. So specifically, we need to make two assumptions. But under those assumptions we will even in the variable difficulty case have probabilistic finality, liveness, et cetera. The first assumption is not going to surprise you.
00:24:15.886 - 00:24:30.450, Speaker A: It just generalizes sort of our ongoing assumption that at most 49% of the hash rate is byzantine. So with the hash rate varying over time, we need at all times the hash rate at that time to be strictly less than 50% Byzantine.
00:24:31.590 - 00:24:33.614, Speaker B: So this first assumption, hopefully this strikes.
00:24:33.662 - 00:25:42.752, Speaker A: You as kind of obviously necessary, right? Like, if the hash rate of the Byzantine nodes ever grows to sort of a majority of the overall hash rate, then there'll be this period of time where those nodes have enough power to basically just kind of fork off the current end of the longest chain and sort of roll back a bunch of blocks that everyone had thought had been finalized. In addition to that kind of, I think, more obvious necessary condition, a slightly less obvious one is we're going to need to impose some kind of bound on how rapidly the hash rate can change over time. All right, so let's talk through exactly why you're in trouble. If you have sort of a massive, massive spike, sudden spike in the overall hash rate, that basically sort of chops time into two periods, right? You have period number one, where you have sort of stable super low hash rate. Period number two, where you have stable super high hash rate. Now in that first period, you're sort of humming along, you have some security parameter K, and you're just considering blocks that are finalized if they're sort of on the longest chain and have been extended K times. So maybe kind of in that first period, the longest chain winds up being 1000 blocks long, maybe K is equal to 100.
00:25:42.752 - 00:26:32.690, Speaker A: So that means there's 900 blocks that are, as we'll see, kind of incorrectly being viewed as finalized at that point. Now the transition comes along and the hash rate goes up by a factor of a billion. And the point is that basically completely washes out everything that happened in period number one. Because even though in period number one you created this sort of chain that has 1000 blocks in it, the work that involved that went into the creation of those blocks is just going to be really small relative to sort of the new hash rates that are available to the nodes after the transition point. And so while you may have been thinking of those 900 blocks as finalized in that first period, actually you shouldn't be thinking of any of the blocks from that first period as being finalized. Potentially stuff can get rolled back right after the transition point because everybody is so much more powerful than they were before.
00:26:33.860 - 00:26:35.632, Speaker B: So I'm sure that seems like an extreme example.
00:26:35.686 - 00:27:40.932, Speaker A: To be honest. I gave you an extreme example on purpose just to make the intuition as clear as possible. But I also hope the fact that it's so extreme suggests that, okay, well, as long as the hash rate isn't doing insane things like suddenly going up by a factor of a billion, it seems like you should be okay, at least if you have sort of if you use a larger security. Parameter to reflect the fact that around sort of transition points where the hash rate goes up by some factor, you are sort of a little bit more vulnerable than you would have been had the overall hash rate and the overall difficulty been staying exactly the same. So let's actually add kind of a third assumption here, just for emphasis, which is that as always, for longest chain consensus, any of the versions that you're talking about, you only get provable properties like Finality Liveness, et cetera, if you choose the security parameter k appropriately so, that's certainly true here. And moreover, the security parameter may need to be a little bit bigger than it was in just sort of our basic discussion, basic analysis of longest chain consensus. How much bigger does security parameter need to be? Well, that's going to depend on how strong a bound on the rate of change in hash rate you make in the second assumption.
00:27:40.932 - 00:27:50.604, Speaker A: So the sort of more wild fluctuations you permit in the overall hash rate, the more you're going to have to increase the security parameter little.
00:27:50.642 - 00:27:53.944, Speaker B: K so for example, you could imagine.
00:27:53.992 - 00:28:18.020, Speaker A: Assuming that the overall amount of hash rate is going to change by most a factor of two in each sort of epic in each period of 2016 blocks. And then you would need to take the security parameter a little bit a small constant factor larger than what we had in our lecture eight analysis. If you allow the hash rate to change by a factor of 1000 over an epic over 2016 blocks, you're going to need to increase K by a larger constant factor.
00:28:18.760 - 00:28:19.876, Speaker B: All right, so I hope all of.
00:28:19.898 - 00:29:02.720, Speaker A: That intuition is kind of quite convincing. So things get a little bit more complicated when you have varying difficulty puzzles. You need to redefine the longest chain rule, but it's sort of in the obvious way and you still get all of the nice properties we've been talking about about longest chain consensus, as long as you sort of make some modest additional assumptions and maybe a modest increase in the security parameter. K, I haven't given you a proof of any of this. Of course, I do hope it's sort of convincing intuition. If you do want to see an actual formal mathematical analysis that makes this precise, then I will write down here a citation, a research paper that you can look at for more details. It's a paper by Gary Kia, Yayas and Leonardos from the Crypto 2017 conference.
00:29:10.130 - 00:29:11.822, Speaker B: So that wraps up pretty much everything.
00:29:11.876 - 00:29:16.254, Speaker A: I wanted to say about difficulty adjustment, varying hash rate, varying puzzle difficulty over.
00:29:16.292 - 00:29:20.806, Speaker B: Time in proof of work blockchains and.
00:29:20.828 - 00:29:42.534, Speaker A: So in particular, in the next couple of videos, we're not going to worry about varying puzzle difficulty or varying hash rate. We're just going to assume that both of those are fixed in the next couple of videos. We wouldn't have to. We could do versions of those videos that did have varying puzzle difficulty. But again, I just want to sort of convey to you all of the main points in the clearest way possible. So that's why I'm just sort of separating these different issues, treating them one at a time, rather than sort of lumping them all together and worrying about.
00:29:42.572 - 00:29:47.326, Speaker B: The most general model we could possibly talk about you. So as a reminder, in the next.
00:29:47.348 - 00:30:23.940, Speaker A: Video, the fifth video for lecture number nine, we're going to be extending all of our guarantees for longest chain consensus, finality liveness, et cetera, from the kind of super synchronous model, the unrealistic model that we've been working on thus far. We're going to extend that analysis to the general synchronous model where you have an OPERATORI known but nonzero bound capital delta on the maximum message delay in the video. After that, the 6th video of lecture number nine, we'll talk about that impossibility result I mentioned, which explains in particular why you pretty much never see proof of work, civil resistance coupled with BFD type consensus. So wherever you want to go next, I will see you there.
