00:00:00.650 - 00:00:17.120, Speaker A: So now that we understand exactly how proof of work works, let's drill down on its properties, beginning with the all important property of civil resistance, meaning that the probability that a node gets selected is independent of the number of distinct IDs like public keys that it might be using.
00:00:18.050 - 00:00:19.822, Speaker B: So, just as a reminder, we're thinking.
00:00:19.876 - 00:01:06.546, Speaker A: About nodes attempting to solve these hard puzzles in the form of partially inverted a cryptographic hash function. So in other words, given a threshold tau, the goal is to find input x that when you feed into the cryptographic hash function like say, shot 256, the result is a small number, the number that begins with a lot of zeros, a number that's at most tau. And remember that to qualify as a puzzle solution, not only does x have to hash to something small, but also x has to basically encode all of that node's choices for what it would propose were it to be elected the leader. So it needs to encode both the block that the node would propose and the predecessor of that block that the node would propose. So sort of a precommitment to what.
00:01:06.568 - 00:01:09.330, Speaker B: Its choices would be were it the leader.
00:01:12.040 - 00:02:06.340, Speaker A: We're also thinking that x encodes the identity of the node, or at least one of its identities. So it's going to include a public key. And then because we expect nodes to be trying out lots of different little x's in order to find an x that hashes to something small, rather than having them, for example, generate zillions of different public keys. Trying to find such an x, it's convenient to just give them some sort of extra sort of free bits that they can experiment with. So that's known as the nonce. Remember also that we're working under the random oracle assumption which says that our cryptographic hash function little h like say, shot 256, it's basically indistinguishable for all practical purposes from a completely random function. So this means we're thinking of this hash function as like that little orange box with a gnome inside sort of flipping coins, right? So whenever you evaluate little h on something, the gnome flips 256 coins and gives you back the corresponding sequence of zeros and ones.
00:02:06.340 - 00:02:54.420, Speaker A: If you evaluate the hash function on an input you've already evaluated it on, well, then it's going to spit back exactly the same answer. It is a deterministic function, but if you ever evaluate it on an input that you haven't seen before, even if it differs from an input you've used before in only one bit any distinct input that you haven't seen it before, it's going to be completely unpredictable. So you can think of the gnome as just flipping a fresh set of 256 random coins and giving you the corresponding sequence of zeros and ones. Now, under this assumption that something like shot 256 is indistinguishable from random, that really means that when you're looking for puzzle solutions, little x there's really nothing you can do that's better than just sort of trying out different possible inputs until you get lucky. So again, you're throwing darts at a dartboard over and over again, hoping to hit, what's? A very small bullseye.
00:02:55.320 - 00:02:56.756, Speaker B: So we're just going to assume that.
00:02:56.778 - 00:03:11.240, Speaker A: When you're doing longest chain consensus with proof of work symbol resistance as we're doing here, we're just going to assume that all of the nodes, all they do is just sort of repeatedly try different little X's as fast as they can. Looking for a solution, looking for something that hashes to at most tau.
00:03:11.980 - 00:03:13.532, Speaker B: And this is in fact a completely.
00:03:13.586 - 00:03:39.780, Speaker A: Accurate description of what happens in practice. So for example, if you're running the Bitcoin protocol and you're trying to solve one of these puzzles, what you're likely to do in practice is just you fix the block B. So do you think of that as fixed once and for all? You fix its predecessor, you fix the public key that's going to go in the solution, and then you just sort of try different choices for the nonce bits, just searching over and over again until you get lucky and find an x that hashes to something at most tau.
00:03:42.040 - 00:03:43.684, Speaker B: So with that setup, we can state.
00:03:43.722 - 00:04:48.120, Speaker A: The key property of proof of work sybil resistance, namely that it is sybil resistant, namely that the probability that any given node is elected as the next leader, meaning as the next one to solve one of these hard puzzles. That probability is proportional to the amount of computational power that it has, how quickly it can keep trying repeated guesses. For little x in particular, it is independent of how many different IDs, how many different public keys that node might be using. So formally, let N denote the number of nodes that are currently running the protocol and trying to find puzzle solutions. Now, let me emphasize that the protocol has no idea what little N is, okay? So the protocol does not need to know N, it does not need to know what these nodes are. But at any given moment in time, you have to agree there is some set of nodes searching for puzzle solutions. Call those nodes one through N and let mu one up to mu n denote the sort of computational power of each of those nodes in the form of the number of guesses they're able to make per second.
00:04:48.810 - 00:04:50.214, Speaker B: And let me emphasize when I say.
00:04:50.252 - 00:05:05.366, Speaker A: Node here, I really mean a physical machine. I don't mean a public key, I mean a physical machine which is possibly running the protocol under multiple IDs that's going to turn out not to matter. So n physical machines running the protocol able to make guesses at these rates.
00:05:05.398 - 00:05:07.918, Speaker B: Mu one up to mu n and.
00:05:07.924 - 00:06:06.228, Speaker A: Then the conclusion is just that each node's probability of being elected leader of any given round is proportional to its hash rate, meaning its value of mu sub I. So, for example, if you have a physical machine capable of making 1 million guesses per second, if you want, you can spin up sort of two versions of the protocol running on your same machine. But the machine still only has the power to make a million guesses per second total, right? The fact that you spun out two different threads doesn't somehow increase the number of guesses that the physical machine can actually make. So whether you want to spend a million of those guesses with a single public key or split it up into half a million, half a million between two different public keys, it's not going to matter, right? Your overall sort of hash rate, the overall number of guesses that this physical node is making per second is going.
00:06:06.234 - 00:06:09.316, Speaker B: To remain the same. I think there's a good chance this.
00:06:09.338 - 00:06:29.740, Speaker A: Seems sort of intuitively obvious to you, right? This is basically saying like, if you have two machines, the second one has twice the hash rate as the first is able to make twice as many guesses as the first per second. Given that each guess is equally likely to be a puzzle solution, sure seems like that second machine should be twice as likely as the first to be elected the next leader.
00:06:30.400 - 00:06:31.996, Speaker B: So just for completeness, let me sort.
00:06:32.018 - 00:06:41.490, Speaker A: Of write down kind of a one line proof that this just follows from Bayes'rule from probability. Again, if you find this intuitively obvious, feel free to skip the next couple of minutes of the video.
00:06:42.420 - 00:06:43.632, Speaker B: Alright, so let me have a little.
00:06:43.686 - 00:07:28.524, Speaker A: Sidebar just to sort of remind you what Bayes rule is and how it just really kind of has a one line derivation. So Bayes rule concerns probabilities involving two different events. So two different things that might or might not happen visually. I always like to sort of think of events as kind of like circles, as if in an event diagram or something like that. So what are these events A and B for us? Well, imagine all of the various guesses that all of the nodes are producing and focus in on one particular guess. I'm not telling you which node it's from, just some guess by some node at some moment in time. So the event A, that's going to be the event that this particular guess, this particular choice of little X that we've zoomed in on is in fact a puzzle solution that does hash in fact to something that's at most tau.
00:07:28.524 - 00:07:49.060, Speaker A: Obviously for any given guess, x that may or may not be true, may or may not be a sort of dart that hits the bullseye. The event B, that's going to be the event that this guess was made by node I, node number 17, say. And of course that may or may not be true because the other nodes are making guesses as well. But any given guess, there's some probability that that came from node number 17.
00:07:49.880 - 00:07:51.128, Speaker B: And now in Bayes Rule, we want.
00:07:51.134 - 00:07:56.730, Speaker A: To think about the probability that both of these two things happen, both event A and event B happens.
00:07:57.580 - 00:07:57.976, Speaker B: All right?
00:07:57.998 - 00:08:03.390, Speaker A: So in terms of the diagram, that's the overlap between the sets A and B.
00:08:06.260 - 00:08:08.144, Speaker B: So here's two different ways we can.
00:08:08.182 - 00:08:22.336, Speaker A: Write a intersect B in terms of conditional probabilities. So for both A and B to happen, it has to be the case that first A occurs, and then given that A occurs, B then occurs.
00:08:22.528 - 00:08:24.064, Speaker B: So in math, that's just the probability.
00:08:24.112 - 00:08:28.310, Speaker A: Of the event A times the probability of the event B conditioned on event A.
00:08:29.000 - 00:08:30.056, Speaker B: On the other hand, we can make.
00:08:30.078 - 00:08:39.370, Speaker A: The same argument, exchanging the roles of A and B, right? So for A and B to both happen, it has to be that B happens and then that A happens, given that B happens.
00:08:40.860 - 00:08:42.472, Speaker B: And so now to get the famous.
00:08:42.536 - 00:08:54.450, Speaker A: Bayes Rule, we just kind of isolate one of the conditional probabilities. So we take the two different ways of writing probability of A and B. We know those have to be exactly the same number. We divide both sides by the probability of B.
00:08:55.380 - 00:08:56.748, Speaker B: So in other words, Bayes Rule lets.
00:08:56.764 - 00:09:11.190, Speaker A: You calculate a conditional probability probability of A given B, given the sort of converse, if you like, conditional probability, the probability of B given a plus, sort of the prior probability, meaning the probability of just A and B with no conditioning at all.
00:09:12.040 - 00:09:12.436, Speaker B: All right.
00:09:12.458 - 00:09:32.940, Speaker A: So now let's go back to the two events that we actually care about and actually, I think I gave them the wrong letters the first time. So let's just switch to letters. Let's say event A is the probability that sort of a given guess little x was actually produced by node number 17. And let's say event B is the event that this particular guess little x happens to be a puzzle solution, happens to hash to something at most tau.
00:09:37.420 - 00:09:39.336, Speaker B: So why is this the probability we're focusing on?
00:09:39.358 - 00:10:08.880, Speaker A: Well, this is exactly the probability that node I is going to be the leader of any given round. So basically you imagine fast forwarding sort of to the next sort of guess little x that actually happens to be a solution that actually hashes to something at most tau. So we're conditioning on x being a puzzle solution, and then we want to say, what's the likelihood that it was node number 17 that actually generated that next winning puzzle solution? So now we can just apply Bayes Rule and rewrite this conditional probability into other terms that are easy to analyze.
00:10:17.270 - 00:10:18.694, Speaker B: So here's where we're going to really.
00:10:18.732 - 00:11:04.980, Speaker A: Use the random oracle assumption. So let's look at the second term on the numerator and also the denominator. So what's the probability that x is a puzzle solution, either unconditionally or given that it was proposed by a particular node number 17? And the point is, it doesn't matter it's the same either way, right? There's some threshold tau by assumption, every guess results in a sort of uniformly random output. And so, for example, if it's 256 bits of output and tau equals two raised to the 176th, then every guess has a two to the -80 probability of being a puzzle solution that's the probability that any of these darts hit the dartboard, all of the darts are exactly the same. All of the nodes are throwing darts at exactly the same dartboard with the same size bullseye. So, whether it's proposed by node I or not, this probability is always the same thing.
00:11:05.830 - 00:11:07.686, Speaker B: So those two terms cancel out and.
00:11:07.708 - 00:11:28.010, Speaker A: We'Re just left with the unconditional probability that any given guess, little x was proposed by node number 17. And that, of course just by definition is the fraction of guesses that are made by node i. So it's going to be mu sub i, the total number of guesses per second by node I divided by all of the guesses made by anybody. So the sum of the Mu j's.
00:11:28.430 - 00:11:31.290, Speaker B: And that's exactly what we set out to prove.
00:11:33.070 - 00:12:54.922, Speaker A: So this key property, again, this does directly imply sybil resistance because notice, I mean this calculates the probability of being elected as a leader as a node. And that probability depends in absolutely no way on how many IDs you might be using, right? So, if you have a million hashes per second, doesn't matter if you split them 50 50 between two IDs or concentrate them all in one ID, either way you're going to have exactly the same hash rate and therefore by this key property, exactly the same probability of getting elected a leader. Now, as I'm sure you've noticed, you can influence the probability that you'll be elected a leader by changing the amount of hardware that you own. So, if you buy a more powerful machine or if you buy more machines, that has the effect of increasing your value of mu. And if the muse of the other nodes are held fixed, that's going to increase your share of the overall hash rate and accordingly increase your probability of leader election. So the statement of civil proofness says consider everybody's hardware fixed, fix muse for all of the nodes. And given that the probability is unaffected as you vary the number of IDs and it's a big difference, remember, because IDs are totally costless to generate just type in Sshgen at a Unix prompt, whereas buying more hardware, you can do it, but it's obviously sort of economically expensive.
00:12:54.922 - 00:13:02.030, Speaker A: So you can change your probability, but only by sort of investing in computation, not by doing something totally costless like.
00:13:02.100 - 00:13:05.246, Speaker B: Generating a bunch of new IDs. All right?
00:13:05.268 - 00:13:29.430, Speaker A: So that's corollary number one of this key property, civil proofness. So what's corollary number two and more generally, let's talk about some other really cool properties that proof of work civil resistance has in the context of longest chain consensus to begin, let me remind you where we left off, lecture number eight. Let me remind you what it was we needed to really have all of the desired finality and liveness properties of longest chain consensus.
00:13:31.450 - 00:13:32.934, Speaker B: So I've already said this several times.
00:13:32.972 - 00:14:09.780, Speaker A: But it's a super important point. So let me just sort of say it again. If you go back to lecture number eight, where we were kind of analyzing this abstract version of longest chain consensus, all we used in all of our results, theorem one, theorem two, theorem three, theorem three, prime. All that we really needed for all of those results to be true is that you have a magenta box. So a way of selecting a leader of each round so that the probability that you get an honest node as the leader is more strictly bigger than 50%, so it exceeds the probability that you're going to get a Byzantine node. As long as you can implement a magenta box so that that property holds, then you are done.
00:14:10.470 - 00:14:12.214, Speaker B: So why are we done? Well.
00:14:12.252 - 00:14:39.150, Speaker A: So, first of all, in the fourth video of lecture number eight, we showed that as long as you have this probabilistic process, so as long as you're generating a leader sequence such that each leader is more likely to be an h than an A. Then with high probability, you get a sequence of h's and A's where you have proportional representation in every sufficiently large window. So in particular, for every sufficiently large window with high probability, you're going to have more honest leaders. They will outnumber the number of byzantine leaders in that window.
00:14:39.890 - 00:14:41.354, Speaker B: So in that part of the analysis.
00:14:41.402 - 00:15:26.906, Speaker A: We were using the permissioned assumption only inasmuch as we needed it to construct the magenta box that we had in mind at that time, which was selecting one of the nodes running the protocol uniformly at random. To select a node uniformly at random, we were using the fact that we knew what the set of nodes were other than the construction of the magenta box. We did not use the permissioned assumption in that probabilistic analysis. Furthermore, if you look at the videos where we actually proved all the cool finality sort of liveness chain quality guarantees for longest chain consensus videos five through seven of lecture number eight, we actually, again, never used the permission setting at all. We never used anything other than the assumption that the leader sequence that gets generated is W balanced and that you set the security parameter K accordingly.
00:15:27.098 - 00:15:30.366, Speaker B: According to that w putting that all.
00:15:30.388 - 00:16:27.918, Speaker A: Together, it means the only missing ingredient from having a permissionless version of longest chain consensus that satisfies all those guarantees from lecture number eight the only missing ingredient is a permissionless version of the magenta box. So a magenta box that does not need to know the current set of nodes, but that nonetheless guarantees that the probability that the output is going to be byzantine is strictly less than 50%. And in this proof of work symbol resistance subroutine, we have just generated exactly that type of magenta box. I hope you'll agree it's a permissionless magenta box, right? The protocol doesn't know anything about how many nodes or what are the nodes running the protocol. We've put the burden of proof onto the nodes themselves. They are working unbeknownst to the protocol, trying to come up with a puzzle solution. And of course, the outcome of this process, all of these nodes sort of trying all these choices for little X, looking for a puzzle solution that has the effect of generating the output of this magenta box of electing a leader.
00:16:27.918 - 00:16:37.250, Speaker A: If node number 17 is the first one who finds the puzzle solution, then that's in effect the output of this implicit magenta box that the nodes are all implementing.
00:16:37.830 - 00:16:38.258, Speaker B: All right?
00:16:38.264 - 00:17:04.110, Speaker A: So I hope that convinces you that with a proof of work symbol resistance can be thought of as a permissionless magenta box, right? It sort of randomly samples a node to be the next leader, the leader of each round. And so now the question is just does this permissionless magenta box satisfy the property that we need? So the sort of condition in blue on the lower right part of the slide is it the case that the output of this magenta box is more likely to be honest than Byzantine.
00:17:06.050 - 00:17:06.366, Speaker B: And.
00:17:06.388 - 00:17:35.240, Speaker A: It'S going to depend on what assumption we make. But the key property tells us exactly what assumption we need. The key property tells us that nodes are elected with probability proportional to their hash rate, which means the probability of electing an honest leader is going to be equal to the fraction of the hash rate contributed by honest nodes. So to get the assumption we want that leaders are elected more often than Byzantine nodes, we're going to need to assume that the amount of honest hash rate is more than the amount of Byzantine hash rate.
00:17:45.360 - 00:17:46.796, Speaker B: So this tells us sort of the.
00:17:46.818 - 00:18:23.848, Speaker A: Key parameter, which is the fraction of the overall hash rate contributed by Byzantine nodes and it tells us what's going to be the assumption that we need. We're going to need to assume that alpha is strictly less than a half. That a strict minority. The overall hash rate is Byzantine. And that was a condition identified all the way back in the original Bitcoin white paper. So in other words, this really has extended our guarantees from permissioned longest chain consensus to permissionless longest chain consensus, coupled with proof of work civil resistance. And so in particular, we are inheriting all of the cool guarantees that we proved in lecture number eight.
00:18:23.848 - 00:19:18.096, Speaker A: So, for example, finality liveness and chain quality. So a couple actually pieces of fine print. So first of all, even back in the permissioned setting, when we were talking about sort of selecting random leaders, there's always some chance that you're just going to get super unlucky and have a really long sequence of Byzantine leaders that messes everything up. So as usual with probabilistic leader selection, with longest chain consensus, we can only speak about probabilistic finality, probabilistic liveness, probabilistic chain quality. So the statement is that all of these properties will hold with high probability. I guess another just minor caveat to just remind you of is that these guarantees are always subject to choosing the security parameter little K sufficiently large. Remember, K is sort of how deep on the longest chain a block needs to be before it's considered finalized.
00:19:18.096 - 00:19:42.540, Speaker A: And so depending on sort of how close alpha is to one half, depending on how long a duration of time you're sort of worried about, those are all going to correspond to needing to take little K to be bigger. But for any fixed alpha, less than one half for any fixed duration that you're worried about, and for any fixed failure probability, little delta, there exists a sufficiently large K. So that with high probability, you get all of these properties.
00:19:43.840 - 00:19:44.236, Speaker B: All right?
00:19:44.258 - 00:20:47.740, Speaker A: So the final thing that we really need to discuss and the reason there's sort of some white space that I've left in this statement, remember in lecture number eight, right, we had all these proofs of finality a common prefix property, all that stuff. And remember, we did have this list of assumptions, five assumptions, a one through a five that really we sort of leaned pretty heavily on in those proofs from lecture number eight. In lecture number eight, we sort of talked through why most of those assumptions would be reasonable in a permissioned plus PKI setting. But we're not currently in that setting, right? We're now in the permissionless setting. We've got a particular permissionless implementation of longest chain consensus by coupling it with proof of work symbol resistance, a combination which is sometimes referred to as Nakamoto Consensus. But to really sort of assert that we get all of those nice guarantees from lecture number eight, we need to go back and actually check if our proof of work implementation, if our proof of work symbol resistance actually meets those assumptions, a one through a five. All right, so here are those five assumptions.
00:20:47.740 - 00:21:06.628, Speaker A: This is the same slide from lecture number eight. All five of these assumptions played important roles of the proofs of the sort of finality liveness, et cetera, guarantees from lecture number eight. So let's go through them one by one and assess to what extent or under what conditions they would hold for Nakamoto Consensus for sort of proof of work civil resistance coupled with longest chain.
00:21:06.804 - 00:21:09.272, Speaker B: Consensus assumption a one.
00:21:09.326 - 00:21:39.730, Speaker A: That's our trusted setup assumption, right? And we use this in our proof of the common prefix property. So the assertion here is that Byzantine nodes did not have advanced knowledge of the genesis block and in particular in our proof of work implementation we're talking about right now. This is basically saying, like, it can't be the case that somehow byzantine nodes were already trying to kind of solve hard puzzles sort of long ago before any of the honest nodes even knew kind of what the genesis block was. So it's important that that was not the case.
00:21:41.460 - 00:21:43.148, Speaker B: So the first assumption a one, there's.
00:21:43.164 - 00:22:51.604, Speaker A: Really nothing to check, right? A trusted setup assumption sort of by definition, you don't really verify those, at least not within the protocol analysis itself. That's just something that you somehow assume happened in advance of or somehow outside of the protocol. Now, when you actually deploy a new blockchain protocol, in practice, you can try to take pains to sort of build confidence in others that the requisite trusted setup assumptions really are satisfied. Interestingly, Nakamoto was really well aware of this point, and in particular this trusted setup assumption, a one that was important for the Bitcoin protocol. And so here's what Nakamoto did to try to build confidence in others that in fact, the genesis block was not known to anybody way in advance, which was the day that the Bitcoin protocol was released. So the protocol description has to include in it a description of the hard coded genesis block. And so part of that hard coded genesis block was in fact a headline from the Financial Times that was either I forget, it was either from that morning or from the day before, but a very recent headline from the Financial Times referring to sort of a big bailout of the British banks.
00:22:51.604 - 00:22:55.400, Speaker A: Remember, this is kind of basically in the midst of the Great Recession.
00:22:56.140 - 00:22:57.276, Speaker B: And so you look at this and.
00:22:57.298 - 00:23:32.150, Speaker A: You'Re like, oh, okay, so the hard coded genesis block presumably could not have been created before the day that that headline, that newspaper headline came out, january 3, 2009. Presumably, neither Nakamoto nor anybody else could have magically guessed what the sort of January 3 headline would have been way in advance. So that's pretty compelling evidence that in fact, the trusted setup assumption does indeed hold specifically for the Bitcoin protocol. And many other protocols, including, for example, Ethereum, have sort of followed suit, have sort of followed this pattern to build confidence that a one really is true.
00:23:40.560 - 00:23:41.996, Speaker B: So returning to our list of five.
00:23:42.018 - 00:24:23.688, Speaker A: Assumptions, in some ways, like the first assumption is the easy one because we just take it as an assumption, right? We're not actually trying to prove it about our protocol. Whereas a two, a three, and a four, these are sort of properties that we assumed the protocol had when we did all of that analysis in lecture number eight. And so now we really need to check that those assumptions are in fact satisfied that those properties do hold for the version of longest chain consensus. That's, coupled with proof of work, symbol resistance. Happily, we can very quickly see that actually all of a two, a three, and a four do hold. So let's start with a two. The assumption that leaders are verifiable.
00:24:23.688 - 00:24:57.124, Speaker A: So if you are the leader of around R, you can convince everyone else of that fact. And if you're not the leader of around R, you cannot trick anybody into thinking that you are in fact the round R leader. And if you think about it, that's just true immediately by our definition of who is the leader of a round, right, you're a leader of the next round. If you're the next node to solve the hard puzzle, either you have the solution to the puzzle or you don't. If you have it, everyone can say, wow, good job, you found an X that hashes to something at most Tau. I agree, you're the leader. Or if you don't, everyone can check your X hashes to something bigger than tau, you're not the leader.
00:24:57.124 - 00:26:15.436, Speaker A: Sorry. So, moving on, let's go to assumption A three. So this assumption states that nodes shouldn't be able to manipulate the magenta box, the intended way of selecting a leader of around. So assumption A three, at least if we're thinking about all of the nodes hash rates as being fixed, all of those muse as being fixed a three would then follow pretty much immediately from the civil resistance property that we just showed, which remember, basically follows from the random oracle assumption. So keeping the hash rates fixed as a node, there's really just like nothing you can do, right? So just like basically the laws of nature just say that if you're making a certain fraction of the guesses amongst all of the nodes because you're all, in effect, just sort of like randomly guessing for puzzle solutions, little x, the likelihood that you're the first one to do it is simply equal to the fraction of guesses that belong to you. Now, it is true that as a node you can manipulate, if you want to call it that, your probability of being selected as a leader by changing your mu. So if you buy a sort of more powerful machine or additional machines, it boosts your mu, keeping everyone else fixed, that boosts the fraction of hash rate that belongs to you.
00:26:15.436 - 00:27:09.190, Speaker A: So it also boosts the probability that you'll be selected as a leader. But that of course is costly. So really, assumption a three, think of it as nodes cannot costlessly manipulate their probability of being chosen as a leader. So for example, if you could manipulate it by generating multiple IDs, that would be basically costless. So the only way you can manipulate it is by sort of changing your hash rate, which of course is an economically expensive thing to do. All right, so let's move on to the fourth assumption. So assumption A four says that it's impossible for the leader of a round R to propose blocks that have predecessors either to each other or to blocks that are going to be created in some later round.
00:27:09.190 - 00:27:23.962, Speaker A: And this assumption is like in a proof of stake. Context, which we'll talk about in lecture number twelve, this assumption can actually be a little bit annoying if you're using proof of work civil resistance. This assumption just takes care of itself in a very convenient way.
00:27:24.096 - 00:27:24.442, Speaker B: Right?
00:27:24.496 - 00:28:15.866, Speaker A: Because remember we had this sort of nice idea that to qualify as a puzzle solution, little X has to not just hash to something small, but it also has to encode sort of valid choices for what that node would do in what we were calling step two B, were it to be elected a leader. And so in particular, an honest node is supposed to just propose one block with one predecessor. So we're just going to say, hey, we're going to force you to pre commit to one block, one predecessor. That needs to be encoded in sort of the data of X. And you can only be a puzzle solution if X has that format and hashes to something small. So our implementation of the magenta box in the proof of work context, not only is it permissionless, which is really cool, but also it actually forces nodes to propose only a single block in.
00:28:15.888 - 00:28:18.154, Speaker B: A given round, R, right?
00:28:18.192 - 00:28:42.690, Speaker A: So the consequence in light blue here on this slide says okay, under the general version of assumption A four, we know that the leader of a round is not going to be able to propose multiple blocks on the same chain, but it leaves open the possibility of proposing blocks across different chains, whereas with the proof of work implementation, actually, the leaders only get proposed one block, period. So obviously in particular it's adding a most one block to any given chain.
00:28:43.130 - 00:28:45.106, Speaker B: So that's the stronger version of assumption.
00:28:45.138 - 00:29:23.600, Speaker A: A four, which actually just holds for this proof of work implementation of longest chain consensus, each round only one block gets proposed. The predecessor for that block has to be specified at the time of creation. So the only blocks it could conceivably refer to were the blocks that already existed at that time, which of course are from rounds earlier than R. So no problem with assumption A four for Nakamoto Consensus. So what about that fifth and final assumption?
00:29:24.260 - 00:29:26.176, Speaker B: All right, so the fifth assumption you.
00:29:26.198 - 00:30:27.892, Speaker A: Might recall was actually sort of so strong as to be totally unreasonable even in the permissioned and PKI setting that we were talking about in lecture number eight. So A five asserts that we're working in, you might call it the instant communication model, or maybe the super synchronous model, but basically like the synchronous model with delta equals zero. So basically whenever an honest node learns something like here's a message, assuming that honest node echoes it to everybody else, all of the honest nodes are going to be aware of exactly the same information at all moments of time. So that assumption kind of trivializes consistency across different nodes, right, to really just sort of by assumption, any two honest nodes know exactly the same information at all times. So it's very easy for them to just be in sync because they have exactly the same views of the world. So what we worried about in lecture number eight, and which was nontrivial even under assumption five, was first of all, sort of finality. So self consistency, right? So it's important that every node is consistent not just with the other honest nodes at that moment in time, but also with versions of itself sort of in future moments of time.
00:30:27.892 - 00:30:52.860, Speaker A: So in other words, you don't ever want to have a supposedly finalized block getting rolled back. So we worried a lot about finality and we also worried about liveness and in its stronger form, chain quality. And those properties were all actually nontrivial to prove even in the supersynchronous model. And moreover, this assumption a five was convenient because it allowed us to actually really isolate the most important conceptual ideas of those proofs.
00:30:53.280 - 00:30:54.796, Speaker B: So, as I said several times in.
00:30:54.818 - 00:31:02.156, Speaker A: Lecture number eight, it's actually, even though it seems like a ridiculous assumption, it seems like maybe you'd be throwing out the baby with the bathwater.
00:31:02.348 - 00:31:02.800, Speaker B: Actually.
00:31:02.870 - 00:32:02.070, Speaker A: You can extend all of those guarantees from lecture number eight with some work, but not that much work, frankly, to the general synchronous model, where you have a nonzero but OPERATORI known bound capital delta on the maximum message delay. Now, for that to be true, it is important that the max message delay capital delta, that should be small relative to the typical duration of a round. So ideally, the max message delay would be something like a few percent of a typical duration of a round. So in fact, in the video after the next one, so the fifth video for lecture number nine, we'll actually talk in more detail about exactly why this is true. So we'll do some math which basically extends what we did in lecture number eight, and you'll see quite kind of vividly exactly why everything still goes through as long as delta is small relative to the typical duration of a round. So if you're interested in more details on that point, if it sounds sort of implausible that a five is actually easy to relax, check out the video after the next one.
00:32:12.450 - 00:32:14.046, Speaker B: All right, so this is all actually.
00:32:14.148 - 00:33:03.438, Speaker A: Pretty great news for us, right? So assumptions a two through a four simply hold for permissionless longest chain consensus if you use proof of work, civil resistance, or at least they hold under the random oracle assumption. Let's take on faith for the moment that we're going to the video after next, relax assumption A five and extend all of the analysis from lecture number eight to the general synchronous model with arbitrary delta. And given all that, it basically means we have a permissionless consensus protocol with kind of all of the properties that we want, namely consistency, at least probabilistic, finality, liveness, et cetera. So we do need the trusted setup assumption. So Byzantine nodes don't have advanced knowledge of the genesis block. That's important. Obviously we need to bound on the fraction of hash rate that's Byzantine, so it should be almost 49% byzantine hash rate.
00:33:03.438 - 00:33:27.654, Speaker A: I guess we're making the random oracle assumption. So we assume that shot 256 has not been broken and we assume we're in the synchronous model. So there is some bound delta that bound sort of the maximum message delay. So it's still a list of assumptions, but it's a much more modest list of assumptions than we had pretty recently. So under those assumptions, again, permissionless longest chain consensus has all of the properties that we want.
00:33:27.852 - 00:33:29.538, Speaker B: Let me conclude this lecture by pointing.
00:33:29.554 - 00:33:39.690, Speaker A: Out a couple other, perhaps less important, but still kind of surprising and remarkable properties of permissionless longest chain consensus with proof of work symbol resistance.
00:33:40.030 - 00:33:41.434, Speaker B: So first, while we do have this.
00:33:41.472 - 00:33:51.150, Speaker A: Trusted setup assumption a one, we don't need our usual trusted setup assumption of public key infrastructure. So all of this works even without assuming PKI.
00:33:51.650 - 00:33:53.246, Speaker B: So for example, if you just sort.
00:33:53.268 - 00:34:49.850, Speaker A: Of download the software necessary to run a Bitcoin node, you're not broadcasting your public key to everybody else at the beginning. You can just start sort of trying to solve these hard puzzles. Similarly, you don't bother trying to learn what are the public keys that belong to other nodes that are out there. So really as a node, literally nobody's heard of you while you're sort of humming along trying to find these puzle solutions. No one's heard of you until you actually successfully solve a puzzle and you submit your sort of winning lottery ticket little x which where one of the fields is going to have your public key in it. Strictly speaking, actually in Bitcoin, you don't even need to put your public key into one of the puzzle solutions, but you're probably going to do it because that's sort of the key that allows you to collect a pretty nontrivial block reward after you've successfully solved the puzzle. So this is why proof of work civil resistance kind of gives you a permissionless protocol in kind of the strongest form you could imagine.
00:34:49.850 - 00:36:02.082, Speaker A: You literally just download the software, you don't register anything with anybody and you can sort of run the protocol, follow along with what all the other nodes are doing, and also create blocks yourself if you're lucky enough to solve one of these hard puzzles. As we'll see when we talk about proof of stake designs in lecture number twelve they're a little bit more in between the permissioned and the sort of truly permissionless proof of work setting in the sense that you can definitely be anonymous. You can have multiple IDs, all that stuff. But there is sort of a form of registration in order to be eligible, to be selected as a leader and sort of vote on and create blocks. Whereas proof of work literally nobody's ever heard of you until maybe after you've actually successfully solved the puzzle, created a block and included your public key in your solution, little X. So those of you who have been following this lecture series really, really closely might at this point have a question. That question being why doesn't all of the nice properties I've put on the slide contradict the impossibility result that we saw way back in lecture number three? So let me just quickly remind you of the context at the very beginning of the lecture series.
00:36:02.082 - 00:36:35.918, Speaker A: So in lecture number two, the point there was to show that you can have good consensus protocols if you're willing to make a sufficiently long list of assumptions. So in lecture number two, two big assumptions we made, first of all, the permission setting with public key infrastructure. And secondly, we were working in the synchronous model. But what we showed, we showed something very satisfying. So we solved state machine replication through a reduction to Byzantine broadcast. And we showed that Byzantine broadcast can be solved in the sense that you have a protocol satisfying termination validity and agreement. Byzantine broadcast can be solved even if 99% of the nodes are Byzantine.
00:36:35.918 - 00:37:19.950, Speaker A: So when you assume synchrony and you assume PKI plus permissions, even with 99% Byzantine nodes, they can't do anything to confuse the honest nodes and get them to disagree or fail to terminate. Then with that result under our belt that was the Dole of strong protocol. We then sort of pushed toward further mountaintops and asked, can we also have good consensus protocols with provable guarantees with a shorter list of assumptions? Like I'm sure you remember, we sort of relaxed the synchrony assumption. We looked at the Asynchronous model, talked about FLP impossibility. Then we sort of settled on this sweet spot model of partial synchrony. Maybe you remember that we also actually, even before probing the synchrony assumption, we probed the PKI assumption. That was the point of lecture number three.
00:37:19.950 - 00:38:30.210, Speaker A: In that lecture we were saying let's continue to work in the synchronous model and let's assume the permissioned setting, but let's throw out the PKI assumption. Okay, I'll still give you the cryptography exists if you want, but I'm going to take away from you the assumption that somehow everybody's public keys were sort of distributed to everybody else in advance of the protocol. And the point of lecture three was that the trusted setup assumption really matters. You probably cannot replicate the fault tolerance of the Dolov strong protocol without the PKI assumption. There is no Byzantine broadcast protocol, no matter how complicated that tolerates 99% Byzantine nodes, or even, as we saw that, tolerates 34% Byzantine nodes. This was the result where the argument you might remember was a sort of very hinged on this very clever thought experiment of a bunch of nodes arranged in a hexagon. And so now the potential point of confusion hopefully is clear, right? The lecture three result says that in the synchronous model without PKI, you can't have consensus if there's 34% or more fraction Byzantine nodes.
00:38:30.210 - 00:39:08.980, Speaker A: But what do we have here on this slide? Right, we have state machine replication in the synchronous model with provable consistency and liveness as long as less than half. So up to 49% of the participation is Byzantine 49. The possibility result on this slide is bigger than 34, the threshold in our impossibility result. And that would seem to be a mathematical contradiction, but again, I'd never waste your time. The result in lecture number three is indeed correct. All of the math that we've done here in lectures eight and nine again is correct. So how can all of those results be mutually consistent? That's a little bit of a subtle point.
00:39:08.980 - 00:39:56.846, Speaker A: So to explain, let's remember when we talked about longest chain consensus in lecture eight. On the one hand we were kind of primarily focused on the sort of usual model, the permission setting with PKI and sort of most of the discussion was focused on that scenario. But we also kind of constantly made sort of forward pointers kind of remarks about how longest chain consensus is going to be instantiated in two other scenarios, the proof of work scenario and the proof of stake scenario. Now, in the basic scenario in the permission setting, we really were thinking about PKI. So we had in mind each round has a leader. That leader generates one or possibly many block proposals in what we were calling step two B, along with their predecessors. And it signs all of its block proposals as the leader of that round.
00:39:56.846 - 00:40:41.134, Speaker A: And we needed that signature to be recognized by other nodes. So we were definitely using the PKI assumption. When we thought about implementing longest chain consensus in the permission setting. In that setting you kind of were thinking byzantine nodes are going to engage in the shenanigans that they always do, which is they're going to send conflicting messages to different honest nodes. So like if you're a Byzantine leader of some round in longest chain consensus in the permission setting, maybe you're going to send one block proposal to honest node number one and a different block proposal to honest node number two. There's really no way in that setting to prevent Byzantine leaders from generating lots of different block proposals. And what we saw in all our proofs in lecture number eight is happily, the fact that the leader could generate multiple block proposals actually didn't mess anything up.
00:40:41.134 - 00:41:27.322, Speaker A: So all we needed was this one assumption, assumption A four, which was very easy to enforce. We just needed that however many blocks a leader happens to propose in some round, they need to be pointing, they need to claim as predecessors blocks that come from previous rounds, otherwise they're automatically interpreted as invalid block proposals. Now, all of this discussion for the permission setting, all of this is really leaning on the PKI assumption. We're really thinking that leaders of rounds can prove that they're the leader of the round by sort of signing all of the block proposals. So there's clearly no contradiction between the basic scenario and the 49% fault tolerance versus the 34% threshold in the PSL FLM theorem. Because of that impossibility result, it's really about the no PKI assumption. And all of our discussion of the permission model did use the PKI assumption.
00:41:27.322 - 00:42:24.660, Speaker A: Now, you can, and I encourage you to do this, you can think about how you might extend our implementation of longest chain consensus to the permission setting without PKI. And if you think that through, you'll find that you'll very quickly sort of run into problems and it will not at all be clear to you. And in fact, it's impossible to extend it so that it still has the same properties without PKI. And fundamentally, what's going to go wrong is you're not going to be able to enforce that assumption a four, which was crucial for the consistency and liveness results. So basically, what can go wrong is if you have sort of a Byzantine leader of some round, they can in effect make up block proposals that it claims were made by leaders of previous rounds, even though that they weren't. And so the problem is, because leaders can't kind of authenticate their own block proposals through signatures, it's not clear how you would check a Byzantine node sort of fabricating blocks by other leaders in previous rounds. So that's sort of what goes wrong.
00:42:25.350 - 00:42:26.114, Speaker B: All right, fine.
00:42:26.152 - 00:42:58.038, Speaker A: So in the basic scenario, kind of the permission setting, if you have PKI, then it's sort of fine. If Byzantine leaders sort of broadcast conflicting block proposals to different nodes, all of the proofs still work. If you don't have PKI, it's not clear how you would extend it. And in fact, the Pslflm theorem shows that you can't extend it and get the same guarantees. Now, the proof of stake scenario, I mentioned this in passing that's also always going to be under a PKI assumption. Whenever we talk about proof of stake longest chain, it will be with PKI. So there will be no contradiction to the PSL FLM theorem.
00:42:58.038 - 00:43:43.574, Speaker A: So what about the proof of work scenario, which is where it seems like we'd have a contradiction? Well, remember the proof of work implementation of longest chain consensus? So we alluded to this in lecture number eight, and then we actually made it real in this lecture. Lecture nine, the proof of work implementation has some additional properties beyond what was needed to carry out the consistency and liveness analysis in lecture number eight. In particular, while in the usual permission setting you're worried about the usual Byzantine shenanigans of sending conflicting information like conflicting block proposals to different nodes, in the proof of work implementation, actually we've simply taken away the power from Byzantine nodes to create multiple conflicting block proposals in the same route.
00:43:43.622 - 00:43:43.930, Speaker B: Okay?
00:43:44.000 - 00:44:34.554, Speaker A: As we've seen in Nakamoto Consensus, you're trying to solve a hard crypto puzzle. The format of the solution encodes the decisions a node would make in what we were calling step two B. So it includes its decisions of block proposals and predecessors. And then we just declare potential solutions as invalid unless they name exactly one block and exactly one predecessor for that block. And that then is why there's no contradiction, right? So when we were talking about the PSL FLM model, we were talking about just like the standard kind of permissioned model where basically everybody can send whatever messages they want, and in particular Byzantine nodes are perfectly capable of sending out multiple conflicting messages at exactly the same time. But the properties of proof of work actually mean that we're no longer in that model. We're actually in sort of a more restricted model of what Byzantine nodes can do.
00:44:34.554 - 00:45:45.746, Speaker A: They really just have no choice but to only send one block proposal. So because Byzantine nodes are in effect less powerful in the proof of work setting, that is sort of how we can get away tolerating a higher fraction of them, how we can get away tolerating 49% Byzantine nodes rather than merely 33% Byzantine nodes, which is what the lecture, the impossibility result in lecture number three would suggest. So that's one way of how to think about resolving this apparent contradiction, right? Proof of work basically makes Byzantine nodes weaker and that's why you can tolerate more of them. There's also a second interesting way to interpret it, if you sort of remember the discussion we had in lecture number three around simulation. So in that FLM proof of the PSL impossibility theorem, remember we had that hexagon and a crucial part of the argument is that even if the world is just a triangle, so you really just have three nodes sort of all connected to each other. If one of them is Byzantine, a strategy for that Byzantine node, if it wanted, would be to simulate simultaneously four of the honest nodes in the hexagon thought experiment. Now, back in lecture number three, we didn't really concern ourselves with the computational feasibility of a Byzantine node carrying this out.
00:45:45.746 - 00:46:19.466, Speaker A: It just seemed sort of we just basically assumed it could do it. And it didn't seem like that big an assumption, right? It's just some protocol. It's probably taking a very small fraction of the computer's CPU. So you have to kind of have like four threads running the same protocol instead of one, right? Presumably the computer is going to be totally capable of doing that. Totally different in the proof of work setting, actually, because now simulating sort of four other nodes in particular means sort of simulating the total hash rate possessed by those four other nodes. So like, for simplicity, just imagine all of the nodes have exactly the same hash rate. Everybody has like 1% of the hash rate.
00:46:19.466 - 00:47:31.330, Speaker A: Now, the point is a node is not capable, it can't just fabricate new hash rate for itself out of thin air. So a node with 1% of the hash rate is just physically incapable of simulating four other nodes that each have 1% of the hash rate or 4% of the hash rate between the four of them. So that's another way to think about how the Byzantine strategies that were required to push through the FLM proof of the PSL theorem because simulation is costly and sort of expensive, all of a sudden those Byzantine strategies, those strategies are not available to the Byzantine nodes. And so that's why that particular proof breaks down in the proof of work setting. And as we've seen in this, in the previous lecture, in fact that theorem is just not true in the proof of work setting. Even without PKI, even in the synchronous assumption, you actually, with Nakamoto Consensus, get up to robustness to 49% Byzantine nodes, which is sort of very cool. Another thing which is really quite specific to proof of work civil resistance coupled with longest chain consensus that's not really shared by the other versions of longest chain consensus is you don't need to assume a shared global clock or even approximately correct, approximately aligned clocks across the different nodes.
00:47:31.750 - 00:47:33.378, Speaker B: And this goes back to that property.
00:47:33.464 - 00:47:58.138, Speaker A: Right, for sort of Nakamoto Consensus rounds do not have some fixed duration. It's not like you just have a new round every 10 seconds. It's purely event driven. So whenever a node sort of solves a puzzle, that's the next round. And if you think about it, that means there's no need for nodes to actually keep track of what time it is. You're literally just sort of trying to solve puzzles, listening to hear about other people's sort of puzzle solutions and maintaining the blockchain. I'm oversimplifying a little bit.
00:47:58.138 - 00:48:59.658, Speaker A: There is an aspect of proof of work blockchains known as difficulty adjustment that we'll talk about in the next video, which does sort of need some sort of loose notion of time for it to make sense. But still, compared to all of the permission protocols you've been talking about, compared to the proof of stake blockchains we're going to talk about in lecture number twelve, the proof of work version of longest chain consensus needs much sort of weaker assumptions about the extent to which nodes share a common notion of time. Maybe somewhat negative byproduct of this is there's sort of an unpredictability as far as the amount of time it's going to take before sort of a new block gets created by some node, right? Because maybe all the nodes get super unlucky and none of the darts hit the bullseye for an unusually long time and so there's a long time between consecutive blocks. Other times some node might hit the bullseye just by luck really soon. And so you're going to have two blocks created in quick succession. Obviously, if you thought about a different implementation with a global shared clock where each round was just like 10 seconds long. Then of course, you'd be seeing blocks like clockwork once every 10 seconds, provided.
00:48:59.674 - 00:49:03.426, Speaker B: The nodes are doing their job correctly. So that concludes what I wanted to.
00:49:03.448 - 00:49:50.042, Speaker A: Tell you about the mathematical properties of proof of work. Again, really sort of two big points. First of all, the civil resistance property. So the probability as a node, the probability that you're going to be selected as the leader of a round, it depends on the fraction of the overall hash rate that you contributed and it doesn't depend on anything else. So there's no way you can costlessly manipulate your probability of being selected as a leader. And by the same token, because leader selection is proportional to hash rates, it means that if you assume that less than half of the hash rate is Byzantine, then actually a given leader is more likely to be honest than Byzantine. So in other words, proof of work gives you a kind of implicit implementation of the magenta box, the box that on each round sort of tells you which node is the leader.
00:49:50.042 - 00:50:09.974, Speaker A: So you get an implicit implementation of the magenta box with the property that we want. It spits out honest leaders with better than 50 50 probability. And remember, that was the only property we really needed to sort of start the dominoes falling with the lecture eight analysis. Again, also assuming that we make or verify these assumptions a one through a.
00:50:10.012 - 00:50:12.134, Speaker B: Five, there's going to be four more.
00:50:12.172 - 00:50:45.202, Speaker A: Videos in lecture number nine, videos four through six, you can watch them in any order. In the next video, the fourth video will talk about difficulty adjustment. So that gets to the question of how should the blockchain protocol tune the threshold tau. So how hard should the puzzles be? And we'll learn in the fourth video that the standard way you do that is to tune tau to target a particular rate of block production. In the fifth video, we will learn why the rate of block production is actually quite important. And in particular, the fifth video is where we'll finally relax. Assumption a five.
00:50:45.202 - 00:51:38.226, Speaker A: We'll sort of dispense with this sort of supersynchronous or instant communication model and work in the standard synchronous model with a nonzero value of delta. And we'll see that all of the usual guarantees hold as long as delta is small relative to the average duration of a round. In the 6th video, we'll fulfill a promise I made you way back in the first video. So in the first video, remember, we had the sort of two by two matrix proof of work, proof of stake, longest chain BFT type consensus. And I said that the sort of quadrant where you have proof of work, civil resistance coupled with BFT type consensus, that's sort of a bad idea. And so in the 6th video, we'll talk through a formal theorem which explains the sense in which it's a bad idea and probably explains why you never see projects in production that corresponds to that particular quadrant. The 7th video is a short one that's just going to be a sort of a segue into lecture number ten.
00:51:38.226 - 00:51:50.450, Speaker A: We're going to talk about finally actually introducing native currencies into our blockchain protocols and then overview kind of all of the complications that that creates that we'll be exploring in lectures ten, 1112 and 13.
00:51:50.870 - 00:51:52.386, Speaker B: So in the next video, we'll be.
00:51:52.408 - 00:52:02.550, Speaker A: Talking about difficulty adjust mint but again, feel free to just sort of watch videos four, five and six of this lecture in any order, wherever it is you wind up going next. I will see you there. Bye.
