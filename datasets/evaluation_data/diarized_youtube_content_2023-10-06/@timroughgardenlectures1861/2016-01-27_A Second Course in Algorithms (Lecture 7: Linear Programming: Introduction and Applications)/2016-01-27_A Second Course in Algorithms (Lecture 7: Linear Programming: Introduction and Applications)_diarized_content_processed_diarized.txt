00:00:00.170 - 00:00:00.720, Speaker A: You.
00:00:02.130 - 00:00:32.470, Speaker B: Let's go ahead and get started. So last lecture we wrapped up the first part of the course. The course has four parts. So the first part was about commentarial optimization. So efficient algorithms for searching over discrete structures, so things like flows, cuts, paths and so on. And so with today's lecture, we move into our second part, which is all about linear programming with an emphasis on applications and and duality. So how many of you have seen at least a little bit about linear programming at some point in your life? Just curious.
00:00:32.470 - 00:00:59.386, Speaker B: Okay, I'm not going to assume anything, don't worry. Just for calibration. So it's kind of confusing for computer scientists because this isn't programming in computer programming. So this predates kind of basically modern computers. So it's more like in the spirit of television programming, which means you basically are scheduling things. You're sort of like planning out what's going to happen. So that's the idea of linear programming.
00:00:59.386 - 00:01:51.726, Speaker B: Same reason in dynamic programming, same notion of the word. So why are we going to spend two full weeks on this topic? So, first of all, linear programming is algorithmically really useful both if you're trying to prove theorems and also if you're trying to solve problems in the real world. So linear programming, if you remember only one thing about it after 261, remember that it's a remarkable sweet spot between, on the one hand, generality. So in particular, every problem we've studied so far can be thought of as a special case of linear programming and on the other hand, computational efficiency. Linear programs can be solved fast both in theory and in practice. And we'll see many examples. The problem you care about literally is a linear program.
00:01:51.726 - 00:02:29.370, Speaker B: But there's also cases where even though your problem isn't a linear program, it's still a super useful subroutine to sort of have in your pocket. So we'll have some lectures toward the end of the course on approximation algorithms for NP hard problems. So we're not going to solve an NP hard problem exactly using linear programming unless P equals NP. But still a linear programming solution can serve as the basis from which we recover a near optimal feasible solution. So we'll see that at the end of the course. And then in practice, we don't really discuss this in 261, but in practice, probably the most cycles are used for linear programming actually as a helper subroutine in what's called integer programming. So that's a harder problem.
00:02:29.370 - 00:03:19.234, Speaker B: It's NP hard, but people work hard to solve it exactly in practice because often the viability of their business is depending on having a sort of efficient allocation of resources. And so in those integer programming codes, they invoke linear programming over and over and over again as a subroutine because that's really the workhorse of how those things get solved. So that's the first reason. It's really sort of a very general but still computationally efficiently solvable problem. But there's a second reason, which is also going to be very useful for us, which is just on a conceptual level, it's really helpful. Meaning it just gives you the right way to think about lots of problems. So like for max flow, I tried to explain how the right way to think about the problem is in terms of these optimality conditions.
00:03:19.234 - 00:03:53.320, Speaker B: No st path in the graph. How do you know when you're done? You know you're done when you've saturated a cut. We had a similar thing with matching. So linear programming, duality can be thought of as a generalization of all of these things that we've learned. In some sense, it's one of the more general answers you could ever have to the question, how do you know when we're done? That's really what duality is all about. So we'll start talking about that next lecture. So needless to say, this general answer to how do you know when you're done? It's super useful for proving algorithms are correct, or at least approximately correct, because it sort of gives you a bound on how good any solution could possibly be.
00:03:53.320 - 00:04:31.220, Speaker B: All right, so what is linear programming? Well, one way to think about it. So once upon a time, maybe a long time ago for some of you, but in at least one previous course, you may have forgotten it, but I'm sure you've seen systems of linear equations which can be thought of as an easy special case of linear programming. All right, so what do I mean by a system of linear equations where you have variables x one up to x n, and then you have a linear combination of them.
00:04:40.330 - 00:04:40.694, Speaker A: Okay?
00:04:40.732 - 00:04:51.146, Speaker B: So that's one linear constraint. So here we're thinking of the A's and B's as given and the X's as free variables that we're solving for. And it's not just one linear equation, it's a system.
00:04:51.248 - 00:04:51.514, Speaker A: Okay?
00:04:51.552 - 00:05:32.022, Speaker B: So in general, you're going to have M of these, if you prefer, in matrix form. You might want to write this. Like so you have a matrix A, so the columns correspond to variables. So there's N columns, the rows correspond to constraints. So there's m of those. So this again is given, these are the coefficients, the little A's. This is what you're solving for.
00:05:32.022 - 00:06:03.242, Speaker B: And you're also given a right hand side B, which has length M. So one number per constraint. So if you like matrices, that might be how you'd prefer to think about this. So when you studied this, I'm sure you learned a pretty good algorithm for solving a linear system in the sense of an algorithm that either determines that there's no feasible solution at all, or if there is a solution, it returns run to you. So like Gaussian elimination would be a canonical such algorithm.
00:06:03.306 - 00:06:03.534, Speaker A: Okay?
00:06:03.572 - 00:06:58.614, Speaker B: And this is the kind of thing, maybe some of you even learned it in high school, but certainly by the time of the first year math sequence here one learns this stuff. Okay, so what does this have to do with the problems we've been talking about? Well, and what's, like, a key problem with Gaussian elimination, linear systems? Well, what about inequalities? And if you think about the problems we've been studying so far, if we wanted to try to encode them in this sort of language, it seems hard to escape wanting inequalities. Think about maximum flow. What did our constraints look like in the maximum flow problem? Well, we had conservation constraints. Those are equations, so that's not a problem. But then we had capacity constraints. The flow on an edge should be at most the capacity of the edge.
00:06:58.614 - 00:07:44.458, Speaker B: It shouldn't necessarily be equal, doesn't have to be equal, can be less. And actually, we also had these non negativity constraints, so the flow should be at least zero everywhere. And it's hard to see how you would encode that into just a system of linear equations. It really seems like you capture more problems by adding inequalities to these systems. And again, just thinking about max flow, it's sort of obvious we might want to do that. All right, so one way to think about linear programming is while Gaussian elimination and algorithms of its ilk, they check feasibility of a system of linear equations, linear programming can be used to efficiently check the feasibility of a system of linear equations and inequalities.
00:07:44.634 - 00:07:45.262, Speaker A: Okay.
00:07:45.396 - 00:07:58.310, Speaker B: More generally, if you have a range of feasible solutions, like you have many flows in a network, linear programming will allow you to optimize over all of the feasible solutions. Okay, so that's the purpose of linear programming.
00:08:01.690 - 00:08:02.630, Speaker A: Um.
00:08:04.890 - 00:08:50.182, Speaker B: All right, so we're going to have several lectures on linear programming and going to do lots of examples. I basically want us all to spend some quality time with this idea because it's one of those things where, like, if you just see it in one lecture, it just doesn't sink in. It's something you need to kind of spend some quality time with to really absorb and then actually use in your later studies and work. So what's the ingredients of an LP? So really, you can sort of think of linear programming as this formalism, this sort of almost programming language for encoding certain types of problems. So let me tell you what are the ingredients? And it's very flexible. So it's a recipe, and as I'll try to convince you, even today, it's a very flexible one. So basically, we have to specify a linear program.
00:08:50.182 - 00:08:52.934, Speaker B: You have to say what's allowed, and you have to say what you want.
00:08:53.052 - 00:08:53.718, Speaker A: Okay?
00:08:53.884 - 00:09:19.130, Speaker B: So the first thing you need to decide on are your decision variables. These play exactly the same role as in x one through x n that I just erased. Okay? So each of these is a real value, positive or negative. So these are just free parameters. So it's the job of a linear programming algorithm to figure out the best way to set values for the x's.
00:09:19.210 - 00:09:19.840, Speaker A: Okay?
00:09:21.010 - 00:09:36.070, Speaker B: All right. So then you have to discuss what's allowed, what's feasible, okay? So there's going to be constraints, and no surprise, given that it's called linear programming, the constraints should be linear.
00:09:39.050 - 00:09:39.750, Speaker A: Okay?
00:09:39.900 - 00:10:22.740, Speaker B: So like the ones we had in a system of linear equations, except we're also going to allow inequalities. So these are going to have the form sum over Aij. So a linear combination of the x's, the A's are the coefficients. That's going to be the left hand side. Then we're going to have a bi on the right hand side. And in the space this can be equals or it can be less than or equal to, or it can be greater than or equal to. So all of those are allowed when you're defining a linear program.
00:10:22.740 - 00:11:04.606, Speaker B: Now, again, let me emphasize, when you think about a linear program, you think of the A's and B's as part of the input, okay? So they were just known numbers, seven minus one, et cetera. The x's are the free variables that we're solving for now. You may sort of observe that allowing these three different types really? This is a superfluous encoding, but it's convenient. Remember, I'm not assuming that the A's or B's are negative sorry, are non negative. They can be positive or negative. That's fine. So given inequality of this form, I can translate it into one of this form just by multiplying everything by minus one.
00:11:04.708 - 00:11:05.070, Speaker A: Okay.
00:11:05.140 - 00:11:17.222, Speaker B: And similarly the other direction, even equations, if you think about it, are sort of a redundant encoding, because if I want to say something is equal to ten, I can just have one inequality saying it's at least ten and a second inequality saying it's a most ten.
00:11:17.356 - 00:11:17.942, Speaker A: Okay.
00:11:18.076 - 00:11:22.498, Speaker B: So really without loss, you can think of it as all inequalities and all going the same direction.
00:11:22.594 - 00:11:23.240, Speaker A: Okay.
00:11:28.010 - 00:12:01.038, Speaker B: We don't per se, but it's easy to have a workaround, and one of today's examples will show you the workaround. So when you input it to an LP solver, you're really sort of stuck with this language, but it's not a big deal, as we'll see. Good question. Other questions. So again, AIJS and BIS are constants, meaning numbers provided as part of the input.
00:12:01.134 - 00:12:01.778, Speaker A: Okay?
00:12:01.944 - 00:12:07.690, Speaker B: So for example, like an edge capacity. That's what I mean, right? That's just going to be like twelve or whatever in some given input.
00:12:07.790 - 00:12:11.094, Speaker A: Okay? All right.
00:12:11.132 - 00:12:49.674, Speaker B: And then you have to say what you want. So just like in the specific problems we've been talking about, we need to posit an objective function more in a second. So this should also be a linear objective function. It can be a minimization or a maximization function. It really doesn't matter. And then you get to choose some other linear combination with different coefficients CJS, and you want to maximize the sum of the Cjxj sum to over J. Now, again, there's no assumption that the C's are non negative.
00:12:49.674 - 00:13:05.682, Speaker B: They could be positive or negative. So then it's sort of immediately clear that it really doesn't matter whether you write it min or max, because if you have a max, you multiply it by minus one. You may as well minimize that instead. Okay, so you can transform one objective into the other objective just by taking the minus times, the coefficients.
00:13:05.826 - 00:13:06.520, Speaker A: Okay.
00:13:08.650 - 00:13:21.034, Speaker B: All right? And that's it. So you specify those things. You've specified a linear program. You can start talking about solving it and what it means when you solve it and so on. So linear, what does linear mean here?
00:13:21.072 - 00:13:21.226, Speaker A: Really?
00:13:21.248 - 00:14:21.422, Speaker B: How should you think about it? Well, so maybe it's maybe it's simpler to say, like, what would be a violation of these rules, of the linearity rules? Okay? So it's certainly not allowed, is to have one of your decision variables, an x, embedded in some nonlinear function. So, like, if log one plus x showed up, that would be nonlinear because log is not a linear function. Similarly, if you had x one squared, that would not be linear in x, right? That's a quadratic function, not a linear function. Also, if you took, like, two different variables, x I and XJ and multiplied them together, again, that would not be linear, okay? It would be a quadratic term just with two different variables. So what's special about linear inequalities and linear objective functions is that whenever you see a decision variable, whenever you see an x, it's just alone, except possibly multiplied times some constant. Again, remember, the A's, B's, and C's are constants provided as part of the input, okay? And then, of course, you can sum them. That's fine for a linear function.
00:14:21.556 - 00:14:22.400, Speaker A: All right?
00:14:23.330 - 00:15:11.790, Speaker B: Okay, so if you want to just check, is something linear? Is it not linear? Just look at the decision variables, make sure they all appear by themselves, multiplied by constants, and then added up. And nothing else in particular? No products. Okay, so the restriction to linearity, it's admittedly significant. There's problems we care about which are not fundamentally linear, but still, as we'll see, there's a ton of applications that are exactly modeled or approximated by linear programs. Sometimes the translation is super direct, and we'll see an example of that when we go to max flow and min cost flow. But I'll also give you a couple of examples today motivated by machine learning, where you need a couple of tricks to really get the linear programming formulation. So sometimes you need some ingenuity, but I'll teach you some of the tricks of the trade in these lectures.
00:15:11.790 - 00:15:54.026, Speaker B: All right, so I do want to do one toy example. All of our other examples will be actually quite real examples. So this is the only sort of simple toy example. Why am I doing it? Reason one is just to make all this stuff a little less abstract, so you're sort of sure what we're talking about. But secondly, because I want to develop some of your geometric intuition about what it means to solve a linear program. And of course, it's much easier to have geometric intuition when you have two or three variables, you're in two or three dimensions than if you have 100 dimensions. Okay? So consider the following linear program.
00:15:54.026 - 00:16:30.326, Speaker B: So this is going to be just two decision variables, x one and x two. We want to maximize their sum, okay? That's the objective. So I have to tell you about their constraints. So let's say that I'm going to require both to be non negative. Those are two of the constraints. And then I'll posit an upper bound on two specific linear combinations. That's evidently a linear program.
00:16:30.326 - 00:17:23.398, Speaker B: Two variables, four constraints. Okay, so among everything so what is it asking us to do among everything that satisfies all four of these inequalities? Find the one with the biggest sum. So what would that look like in a picture? Since there's only two dimensions, we can think about candidate solutions as points in the plane, okay, with their x one and x two coordinates. So first, before we worry about what's optimal, let's just worry about what points actually satisfy all four of these inequalities. Well, let's just go through the inequalities one by one. So first of all, the first two inequalities say we basically are only looking in the non negative quadrant, right, the northeastern quadrant. So wanting x one to be greater than zero says we should be on the right hand side of this line of the y axis.
00:17:23.398 - 00:18:09.130, Speaker B: And saying that x two should be non negative says we should be north of the equator. Okay, so above the x axis. So how about these two constraints? So let's start with this one. It's probably simplest to just say, okay, where does the line defining this inequality hit the axes? So for what values of x one and x two? Do we have two x one plus x two equal to one? Okay, so that's a line, right? The solutions to two x one plus x two equal one. So we sort of think it might hit this zero one or two times and hits it twice, right? So if you take x one to be zero and x two to be one, this constraint satisfied with equality.
00:18:12.990 - 00:18:13.258, Speaker A: On.
00:18:13.264 - 00:18:19.542, Speaker B: The other end, if you take x one to be a half and x two to be zero, it's also satisfied with equality on this axis.
00:18:19.686 - 00:18:20.380, Speaker A: Yeah.
00:18:21.070 - 00:19:02.826, Speaker B: And so the entire line segment between these will also satisfy that with equality. So the set of solutions that satisfy an equation is just a line in the plane. And so what does the constraint say? It says you have to be southwest of this line. So two x one plus x two should only be less than one. Okay, so this is two x one plus x two at most, one, and then symmetrically. You've got this. This is not really drawn very well.
00:19:02.826 - 00:19:57.290, Speaker B: To scale in your mind, think of this as symmetric. Okay, so what's the feasible region? Well, it's points that satisfy every single one of these inequalities. So it has to be in every single one of the four half planes that we mentioned has to be above here, to the right of here, below here, and below there. So that's going to be this parallelogram or this trapezoid. So it's called this feasible region. So the feasible region is the set of all ways to set the decision variables so that all the inequality constraints are satisfied, possibly with equality, possibly not with equality, possibly with slack. So any questions about that? Everyone see that? That's the feasible region.
00:19:57.290 - 00:20:18.686, Speaker B: Okay, so how about optimizing over the feasible region? So we're supposed to maximize the sum. So a good way to think about that geometrically is if you think about it. So that's saying we want to go in the direction of the vector one, comma one as far as possible I e. We just want to go northeast as far as we can.
00:20:18.868 - 00:20:25.640, Speaker A: Okay? It it.
00:20:28.090 - 00:20:33.540, Speaker B: So what can we imagine doing? Well, think about the level sets of this objective function.
