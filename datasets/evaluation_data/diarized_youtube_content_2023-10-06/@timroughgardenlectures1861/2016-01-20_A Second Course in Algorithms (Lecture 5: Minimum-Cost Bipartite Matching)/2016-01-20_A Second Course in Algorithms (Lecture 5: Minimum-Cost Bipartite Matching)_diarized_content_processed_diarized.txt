00:00:00.330 - 00:00:26.706, Speaker A: All right, let's go ahead and get started. So the topic today is again going to be bipartite matching, but now a version with edge cost. So the min cost perfect matching problem. Just a quick reminder, problem set number one is due a week from today. Hopefully you've already looked at it and started thinking about the problems. It's not that easy. The kind of problems which you know, often take a few days to sort of stew and talk to other people about before you see it a crackham.
00:00:26.706 - 00:01:00.634, Speaker A: So definitely do not wait until the night before or you will be in a world of pain, unfortunately. And exercise set two is out. That was posted the end of last week. Exercise set three should be posted a bit next week. So let me remind you where we left off on Thursday, which will segue into our next topic. So we concluded our discussion of maximum flow applications with the discussion of the bipartite matching problem. So remember, what's a matching? So if you have an undirected graph, a subset of edges is a matching if there's no shared endpoints.
00:01:00.634 - 00:01:27.190, Speaker A: So the edges in the matching need to be disjoint. And we studied the problem of finding a max cardinality matching in bipartite graphs. And again in matching, a majority of the applications are already captured by the bipartite special case. And theory is much nicer and the algorithms are much faster for the bipartite case. So that's what we're focusing on. So for example, the squares of bipartite graph, these green edges would be a particular matching. It would be a perfect matching because it actually matches all of the vertices.
00:01:27.190 - 00:02:11.670, Speaker A: And what we saw on Thursday was that if you want to compute a maximum cardinality matching on a bipartite graph, it reduces to the maximum flow problem. And of course, we already know how to solve the maximum flow problem. And the reduction was very simple. We just added a new source, a new sync, directed all the edges from left to right, computed a max flow. And there was a very immediate correspondence between matchings in the original undirected graph and integral flows in the directed flow network that we constructed. So the motivation for this lecture is so if you have a bipartite graph and maybe you have the happy case where it has a perfect matching, but maybe actually it even has like lots of perfect matchings. So maybe you're trying to assign courses to time slots, jobs to workers, whatever, and maybe there's many ways you can do it.
00:02:11.670 - 00:03:08.120, Speaker A: Is there some reason why you might prefer one of these perfect matchings over another? And in applications the answer is often yes. You actually do have a preference over exactly which perfect matching you pick. You'll see some applications on the second problem set, but you can already imagine if you're assigning jobs to workers, maybe a given job can be done by lots of different workers, but maybe some workers are better at it than others. As you'd like to incorporate those preferences into your matching computation, there's the formal definition of minimum cost. Bipartite matching should say perfect bipartite matching. So the input so like last time you have a bipartite graph. So there's vertices V and vertices W.
00:03:08.120 - 00:03:36.370, Speaker A: What does it mean to be bipartite? It means each edge has one endpoint at each of V and W. Neither V nor W has any internal edges equivalently, a bipartite graph is one without any OD cycles. Every cycle has to be even in a bipartite graph. And unlike last time, it's going to be a more general problem where we're also told a cost C sub E for all edges.
00:03:37.990 - 00:03:38.740, Speaker B: Okay.
00:03:47.270 - 00:04:16.910, Speaker A: Now next I'm going to tell you about some assumptions that we're going to have throughout the lecture. None of these assumptions are important. All of these assumptions are for convenience. There are easy reductions that show that each of these assumptions is basically without loss of generality. There'll be some details on this week's exercise set and I'll talk through why right now. So assumptions and again for convenience only. So first of all, I'm going to assume that the two sides have the same cardinality.
00:04:16.910 - 00:05:04.058, Speaker A: Call it N. So there's two N vertices N in V and N and W. Why is this without loss of generality where if this is violated, just add some dummy vertices to the smaller side to make it true. It doesn't change the problem that's assumption one, two sides have the same cardinality. I'm going to assume that G has a perfect matching. This might bother you because last lecture, the whole point was to check if there's a perfect matching. But actually once you have costs, if you think about it, this is also a trivial condition to enforce, namely, just add some dummy edges that have very high cost, okay, so that enforces a perfect matching.
00:05:04.058 - 00:05:09.470, Speaker A: But if there was a perfect matching in the first place, it doesn't change the cost of an optimal solution.
00:05:11.170 - 00:05:11.614, Speaker B: All right?
00:05:11.652 - 00:05:18.530, Speaker A: So assuming that it has a perfect matching, not a big deal. And finally I'm going to assume that all the edge costs are non negative.
00:05:20.150 - 00:05:20.900, Speaker B: Okay?
00:05:22.070 - 00:06:02.830, Speaker A: Again, for some problems this matters, like for shortest paths, as you know, but for matching it doesn't matter the non negativity constraint, right? Because if you have a graph with a perfect matching and you're trying to find the best perfect matching, well you could just add like if you have negative edge costs, you can just add 10,000 to every edge. That changes the overall cost of every perfect matching by exactly N, the number of edges in a perfect matching times 10,000. So it has no effect on the ordering of which matchings you're regarding as better than others. Okay, so that was fine. If that was all a little too fast. In real time, I'll ask you to think about this a little bit more in the exercise set. And for the lecture, let's just adopt these as assumptions about the input.
00:06:02.830 - 00:06:36.380, Speaker A: And then the goal, as I've indicated, is among all the perfect matchings and there's at least one find the one with the minimum sum of edge costs. So the goal is to find it perfect matching, minimizing some over the edges and the matching of the edge costs. So I insist that you pair up all of the vertices and I want you to do it as cheaply as possible.
00:06:36.750 - 00:06:37.500, Speaker B: Okay?
00:06:41.870 - 00:06:54.010, Speaker A: So for this formulation of the problem, we're really thinking of the feasible solutions not as matchings but of perfect matchings. And then amongst all the perfect matchings there's an objective function you want to optimize, namely the sum of the edge costs.
00:06:54.090 - 00:06:54.720, Speaker B: Okay?
00:06:56.710 - 00:07:37.562, Speaker A: All right, so hopefully it's also clear. Last time we were talking about max cardinality, now we're talking about minimizing. But again, for this problem it doesn't matter like the maximum weight perfect matching problem, that's the same as this problem. You just multiply all of the edge costs by minus one, call them weights, and now find a max weight perfect matching. But for a convention we're going to focus on min cost. That's how people usually talk about this problem. All right, so in the special case where all of the edges were the same, so say unit cost, then what we saw last time was that you can solve this problem just by reduction to maximum flow.
00:07:37.562 - 00:08:12.166, Speaker A: So that was when we didn't have edge costs. Now we do have edge costs and it doesn't really feel like the maximum flow problem is going to be able to encode the min cost perfect matching problem. And the reason is it's not clear what parameters in a max flow instance would encode these edge costs. Now maybe you think, well, I mean, edges and maximum flow, each of them also has a number, right? The capacity. But really that's different capacities and costs are not the same thing. A capacity on an edge that actually restricts what solutions are feasible. The edge costs, they don't restrict which ones are feasible.
00:08:12.166 - 00:08:44.866, Speaker A: The feasible solutions are just the perfect matchings. The edge costs dictate. What are your preferences over the different perfect matchings? So the capacities are about feasibility, the edge costs are about optimality. So that's the intuition for why this seems like a new problem. You shouldn't be expecting me to just give you a sort of easy reduction to maximum flow. Okay, so it's a new problem, seems sort of incomparable to the maximum flow problem. But again, maximum flow taught us that the types of problems we're dealing with.
00:08:44.866 - 00:09:30.766, Speaker A: Now you can't just sort of propose random algorithms and hope they're going to be correct. You need to have a strategy, you need to have some discipline, you need to say, okay, well, how am I going to prove the correctness of my algorithm at termination? What are sufficient conditions for optimality? And now how do we use that as a guideline for iteratively, achieving those conditions and also feasibility. That approach served us very well for the maximum flow problem. So remember, what was the maximum flow approach? Well, first we had our optimality condition. How do we know that a flow is maximum? Well, you construct the residual graph and you check if there's an st path or not. And we proved if there's no st path in the residual graph, then the flow has to be maximum. So given that we then looked at two different algorithmic paradigms for maximum flow algorithms.
00:09:30.766 - 00:10:00.170, Speaker A: In the first one, we maintained the invariant of feasibility. We always kept a flow. These were the Augmenting path algorithms, the first three that we studied. And then we worked toward satisfying the optimality conditions. We worked toward disconnecting S and T in the residual graph. If you'll recall in lecture three in the push relabel algorithm, we flipped them around. So we maintained as invariants the optimality conditions, but we relaxed feasibility and then we, over time, worked toward recovering a feasible solution.
00:10:00.170 - 00:10:57.230, Speaker A: So today for the min cost bipartite matching problem, we're going to follow that second paradigm, something very similar to what we did for push for label. So we need to come up with some optimality conditions. So given a perfect matching, how do you know whether or not it actually really is optimal? Or could there be some matching that has even smaller cost? Then once we have the optimality conditions, I'll specify some invariants which imply the optimality conditions, just like we had invariants in push for label. And then we'll design an algorithm which always maintains those invariants and eventually restores feasibility, that is eventually actually comes up with a perfect matchup. So that's sort of what's going ahead. And the reason I sort of say all this up front is I want it to be clear that even though these are pretty hard problems and there are lots of problem specific differences between them, there still really is some kind of high level recipe for how you would prove all of these greatest hits of algorithms. I mean, really there's a mindset which leads you to think about and understand properly these algorithms.
00:10:57.230 - 00:11:41.786, Speaker A: Okay, so step one is optimality conditions. That's where we got to start. So we're not going to talk about algorithms at all at the moment. We're just going to say, suppose you handed me a perfect matching. How would I know if it was the best one possible? Let's start a new board for this. So what we're looking for is an analog of the no st path in the residual graph characterization of maximum flows. And so just like for the max flow min cut theorem, we needed some preliminaries.
00:11:41.786 - 00:12:12.834, Speaker A: We needed to define residual networks. We needed to define cuts. We're going to need some sort of equally natural definitions here that concern matchings rather than flows. So suppose we have some matching M, not necessarily perfect. It could even be the empty set. That's fine, just some matching m. Then we're going to call a cycle C of the graph.
00:12:12.834 - 00:12:58.978, Speaker A: And remember it's a bipartite graph, so cycles have to be even. So it could be something like a six cycle. So a cycle C of G is called m alternating. If as the name would suggest, every other edge of C is in the matching. So you maybe have some really big graph here and you have some matching and maybe these three edges are also in the matching, then that would be an example of an m alternating cycle.
00:12:59.154 - 00:12:59.880, Speaker B: Okay.
00:13:02.170 - 00:13:23.178, Speaker A: Now every other edge of the cycle is in the matching. Notice you're never going to have more than every other edge of a cycle in the matching. This is as full as a cycle can get in a matching, right, because otherwise someone would be matched twice. But it is totally possible that you have some matching, even a perfect matching, and that there are some even cycles which don't have any edges in the matching.
00:13:23.274 - 00:13:23.582, Speaker B: Okay?
00:13:23.636 - 00:14:20.586, Speaker A: That can happen too. So this just says the cycle is as full as it could get. So one thing that's cool about these m alternating cycles is they allow us to take one matching and easily transform it into a second matching with the same cardinality. Namely we just toggle amongst the cycles, the edges in the cycle, which ones are in the matching and which ones are not in the matching. So by toggle I just mean you could take this six cycle which is alternating, remove the three pink edges and replace them by the three blue edges, okay, that would yield a new matching. And notice that the vertices in this new matching that are matched up well, they're exactly the same vertices that were matched up before we toggled the edges. So obviously there's no effect outside the cycle.
00:14:20.586 - 00:14:25.618, Speaker A: Inside the cycle everyone was matched both before and after, just in a different way to each other.
00:14:25.784 - 00:14:26.210, Speaker B: Okay?
00:14:26.280 - 00:15:17.330, Speaker A: Is everyone clear on that? So an m alternating cycle allows you to do this toggle operation. We'll see why that's useful in a second. That's the first part of the definition, an alternating cycle, an alternating cycle is negative. It if the costs of the edges in the matching in the cycle exceed the costs of the edges of the cycle, not in the matching. So if sum over the edges that are in the cycle and also in the matching exceeds the sum over the cycle edges, not in the matching of their costs.
00:15:18.470 - 00:15:19.220, Speaker B: Okay?
00:15:23.750 - 00:16:00.590, Speaker A: So for example, here's an even cycle, there's a perfect matching. And this fourth cycle is indeed a negative cycle with respect to the pink matching. So first of all it's alternating edge in, edge out, edge in, edge out, and secondly the edges in have cost eleven, the edges out have cost nine. Okay, so that's the negativity condition.
00:16:01.330 - 00:16:02.080, Speaker B: Okay.
00:16:04.070 - 00:17:10.066, Speaker A: So negative cycle all right, everyone clear on those definitions? With those definitions, I can state the optimality condition. Okay, well, actually, let me ask you so any guesses? So what are we trying to do? We're trying to understand how do we know whether a given perfect matching is minimum cost or not? That's what the optimality condition is supposed to help us with. So why would I make this definition of negative cycles? What does the negative cycles have to do with the question of whether or not a perfect matching is minimum cost? Anything, or I just pulled this out of a hat. It's sort of like the hall problem good. Where if we get none of these and it's probably going to be optimal if we have no negative cycles. Right, good. So a suggestion was drawn with the Hall's theorem, which was giving us a way how do we know whether matching is maximum cardinality or how do we know if a bipartitect graph has a perfect matching? And so what we saw with hall theorem was we saw well, there's one simple kind of obstruction, or what I was calling constricting sets.
00:17:10.066 - 00:17:17.540, Speaker A: So you have a subset of vertices on the left hand side so that the number of neighbors on the right hand side is strictly smaller. So if you have ten vertices on the left with only.
