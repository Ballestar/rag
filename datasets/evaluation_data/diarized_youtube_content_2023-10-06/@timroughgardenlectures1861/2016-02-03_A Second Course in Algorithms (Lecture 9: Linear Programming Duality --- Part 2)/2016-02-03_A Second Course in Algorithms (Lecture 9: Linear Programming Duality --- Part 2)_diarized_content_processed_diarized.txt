00:00:00.490 - 00:00:34.040, Speaker A: So let's go ahead and get started. So this is our third lecture on linear programming. A week ago, I tried to convince you that linear programs model lots of different problems that we care about. So we should care about understanding them, we should care about algorithms for them. And then on Thursday, we started our discussion of linear programming duality, which we're also going to be discussing all day today. And really LP Duality. I think in hindsight, after you finish the course, you'll recognize it as really sort of heart and soul of CS 261, really kind of the concept that binds an enormous overwhelming majority of the concepts that we're going to discuss.
00:00:34.040 - 00:01:10.558, Speaker A: So I know that was five days ago and it was a pretty conceptually rich lecture on Thursday. So let me help you page back in sort of where we left off. So if you have a linear program of the following form. So a maximization linear program subject to inequality constraints with non negative random variables. We talked about the idea of generating upper bounds on the best possible objective function value of this linear program by taking suitable linear combinations of the constraints. And then we noticed that looking for the tightest upper bound of this form is itself a linear program. And that linear program is called the dual.
00:01:10.558 - 00:01:46.010, Speaker A: And in general, what happens when you pass to the dual, min goes to max, max goes to min, variables turn into constraints, constraints turn into variables. The objective function on the right hand side gets swapped, and the constraint matrix gets transposed. That's sort of what's happening here. So you can see some of it more clearly in the vector matrix notation, which I've listed in pink. So for example, you can see here that the constraint matrix gets transposed when you pass to the dual. So this is just one particular form of a linear program. And as we've discussed, there's many forms.
00:01:46.010 - 00:02:02.510, Speaker A: You could have equality constraints instead of inequality constraints. You could have general real valued variables instead of non negative variables. You could have a min instead of a max. And last time we talked about how to take the dual of any linear program, and we'll actually use that general recipe today when we return to bipartite matching.
00:02:02.590 - 00:02:03.220, Speaker B: Okay?
00:02:04.310 - 00:02:43.454, Speaker A: All right, so once we understand the meaning of the dual linear program, once you understand that every feasible solution to the dual is by construction gives you an upper bound on the primal objective function value, at least for the maximization case, then weak duality is self evident, right? So basically, every dual solution has an objective function value which upper bounds the objective function value of every primal solution. Fusible primal solution that holds in particular for the optimal solutions to the primal and the dual, from which we can conclude that the optimal solution of the primal is at most the optimal solution of the dual.
00:02:43.582 - 00:02:44.162, Speaker B: Okay?
00:02:44.296 - 00:02:50.846, Speaker A: If the primal is a minimization linear program rather than maximization, then this inequality goes the other direction.
00:02:50.958 - 00:02:51.620, Speaker B: Okay?
00:02:52.710 - 00:03:34.142, Speaker A: So just to remind you, and also for reference, here's the proof of weak duality. And so, again, what this is showing, it's saying take anything, any x feasible for the primal, take any y feasible for the dual. Let's show that the objective function value of x, which is this, is bounded above by the objective function value of the dual solution y, which is this. And again, it holds in particular for the optimal x and the optimal y. And so how do you do this? Well, you just invoke for this inequality. You invoke that y is feasible for the dual. So these linear combinations, a transpose y of the constraints dominate, the objective dominates C.
00:03:34.142 - 00:03:52.882, Speaker A: We're also using here that x is non negative. So by making this bigger, we only make the overall quantity bigger because all of these are non negative. This is just association. And over here we're using similarly that x is feasible and that y is non negative.
00:03:53.026 - 00:03:53.814, Speaker B: Okay?
00:03:54.012 - 00:04:24.622, Speaker A: So we did that derivation last time. This is just a succinct reminder. And again, that picture you might want to have in mind is the same one I drew when we talked about the value of a flow being at most the value of a cut. So again, this is for the case of a maximization primal, otherwise it would be reversed. So we can plot on a line sort of of objective possible objective function values. The x's are the objective function values that can be attained by feasible solutions to the primal. The O's are the objective function values in the dual that can be obtained by feasible solutions for the dual.
00:04:24.622 - 00:04:50.358, Speaker A: Weak duality says all the O's are only to the right of all the x's. And one thing that gives us immediately is a sufficient condition for optimality. It's not clear if it's necessary yet, but it will be necessary in the end. So a sufficient condition for optimality would be that you have a superimposed x and an O, that is, that you can find feasible solutions with equal objective function value.
00:04:50.524 - 00:04:51.240, Speaker B: Okay?
00:04:51.770 - 00:05:11.230, Speaker A: This proves that both x and y are optimal for their respective linear programs. Y. Well, no x can be strictly to the right of any O. So if you have an x exactly with an O, then that's got to be as rightmost as any x could be. It's got to be an optimal primal solution. Similarly, you know, o can be the left of an x. So if you can actually superimpose with an x, you got to be optimal.
00:05:11.230 - 00:05:27.720, Speaker A: And then the last thing that I left you with on Thursday was complementary slackness conditions. So let me remind you about those and also explain why they're true.
00:05:29.610 - 00:05:29.926, Speaker B: Or.
00:05:29.948 - 00:06:20.882, Speaker A: Why they're a sufficient condition for optimality. So complementary slackness is sort of a corollary of that corollary. So the complementary slackness conditions. So again, suppose you have XY feasible for the primal and dual, respectively. And suppose it's the case that over in the primal, the only time you give a non zero value to a decision variable is when the corresponding constraint in the dual is tight, meaning holds with equality. So remember, every variable in the primal corresponds to a dual constraint in the dual. Every constraint in the primal corresponds to a variable in the dual.
00:06:20.882 - 00:07:02.370, Speaker A: So you can talk about saying a primal variable XJ. It is only nonzero when the corresponding constraint. So the jth constraint of the dual is tight I E holds with equality. Okay, that's one of the two conditions. Primal variables are only nonzero when the corresponding dual constraints are tight. And then we also have a second condition which goes the opposite direction, which says that a dual variable should only be nonzero when the corresponding constraint of the primal is tight.
00:07:05.670 - 00:07:06.420, Speaker B: Okay?
00:07:06.790 - 00:07:16.162, Speaker A: So again, the ith decision variable in the dual is in correspondence with the ith constraint in the primal. So we can talk about the corresponding constraint in the primal being tight.
00:07:16.306 - 00:07:17.000, Speaker B: Okay?
00:07:17.790 - 00:07:28.410, Speaker A: All right, so that's the hypothesis. Suppose you have feasible solutions that satisfy one and two. Then we get the same conclusion as before. Then x, y, both optimal.
00:07:30.910 - 00:07:31.660, Speaker B: Okay?
00:07:33.310 - 00:08:32.430, Speaker A: So what I want to do next is prove this before that. So this brings us back up to date with where I left off on Thursday. So any questions about the recap or about the big picture? All right, so the proof of this is actually staring at us on the top board. So suppose x and Y are feasible, and suppose one and two hold. Let's stare at that circle derivation up there. That proves that the objective function value of the prime was always at most that of the dual. I claim that the point of conditions one and two, what it buys us is it allows us to conclude that both of the inequalities in the circle derivation are actually equalities.
00:08:32.430 - 00:09:22.960, Speaker A: So in the weak duality, proof one plus two imply that both inequalities are actually equalities. So this might be a little bit easier to see if I switch back from matrix vector notation back to the sums. So, for example, focus on this first inequality here. C transpose x is at most this other thing. So y transpose ax, y transpose a, transpose x. And consider a single term, right? So C transpose x is the sum over j of Cjxj. Just consider some fixed j.
00:09:22.960 - 00:09:48.598, Speaker A: So if one holds, then I claim that for all J-C-J XJ just equals XJ times sum over I AI J-Y-I.
00:09:48.684 - 00:09:49.174, Speaker B: Okay?
00:09:49.292 - 00:10:08.766, Speaker A: So again, this is for a fixed j. That's just one ingredient of each of these numbers, right? So these are all scalars. This is a sum of a bunch of scalars summed over j. Same thing here. I'm saying coordinate by coordinate. The terms match, okay? And if you look at it this way, it should now sort of be obvious.
00:10:08.948 - 00:10:09.680, Speaker B: Okay?
00:10:10.050 - 00:10:30.406, Speaker A: Case one, XJ is zero, both sides are zero, no problem. Case two, XJ is not zero. Well, then just the condition tells us that the corresponding dual constraint holds with equality I E. This number actually equals this number.
00:10:30.588 - 00:10:31.366, Speaker B: Okay?
00:10:31.548 - 00:10:45.990, Speaker A: So either XJ is zero and you're done, or the equality or the thing is tight and you're also done. So it's called complementary slackness because it's just asserting that you don't have slack simultaneously in the primal and in the dual.
00:10:46.070 - 00:10:46.314, Speaker B: Okay?
00:10:46.352 - 00:11:16.898, Speaker A: If you think about a nonzero value corresponding to having slack in the primal and you think about being strict, inequality, having slack in the dual, you'd never see both of those at once. That's the condition. If the condition holds, then because the circle derivation holds with equality, that proves that the objective function value of the primal and the objective function value of the dual are the same. And by our previous corollary, that says they both have to be optimal just by the X's and O's argument.
00:11:17.074 - 00:11:17.800, Speaker B: Okay?
00:11:23.210 - 00:11:26.840, Speaker A: So that's complementary slackness and that's why it's true.
00:11:27.290 - 00:11:28.040, Speaker B: Okay?
00:11:30.090 - 00:12:36.270, Speaker A: So next I'm going to do a couple of different things which are less abstract, more concrete, should help you understand this better. But any questions just about sort of the abstract versions of these concepts before I continue? Okay, so one thing which I offer to you sort of purely as a metaphor, not as anything formal or precise, but that seems to be helpful for students sometimes, is a way you can kind of physically think about or interpret complementary slackness conditions. So suppose you have a linear program and you've got some direction, some objective function, and then you're going to have some optimal solution, x star, the point furthest in the feasible region in that particular direction. So if you want, you can think about the objective function as effectively exerting force, force in the direction C. Okay? And you can think of these as sort of physical walls, right? So the sides of the feasible region.
00:12:37.410 - 00:12:38.160, Speaker B: Okay?
00:12:38.690 - 00:13:32.938, Speaker A: So basically in optimizing this linear program, as we've sort of discussed before, you're pushing in a particular direction until you can't go any further. And in general, except for some edge cases, you're going to wind up at a vertex at a corner. Now at this point, once you get to x star, there's no longer any movement. X star is just holding there despite the fact that the objective function is still exerting force in the direction C. So if there's no motion, then somehow the sum of the forces acting on this point should add up to zero. So what else is exerting force on the point other than the objective function? Well, the sides, or if you like, the constraints, can also exert force backward in the opposite direction. The direction of that force exerted by one of these walls is going to be given by the normal to that hyperplane.
00:13:33.034 - 00:13:33.680, Speaker B: Okay?
00:13:34.530 - 00:14:12.178, Speaker A: You can then interpret these yis. So previously we were interpreting the dual variables as multipliers of the primal constraints. For this physical intuition, you can interpret the yis as the magnitude of the force being exerted in this particular direction, being exerted in the direction of the normal vector of the ith constraint. So remember, dual variables correspond to constraints. So we have one dual variable for each of these sides. So if this is motionless, the sum of the forces should be zero. Now, what does complementary slackness say? So the version I'm looking for is this version.
00:14:12.178 - 00:14:47.782, Speaker A: So let's look at condition two. So yi is strictly positive only if that corresponding constraint in the primal is tight. That just says that any of the sides of the feasible region which don't actually touch where the point gets stuck, they're not allowed to exert any force on this point, okay? Because they don't touch it. On the other hand, anything which is tight, so knows these correspond to two tight constraints, these two sides of the feasible region. So anything that is tight can exert force. And then you're looking for a sum of those forces which equals sort of the opposite of the objective function value.
00:14:47.916 - 00:14:48.358, Speaker B: Okay?
00:14:48.444 - 00:16:05.220, Speaker A: So if you like, this is one way to think about complementary slackness. So basically, the only things that can exert force on a solution are the constraints that are binding, the constraints that are touched by the optimal solution. So why am I telling you about complementary slackness? What are they good for? Well, if you think about it, if we take a step back so we talked about when we were solving max flow and then again when we were solving matching, how these sort of harder problems in P required sort of discipline. They require really, a strategy to solve them correctly. So you almost have to sort of be thinking about how you're going to prove an algorithm correct when you're designing the algorithm itself. Complementary slackness provides a very general paradigm, really three different paradigms for these kind of discipline strategies for solving problems. So what do I mean? Well, consider three conditions which may or may not be satisfied at any given point.
00:16:05.220 - 00:16:30.866, Speaker A: You have some kind of attempted primal solution. You have a Y, and you have the combinary slackness conditions. What do we just say? We just said if one through three are all satisfied, then X and Y are both optimal.
00:16:30.898 - 00:16:31.046, Speaker B: Okay?
00:16:31.068 - 00:16:57.090, Speaker A: So we know that. Now when you want to solve a problem from scratch, you don't expect to just zip straight to the optimum. So the general paradigm would say, well, of the three things which together are sufficient for optimality, pick two of them that you're going to keep satisfied at all times in your algorithm, you're going to violate the third one initially and work toward restoring the third condition.
00:16:57.750 - 00:16:58.500, Speaker B: Okay?
00:16:59.830 - 00:17:14.486, Speaker A: So the paradigm would say pick two to maintain at all times so as invariance work toward the third.
00:17:14.668 - 00:17:15.400, Speaker B: Okay.
00:17:17.770 - 00:18:07.750, Speaker A: So you could start with a feasible primal and a feasible dual, presumably not optimal, maybe just sort of initializations and then work toward complementary slackness. You could start with a primal solution that's feasible, an infeasible dual and complementary slackness, and work toward dual feasibility. Or you could start by violating the primal feasibility constraints, maintain a feasible dual and complementary slackness, as invariance, okay, so it turns out an enormous fraction of the polynomial time algorithms, or just the useful algorithms for linear programming. And also special cases like we've been talking about are in fact instantiations of this general paradigm. That's exactly what they do. In fact, maybe one of the most transparent examples of this design paradigm is an algorithm. You already know the Hungarian algorithm.
00:18:07.750 - 00:18:33.514, Speaker A: So what I'm going to show you next is that the Hungarian algorithm indeed maintains two and three. It maintains a dual feasible solution at all times, it maintains complementary slackness at all times, and it works toward primal feasibility. And the reason now, in hindsight, now that we understand linear programming duality, we can say the reason the Hungarian algorithm is correct is because it halts with primal feasibility, dual feasibility and complementary slackness.
00:18:33.642 - 00:18:34.366, Speaker B: Okay?
00:18:34.548 - 00:18:41.710, Speaker A: So that's really kind of the sophisticated way to think about the Hungarian algorithm, and we're now in a position to have that kind of understanding.
00:18:44.530 - 00:18:44.940, Speaker B: All right?
