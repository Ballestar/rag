00:00:00.410 - 00:00:00.766, Speaker A: It.
00:00:00.868 - 00:00:43.500, Speaker B: All right, so having now essentially completed CS 261 with the skills you've learned this quarter, you're now in a position to sort of learn and appreciate, you know, a lot of stuff in advanced algorithms. And so this lecture is just going to be sort of one illustrative point. I'm going to show you a really cool and quite famous approximation algorithm, which will build on a number of the ideas that we've seen over the quarter. And I could pick many other examples of cool results which you're now well positioned to understand. All right, so the lecture is going to be about the maximum cut problem. So we talked a lot about minimum cut at the beginning of the class. This is going to be about maximum cut.
00:00:43.500 - 00:01:31.910, Speaker B: So the input is an undirected graph and a non negative weight for all edges. And the goal is to compute the cut AB. So this is just a partition of the vertices into two different sets, maximizing the total weight of the cut edges, where an edge is cut if it has one endpoint in each of the two sides, one endpoint in A, one endpoint in B. Now if this were the minimum cut problem, we'd know what to do. So then we could reduce it to the maximum flow problem.
00:01:31.980 - 00:01:32.214, Speaker A: Okay?
00:01:32.252 - 00:02:09.400, Speaker B: So we don't have an S and T here, but even so it reduces to the maximum flow problem if we wanted to minimize, but instead we want to maximize. So one obvious sort of hack would be, well, let's just negate all of the edge weights and then solve a min cut problem. And that's actually worked for us in the past on occasion. But if you actually go back to all of our maximum flow and minimum cut algorithms, they're assuming non negative edge capacities. And that hypothesis is indeed used in the correctness of all of those algorithms. So if we take a max weight instance, negate the max cut instance, negate the edge weights, we get a minimum cut instance with negative edge weights. And it's not totally clear what's up with that.
00:02:09.400 - 00:03:14.362, Speaker B: And indeed, this is an empty hard problem, it turns out, which is not hard to prove. So as a result, we're going to talk about an approximation algorithm. So restricting attention to polynomials and algorithms, how close to fully correct can we get? What approximation ratio can we get? Well, pretty much at the very dawn of approximation algorithms, so like kind of early 70s or so, it was observed that it's really not difficult to get a one half approximation, that is, to have a polynomial time algorithm that outputs a cut where the weight of the crossing edges is at least half of the maximum possible. Pretty much anything you might think to try gives you a one half approximation. Greedy local search, picking a cut uniformly at random, linear programming, rounding, all that stuff, gives half approximations. So that's the good news. There's all these different ways to get zero five.
00:03:14.362 - 00:03:49.090, Speaker B: But it was pretty frustrating. It sort of seemed like the whole algorithmic toolbox was getting stuck at zero five, and the concern was that maybe actually it was NP hard to do better. We've seen some other examples where it's sort of NP hard to beat the sort of obvious algorithm like set cover and set coverage, for example. So that's why it was sort of a really cool algorithmic breakthrough when in 94, Gomez and Williamson developed an approximation algorithm which does significantly better than one half. So what was the breakthrough? The breakthrough was to make use of something called semidefinite programming, which is kind of like linear programming, but taken to the next level.
00:03:49.160 - 00:03:49.394, Speaker A: Okay.
00:03:49.432 - 00:04:01.100, Speaker B: So it's again going to be of the template where we have a relaxation, although it'll be a different kind of relaxation as I'll explain. And then again, we're going to round it to bona fide cut without losing too much in the rounding process.
00:04:01.470 - 00:04:02.314, Speaker A: Okay.
00:04:02.512 - 00:04:53.340, Speaker B: So that's what we're going to do. Now actually, most of what I need to do is explain to you what this relaxation is and provide you some evidence about why you can actually efficiently solve the relaxation. Because again, it's going to be kind of an LP relaxation on steroids a little bit. So I want to introduce you to it in two steps. So first I'm going to give you actually an exact formulation of the maximum cut problem. So this is going to be an NP hard formulation to solve, but this will be the form in which we can figure out how to relax it. So there's going to be decision variables one per vertex yi, and the constraint will be that it's plus minus one.
00:04:53.340 - 00:05:00.346, Speaker B: Okay, so the plus or minus one is just indicating which side of the cut the vertex lies on.
00:05:00.448 - 00:05:01.002, Speaker A: Okay.
00:05:01.136 - 00:05:29.598, Speaker B: So you might say y zero one. You could do zero one. It's just sort of more convenient to do minus eleven. Good. So in this formulation, there's going to be no constraints other than this that each yi is plus minus one. Now, what's the objective? So I claim we can express the maximum weight objective function exactly in terms of these variables. Okay, so notice there's already a one to one correspondence between cuts and feasible solutions, right.
00:05:29.598 - 00:06:05.620, Speaker B: For any plus minus one assignment to the labels to the vertices, you get a cut and vice versa. So how do we write down the objective function in terms of the yis? Well, so we want to maximize the sum of the cut edges. So presumably we have like a sum over all edges. If we happen to cut IJ the edge I j, then we get a prize of its weight W-I-J and then we want this multiplied by something which is one if the edge is cut, and which is zero if the edge is not cut.
00:06:06.070 - 00:06:06.820, Speaker A: Okay?
00:06:07.910 - 00:06:27.878, Speaker B: And so the point here. And the reason this is a quadratic formulation is if you think about the product of yi and YJ, this product is one if the two vertices are on the same side of the cut. Doesn't matter which side, just the same side, and it's minus one if they're on different sides of the cut.
00:06:28.044 - 00:06:28.710, Speaker A: Okay?
00:06:28.860 - 00:06:52.000, Speaker B: And so in that second case, one on different sides of the cut, and that's a minus one. We want to get this bonus of the weight of the edge. And so the slick way to do that is you say, okay, well it's good for us when this is a minus one. So we want a minus sign. So now this is going to be plus minus one with the opposite semantics. And so the way to get that back to a zero one is just to take one minus the product and then times one half.
00:06:52.610 - 00:06:53.360, Speaker A: Okay?
00:06:55.250 - 00:07:30.826, Speaker B: So notice if the vertices are on different sides of the cut, that is yi and YJ have opposite signs. This evaluates to one if they're on the same side of the cut, and YYJ evaluates to one. This evaluates to zero, okay? Which is what we wanted. So this is just really a rephrasing of the maximum cut problem as a quadratic program. So if you like, this is a proof that quadratic programming is NP heart max cut is a special case. So any questions about that first step? So again, I went through this because the relaxation is more natural when you're staring at this formulation.
00:07:30.938 - 00:07:31.600, Speaker A: Okay?
00:07:33.010 - 00:08:16.778, Speaker B: All right, so the relaxation with linear programming, we would say stuff like maybe yi is going to be either zero or one. And then when we relax it, we say yi is some real number between zero and one. So that's what we're used to. What we're going to do here is actually a little bit, it's a bit of a leap, so it's cool, you can get away with this, and I'll have to do some explaining about why we can get away with it. So here's what we do. So again, it's going to be a relaxation in the sense that we're going to have something where each of the yis might be minus one or plus one, but they might be other things as well, which don't necessarily correspond to a cut.
00:08:16.864 - 00:08:17.162, Speaker A: Okay?
00:08:17.216 - 00:09:12.250, Speaker B: So we're going to get other stuff. So yi can be not just minus one or plus one, but actually we're going to allow it to be any unit vector in RN. So any vector on the unit sphere in N dimensional space, n, here is the number of vertices to be any unit vector in RN. So of course, plus one and minus one can be thought of as a special case of this, right? If you just take plus one and then pad it with N minus one zeros, or same thing with minus one, pad it with N minus one zeros, then indeed you get points on the sphere and n dimensions. But of course there are a lot of vectors on the sphere and n dimensions other than those two. Okay, so that's going to give us, okay, right. So that's the first thing.
00:09:12.250 - 00:10:01.790, Speaker B: So I'm going to call these now x eyes when I switch to the vectors. So the y eyes are the plus minus ones. The x eyes are just sort of the general unit vectors. So intuitively we just want to sort of take this and plug in x eyes for yis. Now if you think about it, we have to understand what we might mean here, right? So when each of yi and y j was plus minus one, this was just multiplying two numbers. If these are now vectors, what should the product correspond to? The obvious thing to try is the inner product and that's what we're going to do. Okay, so now what do we get? We get maximum, and this will be the relaxation sum over all the edges, W-I-J one, minus the dot product of xi and XJ.
00:10:03.170 - 00:10:03.920, Speaker A: Okay.
00:10:05.170 - 00:10:08.430, Speaker B: Subject to xi unit vector.
00:10:11.650 - 00:10:12.400, Speaker A: Okay.
00:10:13.830 - 00:11:00.254, Speaker B: And actually, let me write this in a particular way. So obviously what it means to say something's, a unit vector is that, sorry, that it's two norm, euclidean norm is one. So obviously it's the same to say that the euclidean norm squared is one. It'll be clear why I'm doing that later, but sort of doesn't really matter. Okay, so unit vectors maximize this. Now how should you think about this geometrically? What's not at all clear is how you would solve this relaxation or how it would relate to the optimal solution of the original problem, which are kind of like the two crucial points. But before we do that, let's just try to get some understanding of how to think about this.
00:11:00.254 - 00:11:49.680, Speaker B: So you have this graph, there's vertices, n vertices, there's edges. What does solving this thing do? Okay, it takes the n vertices of the graph and it embeds them, it gives them geometry, it puts each of those vertices on the unit sphere in n dimensions. Now what is it trying to optimize? It's trying to maximize this weight times one minus the dot product of these. Okay, so in other words, it's trying to get this dot product to be as close to minus one as possible. It's trying to take the endpoints of an edge and make them in tipital on the sphere. It doesn't have to be plus one, minus one, but just if they're in tipital, you're going to get the full W-I-J for that edge. So you're taking the n vertices, you're distributing them on the n dimensional sphere, trying to make each edge its endpoints as close to intipital as possible.
00:11:49.680 - 00:12:29.642, Speaker B: So that's what this is saying geometrically. Now one thing that I hope is clear is that this is indeed a relaxation. Okay? So let me call it the vector opt, the biggest this number could be for any unit vectors is at least as big as the opt that we care about as the best max cut. Why? Because every cut corresponds to one particular feasible solution of this, what's called the semidefinite program or vector program.
00:12:29.776 - 00:12:30.362, Speaker A: Okay?
00:12:30.496 - 00:12:47.266, Speaker B: Namely, if you have a cut for everything on the A side, just give it plus one and then n minus one zeros. For everything on the B side, give it minus one and n minus one zeros. That's a feasible solution. This optimizes only over, only more stuff, and it's maximizing, so it's only going to get a bigger number than the original problem.
00:12:47.448 - 00:12:47.986, Speaker A: Okay?
00:12:48.088 - 00:12:54.580, Speaker B: So in the same sense that we had linear programming relaxations, this is a vector or semidefinite programming relaxation of max cut.
00:12:55.130 - 00:12:55.880, Speaker A: Okay?
00:12:58.250 - 00:13:34.958, Speaker B: Now, here's what's kind of crazy. So this, I hope, is clear. The following fact may strike you as quite counterintuitive, certainly not obvious, which is that we can solve this relaxation in polynomial time. And what I'm going to do next is I'm going to explain why this is true. It's not going to be like a full blown proof, but I'll try to give you a very plausible argument for why it should be polynomial time solvable.
00:13:35.054 - 00:13:35.746, Speaker A: Okay?
00:13:35.928 - 00:13:48.790, Speaker B: Any questions before we do that's? N is the number of vertices.
00:13:52.570 - 00:13:53.320, Speaker A: Yeah.
00:13:54.570 - 00:14:38.514, Speaker B: So the number of points you're putting on the sphere is the same as the dimension of the sphere. It'll be kind of more clear why that happens in a little bit. Other questions? All right, one reason I found this surprising the first time I saw it is if you look up. So why was this clearly not a linear program? Okay, so one issue is like the plus one minus one thing, right? That's not a linear constraint, but whatever. The other thing is like that yi times y j, you're multiplying two decision variables together. And remember, that was like the number one forbidden thing. When we're talking about linear programs, we're having like two different variables in the same term, and there they are.
00:14:38.514 - 00:15:00.250, Speaker B: But in the relaxation, we've got that too. We've got this dot product of x I and XJ. So it again feels like totally quadratic. So how do we escape the quadratic quandary, if you will. Okay, so the moral reason for why this is tractable is convexity.
00:15:02.370 - 00:15:03.120, Speaker A: Okay?
00:15:03.970 - 00:16:00.510, Speaker B: So we talked about convexity a little bit back when we were talking about strong linear programming duality. So remember, a convex set just means that you contain all the chords. And if you're not convex, then there are chords which do not lie entirely in the region. Okay, so that's convex. That's not convex, right? It okay. And in fact, actually a really good, just sort of rule of thumb for you to remember kind of like forever, is that convexity is usually very closely associated with computational tractability. And many of you who go on to work in, say, machine learning will hear this over and over again, or in convex optimization, or in a lot of other fields, convexity is usually fundamentally what's driving computational tractability.
00:16:00.510 - 00:16:33.850, Speaker B: Now, convexity, I actually mean it in two senses. So the first is in terms of the feasible region. So then we're talking about convex sets. So things like this. And so linear programs, as we saw, had convex feasible regions, and we're going to have that here, too, as we'll see in a second. The other part of convexity which is useful is in the objective function. So you can be computationally tractable, not just with a convex feasible region, but even if you have not a linear objective function, but a convex objective function.
00:16:33.850 - 00:17:04.142, Speaker B: So minimizing a convex objective function over a convex set is a tractable problem. Now, in linear programming, you have a linear objective function, which is a very special case of a convex objective function. Here, when we reformulate, we'll also have a linear objective function. So for us, we're not actually using the power of optimizing convex objective functions, though you'll certainly do that in machine learning and regression and so on. But we will use the fact that we can rephrase this relaxation so that the feasible region is convex.
00:17:04.286 - 00:17:04.980, Speaker A: Okay?
00:17:05.850 - 00:17:38.750, Speaker B: All right, so that's just something to remember. Convexity and combination attractability almost always go together, but still you stare at this, and I'm talking here about these convex feasible regions and containing the cords and all this kind of stuff. All right, so what's the feasible region here? The feasible region here? Points on the sphere. You take an average of two points on the sphere. You don't get a point on the sphere. So what am I talking about? Where's the convexity.
00:17:46.210 - 00:17:46.862, Speaker A: It?
00:17:46.996 - 00:18:21.310, Speaker B: So to expose the convexity, we're going to have to reformulate this vector program. Okay, we're going to get something which is equivalent, but where the convexity will be very transparent. And so the first idea and again, so why does it not feel linear? Well, several reasons. Okay. But again, there's this dot product that we're worried about. So we're going to do something we've done before when we were talking about linear programs, which was add extra variables that represent or intended to represent the sort of nonlinear quantities. So it's a linearization.
00:18:21.310 - 00:18:27.950, Speaker B: So for each dot product, we're going to introduce a decision variable, PIJ.
00:18:29.570 - 00:18:30.320, Speaker A: Okay?
00:18:30.770 - 00:18:54.710, Speaker B: So this is for every pair of vertices, whether they're connected by an edge or not. So we're going to have PIJs, and the intended semantics is for PIJ to be equal to the corresponding dot products, inner product of Xi and XJ.
00:18:55.530 - 00:18:56.280, Speaker A: Okay?
00:18:58.410 - 00:19:34.290, Speaker B: So when we introduce these decision variables, this is what we have in mind for them. Now, of course, if we just stopped here and we don't okay, so you can imagine just taking that linear program and replacing each of those dot product of X I with XJ, with PIJ, you might say, what do we do with that norm squared constraint? Well, actually the norm squared is just Xi dot product with itself. So that would correspond to a PII in the second set of constraints. So we could do that, we could just substitute, but then the problem is we've really sort of done a relaxation of our relaxation.
00:19:34.870 - 00:19:35.474, Speaker A: Okay?
00:19:35.592 - 00:20:00.910, Speaker B: So while it's certainly true that if you have a set of vectors these XIs, you can define these corresponding PIJs, if you just solve for a bunch of PIJs, who's to say that they actually arise as the inner products of some vectors x one through x n? If we don't add any more constraints, the PIJs could define any n by n matrix that they want.
00:20:01.060 - 00:20:01.760, Speaker A: Okay?
00:20:03.970 - 00:20:29.910, Speaker B: So the question then is how can we enforce the intended semantics? So we've added our variables. Now we've got to add some constraints. And the meaning of the constraints will to say the PIJs must arise as the inner products of n vectors x one through x n. Okay? So we need to impose a constraint which says that, and surprisingly perhaps, if you haven't seen it before, that's a convex constraint.
00:20:35.290 - 00:20:36.120, Speaker A: All right?
00:20:38.590 - 00:21:23.810, Speaker B: So let me remind you some linear algebra. You've probably seen this before, you've probably forgotten it. That's okay, it's easy to teach to page it back in in sort of, you know, 20 minutes with your linear algebra textbook or Wikipedia. So let me just sort of jog your memory. So suppose you have an n by n matrix of PIJs and wondering is that of the form of a bunch of inner products with the ijth entry being the inner product of the ith and jth vector.
00:21:23.890 - 00:21:24.134, Speaker A: Okay?
00:21:24.172 - 00:21:56.260, Speaker B: So that's what we're wondering. Clearly, some matrices will be of this form and others will not. So in particular, any matrix of this form is going to be symmetric because the inner product of x, I and XJ is the same as XJ and Xi. And obviously some matrices are not symmetric. So this will be true for some matrices, but not all of them. So first let's just sort of observe something this we should be able to understand in real time. So this is really just rephrasing this thing.
00:21:56.260 - 00:22:13.400, Speaker B: So P has this form if and only if it can be written as basically a square, okay? Meaning as x transpose x for some n by n matrix x.
00:22:14.810 - 00:22:15.560, Speaker A: Okay?
00:22:17.610 - 00:22:47.310, Speaker B: And this, once you sort of remember all the definitions, is just tautological. So think about x transpose x, what's the I jth entry of x transpose x, what's the dot product of the ith row of x transpose with a jth column of x? That's the same as the dot product of the ith column of x with the jth column of x. So indeed, the entries of x transpose x are exactly the dot products between x I and XJ.
00:22:47.470 - 00:22:48.178, Speaker A: Okay?
00:22:48.344 - 00:22:52.654, Speaker B: So this is really just kind of, if you like, by the definition of matrix multiplication.
00:22:52.782 - 00:22:53.620, Speaker A: All right?
00:22:56.010 - 00:23:04.120, Speaker B: And then what are the XIs? So the XIs, as we discussed, are just the columns of your n by n matrix X.
00:23:05.130 - 00:23:05.880, Speaker A: Okay?
00:23:06.970 - 00:23:24.378, Speaker B: All right, so this is a famous type of matrix, these sort of things which are like squares, all right? So they're going to be symmetric as we discussed. But then the keyword here is positive semidefinite or PSD.
00:23:24.554 - 00:23:25.280, Speaker A: Okay?
00:23:27.890 - 00:23:45.620, Speaker B: So if you like we'll say by definition p is symmetric and positive semidefinite. And this is where the semidefinite programming terminology comes from.
00:23:47.350 - 00:23:48.100, Speaker A: Okay?
00:23:52.710 - 00:24:33.502, Speaker B: So positive semidefinite matrices are these squares can be written as x transpose x or equivalently, they're matrices of inner products of vectors. Now PSD matrices are super nice, super nice. So one proof that they're super nice is that there's like a zillion different ways to characterize them which feel very different. But actually all of the characterizations are quite easy to go back and forth between. So in particular, so for example, one characterization of PSD matrices, they're the matrices that have all non negative eigenvalues. It's a symmetric matrix. So it has a full set of real valued eigenvalues and it's PSD if and only if all of the eigenvalues are non negative.
00:24:33.502 - 00:25:06.334, Speaker B: So it's a really nice characterization of them. So I'm going to write down one of those characterizations which exposes the convexity in the clearest way. So it turns out a matrix is positive semidefinite if and only if, what's called this quadratic form. Meaning you look at z transpose PZ here, z here is an n vector. If you think about what this is, right, so you multiply P times z, you get a vector. Then you multiply that times another vector, you're going to get back a number.
00:25:06.452 - 00:25:06.734, Speaker A: Okay?
00:25:06.772 - 00:25:55.230, Speaker B: So this is a real valued number. It's a scalar called the quadratic form. So if and only if z transpose PZ is non negative for all possible so called test vectors z, okay? Now so again, just to make this maybe a little less mysterious, if you really don't remember it, so let me just point out that if so one direction here we can just immediately see on the board, okay? If P is a square, is x transpose X. Suppose P has the form x transpose X. Now think about this quadratic form. Suppose we hit it by Z transpose on the left and z on the right. Well, now we just have XZ transpose times XZ.
00:25:55.230 - 00:26:02.218, Speaker B: That is, we have a vector dot product with itself that's always non negative, right? That's just the norm of the vector.
00:26:02.394 - 00:26:03.070, Speaker A: Okay?
00:26:03.220 - 00:26:09.874, Speaker B: So if P is like this stick on a Z transpose, there a z there, it's obvious that you get a non negative number.
00:26:09.992 - 00:26:10.706, Speaker A: Okay?
00:26:10.888 - 00:26:17.058, Speaker B: The converse is not obvious, but especially if you have this eigenvalue characterization, it's kind of very easy to prove.
00:26:17.154 - 00:26:17.800, Speaker A: Okay?
00:26:19.290 - 00:26:28.540, Speaker B: All right, so any questions about that? That was the sort of linear algebra recap that I had to do questions.
00:26:31.710 - 00:26:32.460, Speaker A: Okay.
00:26:33.470 - 00:27:04.160, Speaker B: All right. So why does this make us happy? So again, remember, a matrix, p, these PIJs, right? They're PSD. That is, they really represent the inner products of a bunch of vectors if and only if this quadratic form is always non negative. So no matter what z is. So fix a z, your favorite z, I don't care which one. And think about what this constraint means. Think about what it looks like.
00:27:04.890 - 00:27:05.640, Speaker A: It.
00:27:18.110 - 00:28:09.942, Speaker B: Well, if you just do the matrix vector multiplication for this quadratic form, you just get that you sum over all choices of inj of P-I-J times Z-I-J. Okay, that's just expanding this out. Remember, capital P is a PSD matrix if and only if this is always non negative, no matter what z is. Now, for fixed z, for fixed z, this quantity is linear in the PIJs. So it's true, there's this quadratic aspect to it, z I ZJ. But if we actually fix the vector z, these become numbers two and minus three. Whatever.
00:28:09.942 - 00:28:23.690, Speaker B: Okay, this is just some number. And as an expression, this is just linear in p, which means that for each fixed choice of, we have a single linear constraint in the decision.
