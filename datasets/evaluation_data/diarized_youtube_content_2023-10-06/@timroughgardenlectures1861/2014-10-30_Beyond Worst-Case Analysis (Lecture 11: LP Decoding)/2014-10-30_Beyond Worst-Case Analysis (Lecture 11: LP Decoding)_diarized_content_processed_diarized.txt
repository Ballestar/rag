00:00:00.170 - 00:00:41.470, Speaker A: Today is going to be the last subtopic within the exact recovery part of the class. I got kind of obsessed with this topic this year, so for better or for worse, we can give you a lot of lectures on it. So today I want to introduce the idea of LP decoding, and this will spill out over a little bit into the beginning of Wednesday, and then we'll start a new topic on Wednesday. So this is motivating by a topic hopefully some of you have seen at least a little bit of, although I'm not going to assume any background, namely error correcting codes. So error correcting codes is when you want to encode information in a way that it's resilient to errors, things like bitflips.
00:00:41.570 - 00:00:42.220, Speaker B: Okay?
00:00:43.710 - 00:01:08.978, Speaker A: So we're only going to be talking about binary codes today. So it's just going to be things over the alphabet zero one and a couple of things that you should remember. So first of all, the hamming distance between two vectors, that's just the number of coordinates in which they differ. And then the distance of a code, which is an important parameter, is the minimum distance between the minimum hamming distance between any two code words.
00:01:09.064 - 00:01:09.794, Speaker B: Okay?
00:01:09.992 - 00:01:59.394, Speaker A: So formally a code is just a subset of binary n vectors. So it's just a subset of zero one to the n. And one of the parameters you care about is amongst all vectors in your code in this subset, what's the minimum hamming distance between any two? So for example, so here's a code you could just think about all the binary n vectors that have even parity meaning the number of ones is even. So what would be the distance of this code? Yeah, it'll be two, right? So if you flip one bit, then it's not a code word because all of a sudden you have an OD number of ones, right? But if you flip two bits, you can get back to a code word again.
00:01:59.512 - 00:01:59.842, Speaker B: Okay?
00:01:59.896 - 00:02:22.294, Speaker A: So code words need to differ by two bits, two coordinates, but indeed some code words do differ only by two coordinates, okay? And one way to think about this is you could take just all zero one vectors of length n to the minus one and then append a parity bit. So just whatever the parity is at the first n minus one, you stick that at the end to make it even. Overall, that's one way of thinking about this code.
00:02:22.412 - 00:02:23.080, Speaker B: Okay?
00:02:23.770 - 00:03:24.666, Speaker A: So this is a code that does a lot of words in pretty small distance, and a lot of the codes that people think about and what we're going to be thinking about today are code where the distance is much bigger between two different code words. In particular, we'd not to not just detect an error, okay? So if you have distance two, that means if in transmission exactly one bit got flipped, you'll know, something went wrong, okay? Because you'll see an OD number of ones. And you're like, you basically say, hey, sender, retransmit. I don't know what you said, okay, but if you think about it, what about the distance is a lot bigger than two? Okay, what if it's just D in general? Well, then certainly, even if there's as many as D minus one errors, and again, adversarial, okay, not probabilistic, just adversarial errors. If there's D minus one or less, you'll certainly notice because what you get won't be a code word by the definition of distance. On the other hand, if less than D over two bits were flipped, then actually, without asking for a retransmission, you can reconstruct what the sender meant because you just look at what is the nearest thing, which actually is a code word. So you just say, how many coordinates do I have to flip to get back to a code word? Let's just flip them.
00:03:24.666 - 00:03:51.126, Speaker A: That must be what they meant. Okay, so by the triangle inequality, if everything is D apart and you get something and there's the most D over two errors, it's closer to the original code word than any of the others. Okay, so distance D allows you to detect D minus one errors, and it allows you to correct less than D over two errors. And so the point of this lecture is to study the computational problem of given a corrupted code word, efficiently reconstruct what is the nearest code word to it.
00:03:51.228 - 00:03:51.974, Speaker B: Okay?
00:03:52.172 - 00:04:27.422, Speaker A: And to continue the theme of some of the recent lectures, we're going to be asking, when can you do this via linear programming? And again, there's going to be actually quite nice sort of nontrivial linear relaxation of this problem. And we'll again be seeking conditions under which this linear program is exact. Okay, so this idea of decoding via linear programming works nicely for a family of codes, which is what I want to introduce next. And this is already one of the fun things about this topic is you'll see some ideas which if you haven't seen them before, they're just cool ideas.
00:04:27.486 - 00:04:27.954, Speaker B: Okay?
00:04:28.072 - 00:05:09.280, Speaker A: So idea number one is there's a very nice way to use graphs to naturally induce codes. So I want to tell you about that next. It so this is sort of a very old idea. I mean, over 50 years due to Gallagher, and then these graphs are often called tanner graphs for work. In the 80s, really, it sort of took off big time, at least in the computer science world, with a paper of Sipser and spielman about 20 years ago. And then there's been a lot of work on these connections between graphs and codes in the last couple of decades. So here's the idea.
00:05:09.280 - 00:05:19.358, Speaker A: So we're going to have a bipartite graph, and these bipartite graphs are going to play an important role in our discussion of this topic. The left hand side are the variables.
00:05:19.454 - 00:05:19.762, Speaker B: Okay?
00:05:19.816 - 00:05:30.310, Speaker A: So there's n of them. Okay, so these are just like the coordinates. And then on the right hand side, we're going to have a bunch of parity checks.
00:05:31.930 - 00:05:32.680, Speaker B: Okay.
00:05:34.250 - 00:06:24.214, Speaker A: And there'll be edges in this bipartite graph. And so for a given binary vector of length, n. So if you like an assignment of zero or one to each of these vertices, we'll say that such an x satisfies the jth check if when we zoom in on just the coordinates of x. Okay? So j. Remember, j is going to be one of these vertices. So there's like a canonical j, and so a parity check. A node on this side is going to have neighbors on the other side.
00:06:24.214 - 00:06:25.362, Speaker A: It's a bipartite graph.
00:06:25.426 - 00:06:25.606, Speaker B: Okay.
00:06:25.628 - 00:07:13.446, Speaker A: So there's some subset of variables that neighbor this check. J n of j denotes the neighbors of a vertex. So this is saying for the jth check, the jth parity check, which variables are connected to it, involved in it. Project this n vector to that subset of coordinates, and then this constraint insists that there's an even number of ones amongst these coordinates. So if j has five neighbors, then we're saying necessary. Well, sort of. For this particular parity check to be happy to be satisfied, zero, two or four of those five neighbors should be set to one.
00:07:13.446 - 00:07:27.990, Speaker A: Not one, not three, not five. So that's how you satisfy just one of the parity checks. And then the code words are exactly the vectors which satisfy every single one of the parity checks.
00:07:29.530 - 00:07:51.454, Speaker B: It's it. So the variables are like boolean variables.
00:07:51.502 - 00:08:14.410, Speaker A: Yes. Everything's going to be everything's going to be binary. Yes, exactly. So at the end of the day, you should think of each of these as an x is basically a zero one labeling of these vertices. So that's sort of what x looks like, and then x projected onto n of j. That's just some five coordinates, some five components. So it's going to be like a five bit vector.
00:08:14.410 - 00:08:31.360, Speaker A: So this is basically establishing evenness of parity of various subsets of the left hand side. Okay, that's what's going on. So what about this set? Could you imagine a bipartite graph which corresponds to this code here?
00:08:32.370 - 00:08:36.014, Speaker B: Yeah, one vertex on the right. That's connected to everything I've lost.
00:08:36.062 - 00:09:06.326, Speaker A: That's exactly right. So this corresponds to having a single constraints. Which variables are involved? All of them. Okay, so it's one node on the right, and it's like a star to the left hand side. In general, throughout this lecture, I want you to think of these graphs as being sparse. So think, for example, that maybe each parity check involves like a random subset of 20 variables ballpark and maybe each of the variables is involved in like, ten different parity checks.
00:09:06.438 - 00:09:06.810, Speaker B: Okay?
00:09:06.880 - 00:09:09.126, Speaker A: So think of it as a sparse bipartite graph.
00:09:09.238 - 00:09:09.834, Speaker B: All right?
00:09:09.952 - 00:09:37.542, Speaker A: That's what you should have in mind. So in particular, we're going to have lots of parity checks and they will be overlapping in their variables. Okay, so this 1 may have ten variables, and maybe two of those variables are also in some other parity check. That's fine. Okay, so the goal then, okay, literally any bipartite graph without any further assumptions induces a code.
00:09:37.676 - 00:09:38.070, Speaker B: Okay?
00:09:38.140 - 00:10:22.100, Speaker A: So this defines a code. It's exactly the subset which satisfies these parity checks. But now to have a good code and your favorite definition of good, presumably it's going to be a subset of all possible graphs. So let's think about what kind of graphs would lead to codes that are good in some sense. And so the focus in the lectures is to be sufficient conditions on the graph g. So that well, first of all, it should be that we can correct many errors I e. The minimum distance between any two code words should be big, and here by big I mean a constant fraction of N.
00:10:22.100 - 00:10:52.490, Speaker A: Okay? So let's say at least 1% of N or something like this, okay? So even if one out of 20 of your bits are getting flipped for this very long message, then you're still closer to the original code word than any other code word. So you'd really need a ton of corruptions to actually push something to get confused with some other code word. So that's the first thing. But then we also want to actually do the correction so the decoding efficiently.
00:10:53.550 - 00:10:54.300, Speaker B: Okay?
00:11:00.770 - 00:11:02.426, Speaker A: So that's what we mean by good.
00:11:02.548 - 00:11:03.220, Speaker B: Okay?
00:11:04.150 - 00:11:37.610, Speaker A: All right, so here are the conditions we're going to be looking at. And these conditions basically correspond to what people call low density parity check codes. So LDPC, so if you do a search on that, you'll find lots and lots and lots of papers. The parity check, it's kind of obvious what that comes from. The constraints are all these parity checks. The low density refers to the fact that the graph is sparse. Okay, so bounded degrees, all right, so LDBC codes.
00:11:37.610 - 00:11:56.226, Speaker A: So the version we're going to look at is assume that the left hand side is regular. So deregular. So in other words, every variable participates in exactly the same number. D of parity check constraints. Again, think of d as ten, certainly o of one.
00:11:56.328 - 00:11:56.980, Speaker B: Okay?
00:11:59.030 - 00:12:21.130, Speaker A: Similarly, the right hand side, we want to have constant degree. It may or may not be regular, it doesn't really matter, but degree, o of one, again, think of it as maybe like 20. So think of maybe the right hand side as being half the size as the left hand side and having double the degree. And then the important thing is we want an expansion condition.
00:12:24.430 - 00:12:24.794, Speaker B: Okay?
00:12:24.832 - 00:13:17.930, Speaker A: So if you haven't seen expanders before, this is another very nice concept that this lecture will introduce you to. So the third condition we want says the following for some constant delta independent of N. So that for all not too crazy large subsets of S. So let's say 1% of the nodes or something like that. So for all subsets of the nodes but as large as linear in n. So remember, delta is a constant independent of n. The condition talks about the number of distinct neighbors the set has.
00:13:18.000 - 00:13:18.186, Speaker B: Okay?
00:13:18.208 - 00:13:23.078, Speaker A: So I introduce you to the notation n of I or n of j. That's the neighbors of a single vertex.
00:13:23.174 - 00:13:23.434, Speaker B: Okay?
00:13:23.472 - 00:14:00.566, Speaker A: So when I say n of a subset of vertices, I just mean the union of all of their neighbor sets. Okay, so N of S is everybody that has at least one neighbor in S. Now if you take a set S and every node in S has degree d, what is the maximum number of distinct neighbors that you could possibly have? So what's an obvious upper bound on the size of n of S-D-N of s d times s. Yeah, exactly. So each vertex in S could have the most d neighbors. So this is going to say that it has almost the maximum possible number of distinct neighbors.
00:14:00.678 - 00:14:01.340, Speaker B: Okay?
00:14:02.270 - 00:14:41.650, Speaker A: So at least I'm just going to instantiate all the parameters. So let's just say three quarters, anything above two thirds would work for all of the arguments today and Wednesday, but at least 75% of the maximum possible. So there's no way it's bigger than d times cardinality of S. It's actually going to be 75% of that maximum amount. And the point here is that this is true not just for some set S, but literally all exponentially, many subsets of size up to delta times N. Okay, so not even a single set of size. This or less fails to have tons and tons of distinct neighbors.
00:14:41.810 - 00:14:44.600, Speaker B: What's the intuition behind why this is a useful property?
00:14:45.610 - 00:14:49.766, Speaker A: We'll get to that. A lot of the lectures about that explicitly.
00:14:49.958 - 00:14:50.700, Speaker B: Okay.
00:14:52.990 - 00:15:22.980, Speaker A: All right, so one thing I should say is this property is so strong you should sort of demand that there are examples. It's really not at all obvious that this is a non vacuous definition. It's not obvious that any graph satisfies these properties. So, all right, so everyone's clear on the concept, I hope. So here's your set S.
00:15:24.790 - 00:15:25.314, Speaker B: And here.
00:15:25.352 - 00:16:17.714, Speaker A: Is n of S, and N of S should be big no matter what S is. So the definition is clear, it's important definition. Okay, all right, so one thing I'll put on homework six is a very classic application of probabilistic method which says that these graphs do exist. In fact, if you choose one at random, it's overwhelmingly likely to satisfy this property. Okay, so almost all graphs are expanders in some sense. So with high probability a random graph that satisfies one and two. So the conditions on the right also satisfies three as N grows sufficiently large.
00:16:17.714 - 00:16:51.200, Speaker A: Okay, so with high probability means as N grows large, the probability is approaching one. Okay, so intuitively this is roughly the same you can think about like you just take D random functions. So you take a random function from S to C and you just superimpose d of those. And this is roughly claiming that what you'll get if you superimpose D random functions, then you'll get this expansion property. Okay, so that takes a calculation. It takes a churn off bound, then a union bound, some of the stuff that's also in homework five. And so it's a good thing to leave to you.
00:16:51.200 - 00:17:24.154, Speaker A: But so that turns out we're not going to worry about that. So people do worry a lot about, okay, can you have a derandomized explicit construction of expanders? That's a hard problem. But if you're happy with a randomized construction, monte Carlo construction, then just pick one at random and you're done. Okay, so that sort of mirrors what we're talking about with compressive sensing too, right where we wanted these matrices whose kernels had this almost Euclidean subspace property. And it wasn't at all obvious, but it turned out a random matrix works so similar to theme here. A random graph gives us the property that we want.
00:17:24.272 - 00:17:25.100, Speaker B: All right?
00:17:26.830 - 00:17:42.186, Speaker A: Okay, so given a graph like this, again, we get a corresponding LDPC code. And so now I want to develop the idea that these codes are good in various senses.
00:17:42.298 - 00:17:42.960, Speaker B: Okay?
00:17:43.590 - 00:17:58.610, Speaker A: So the first thing I want to prove is that the minimum distance between any two code words is in fact linear. In n, in fact it's lower bounded by the same delta times n in the expansion parameter.
00:17:58.770 - 00:17:59.526, Speaker B: Okay?
00:17:59.708 - 00:18:03.126, Speaker A: So the point is it's growing with n at the same rate.
00:18:03.228 - 00:18:03.880, Speaker B: Okay?
00:18:05.050 - 00:18:51.240, Speaker A: So distance of such a code is at least delta times n. Okay? In code, sort of one of the things you're looking for is you're sort of asking, okay, to be resilient to errors, you need redundancy. So always the game is sort of minimize the amount of redundancy subject to having a certain amount of robustness properties to error. So this isn't optimal in any sense, but this is like a good sanity check. Okay, so it says basically as n grows, you're basically still using a constant fraction of the coordinates for information as opposed to just for redundancy. Okay, that's sort of the point here. So delta here is sometimes called the rate.
00:18:51.240 - 00:19:36.766, Speaker A: Good. So why am I doing this? Why am I doing this proposition? Because in some sense, I mean, this is not a computational result, right? So just arguing about the distance just says this is going to imply in principle, if you have strictly less than delta n over two errors, then by just computing the nearest code word to what you received, that's going to be the intended transmission. So that's what we know. So this gives us an information theoretic result that says we have enough information to figure out what the person sent. But again, sort of the goal for the whole class, in particular for these lectures, is to focus on computationally efficient solutions. But still, this is kind of obviously a prerequisite for having a computational efficient solution. You have to at least be able to do it in principle that we'll have a polynomial time algorithm.
00:19:36.766 - 00:19:48.778, Speaker A: Also, I think this proof starts really showcasing why expansion is a useful property for coding and in particular for decoding. That's another reason why I want to do the proof, which is not long.
00:19:48.864 - 00:19:54.700, Speaker B: Okay? All right.
00:19:57.470 - 00:20:42.070, Speaker A: So remember, the code words are exactly those that satisfy all of the parity checks corresponding to the right hand sides. We're going to use that in the proof. So suppose X is a code word, and suppose Z is close in Hamming distance to X. Whoops. So I'm going to write D, sub H for the Hamming distance number of differing coordinates. So what we need to show then is that Z is not a code word.
00:20:42.220 - 00:20:42.630, Speaker B: Okay?
00:20:42.700 - 00:21:17.590, Speaker A: That's exactly what the proof boils down to. Okay, show me anything that's not a code word. It can't be too close to anything that is a code word. Sorry, if it's too close to a code word, it can't itself be a code word. Okay? So consider the DH of XZ. Consider the Hamming distance number of coordinates in which they differ. The claim is that there exists.
00:21:19.290 - 00:21:19.606, Speaker B: A.
00:21:19.628 - 00:22:11.830, Speaker A: Parity check, so a note on the right hand side. So, again, don't ever lose sort of sight of the picture. So B, C, and edges whenever a particular variable participates in a particular parity check. So there exists a parity check, J, such that exactly one coordinate, let's say variable, let's say yeah, coordinates, fine, coordinate of S participates in the constraint. So take 30 seconds and convince yourself that if we prove the claim, then we've proved the proposition.
00:22:13.370 - 00:22:36.930, Speaker B: Okay? It.
00:22:40.800 - 00:22:49.948, Speaker A: So what's the reason? Well, X is a code word, so by definition it satisfies every single parity check, including parity check J.
00:22:50.114 - 00:22:50.830, Speaker B: Okay?
00:22:51.620 - 00:22:57.228, Speaker A: Z on the coordinates involved in J differs on exactly one coordinate.
00:22:57.324 - 00:22:57.776, Speaker B: Okay?
00:22:57.878 - 00:23:07.540, Speaker A: So if X satisfied this parity check, and this other thing is off in exactly one coordinate, it's going to not satisfy the parity check, and that means it's not a code word.
00:23:07.690 - 00:23:08.292, Speaker B: Okay?
00:23:08.426 - 00:23:49.730, Speaker A: So remember, if you just flip one bit involved in a parity check, it's going to go from satisfied to unsatisfied or vice versa. So this basically says, like, from J is local view. So think of one of these constraints, these parity checks, as just pretending like there's only these ten variables in the world, right? And if it sees exactly one of those get flipped, it's going to go from happy to unhappy or vice versa. So this says if they differ sufficiently, a few coordinates, there always will exist a parity check which has that property. Okay, so proof of claim then. And this is where we use the expansion property. Well think about s, right? So S is a subset of the variables of small size.
00:23:49.730 - 00:24:36.370, Speaker A: So S corresponds to some region of the left hand side, okay? And the size is small enough that the expansion condition holds is triggered. Okay, so first of all, the total number of edges sticking out of S, well, it's deregular on the left hand side. So we have the number of vertices times the degree sticking out on the other end. By the expansion property, we know that the number of distinct neighbors of S is at least 75% of this number.
00:24:37.540 - 00:24:38.290, Speaker B: Okay.
00:24:41.620 - 00:25:09.128, Speaker A: So there's something like 100 edges sticking out. They have to be going to at least 75 different nodes. So that leaves only 25 edges left over. That could be second edges to any of these nodes. These 75 ones to distinct nodes have to be distinct 25 left over. Those could take 25 of those 75 and make them not have a unique neighbor. But that still leaves 50 that have a unique neighbor.
00:25:09.304 - 00:25:10.030, Speaker B: Okay.
00:25:15.200 - 00:26:25.822, Speaker A: So there's only D over four cognitive S edges left over after accounting for all the distinct neighbors. So in fact, there's not just one parity check that has a unique neighbor. There's tons of them. Okay, so at least three quarters minus one quarter times DS nodes of N of S have a unique neighbor in S. That's where the claim is true, and then the claim implies the proposition. So intuitively, this expander property is sort of saying you have kind of maximal disjointness between what the different parity checks are doing, subject to the fact that there's enough of them that they have to overlap to some degree.
00:26:25.966 - 00:26:26.660, Speaker B: Okay.
00:26:31.110 - 00:27:30.206, Speaker A: So agree. Okay, so they've got big distance. At least. There remains the question of how can we actually do this decoding efficiently. So any questions before we get to the linear programming relaxation and some discussion of when it might be exact? Everything clear. Okay. All right, so the same way we did with, say, the cut problems a few lectures ago.
00:27:30.206 - 00:28:13.650, Speaker A: Let's start with an integer programming formulation. This is already actually sort of interesting. And then the linear programming formulation will be sort of obvious given the integer programming formulation. Okay, so IP formulation, so intuitively, right? So we're given some received vector Z. Okay, so given Z, which like everything else, is a binary N vector, okay, we're assuming this is sort of not a code word. It was obtained from a code word that got flipped in some small number of coordinates less than delta n. We want to know what is that code word?
00:28:13.720 - 00:28:14.340, Speaker B: Okay.
00:28:16.230 - 00:28:34.760, Speaker A: So just very high level. This is the optimization problem we want to solve. Okay, so we're given Z, we want to solve for X and with minimizing the hamming distance. And what are the feasible solutions? The feasible solutions are just.
