00:00:00.330 - 00:00:38.342, Speaker A: You. Hi everyone, and welcome to the portion of the video playlist that accompanies chapter 20 of the book algorithms Illuminated part Four. This is a chapter about efficient inexact algorithms, so you can't have it all. With NP hard problems, you have to give up on at least one of generality speed or correctness. So in applications where generality and speed are both mission critical, but you, you have no choice but to relax, correctness and consider fast heuristic algorithms that are not going to be correct on every input. That said, you'd still like to minimize the damage. You'd like to, if you can, use a fast heuristic algorithm which is approximately correct in some sense.
00:00:38.342 - 00:01:25.158, Speaker A: So maybe you can prove that it's approximately correct on every input, or maybe at least you're empirically doing really well on the instances that you care about. So we'll revisit an old algorithm design paradigm, the paradigm of greedy algorithms. Those turn out to be particularly well suited for the design of fast heuristic algorithms, including some that have provable guarantees. And we'll also augment your toolbox with a new tool, Local Search, which often lacks provable guarantees, but it's nonetheless extremely effective at tackling a number of different NP hard problems in practice. So the case studies we'll look at throughout this chapter include a scheduling problem, a problem in selecting a team, the analysis of social networks, and finally, we'll revisit the famous traveling salesman problem. The first section, section 20.1, concerns the Makespan minimization problem.
00:01:25.158 - 00:01:56.690, Speaker A: So this is a case study in scheduling. We're going to be thinking about assigning tasks to a bunch of shared resources. So for concreteness, you could think of the resources as being various computer processors, and we're assigning the tasks being computer jobs. You could think of the resources as being different classrooms where we're assigning different classes that have to take place to those classrooms. Or you could think about the resources as different days on your calendar. And what has to be assigned are various meetings that you have to have during the week. So let's get straight into the problem definition.
00:01:56.690 - 00:02:39.120, Speaker A: So in scheduling problems, the tasks to be assigned are usually referred to as jobs, and then the resources are referred to as machines. And what we mean by a schedule is just an assignment, a specification for each job of which machine processes it, which resource it is assigned to. So there's a lot of different schedules we could implement. So which one should we prefer? Well, we're going to assume that different jobs have different lengths. So we'll denote the length of a job J by L sub J. And so you could think of this as just like the length of a class or the length of a meeting. We're going to be thinking about the objective that occurs probably the most commonly in practice, which is how to assign the job so that they all complete as quickly as possible.
00:02:39.120 - 00:03:19.830, Speaker A: So to formalize this idea. We need to define a precise objective function which assigns a numerical score to each schedule and quantifies exactly what it is we want. So first, let's define the load of a machine that's just going to be the sum of the lengths of the jobs that are assigned to that machine. So we're going to be interested in the largest of the machine loads, the most heavily loaded machine, and that's going to be known as the make span of a schedule. So this the make span of a schedule, that's exactly what we're going to want to minimize. Now notice the load on a machine, it's the same no matter what order the jobs are processed on that machine. It's just the sum of their lengths.
00:03:19.830 - 00:04:09.542, Speaker A: And so therefore, machine loads and therefore the make span don't depend on the ordering of jobs on machine. So we're not going to worry about that. We're just going to worry about which machine each job gets assigned to. And we want to do that to make this makespan as small as possible. So just to make sure these definitions of machine loads and of make span of a schedule that they're crystal clear, let's pause for a quick quiz. All right, so if the definitions on the previous slide were clear, then this quiz should have been straightforward. The answer is C.
00:04:09.542 - 00:04:36.574, Speaker A: So remember, the load of a machine is the sum of the links of the jobs assigned to it. So in this first schedule, the first machine has load two plus two equals four. The second machine, one plus three equals four. Whereas in the second schedule the first machine has load two plus three, which is five, whereas the second machine has load one plus two, which is three. The make span, remember, is the largest of all of the machine loads. So in the first schedule, both the machines have load four. So the maximum is also four.
00:04:36.574 - 00:05:32.750, Speaker A: In the second schedule the loads are three and five and the maximum those would be five. So now you should be able to guess exactly what the makespan minimization problem is. I tell you that there's m different machines, I tell you that there are n jobs and I tell you their lengths. And your goal is to assign each job to a single machine so that the make span of the resulting schedule, the maximum machine load is as small as possible. So for example, if the jobs represent parts of a computational task which are to be processed in parallel, like the jobs that would make up a MapReduce or hadoop program, then it is the make span of the schedule that governs when the entire computation completes. So those applications are some of the reasons why this is possibly the most commonly studied objective function for scheduling in practice. Now, like all of the problems we're going to be discussing in this video playlist, the make span minimization problem is an NP hard problem.
00:05:32.750 - 00:06:17.422, Speaker A: We will actually prove that once we get to the part of the playlist where we talk about how to prove the problems are NP hard. So we need to give up on a general fast, always correct algorithm. But we can ask, could there be a fast heuristic algorithm? An algorithm which works on all inputs, is always fast, and is in some sense, approximately correct. So as we've discussed in previous parts, the greedy algorithm design paradigm, that's usually a great algorithm design paradigm to start with when you're first trying to understand a problem. So they're really good for brainstorming, greedy algorithms. So we've seen that so far for polynomial time solvable algorithms like the minimum spanning tree. But it turns out greedy algorithms are equally well useful for brainstorming about what to do with NP hard problems, including the make span minimization problem.
00:06:17.422 - 00:07:05.230, Speaker A: So let me just jog your memory real quick about what is the greedy algorithm design paradigm. So the philosophy in designing a greedy algorithm is you're going to construct a solution, iteratively piece by piece via a sequence of myopic decisions, and then you hope that everything works out in the end. So in previous books in this series, by works out in the end, we meant that hopefully you correctly solve the problem like we did with, say, dejkstra's algorithm or prim's algorithm. In this part, with NP heart problems, we're going to be hoping by working out in the end, we're going to hope that we're reasonably close to a best possible solution. So the two main selling points of greedy algorithms are the first of all, as we've said, good for brainstorming, so easy to come up with in many cases for different problems. And second of all, because they're so simple, they're often very fast. So it's common to see greedy algorithms that run in linear or near linear time.
00:07:05.230 - 00:07:48.342, Speaker A: Now, the main issue with greedy algorithms is they're often too simple to be correct. And that's true even for problems where more sophisticated approaches like dynamic programming can solve a problem correctly. In polynomial time, greedy algorithms are often too simple to be correct in all cases. Now, with NP hard problems, on the other hand, if we're looking at polynomial time algorithms, we're going to be stuck with an algorithm which is not always correct no matter what. So this kind of flaw of greedy algorithms would be shared by any other polynomial time heuristic algorithm for the problem. And that's one of the reasons why greedy algorithms are such a great starting point for fast heuristic algorithms. Their weakness that they're not always correct is actually the perfect fit for the compromise you're making with a fast heuristic algorithm.
00:07:48.342 - 00:08:44.058, Speaker A: And for this reason, greedy algorithms are going to play a starring role in this part of the video playlist. So we're going to look at three successive problems, all of which have a good fast heuristic algorithm based on a greedy algorithm. So let's go ahead and apply the greedy algorithm paradigm to the current problem, the make span minimization problem. So in this problem, we have to assign each job to one machine. The greedy paradigm says, you know, build up a solution piece by piece. So it'd be natural to go through the jobs one by one. So when a single pass over the jobs, assigning each job irrevocably to one of the machines, the question then is, okay, so when it's time to assign job number 17 to some machine, where should you assign it? Well, given that we're trying to have the machines as balanced as possible, that we want to minimize the maximum load, the obvious greedy criterion would be to say, well, look at the machine that can best tolerate this new job, the machine that has the smallest load of all m machines.
00:08:44.058 - 00:09:25.226, Speaker A: And let's put job number 17 there. That greedy algorithm is actually a famous one known as Graham's algorithm. So that's going to be our first heuristic algorithm for an NP hard problem. Graham's simple algorithm, which just does a single pass over the jobs in arbitrary order, always assigning the next job to the machine that currently has the lightest load, given where the previous jobs had been assigned. Now, as usual with greedy algorithms, it's not that difficult to figure out that this is a fast algorithm. So suppose you implemented the algorithm in the most straightforward possible way, right? So you'd have an outer for loop which has n iterations. It goes through the n jobs one at a time.
00:09:25.226 - 00:10:00.946, Speaker A: So n loop iterations, what happens inside an iteration? Well, you have to figure out which of the m machines is currently the lightest. So that would be a simple linear search through the m machines. I'm assuming here that you just keep a running count of the current load of each machine. So every time you assign a job to a machine, you increment that machine's load by the appropriate amount by the length of the job. So overall n loop iterations, within each, you do a linear search over the m machines to check which one has the smallest load. Boom. You've got an O of m times n algorithm, where m is the number of machines and n is the number of jobs.
00:10:00.946 - 00:10:35.678, Speaker A: Those of you that have some experience with data structures I hope, will recognize an opportunity for improvement. So fundamentally, what is the work that we're doing in each of these n loop iterations? We're just computing the minimum of m numbers. We have these m machines. We're keeping track of a load of each machine in each iteration. We want to know what is the minimum of those m numbers of the m machine loads. So the work done by this algorithm is just repeated minimum computations when you say it that way. So hopefully like a bell rings and you're like, ah, repeated minimum computations.
00:10:35.678 - 00:11:25.630, Speaker A: This is an algorithm that calls out for a heap data structure, because the raison detra of a heap is exactly to speed up repeated minimum computations from linear time to logarithmic time. So I'll leave it to you to think through. But with a pretty straightforward heap based implementation of this algorithm, you still pay the factor of n for the n loop iterations. But now in each loop iteration, it's going to run in time. Logarithmic in the number of machines rather than linear in the number of machines for an overall running time of o of n log m. So, as usual with greedy algorithms, it was not that hard to analyze the running time and it wasn't that hard to implement the algorithm so that it runs blazingly fast so that it runs in almost linear time. Let's turn to the trickier issue, which is the quality of the solution, the quality of the schedule returned by Graham's algorithm.
00:11:25.630 - 00:12:24.054, Speaker A: Let's get an initial feel for this question in the following quiz. So the question is, suppose I give you an input. There's five machines and there's 21 jobs. And the list of jobs, the first 20 of them, all of those jobs have length one, and then the 21st job has length five. So figure out the schedule computed by the Grams algorithm and its makespan and figure out also, in a perfect world with a perfect schedule, how small could the make span possibly be? I'll let you think about that for a few seconds. So the answer is c. So let's see why.
00:12:24.054 - 00:12:59.494, Speaker A: Let's first start and understand what Graham's algorithm is going to do on this particular input. Well, Graham's going to process those 1st 20 jobs and it's going to spread them as equally as possible. So it'll assign each of the first five linked one jobs to different machines. The next batch of five will also go to different machines, similarly for the third and the fourth batches of five. So after those 1st 20 linked one jobs, you're going to have four length one machines on each of the length one jobs on each of the five machines. So that's the status when Graham comes to the 21st job, which unfortunately is a big job of length five. And there's nowhere to put it.
00:12:59.494 - 00:13:33.634, Speaker A: All the machines are at load four. So you've got to stick it on one of them. Say the first machine that machine's load is going to jump up to nine, and then that'll be the final mix band of the schedule. So what about in a perfect world, could we do better? Is there a schedule out there with mix bands smaller than nine? Well, yeah, sure. Basically the idea is you're going to save one of the machines to process the big job, the length five job that has four machines left over and they have to spread out the 20 length one jobs among them. So each of them will also have total load five. So you wind up with five machines each with load five.
00:13:33.634 - 00:14:04.960, Speaker A: And so that's going to be the make span as well. So the example in that quiz shows that the Gram algorithm may output a schedule with makespan strictly larger than the minimum possible. So it's not always, you know, we're not actually surprised to see an example like this. I've told you that the makespan minimization problem is NP hard. Graham's algorithm clearly runs in polynomial time. So we're certainly not expecting it to be correct on all inputs. If it were correct on all inputs, that would refute the P not equal to NP conjecture, and we're not expecting that to happen.
00:14:04.960 - 00:14:41.526, Speaker A: That said, it's a very simple example and already sort of graham's algorithm is off by four. It has a makespan of nine instead of a makespan of five. So it's almost a factor of two bigger. And you might well be know that was such a simple example. Maybe there's other more complicated examples out there where the schedule output by Graham's algorithm is really just totally horrible. It's just like completely unacceptable. But it turns out what we're going to prove next is an approximate correctness guarantee for Graham's algorithm, which says that that example in the quiz and the corresponding generalizations to more machines are actually the worst possible examples.
00:14:41.526 - 00:15:24.518, Speaker A: You will never see a more serious flaw in Graham's algorithm than is already apparent in the example in that quiz. So precisely, Graham's algorithm is approximately correct in the sense that no matter what the input is, no matter how many machines you have, how many jobs, what the job lengths are, the make span of the schedule produced by the Gram algorithm will never be too much worse, too much higher than the minimum possible. At worst it will be off by a factor of two minus one over little m, where again m is the number of machines. So for example, if m equals two, this would say that the heuristics schedule has make span at most 1.5 times the minimum possible. If m equals five, it would be at most 1.8 times the minimum possible.
00:15:24.518 - 00:16:07.590, Speaker A: That's exactly what we saw in the quiz when the best possible make span was five and Graham's makespan was 1.8 times larger equal to nine. And you can generalize the example in that quiz to any number of machines m, showing you that you get worst examples possible for any number of machines m. So that's as bad as it gets in the quiz. You're always at least as good with this fast heuristic algorithm as you are in the quiz. So how should you interpret an approximate correctness guarantee like this? Well, think of it as an insurance policy. What this guarantee is saying is that even in the doomsday scenario of a super contrived input like the one we saw in the quiz, even then Graham's heuristic algorithm is going to give you makespan no more than twice the minimum possible.
00:16:07.590 - 00:16:45.882, Speaker A: So not that bad. Also empirically, on more realistic inputs, the algorithm is going to over deliver. And you're not going to necessarily see the optimal make span, but you will, on real world instances, typically see the make span of the heuristic algorithm. Be much closer than to the minimum possible off by maybe you might expect to be off by 10% or 20% or something like that. So we will be proving this guarantee in full over the next two slides. So if you want to see that proof, great, just go ahead and watch those two slides. For those of you that may be math phobic or time constrained, let me just give you sort of three steps of brief but accurate intuition.
00:16:45.882 - 00:17:46.302, Speaker A: And if then this is satisfying enough for you, you can skip over the next couple slides and then we'll get to another algorithm. So the first part of the intuition has nothing to do with Graham's algorithm. It's just about sort of scheduling in general, which is that any schedule you look at there will be one machine at least whose machine load is small, is no more than the minimum possible make span, right? Because sort of the best case scenario for minimizing the make span is that you spread the jobs perfectly evenly across the machines. And then in any schedule you might have some machines that are sort of bigger than that, but then other machines will be smaller than that. So any schedule doesn't matter how smart or how stupid, there will be at least one machine whose load is small at most the minimum possible make span. So the second piece of intuition does concern Graham's algorithm and its greedy criterion, which know, think about the machine that winds up having the maximum load at the end of the day when Graham's algorithm completes by the algorithm's greedy criterion. At the time that that machine got its last job, at that time it was actually the least loaded machine.
00:17:46.302 - 00:18:42.582, Speaker A: So the final job we scheduled on that machine flipped its status from being the least loaded machine to the most loaded machine. So what that means is that Graham's algorithm will ensure this sort of approximate balancing of the machine loads. There may be a difference between the least loaded machine and the most loaded machine, but that difference will be at most the length of some job, again, because a single job toggled that machine status from the minimum load to the maximum load. Also if you think about it, the length of a single job, that can't be more than the minimum possible make span, right? If you have a job that has length twelve, every schedule has to put that length twelve job somewhere and therefore every schedule has to have make span at least twelve. So there can be a difference between the most and least loaded machines at the end of Graham's algorithm, but that difference is going to be bounded above by the minimum possible make span. Of any schedule. So if we put those two things together, on the one hand, the least loaded machine at the end of the day, at the end of Graham's algorithm, that can't be that big.
00:18:42.582 - 00:19:13.034, Speaker A: That's the minimum make span possible. Now, the maximum machine load might be bigger, so the make span might be bigger than that, but it's only going to be bigger by something which is at most the minimum possible make span. So you put those two things together, at the end of the day, the make span, the max machine load, can't be more than double the minimum possible make span. And in the more careful analysis we'll do in the next couple of slides, we'll see that actually you get a bound better than two. You get a bound of quantity two minus one over M, where M is the number of machines. But if that's enough for you, great. Skip the next two slides.
00:19:13.034 - 00:19:17.420, Speaker A: For the rest of you, let's now really kind of do this for real. All.
