00:00:00.330 - 00:01:24.406, Speaker A: So in this final video for lecture eight, I want to do two quick things. First thing I want to do is stress test longest chain consensus by examining it in the partially synchronous model and in particular seeing exactly kind of what breaks down when you can have unbounded network delays. The second thing I want to do is I want to just kind of give a very brief recap of some of the key points of Lecture Eight and package them in a way that you can take them directly forward into Lecture Nine and basically poured over all of this lecture's results into the permissionless model that we're going to discuss next. So for this entire lecture we've been analyzing longest chain consensus in the synchronous model. Actually we've even been analyzing it in the super synchronous or instantaneous communication model which corresponds to the synchronous model with a parameter delta equal to zero. But as I've said many times and as we'll talk about in more detail in lecture nine, as long as the time of around so the length of time that elapses in between consecutive leaders as long as that is large relative to the network delay capital Delta large here meaning one to two orders of magnitude bigger then in fact, everything we talked about today will hold more generally for the general synchronous model with a known bound Delta on the maximum message delay. But what about the partially synchronous model? Right? In previous lectures we made sort of a big deal that if you're running a consensus at the internet scale you have to be ready for sort of network outages that maybe last a long time.
00:01:24.406 - 00:02:14.310, Speaker A: You have to be ready for denial of service attacks that maybe last a long time. So what happens with longest chain consensus when you're dealing with those outages or attacks? And it's actually not that hard to see that some of the guarantees we proved for longest chain consensus in the synchronous model are not going to remain true unfortunately in the partially synchronous model. Take finality for instance, this was our theorem two which said that for a suitably chosen parameter k, once a block is ensconced in the longest chain with at least k blocks after it, that will be true forevermore. So that's what allowed us to say that in our notation we had b sub k of g. So a longest chain with the last k blocks lopped off. Theorem two said that that only grows over time. In other words, once a block belongs to that set it will never be kicked out again as long as you have a balanced leader sequence.
00:02:14.310 - 00:03:13.596, Speaker A: In the partially synchronous model. However, longest chain consensus does not achieve finality in the partially synchronous model, messages can be delayed arbitrarily it's true there is this GST, this global stabilization time after which you kind of revert back to the synchronous mode. But remember, GST is unknown operatori the protocol has no knowledge of it and it could be arbitrarily large, finite but arbitrarily large. And let's think about a message delivery adversary kind of using the power of the partially synchronous model in sort of the simplest possible way, which is through a network partition. Network partitions, you may recall from our discussion of the Cap theorem back in lecture six. And so remember this, what this means, we think about the nodes running the protocol and we divide them into two groups, group A and group B. And the message delivery adversary is just going to stop all communication between the two groups of nodes.
00:03:13.596 - 00:03:56.320, Speaker A: So nodes of A can talk to each other, nodes of B can talk to each other, but they can't compare nodes. No messages are going to be allowed to cross that boundary. So just like when we discussed the Cap theorem and network partitions there, beyond this sort of adversary controlling message delivery, we're not going to need any other adversaries. So there are criticisms of longest chain consensus are going to apply even when all of the nodes are honest, even with no Byzantine nodes, if you can have unbounded message delays, then even with all honest nodes you for example lose finality. So what goes wrong? So let B not denote the genesis block. And imagine some number of blocks, I don't know, 10,000 blocks have been created to date. And then all of a sudden there's a network partition.
00:03:56.320 - 00:04:34.604, Speaker A: Well so now all of a sudden what happens is that each set of nodes A and B, they're going to just continue to operate independently and unfortunately unaware with what's going on in the other group. So for example, imagine we're just doing simple round robin leader selection like we usually have been in the permission setting under the PKI assumption. And imagine this network partition happens at, let's call it time step 100. So we start out with this really nice long single chain that has 100 blocks in it, no problems at all. Timestep 100 there's a network partition. The groups A and B no longer can talk to each other. Then time moves forward.
00:04:34.604 - 00:04:52.800, Speaker A: It's time step 101. Remember, it's a global shared clock. All of the nodes in A know it's timestamp 101. All of the nodes in B know that it's timestep 101. All of those nodes both in A and B have some expectation of which leader it's supposed to be right now. So for example, maybe N equals 40. There's 40 nodes total.
00:04:52.800 - 00:05:27.912, Speaker A: And all of the nodes are therefore expecting node 21 to suggest a block to everybody else at this time step 101. Now node 21, of course it's going to either be in A or it's going to be going to be in B. It's not going to be in both. Imagine it's in the set A for example. Then from the perspective of the nodes in A, everything looks totally normal, right? All of them are going to hear a message from node 21. The leader of the current timestep in all of the nodes in A are like, great, this is the new block, let's call it B sub l plus one. Now, node 21 tries to tell everybody about this block, b sub l plus one that it proposes.
00:05:27.912 - 00:06:09.544, Speaker A: It tries to tell the nodes in B. But again, remember our message delivery adversary is delaying all of those nodes. So while things look normal for nodes in A, nodes in B are like, I was sort of expecting to hear from node 21 in this time step and we didn't. So I guess we're just going to skip this block and move on to node 22 and timestep 102. Because remember, for all the nodes in B, node 21 is Byzantine and chose to sort of waste its slot intentionally by not broadcasting any messages. So the nodes in B just sort of carry on assuming that something went wrong with node 21, not worrying about it, and then proceeding to listen to the next block which should be coming in from node 22. So the clock then moves forward.
00:06:09.544 - 00:06:35.104, Speaker A: It becomes time step 102. And so node 22, let's say, is the leader of that time step. And node 22 could be anywhere, but it might well, for example, be in the set capital B. Now, node 22, let's assume it's honest. But even though it's honest, it has stale information. So B has not heard, node 22 has not heard about this block, B sub l plus one. It thinks the longest chain ends in B sub l.
00:06:35.104 - 00:07:10.734, Speaker A: As an honest node. It's going to propose a block that extends the longest chain it knows about that extends B sub l. So it's going to create a block, let's call it B sub l plus one prime that also has B sub l as its predecessor. So after node 22 creates this block, this block, B prime sub l plus one, tries to inform everyone about it. So it broadcasts this block to everybody on the same side of the partition. So other nodes in capital B, they're all going to learn about this block, b prime l plus one. And they all have a common view of what the blockchain looks like.
00:07:10.734 - 00:07:42.666, Speaker A: No nodes of B have heard about BL plus one. They've all only heard about B prime l plus one. So all of them will think that the blockchain currently looks like a single chain starting at B zero and terminating at B prime sub l plus one. Nodes in A, meanwhile, because of the ongoing network partition, are not going to hear about node 20 two's block. So from their perspective, basically they were expecting to hear from node 22. They don't. So maybe they assumed that node 22 was Byzantine or otherwise just sort of out of commission and they just sort of say, oh, I guess we're just sort of skipping this step.
00:07:42.666 - 00:08:25.446, Speaker A: So after time step 102, all of the nodes in A will also have a common view of the blockchain that it goes from B zero to BL plus one. So all of the nodes in A will think that the top branch of this fork is the entire blockchain. All of the nodes in B will think that the bottom branch of this fork is the entire blockchain. And the problem is not just that somehow redundant work is being done. I mean, remember, these two blocks in general are not going to be the same. I mean, even if the node, even if node 21 and node 22 knew about exactly the same set of transactions, even if they packed exactly the same transactions in their blocks, they might well have put them in in different orders because remember, they order them arbitrarily. And I'm sure you can see sort of what happens from here on out.
00:08:25.446 - 00:08:57.282, Speaker A: As long as this network partition exists, as long as sort of nodes in A and nodes in B just can't communicate and live in sort of parallel but sort of self consistent worlds, the two branches of this work are just going to grow, right? So maybe node 23 belongs to set A. Let's say it's an honest node, so it thinks the unique end of the longest chain is BL plus one. That's what node 23 will extend. All of the nodes in A will hear about it, et cetera. Node 24, maybe that's also part of A. It will also extend the top branch. Maybe node 25 belongs to set B.
00:08:57.282 - 00:09:36.398, Speaker A: It's going to extend the bottom branch because the bottom branch is the only thing it knows about. Now initially when we just have the initial fork, BL plus one and B prime l plus one, that's actually not a violation of finality. Because remember, for finality we always lop off the last K blocks of the longest chain. So the fact that there we have two sort of longest chains that disagree on just their last block, I mean, as long as K is at least one, there's no disruption to finality. But again, remember, partially synchronous model message delays can be unbounded. So the duration of this network partition can be unbounded. And as long as it persists, both branches of this fork are going to get extended, the top branch by nodes of A, the bottom branch by nodes of B.
00:09:36.398 - 00:10:23.114, Speaker A: If that network partition goes on long enough, the length of the two branches is going to start exceeding K. And at that point the nodes in A and the nodes in B have different opinions about which blocks have been finalized to that point. Okay? So already at this point we see that we lose the common prefix property, which was the conclusion of theorem one, which holds in the synchronous model. Once these two branches diverge and have gone on for more than K blocks, all of a sudden you do have these potentially two different longest chains which disagree even after lopping the last K blocks. Okay, so that's a failure of theorem one in the partially synchronous model. So what about finality? When does that break down? Well, that breaks down when the network partition ends. So suppose the network partition goes on for a long time.
00:10:23.114 - 00:10:54.258, Speaker A: Nodes in A nodes in B sort of operate in their own sort of self consistent but different universes. Maybe during the network partition, nodes of A added 125 blocks to their branch. And maybe nodes and B didn't make quite as much progress. Maybe they added 119 blocks to their branch. Let's say that the parameter k is something like 50. So let's say that like 69 of those blocks in B's branch are regarded as finalized by the nodes in B. Well then, when the network partition ends and all of the nodes compare nodes, right? The nodes in a catch up with what nodes in B have been doing and vice versa.
00:10:54.258 - 00:11:21.550, Speaker A: The nodes in B. You're like crap. We have to throw out this 119 block branch that we built because we're going to be extending the longest chain. And the actual longest chain is this 125 block branch created by the nodes in A. So all 119 blocks created by nodes in B during the network partition are going to be thrown out. And so that means the 69 of those blocks that nodes in B had thought were finalized were not in fact finalized. The transactions in those 69 blocks are going to be rolled back.
00:11:21.550 - 00:11:56.938, Speaker A: So that's the violation of finality. So this then is one of the biggest issues with longest chain consensus, one of its biggest flaws. Longest chain consensus is still useful. It still powers many of the world's biggest blockchains. But it is important that everyone understand this weakness that it has. Namely, no matter how you set K, no matter how many blocks you wait before you consider a block finalized, that finalization might get violated if you have sufficiently long network outages or denial of service attacks. So for example, in our example, we took K to be 50, which is a pretty sizable number.
00:11:56.938 - 00:12:46.022, Speaker A: But if the network partition was long enough so that you had these branches that were of the order of 119 and 125, you got 69 blocks getting rolled back, not having finality. So this is a big contrast to the BFT type protocols that we were focused on prior to lecture eight. So for example, using tendermint as an example, so remember the way that protocol worked is that a block did not get finalized until you had a supermajority of votes in favor of it, right? So it's the permission mission model. You know, there is say, 100 nodes total and no block is considered finalized until you've heard from 67 distinct nodes voting for it to be finalized. And so what that means is that when you have a network partition like this one, so say half the nodes in a half the nodes in B. A protocol like Tendermint is just going to stall, okay? So it's not going to have any violations of finality. It's just not going to finalize any new blocks.
00:12:46.022 - 00:13:16.222, Speaker A: The nodes in A will only ever hear about 50% of the votes. They'll only hear about votes from A, so that's not a supermajority. They'll make no progress. Nodes in B will never hear from more than half of the nodes during the network partition, so they will also make no progress. So in this sense, BFT type protocols like Tendermint favor safety over liveness. So remember that when a protocol is under attack, we know by the FLP impossibility theorem that you can't have it all. You can't ensure both safety and liveness throughout an attack.
00:13:16.222 - 00:13:41.066, Speaker A: So you have to give up one of them. BFT type protocols give up liveness. They just stall. But at least they never violate consistency or roll any blocks back. That's very much the traditional approach, which makes sense, right? Like safety properties stay, that sort of bad things never happen. So it makes sense to say, oh, I guess they should never happen even if we're under attack. Liveness properties traditionally say something good eventually happens.
00:13:41.066 - 00:14:29.526, Speaker A: So it's very natural to say, oh, I guess we should have liveness eventually, meaning after the attack ends. Longest Chain Consensus meanwhile, as we see from this example, it certainly gives up on some safety properties while under attack. Like if you have a network partition, you don't have finality. Like in the example, 69 blocks got rolled back. On the other hand, interestingly, you do still have a form of liveness in longest chain consensus, even when you have, for example, an unbounded network partition, because both sides of the partition, unlike in BFT type consensus, where they don't have enough votes and they can't make any progress, in longest chain consensus, both sides of the partition just kind of plow ahead. Both of them think they're making progress in their own right. Turns out the nodes in B were not making progress, but the nodes in A, those 125 blocks they created added to their longest chain.
00:14:29.526 - 00:15:02.374, Speaker A: And the 75 of those blocks that they considered finalized, once the network partition ends, the nodes in B will be back on board. They'll actually recognize that top branch as the correct one. All 125 blocks that were created, those are going to stick around in the longest chain. Those 75 blocks that were finalized, they're going to still be finalized moving forward. So in the blockchain world, you'll often hear people say longest chain consensus favors liveness over safety or over consistency. And this is exactly what they're talking about. So think about the canonical case of a network partition that goes on a really long time.
00:15:02.374 - 00:15:51.986, Speaker A: Notice that unfortunately, blocks may get rolled back because a set of nodes may think they're extending the longest chain when they're actually not. On the other hand, one of the groups of the network partition is going to actually be making progress on what will ultimately be the true longest chain. And similarly, people will talk about BFT type protocols like tendermints favoring safety or favoring consistency over liveness. And here again, this is exactly what they mean. When you have this network partition, the blockchain stalls, you will lose liveness, but at least you will always have consistency. You'll never have to roll back any blocks. And this connects very directly to the different ways in which different blockchain consensus protocols fail.
00:15:51.986 - 00:16:48.174, Speaker A: And if you go back through the news articles in recent years about different consensus protocols breaking down in different ways, you'll notice that the protocols that are based on longest chain consensus fail in different ways than the ones based on BFT type consensus. For the protocols based on BFT type consensus, like Cosmos would be an example or Salana would be an example. Whenever there's an issue with the consensus protocol, like there's a bug in the implementation or anything else, generally what happens is they just go down because you will hear about one of those blockchain protocols literally just not processing any transactions for whatever, 12 hours or some sort of long period of time. And that's exactly because they favor safety over liveness. Liveness is what they lose when something goes wrong. Longest chain protocols, meanwhile, whenever you hear them failing, what you hear about is large reorganizations and a reorg in a longest chain blockchain. That's just the same thing as these 69 blocks that one thought was finalized actually getting rolled back.
00:16:48.174 - 00:17:36.874, Speaker A: That's what it means to have a reorganization. It's a rollback of blocks that were previously thought to be finalized. You don't actually hear about those reorgs much in, say, Bitcoin or Ethereum just because they're so big and sort of battle tested at this point. But if you look at more sort of obscure longest chain protocols like, say, Ethereum Classic, you will see that they've failed through reorganizations many times over the years. And so this is one of the big reasons that I think longest chain consensus, even though the killer application is in permissionless consensus, as we'll start talking about in the very next video in lecture nine, I still think it's a super interesting design point in the design space, even in the permissioned setting under the PKI assumption. Right. On the one hand, if you only talk about the synchronous model, you say, okay, longest chain consensus, it has liveness and consistency.
00:17:36.874 - 00:18:33.370, Speaker A: That's good for up to 49% Byzantine nodes. Okay, so that's nice, right? That's higher than some of the 33% results we've seen elsewhere. So 49% is definitely not trivial. On the other hand, have to concede that in lecture two, we did have this state machine replication protocol based on the dole of strong Byzantine broadcast protocol, which in the same situation, synchronous model permissioned PKI assumption tolerates 99% Byzantine nodes. So it seems like we traded in 99% for 49%, which doesn't seem that interesting. But then once you start stress testing by relaxing the synchrony assumption, the protocols start looking a little bit less directly comparable. It's not to say one's better than the other, but just like in our discussion of the Cap theorem, based on your application, you may have reasons to favor consistency versus liveness when you have sort of network outages and attacks.
00:18:33.370 - 00:19:26.780, Speaker A: And so longest chain consensus allows you to pick a different point on that trade off curve. So historically, up to the point that longest chain consensus was introduced by Nakamoto in 2008, up to that point, there wasn't really a choice to be made, right? If there's an attack, you might stall. That was sort of the only option. Whereas with longest chain consensus, you now have a second option, which is maybe you keep making progress with the understanding that some of the nodes will wind up having to roll back some of the blocks that they produce. All right, so I think underappreciated point about longest chain consensus, you only kind of see it when you stress test it in the partially synchronous model, which is why I wanted to spend a fair amount of time on this slide talking through this. But it really is even the permissioned special case, a very interesting consensus protocol. So that brings us to the end of lecture eight.
00:19:26.780 - 00:20:28.872, Speaker A: This was a rather important and correspondingly rather long lecture on longest chain consensus. And throughout lecture eight, I've I've tried to, you know, focus you primarily on the same setting we were looking at, at lectures two through seven, namely the permissioned setting, where there's a known set of nodes up front that remains the same and just runs the protocol sort of from the beginning till the end of time. The permissioned setting. I did that for two reasons. One reason is just seemed natural to maintain consistency with the previous six lectures that we discussed, all of which were in the permissioned setting. But a second reason is I actually do think it's worth just sort of appreciating the innovations of Longest chain consensus in the safe confines of the permissioned model without worrying about all of the additional details required to make it work in the permissionless setting, which is what we're going to start talking about next lecture in lecture nine and conceptually the difference between permissionless and permissioned. Consensus is actually really big, right? So permissioned consensus you should have in mind, like IBM buying their seven servers.
00:20:28.872 - 00:20:59.668, Speaker A: Of course, those seven servers you initialize with the names of the other six servers. And that's all fine. But I also encourage you go look into what you have to do to run what's called a full node for, say, the Bitcoin or Ethereum protocols and you'll see that you don't have to register with anybody. You're not giving anybody a credit card or a Social Security number. You're just downloading some software from the web and firing up and boom, you get to start running the protocol. You get to join the party. So that should seem like a fairly radically different setting.
00:20:59.668 - 00:21:57.292, Speaker A: And it is. And as we've seen, there's a rich literature for many decades there's been a rich literature on distributed computing, but it really wasn't until Bitcoin came out in 2008 that we had any reasonable solutions to permissionless consensus. Thinking back to the BFT type protocols, we discussed like, say the tenderman protocol from lecture seven, you can kind of see pretty quickly why it's not so clear how to make those work in the permissionless setting because those are sort of voting based protocols. So basically nodes are kind of hanging around waiting to hear from enough other distinct nodes before they sort of agree to move forward, like collecting a supermajority of votes on some block before finalizing it. So you need like 67 out of 100 nodes, say, permissionless setting, right? It's not just that you don't know who the 100 nodes are. You don't know that there's 100 nodes, right? Maybe there's 100 nodes today and there's 1000 nodes tomorrow. So how would you ever know how many votes to be waiting for now? We will see later when we talk about proof of stake blockchains in lecture twelve.
00:21:57.292 - 00:23:03.890, Speaker A: There are ways you can take a permissioned consensus protocol like tendermint and make it sort of permissionless basically by having an outer wrapper which selects which nodes get to run the permissioned protocol for some period of time. So we'll discuss that in some detail in lecture twelve. But really it's longest chain consensus that extends just amazingly naturally to the permissionless case, which presumably, if I had to guess, is the main reason that Nakamoto invented it. My guess would be Nakamoto fundamentally took it as an objective of their work to have permissionless. Consensus really wanted kind of an open network that anybody could use to do digital payments and then really was forced to invent longest chain consensus given the limitations, the immediate limitations of the BFT type protocols that had existed up to that point. So starting in the next video in lecture nine, we'll get into the details of how you extend longest chain consensus into the permissionless setting. But let me just conclude with a couple of things from lecture eight you should remember going forward.
00:23:03.890 - 00:23:52.470, Speaker A: So the first thing is you just want to remember kind of operationally how longest chain consensus works. So you want to remember that you start with a genesis block, which we're calling B zero, which is sort of hard coded into the protocol. And remember, this is where we have a trusted setup assumption. This is what we were calling assumption A one earlier in this lecture. We're assuming that this genesis block was created only at the time of the protocol's deployment. So in particular, Byzantine nodes should not have any advanced knowledge of what the genesis block is. And then sort of in the spirit of our earliest state machine replication protocols we were talking about back in lecture two, we're going to have a notion of sort of rounds with each round having a leader who's responsible for creating one or potentially more blocks.
00:23:52.470 - 00:24:33.300, Speaker A: Now, in lecture eight, we did not really discuss how this choice was made. We talked about some options like if you're in the permissioned and PKI model, maybe it's just sort of a global shared clock and you're just doing a round robin order. Just like in our protocols in the past. In lectures nine and twelve for proof of work and proof of stake blockchains, we're actually going to talk in quite some detail about exactly how this leader selection is done. It's going to be a sort of very important point. So what I want you to think about longest chain consensus sort of having this module in it, which is kind of the leader selection module. So there's some sort of black box which conceptually at least sort of takes in a round number, like round 17 and outputs the identity of some node, node number 173.
00:24:33.300 - 00:25:20.494, Speaker A: Now remember, two of our five assumptions earlier in this lecture actually concerned the implementation of this magenta box of the leader selection box. So first of all, it should be evident to everybody who the leader is. So the leader should be able to prove that they're the leader and no one who's not the leader should be able to trick other nodes into thinking that they are the leader. So that's something when we actually get around to implementing the magenta box, we're going to have to make sure that that property holds. Also, it's going to be important that nodes are unable to manipulate how this box works. So for example, if it's a randomized box, it's important that nodes are unable to manipulate the distribution from which it's sampling a particular node. So finally, what is it that a leader is supposed to do in step two B? Well, if it's an honest node, they're supposed to just add a single block to the longest chain.
00:25:20.494 - 00:26:11.232, Speaker A: If they're a dishonest node, they can propose multiple blocks if they want. You remember, we had our assumption A four, which said that each block remember, each block has a predecessor in longest chain consensus. And there is a constraint in step two B that however many blocks you create, you must have a predecessor that was created in some round prior to round R. So remember, this sort of corresponds to what we were. Calling assumption A four earlier in the lecture. This assumption that the leader of a given round, even if they're Byzantine, cannot create, for example, multiple blocks that point to each other, right? That would be a round R block pointing to another round R block. Nor can they somehow delay the announcement of the blocks in order to have predecessors that are created later.
00:26:11.232 - 00:26:51.292, Speaker A: So you. Can't. For example, if you're the round R leader, sort of wait until round R plus ten and announce some blocks that have a predecessor from round R plus five. You can announce them in round R plus ten, but the only blocks you're going to be able to announce then are ones that have predecessors that are created in rounds R minus one or less. If you watched sort of the videos on the proofs of the consistency and liveness properties of longest chain consensus, you'll recall that assumption A four played a really, really important role in those proofs. So again, when we talk about specifically instantiating longest chain consensus in a proof of work blockchain or a proof of stake blockchain, we'll have to make sure that that assumption is indeed met. Proof of work is actually going to take care of itself.
00:26:51.292 - 00:27:46.524, Speaker A: It'll turn out just by the nature of how it works. A node is actually only going to be able to specify one block period in a given round. So that totally takes care of assumption A four very easily proof of stake. We're going to have to be more careful and make sure we're not going to be able to enforce that a leader can only create one block, but we'll still be able to, as we'll see, enforce assumption A four, which was all we actually needed for the consistency and liveness properties of longest chain consensus. So let me leave you with kind of the two key takeaways from this lecture, things we proved in this lecture that are going to immediately port over to the permissionless case and basically just effortlessly take care of consistency and liveness for longest chain consensus even in the permissionless setting. So the first key takeaway is going to basically summarize the work that we did in the fifth, 6th and 7th videos of this lecture. Remember, I factored the analysis of longest chain consensus, consistency and lateness properties.
00:27:46.524 - 00:28:27.768, Speaker A: I factored it into two sort of modules, largely independent. So the first part of it, which was in those videos five through seven, we just sort of assumed that the leader sequence had a certain property, the W balance property for some parameter W and we showed that under that assumption. And under that assumption only, no matter the reason why, your leader sequence might be w balanced. If it's w balanced for whatever reason, you're good. You've got everything you want, you've got consistency and liveness. As long as you choose that parameter K, about how many blocks to wait before finalization. As long as you choose K appropriately based on W, you get consistency and liveness.
00:28:27.768 - 00:29:39.896, Speaker A: So a sufficient condition for getting everything that we want was this balanced leader sequence. What does it mean to be balanced? Well, with respect to a parameter W, that just means that if you look at the leader sequence and you look at any window of W or more consecutive leaders, it should be that a strict majority of them are honest. A strict minority of the nodes selected over that window as leaders are Byzantine. If you remember the statements of those theorems, you'll remember the K, the number of blocks we wait till finalization that was equal to W over two, where W is the parameter in our assumed W balanced condition. And I want to stress, you actually might want to rewatch videos five through seven of this lecture before going to lecture nine just to verify that this is actually true. Nothing we did in those videos truly used our assumption that we were living in the permissioned setting. All that we needed was the balanced condition, right? So for our purposes, we don't care how many nodes there are, there's just some sequence of H's and A's getting chosen in step two A, right? All honest nodes are the same, they're just sort of adding a block to the longest chain, breaking ties.
00:29:39.896 - 00:30:38.096, Speaker A: Arbitrarily. All of the Byzantine nodes are kind of interchangeable, right? We're always assuming that all of the Byzantine nodes are sort of acting in cahoots, so it doesn't matter which one you choose. So all that mattered was this sort of sequence of like where are the H's and where are the A's in the leader sequence that got generated. And knowing only the pattern of H's and A's, knowing only that the W balanced in this condition holds that in any length W or bigger window you have more H's than A's. Knowing only that we were able to carry out all of the proofs of the common prefix property of finality of liveness, of chain quality. Now you should have the question of how on earth are we going to implement the magenta box if it's not the permissioned setting? In the permissioned setting, clearly the magenta box can just be sort of a round robin order based on the shared global clock. But my point is just that if we assume that somehow that magenta box gets implemented, again, we know nothing else, then as long as the balancedness sequence holds, we're good.
00:30:38.096 - 00:31:45.252, Speaker A: We don't need to know how many nodes there are, we don't need to know the names of various nodes. So again, videos five through seven, what we saw that as long as a W balanced leader sequence gets generated in step two A over the course of the rounds, knowing absolutely nothing else, as long as you have more H's than A's in any length W or bigger window, you are good, you have both consistency and liveness. The obvious question then, of course is like, okay, fine, but why should this hypothesis hold? Why should we expect there to be a balanced leader sequence every sufficiently long window more H's than A's? That obviously is going to depend on how these leaders are chosen, right? It's going to depend on the implementation of the magenta box. Presumably. It's also going to depend on assumptions about, for example, the Byzantine versus honest participation in the protocol. So let me remind you of the hard work we did in the fourth video of this lecture of lecture eight, where we thought about the case of randomly selected leaders. So, for simplicity, back then we were again thinking really about the permission model.
00:31:45.252 - 00:32:54.620, Speaker A: Like there were these 100 nodes that everybody knew about and we kind of said, what if we implemented the magenta box? Not through round robin, but what if just every round we sort of independently picked a node uniformly at random, so each of the 100 nodes is a one in 100 chance of being the leader for any given round. So fundamentally, what we proved in that fourth video is that if an alpha fraction of the nodes are Byzantine, then in sufficiently long windows you expect a roughly alpha fraction of the leaders in that window to be byzantine, maybe slightly more than alpha, maybe slightly less than alpha. But as the window size grows large, you're going to have basically proportional representation of Byzantine and honest nodes. So a basically alpha fraction of Byzantine nodes and a basically one minus alpha fraction of honest nodes. And so what that means is that if alpha is less than a half, if you have a strict minority of Byzantine nodes, then at least for a sufficiently long window size, with the window size getting bigger as alpha, gets closer to one half. In any case, for any alpha less than one half for a sufficiently big window, you actually will have this balancedness property with high probability. You really do expect to in all sufficiently long windows have strictly more honest nodes than Byzantine nodes.
00:32:54.620 - 00:34:11.400, Speaker A: Now, amazingly, in that analysis, we also were just barely using the assumption that we were in a permissioned setting, because really we just said, okay, look, maybe there's 51 honest nodes and 49 Byzantine nodes, we're going to select one uniformly at random in each iteration. The only property we needed was that in each round in step two, A, there was a 51% chance of choosing some honest node, there was a 49% chance of choosing some Byzantine node. That's literally the only thing we used about the magenta box implementation that we studied in the fourth video. When you press for a random sample, boop alpaps a random leader, bigger than 50% chance it was honest, less than 50% chance it was Byzantine. If you go back and rewatch that fourth video, that is literally the only property that we used. If you have a magenta box that guarantees the selection of an honest leader with bigger than 50% probability, everything in lecture four holds verbatim. So a little more precisely, if there's some parameter alpha less than a half.
00:34:11.400 - 00:35:00.476, Speaker A: Now, in video four alpha, what it meant was the fraction of nodes that were Byzantine. But really the role of alpha in that proof was just an upper bound on the probability that a given leader would be byzantine. And so if in every single round in step two A, you have at most an alpha chance of selecting a Byzantine leader with alpha bounded below one half, what we showed in the fourth video, literally that exact same proof shows that you're going to get a balanced leader sequence with high probability. The parameter W in the W balance condition, that's going to depend on how close alpha is to a half. The closer alpha gets to a half, the bigger the W you're going to need. But for any given alpha, less than a half. If you can guarantee, if you can implement that magenta box so that every round a Byzantine leader selected with the most alpha probability, you are good.
00:35:00.476 - 00:35:32.100, Speaker A: You will be getting balanced leader sequences with high probability. So the point is that the only missing piece of turning longest chain consensus into a permissionless version that has exactly the same consistency and liveness properties, the only missing piece is the magenta box, right? So in the permission setting with PKI assumption, very clear how to implement the magenta box. You can do round robin. You can just select one of a hundred nodes each. Equally likely, whatever. So the question is, if you're in the permissionless setting, you have no idea how many nodes there are. Nodes are coming and going all the time.
00:35:32.100 - 00:36:19.532, Speaker A: Can you implement a magenta box that simply works even though the nodes running the protocol are changing all of the time? Not obvious, but that is exactly what we will do next lecture when we discuss proof of work. So we need to implement a permissionless version of the magenta box and we need it to have the property, or we need to make assumptions so that it has the property that in each round it selects an honest leader with bigger than 50% probability. Byzantine leader with less than 50% probability. Okay, technically there's also one other thing we need to worry about is we do need to make sure we're enforcing that assumption. A four. So in two B, we have this requirement that the nodes created the blocks created by a leader have to have predecessors that are in previous rounds. So we have to say exactly why our implementations do force that assumption.
00:36:19.532 - 00:36:38.984, Speaker A: But the biggest thing is just where's the magenta box come from. So I hope you feel like close to having permissionless, longest chain consensus and we are. The missing piece is going to be proof of work. Civil resistance. The subject of next lecture. Lecture nine. I'll see you there.
00:36:38.984 - 00:36:39.460, Speaker A: Bye.
