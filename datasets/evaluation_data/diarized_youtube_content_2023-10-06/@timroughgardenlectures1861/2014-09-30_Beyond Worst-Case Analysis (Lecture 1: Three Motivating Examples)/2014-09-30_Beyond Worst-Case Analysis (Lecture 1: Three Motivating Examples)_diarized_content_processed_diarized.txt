00:00:00.250 - 00:00:01.040, Speaker A: Good morning.
00:00:02.370 - 00:00:29.030, Speaker B: All right, so welcome everyone. Let's get started. So this is CS 264. It's sort of advanced algorithms or master's level algorithms class called Beyond Worst Case Analysis. And every lecture I'll just, you know, at the beginning, write on the board what what's the plan for the lecture? So today I'm going to start with a few motivating examples. You all hopefully know what worst case analysis is. I'll remind you why we might want to do it, why we might not want to do it.
00:00:29.030 - 00:01:08.978, Speaker B: And then I'll introduce the first kind of main concept of the course instance. Optimality, somewhere in the middle, I'll break and just talk about sort of course announcements and what's expected and so on. Okay, but I want to start with some motivating examples. And so these three examples are meant to sort of clearly demonstrate the need for a course like this, the need for alternatives other than worst case analysis. So it's lecture number one. This is mostly for intro and motivation, so the description is going to be kind of informal, but these are all topics that we'll cover in more detail later on in the course.
00:01:09.064 - 00:01:09.746, Speaker C: Right?
00:01:09.928 - 00:01:31.610, Speaker B: Let me start by talking about caching a problem that you've probably all seen, if not in an algorithms course than in a systems course. So you have two types of memory, a cache, which is small and fast, and then a larger memory, which is slow.
00:01:36.350 - 00:01:37.100, Speaker C: Okay?
00:01:38.030 - 00:02:26.220, Speaker B: And depending on the context, these could mean different things. Maybe the cache is sort of an on ship cache and the slow memory is main memory. Or maybe actually the small fast memory is main memory and the large slow memory is disk. Those are just two examples. And so what happens is you have a program requesting data, and if the data is in the cache, great, you just read it directly. But if it's not, then you incur what's called a page fault or a cache miss, and you have to pick a page in the cache for eviction, and you have to bring in the requested page so that the program can actually read that data. And so then of course, there's an algorithmic question, which is, when you got to evict something, what should you evict? Okay, and so let me just write this down and then I'll explain with an example.
00:02:26.220 - 00:03:09.866, Speaker B: So consider, say, a cache that, let's say, has room for four pages. So pages are just sort of blocks of memory, if you like, and you're going along and some program is requesting various pages. So let's say the sequence, the request sequence. So maybe the first four pages it requests are ABC and D. So you promptly bring those into the cache. Maybe then D is requested again, no problem. Program has immediate access to it.
00:03:09.866 - 00:03:29.482, Speaker B: Maybe then A is requested again, no problem. Sitting there in the cache available. Now the party is over once E is requested. Okay, so some fifth page, which isn't in the cache. So there's four things in the cache. We got to kick out one of A through D to make room for E. Okay, it might get worse.
00:03:29.482 - 00:04:00.486, Speaker B: Maybe the next request is F. And then again we have to kick out one of the four to make room for F and so on. And it's not clear. I mean, we have to make some decision, and it's not totally obvious how to do it. So as a thought experiment, if it turned out if we were actually clairvoyant and we knew everything that's going to happen in the future, then actually this is a well understood problem. So when we get to E, we have to evict one of ABC or D. Okay.
00:04:00.486 - 00:04:46.406, Speaker B: And what's annoying is whichever one we evict, well, it might get requested again sometime in the future. So, like, if we evict A, maybe later A is requested, then we have to bring A back in. If A is never requested again, then we have no regrets about evicting it now. Now, if all bells being equal, intuitively, what do you want to evict? Well, you want to evict the page you're going to regret as late as possible. Okay, so if A is going to be requested again tomorrow, you certainly don't want to evict it now, because then you just got to bring it back tomorrow. But if A is not going to be evicted for, like, hundreds of thousands of more page requests, then it seems like a good idea evict. Okay, so there's a heuristic known as Furthest in the future which says of all the pages currently in the cache, figure out which one is going to be like it says requested Furthest in the future and evict that.
00:04:46.406 - 00:05:08.266, Speaker B: It's intuitively a good idea. In fact, it's optimal. It minimizes the number of cache misses over anything you could do. So this is a classic example of an optimal greedy algorithm. But recall, this was just a thought experiment. This assumed we had clairvoyance and knew when things were going to be requested next. Mostly in the caching problem, you don't you have to make decisions on the fly.
00:05:08.266 - 00:05:46.938, Speaker B: Okay, so we need heuristics. So what kind of heuristics might we use? Well, let me get the ball rolling and suggest one thing you certainly could try. Which is first in, first out or FIFO? So this would just save everything currently in the cache. You see which one was brought in furthest in the past, and then that's its sort of expiration date. Okay, so fair is fair. You've been here the longest. You have to go.
00:05:46.938 - 00:06:22.758, Speaker B: So which would we be evicting in this case? Once E shows up? Yeah. So FIFO would evict A, replace that with E, and then when the F shows up, what do we evict? So then when the F shows up, we evict B. All right, so that's what Furthest sorry, first in, first out would do at the moment, it's not clear if that's good or bad. All right, so what's another? What have you heard of in some class that's perhaps a better heuristic than FIFO? How about hands least recently used?
00:06:22.844 - 00:06:23.046, Speaker C: Yeah.
00:06:23.068 - 00:06:52.446, Speaker B: So there's one called LRU or least recently used. And so here what you do is just like it sounds, okay? So of everything in the past sorry, everything in the cache you look at, when was the most recent time that it was requested, and then the one that was most recently requested furthest in the past, that's the one you kick out. So for LRU, when the E shows up, what are we going to evict? B.
00:06:52.628 - 00:06:53.178, Speaker C: Right.
00:06:53.284 - 00:06:59.122, Speaker B: So we're not going to evict A because A was requested very recently relative to when E shows up.
00:06:59.176 - 00:06:59.442, Speaker C: Okay.
00:06:59.496 - 00:07:15.400, Speaker B: So B is actually the most least recently used one. How about when the F shows up for LRU, what are we going to evict? Yeah. C. Right? Because we have EAD all requested recently. C, not so much.
00:07:16.330 - 00:07:17.080, Speaker C: Okay.
00:07:19.050 - 00:07:32.170, Speaker B: All right. At the moment, I'm not going to so you could ask which is better? So if you're implementing a Caching policy, which should you use? And I just want to point out at the moment, it's sort of not clear how to answer that question.
00:07:32.240 - 00:07:32.426, Speaker C: Right.
00:07:32.448 - 00:07:59.514, Speaker B: So, for example, which of these so these are two different Caching policies. They did different things when ENF showed up, and depending on what happens next, either way, you might regret what you did. Right. So if in fact, after the F-A-C shows up, right, let's see. So FIFO evicted oh, sorry. The A and the B. So if an A shows up next, you're going to be bummed out that you did FIFO instead of LRU.
00:07:59.514 - 00:08:06.150, Speaker B: On the other hand, if a C shows up next, you're going to be bummed you did LRU instead of FIFO. Or the reverse. Excuse me.
00:08:06.220 - 00:08:06.646, Speaker C: Okay.
00:08:06.748 - 00:08:14.970, Speaker B: So depending on which request comes next, which of course you don't know, either one of these could look better in hindsight than the other, right? Because they made different decisions.
00:08:15.710 - 00:08:16.460, Speaker C: Okay.
00:08:18.030 - 00:08:38.130, Speaker B: So you could say, all right, well, what happens in practice, what's true empirically and what you should have learned in a systems class like 140, is that basically LRU kicks butt. It's really good. In any case, it's certainly better than FIFO.
00:08:39.510 - 00:08:40.260, Speaker C: Okay.
00:08:44.310 - 00:09:51.574, Speaker B: In fact, in practice, usually so LRU is sort of the gold standard, and generally to implement LRU, you actually have to keep track for all the pages in your cache, when were they last requested, and that's actually kind of like a lot of work in some contexts. So what you often learn in systems is just how do you have good simulations of the gold standard of LRU, using few resources that's often what you hear about. Okay, so in some sense, we have this as the ground truth from practice that LRU is a good and excellent caching policy. And all right, so what's the narrative and so the reason why so, first of all, experiments show this, but the intuition is easy to glean, which is that in practical request sequences, practical data, if something was requested recently, it's likely to be requested again quite soon. So maybe, for example, if they're blocks of a program, the program is working in some particular piece of code and doing some for loop whatever, it's using these pages. Another way to think about it is that the LRU assumption is sort of that the past is predicting the future and we know what to do. If you're predicting the future, you want to do furthest in the future.
00:09:51.574 - 00:10:48.390, Speaker B: So LRU is sort of a simulation of that benchmark the Furthest in the future algorithm. If you assume that the future is going to look like the past, the recent future looks like the recent past. Okay, so that's why the intuition why it works now, believe it or not, and this is something I think you'll all appreciate more in the next couple of weeks, it's surprisingly challenging to develop theory that conforms to, in some sense this ground truth that we know from practice. Okay, so a challenge for research is to develop convincing theory that explains this well. So the Caching problem has been around forever. So the Furthest in the future algorithm, that's due to Bellati, that's from the 1960s. It's really only last decade that we started having truly convincing theory about how to think about the difference of why LRU is superior.
00:10:48.390 - 00:11:31.100, Speaker B: And that's some of the stuff we'll be covering in a couple of weeks. So one of the reasons looking ahead, so one of the things we're going to have to do is we're going to have to somehow model data, which is something you wouldn't have learned in an undergraduate algorithms class because in some sense it's properties of real data, which is what makes LRU better than FIFO. So without modeling that aspect of the real world, you actually wouldn't expect theory to predict LRU to be superior. So that's one of the things we're going to have to introduce. All right, so that's example number one. So example number two, I'm going to draw from linear programming. And I'm not going to assume that you've studied linear programming, but I do hope you've heard of.
00:11:31.100 - 00:12:25.578, Speaker B: So what's going to be the issue with linear programming is, again, there's going to be some so here, not only is the traditional theory not very convincing, it's just somehow outright wrong. So the kind of gold standard algorithm for linear programming, just theoretical predictions for its performance are just way too pessimistic. All right, so what's linear programming? So let's start with a picture. So, most interesting, linear programming problems are in high dimensions. But just think about two dimensions for starters. So you've got a feasible region, which in 2D is a polygon. So these are all the things that are possible.
00:12:25.578 - 00:12:30.334, Speaker B: And what you want to do is maximize a linear function with respect to this constraint set.
00:12:30.372 - 00:12:30.526, Speaker C: Okay?
00:12:30.548 - 00:12:35.120, Speaker B: So maximizing a linear function is just like pushing as far in one direction as possible.
00:12:37.010 - 00:12:37.374, Speaker C: Okay?
00:12:37.412 - 00:13:21.550, Speaker B: So that's your objective. So the way I've drawn the picture, she's a little awkwardly the way I drew the picture, but I think that's going to be the optimal point. That's the furthest part in the feasible region in that direction, if you like more algebraically. So this is you maximize a linear function. So c transpose x So here x are the variables and c are the coefficients. So C is telling you the direction. So over an intersection of half spaces.
00:13:22.770 - 00:13:23.134, Speaker C: Okay?
00:13:23.172 - 00:13:31.182, Speaker B: So that's the polygon in general, you would say subject to the linear constraints ax at most B.
00:13:31.316 - 00:13:31.854, Speaker C: Okay?
00:13:31.972 - 00:13:35.382, Speaker B: So here A-B-C and x are all vectors and A is a matrix.
00:13:35.466 - 00:13:36.180, Speaker C: All right?
00:13:37.110 - 00:14:23.454, Speaker B: So this is a very important class of problems. Those of you who have taken CS 261 certainly know about it very useful in practice to have fast algorithms for solving linear programming. It's sort of more general than lots of other famous problems like max flow matching and so on, yet it remains computationally tractable both in both in theory and practice in some sense. Now, again, just like with a caching problem, we somehow knew a good solution in the form of the LRU heuristic. Here we know an extremely good algorithm, which is called the simplex method. And I'm not going to explain right now what this is. If you've studied it, great.
00:14:23.454 - 00:14:41.014, Speaker B: If you haven't, don't worry about it. What you should know is just that this is super fast in practice. So the empirical running time across lots of different data sets, simplex method usually runs in pretty close to linear time, where linear is the number of variables, I e, the dimension of this space, I e, the length of the vector x.
00:14:41.132 - 00:14:41.414, Speaker C: Okay?
00:14:41.452 - 00:15:01.706, Speaker B: That's the typical kind of performance that you see from the simplex method. It's very old, it's from the 1940s. But kind of amazingly, despite the fact that there's been lots of developments in linear programming over the past 70 plus years, still typically today most linear programs are solved using some suitably optimized variant of the simplex method.
00:15:01.738 - 00:15:01.886, Speaker C: Okay?
00:15:01.908 - 00:15:35.080, Speaker B: So it remains just a great algorithm for linear programming. So what does theory or specifically worst case analysis say about the simplex method? Well, so there's a very cool but also very exasperating theorem of clay and minty. This is from around 1970, which says that if you look at the worst case running time over all possible linear programs of a given dimension n.
00:15:36.970 - 00:15:37.286, Speaker C: Of.
00:15:37.308 - 00:15:41.130, Speaker B: The simplex method, then it's exponential.
00:15:44.590 - 00:15:44.954, Speaker C: Okay?
00:15:44.992 - 00:16:08.910, Speaker B: So rather than something like big o of n, it's something like two to the n. Okay? So this is not some minor misprediction of performance. I mean, this is way off. I mean, this basically says simplex if you take this too literally, it says you shouldn't be able to solve problems with more than like 30 dimensions and really simplex can handle problems with millions of dimensions. So it's orders of magnitude off as far as the size of problems that you can solve.
00:16:09.070 - 00:16:09.780, Speaker C: Okay?
00:16:11.190 - 00:17:03.170, Speaker B: Now to make matters worse, at least for the worst case analysis perspective is on the other hand, there are algorithms which do run in polynomial time in the worst case, but empirically are far worse than the simplex method. The most egregious of these is a method known as the Ellipsoid method that was actually the first ever proved worst case polynomial timelinear programming algorithm. And it's actually theoretically very useful and it's a beautiful construction, but you'd never want to run it on real problems. Okay, so we have sort of a double issue here. So first of all, it just says that at least for the specific context of the simplex method, you cannot take worst case analysis very seriously. It gives you a disastrously pessimistic prediction of its empirical performance. But secondly, if you take it too seriously, it actually just recommends the wrong way to solve the problem.
00:17:03.240 - 00:17:03.474, Speaker C: Okay?
00:17:03.512 - 00:17:58.814, Speaker B: You should be using the simplex method. You should certainly shouldn't be using the Ellipsoid method to solve linear programs in practice. So again, this disconnect motivates a challenge, which again is just not very easy, although there's been great progress, mostly 21st century, and we'll talk about it, which is developed theory. So an alternative to worst case analysis to explain perform into the simplex method on real inputs. So I should say the example of clay and minty is a sort of carefully contrived example meant to make simplex perform poorly. It's not like they took it from some industrial data set or something like that. It's a geometric construction explicitly concocted to prove this point.
00:17:58.952 - 00:17:59.640, Speaker C: Okay?
00:18:00.170 - 00:18:14.950, Speaker B: And so what we're going to see here is we'll see here we won't do all the proofs, but I'll definitely tell you about sort of the theory. And so it turns out you really can prove in a rigorous sense that the simplex method runs in polynomial time on essentially almost all inputs.
00:18:15.030 - 00:18:15.274, Speaker C: Okay?
00:18:15.312 - 00:19:03.494, Speaker B: So it's sort of like an average case analysis, but on steroids it's like a much more robust version of an average case guarantee. So that's something called smooth analysis. And we'll spend three to four weeks on smooth analysis, including its application to linear programming deep in the course weeks, seven, seven and eight, something like that. All right, that's example two of why we need alternatives to worst case analysis. Last example I want to talk about is clustering. Or some of you might know this as unsupervised learning. So finding patterns in unlabeled data, right? So you want to think about something like maybe you have a bunch of images and you somehow represent the images in Euclidean space, maybe by their bitmaps or you have some features for them and you're hoping to just sort of identify meaningful groups.
00:19:03.494 - 00:19:07.370, Speaker B: Maybe these are pictures of cats, those are pictures of dogs, something like that.
00:19:07.440 - 00:19:08.060, Speaker C: Okay.
00:19:11.810 - 00:19:30.526, Speaker B: So goal detect meaningful groups. Usually the real problem in clustering has this sort of modeling issue that you sort of know a good solution when you see one, right? If you show me the clustering, like, oh, yeah, it's a good clustering, show me a different you're like, no, that's a bad clustering.
00:19:30.558 - 00:19:30.754, Speaker C: Right?
00:19:30.792 - 00:20:06.234, Speaker B: But to really have an algorithm for it, you need to have some more precise approach. And so the way usually people do this is they take an optimization sort of approach. So they specify some kind of numerical objective function defined on clusterings. Maybe you've heard of things like K means or K median. These are examples, but there's many examples. And then having posited this objective function on clusterings, you just optimize it. You find the clustering, which makes this objective function as high or as low as possible, depending on if it's a minimization or a maximization.
00:20:06.234 - 00:20:12.880, Speaker B: Okay, so posit objective and optimize it.
00:20:14.470 - 00:20:15.220, Speaker C: Okay?
00:20:17.990 - 00:20:46.886, Speaker B: And we'll talk about some concrete formulations a little bit later. But the main points really cut across all the ways that people formulate clustering problems. So what does theory say? Theory says that these are NP hard problems. So for all of the standard objective functions, k means, K median, et cetera, it's NP hard to find the best clustering. And, of course, NP. Hardness. As you know, that means in the worst case, it's a worst case notion, NP hardness.
00:20:46.886 - 00:21:07.698, Speaker B: So what's true in practice? Well, in practice, the clustering problem, maybe it's an exaggeration to say it's viewed as well solved, but people successfully solve clustering problems to a degree that they're happy with the solution all the time.
00:21:07.864 - 00:21:08.386, Speaker C: Okay?
00:21:08.488 - 00:21:23.990, Speaker B: So let's say fast algorithms usually give meaningful results, okay? So maybe they get a few points wrong, whatever, but you basically get your cats and dogs back in lots of different data sets.
00:21:28.490 - 00:21:29.400, Speaker C: All right?
00:21:34.890 - 00:22:36.880, Speaker B: It's so once again, we have a disconnect. So the clustering problem, once you formulate it as an optimization problem and you apply the worst case lens, you get MP hard problems. Yet somehow lots of algorithms are doing something interesting, okay? And we want to understand why. So the challenge here for theory is to formalize the following idea. So the thesis is that clustering problems is hard only when it doesn't matter. So what do I mean? Well, when do we care about clustering? We care about clustering when there is a meaningful clustering to be found. Now, you write down an objective function like an optimization problem, and it's well defined whether or not there's a meaningful clustering, okay? And to have a worst case algorithm, it means you have to solve every single instance whether there's a meaningful solution or not.
00:22:36.880 - 00:23:27.614, Speaker B: And somehow the belief, and we'll look at recent attempts to formalize this, is that, well, if there actually is a meaningful cluster there, then somehow the input should have extra clues, footholds for an algorithm to make use of. And so somehow these cases of the problem should be easier than the worst case. So we might hope the clustering algorithms could do better in the worst case on the instances that we actually care about. So that's been a pretty hot topic for the past five years or so, and I'll definitely survey some of the key results about midway through the course. All right, so those are the examples. Now that we have a quorum, let me just say a little bit about the class. So what are you wanting? You're probably wondering, what do I got to do? All right, so here's what you have to do.
00:23:27.614 - 00:23:46.798, Speaker B: So there's going to be weekly problem sets. They'll go out Wednesdays, starting this Wednesday. They'll be due a week later. They'll be composed of both exercises and problems. Exercises are just sort of lecture details to be filled in. They're not meant to be very difficult. They're just sort of a framework to keep you keeping up with the lectures.
00:23:46.798 - 00:24:14.870, Speaker B: And also it allows me to kind of defer some of the less interesting points from the lecture and focus on the good stuff, the juicy stuff in lecture, and then you can fill in some details on your own. Then there'll also be some problems. They'll be harder, but there'll be few of them. And that's meant to further development in lecture. So often I'll discuss an idea, like today we'll talk about instance optimality at the end of the lecture. And I'm only going to have time to give you really one juicy example about instance optimality. That'll be on Wednesday's lecture.
00:24:14.870 - 00:24:49.314, Speaker B: But through these problems, you'll see that it also can be applied to other problems as well. And that's really one of the things I want you to take away from the course. Here are other ways you can think about designing and analyzing algorithms, but I also want to give you enough examples that you can imagine how you might then apply them to computational problems that come up in your own work. So in the lecture I'll give you some examples, but then the problems will develop more. Okay, so some second order points around the homework. So if any of you are more interested in kind of delving into the research literature, there's an option. It's totally optional.
00:24:49.314 - 00:25:29.778, Speaker B: But if you prefer, you can skip some of the problem sets, namely the last three out of the nine, and instead read a paper or two and write a, say, ballpark ten to twelve page paper summarizing what you've learned. Okay, synthesizing one or two research papers. So that's an option, but again, you don't have to do it. Secondly, I always require less of the pass fail students from the letter grade students. So you can also audit, of course, but for a lot of people, a sweet spot is sort of you take a pass fail and then it's sort of a forcing mechanism. You actually come to class the whole quarter and so on. And so then I'll be explicit on the problem set instructions, but roughly, you have to do kind of half the work of the problem sets for a pass fail grade.
00:25:29.874 - 00:25:30.520, Speaker C: Okay.
00:25:31.370 - 00:25:56.878, Speaker B: All right, other announcements. In the green is Rishi Gupta. He's your trustee Ta. He took this class a few years ago, and he works in this area, so he knows lots of stuff about everything. I'll be talking about office hours will be posted later this week. What else? Oh, yeah, so there's no textbook. Basically, this class doesn't exist except for right here at Stanford, the one you're taking.
00:25:56.878 - 00:26:17.406, Speaker B: Pretty much. There will be videos, as you can see. Those should be up on the website within one to two days after the lecture. I'll also be doing lecture notes that'll take a little bit longer, but know, unless I'm traveling or have a huge deadline, say within two to five days after the lecture. So I'll do my best, but I can't absolutely promise you'll have the lecture notes before homework is due.
00:26:17.438 - 00:26:17.598, Speaker C: Okay.
00:26:17.624 - 00:26:25.670, Speaker B: It's sort of best effort service, if you like. Okay, so questions about the course or about the Motivating examples?
00:26:26.490 - 00:26:27.478, Speaker C: Anything on your mind?
00:26:27.564 - 00:26:28.710, Speaker B: Yeah, you had a question.
00:26:28.860 - 00:26:40.346, Speaker A: The optimization objective. Yeah, I mean, it must be nonlinear because like you said before, that there's the Ellipsoid method, which solves linear optimization generally in non NP heart.
00:26:40.528 - 00:27:10.114, Speaker B: So for linear programming, you need linearity in two senses. So the question was, why can't you apply example number two to example number three? That was basically the question. So linear programming, you need linearity in two senses. So first of all, it needs to be the objective function, which actually is the case for many, but not all of the clustering objectives. But you also need the feasible region to be convex, the intersection of half spaces. And so clusterings are discrete objects. So the feasible region here is more of an integral, discrete flavor.
00:27:10.114 - 00:27:19.740, Speaker B: And if you have linear objectives and you have a discrete solution, those are often NP hard. And that's the case for clustering. All of these clustering problems. Other questions?
00:27:27.250 - 00:27:29.520, Speaker C: Okay, yeah.
00:27:32.370 - 00:27:46.950, Speaker B: We will we'll have one to two lectures on semirandom instances. There's been a lot of recent progress on a topic, but it's very technical. So I'm trying to see if there's some simple nuggets I can distill out. But there'll be one lecture for sure. Yeah, for graphs or for other things or what were you wondering about?
00:27:47.020 - 00:27:48.120, Speaker C: Graphs? Yeah.
00:27:48.890 - 00:27:49.960, Speaker B: Other questions?
00:27:52.730 - 00:27:54.594, Speaker C: Yes. Midterms or finals?
00:27:54.642 - 00:28:01.850, Speaker B: No, we're all adults here. No midterms, no finals.
00:28:03.470 - 00:28:07.660, Speaker C: Other questions? All right. Okay.
00:28:08.590 - 00:28:37.860, Speaker B: All right, so having covered these examples, let me zoom out a little bit, okay? And let's just be clear about something. Let's try to be clear about why are we even doing this? Why are we bothering to try to analyze algorithms mathematically as opposed to just, like, randomly thinking of some and then running them and see how they do on some random instances that we sort of dream up. So what is the point.
00:28:41.910 - 00:28:42.466, Speaker C: Of the.
00:28:42.488 - 00:28:45.680, Speaker B: Mathematical analysis of algorithms and.
