00:00:02.890 - 00:00:57.230, Speaker A: I'm Brian from Raciro, the CEO, and we also have Tim Carsten's here and Paul to answer your questions, but I'm going to be talking about Zest. So this is a type zero Zke EVM, the type zeros in quotes, and we'll get into why that is later. But. What is zest fundamentally? So it's a fully open source ethereum block prover that actually does all of the things necessary to construct a block and proves it in ZK. Supports all of the EVM Op codes, 100% built in Rust, and you can see it costs about $20 to prove a block. Now, the twelve minutes for a proof, that's if you're using, I think, 3000 GPUs, whereas if you scale it down to something more reasonable, like 64 GPUs, it takes about 50 minutes. That said, there's a ton of room for making this system a lot more performant.
00:00:57.230 - 00:02:13.782, Speaker A: And all of this is based on this continuations feature. So if you're interested in ZK proof systems and how to make ZK proof systems, I think, more effective and useful, you should definitely check out the articles that we've written about continuations because this is really the feature that's made this Zeth type zero ZK EVM possible. So I guess I'll talk a little bit more just about that particular feature. So continuations actually lets us take a full risk Five program. How many people here actually know what power zero works? Should I give a little intro? So, it's a Zkvm that interprets RISC Five code or Risk Five instructions and proves them in ZK. Now, RISC Five is a general microarchitecture, so it can ZK prove anything that can be compiled for this general purpose microarchitecture. So when we think about Zke EVMs, people usually think about taking the semantics of the EVM and then translating that directly to a set of circuits and then proving execution of EVM bytecode through these set of circuits and then combining them and aggregating them together.
00:02:13.782 - 00:03:29.002, Speaker A: So instead of doing that, because this involves creating potentially hundreds of custom circuits, and then you have to figure out how to get them to coordinate with each other and then aggregate them all together and then audit all of that. Instead, you might consider just, well, wouldn't it be great if you could ZK prove an EVM that already exists? So, with this approach, you can, by actually taking the EVM code and then compiling the EVM itself to RISC Five, now you effectively can shove EVM bytecode through and state data through this virtual machine and produce an actual EVM proof. Yes. So unlike these sort of EVMs that are built on top of all of these circuit libraries, this Zkevm is based really on the Revm code base and utilizes a couple other libraries. So, interestingly with the performance, even as is. So going back, we have this general purpose ZK VM. However, for parts of it, you might imagine you want to accelerate them just like you have a graphics accelerator in your phone or any other number of cryptographic accelerators in various hardware.
00:03:29.002 - 00:04:15.770, Speaker A: You can build the same thing in ZK systems. So we have the general purpose RISC Five Zkvm, and then attached to that we have accelerators for sort of like Poseidon and Shaw 256 and Big Integers. However, for this particular for Zeth, we didn't actually even use any of those accelerators. We just used the existing crates. So this sort of shows that even for things that are cryptographically intense, this approach of just reusing code that somebody else wrote is entirely effective. If we wanted to make it faster, we could go back and look at how to actually accelerate Ketchak or Shaw Three in a more native ZK context, which would then improve. I think we decided about 60% of the time in mini ETH blocks is currently spent in Ketchak.
00:04:15.770 - 00:04:43.050, Speaker A: So there's a lot of potential improvements for the overall system. So, Tim's, out there, do you want to talk about the Patricia Merkel tree? I don't want to butcher it. This is what happens when you have CEOs talk about Patricia Merkel trees.
00:04:44.370 - 00:05:56.182, Speaker B: So one of the critical things that you have to do when you're running and constructing Ethereum blocks is you want to prove that the state that you're starting with is consistent with the state that the chain is actually agreed upon. Right. And so Ethereum actually provides a standard way to do this, called an EIP One One eightyof, which you can use to prove that a state of a given account or storage at such and such block is equal to whatever you would expect. So for the purposes of ZK construction, it's actually very important that before you start running transactions, you verify the state of the accounts and storage that you intend to read and then act upon. So one of the things that we do is we take the inclusion proofs that are provided to us by RPC providers, for example, and we essentially take the union of those and we construct from that what we call a partial Merkel Patricia tree. So this is a Merkel subset proof, if you will, that represents the rather that verifies that the state that you're acting upon for your computation is in fact the consensus state for those accounts. That's what the structure does.
00:05:56.236 - 00:05:57.974, Speaker A: Excellent. Thank you, Tim.
00:05:58.092 - 00:05:58.486, Speaker B: Thank you.
00:05:58.508 - 00:06:08.394, Speaker A: Brian, do you want to talk about putting it all together as well? I think you're going to do more justice to this than I would, but yeah, sure.
00:06:08.512 - 00:06:39.010, Speaker B: So the process for building an Ethereum block, of course, is that you start from a given state. You verify that that state is consistent with the consensus state. And then from there, you also want to verify that the signatures on the transactions are valid so that you're only doing authorized updates. For EOA accounts, we do those verifications. We apply the transactions in sequence, updating the data as we go, and then at the very end, we calculate the new state route again using this partial Merkel patricia tree.
00:06:43.540 - 00:07:56.200, Speaker A: So one of the key benefits of doing things this way is that if you look at the sort of breadth of the existing various type, whatever ZK, VMs or Zkevms, because you have to build all these custom circuits, you can imagine that the system around it kind of ossifies. You have this very complex interconnected set of code. So if you want to actually modify the EVM to add an EIP or potentially do things like Curio and other teams are doing to actually innovate in the kind of capabilities of your EVM, you kind of at that. Point now are looking at a huge amount of engineering and sort of security budget investment in order to bring your Zke EVMs to all of these different ecosystems. So by building on top of existing code, we're making it really easy for everyone else to sort of innovate in the EVM space while maintaining the maximal amount of security. So rather than re auditing this entire complex ZK system, you can just use something like Lambda's Tool that shows you the diffs between different EVM stacks, then you can really focus on only the things that have changed. And then yeah, so Zest is built really quickly.
00:07:56.200 - 00:08:22.956, Speaker A: Tim here and two other people sort of headed up this effort. Very impressive work. And this is something I think we've been talking about on stage for like a year, that the best way to build an EVM is just to go ahead and use an EVM that already exists if you're going to do it in ZK. So it's really happy that we were able to actually make this sort of feat work. I think it's what 4 billion cycles ish for most blocks.
00:08:22.988 - 00:08:23.932, Speaker B: Yeah, typical proof.
00:08:23.996 - 00:09:01.070, Speaker A: Yeah. So that's basically 4000 chunks. We split this into of a million cycles each and then prove those all in parallel, sort of. Well, as the executor figures out what's happening in the Risk five code, it produces these little chunks to go or segments that then get streamed out to a bunch of other nodes. We could get into the architecture of all of Bonsai separately. Yeah. So if you want to actually play around with any of the kinds of things that you might be able to prove from this client, like the stuff Tim mentioned, you can do this now without needing to really worry about the ZK at all.
00:09:01.070 - 00:10:07.460, Speaker A: And yes, as Stephen Slides here mentioned, we have sort of started working with optimism, really excited about that. And I think there's going to be some really cool hybrids of things like theft being mixed with fraud proofs. So you could think about ZK as a way to if you can produce the correct proof of execution of something, you can also produce the incorrect proof. So you could imagine, even with Zeth, you could effectively make fraud proofs for ethereum because you could produce a proof that the agreed upon answer was not the actual appropriate answer based on what the clients put out. So going to be some really exciting things, I think, coming out there that yeah, that's like the overview of Zeth. I figured there's lots of questions that people might have so we can get into more stuff about Zeth or more stuff about resiro or continuations. But yeah, just like a quick overview.
00:10:12.280 - 00:10:14.310, Speaker C: I guess. Now we're open for questions.
00:10:15.480 - 00:10:23.050, Speaker A: I'm just curious with that sort of implementation logic, does it also apply to other clients that.
00:10:25.900 - 00:10:28.900, Speaker C: Polka dot or even just like, salana?
00:10:28.980 - 00:10:58.500, Speaker A: Yes. Well, it depends on the exact client. Right. I think we support something like 1350 out of the top 1500 Rust crates at this point and that number goes up over time. How do you verify the use of support? So we actually have scripts in GitHub that go download everything, build them and then yeah, actually build it inside the guest code. And then I think we have Rudimentary support for executing some of the tests. So there's some amount of support for all these things.
00:10:58.500 - 00:11:37.120, Speaker A: So to some extent it depends on how much the actual crate you're using supports. Like some of the modifications we had to make for Zest involved building a different database backend to make sure that the serialization across the sort of host guest the thing that's being proven versus the thing that's interacting with the thing being proven is actually efficient. We had to add this custom Merkel Patricia tree code. So you might have to do things like that for particular VMs. I know that the solana I think park got the Solana one working the other day. He said it was gross. But isn't that a judgment on Solana?
00:11:40.180 - 00:12:12.540, Speaker B: So continuing on from that, it's actually one of the really fun things about having a general purpose architecture available to us is that it does give us the vast majority of existing Rust libraries, which in this year actually covers a significant amount of the use cases in blockchain and also off chain. So in addition to Solana, we also have support for WASM not a problem at all. And I've also heard a rumor that we have support for Cairo. As of open source Cairo prover.
00:12:16.480 - 00:12:38.580, Speaker A: People have played around with move. I don't know if anybody's gotten that torque and sway also. And then also of course, Linux. So if you haven't seen this, it's totally worth checking out. I don't know why people need to ZK prove Linux. I hear it has something to do with gold bars. But yeah, you can now prove that ZK prove the execution of Linux programs using the exact same approach.
00:12:39.080 - 00:12:55.176, Speaker B: And you can also roll up existing roll ups into RISC Five quite easily. Right? Like if there is a ZK proof Verifier for a Snark or a Stark or anything else, if it's written in Rust, you can run it inside our system. And in this way you can make.
00:12:55.198 - 00:12:56.840, Speaker A: Everything an L two of everything else.
00:12:56.910 - 00:12:57.384, Speaker B: Absolutely.
00:12:57.502 - 00:12:58.170, Speaker A: Yeah.
00:12:58.700 - 00:13:00.964, Speaker C: How do you decompose the existing Rust.
00:13:01.012 - 00:13:50.456, Speaker A: Trace into Risk Five format? So you don't actually have to do anything for that. That's the kind of beauty of this approach. So Rust has the compiler for Rust is built in sort of two layers. It has an initial compiler that sits above LLVM and then it effectively just bolts onto LLVM and then everything gets lowered from there. So right now our tool chain, we support GCC and LLVM for production of Risk Five, but Rust pretty much already knows how to because it uses LLVM and because they decided to support the subtype of Risk Five that we use RV 32 im, it just works. I mean, there's a lot of hard work went into making it just work, but from a developer's standpoint, you don't need to do anything special. You did have to run the latest version of Rust, but I think we fixed that yesterday too.
00:13:50.456 - 00:13:53.772, Speaker A: So now you can just use standard Rust question here.
00:13:53.906 - 00:14:07.470, Speaker C: So from the compiler perspective, there are usually some UN accessory assembly level instructions. Right. So how do you guys handle it? Because it's very expensive in terms of.
00:14:08.100 - 00:14:09.472, Speaker B: This is the surprising thing.
00:14:09.526 - 00:14:09.840, Speaker A: Right.
00:14:09.910 - 00:14:51.820, Speaker B: So I think that when we first released our VM, a lot of the cryptographers commented to us with a little bit of skepticism. They said, how can you possibly compete in performance against virtual machine architectures that were designed specifically for ZK? And I have to say I understand where those questions were coming from. Right. Like intuitively, the more optimized the better. And you would think that would be really important. But empirically, with the actual data in hand, what we have found is that RISC Five is primarily just doing simple control flow operations which cost exactly the same regardless of what your VM design actually is. Where the performance really starts to matter is when you want to do expensive cryptographic operations.
00:14:51.820 - 00:15:27.400, Speaker B: Right. And we see this exactly in Zeth. So Zeth actually takes advantage of I should back up. The VM that we provide is a RISC V core together with accelerated circuits for specific cryptographic operations, which incidentally is exactly how real processors work. Right. And so if you were to be producing traditional computer systems, you do have the option of building your own asics for your specific application. But you'll notice that nobody does this and they don't do it because if you need control flow, there's actually no benefit to it.
00:15:27.400 - 00:15:38.172, Speaker B: The only benefits come from hyper optimized circuits for things like ECDSA, which we provide, or for Shaw Two, which we provide, or for Ketchak, which we will.
00:15:38.226 - 00:15:39.950, Speaker A: Provide, or someone will. Yeah.
00:15:41.120 - 00:15:53.696, Speaker B: So our architecture is very much incorporating the lessons learned from the last hundred years of computer architecture. And our experience has been that actually translates very well into ZK. Perhaps running against some of the narrative that has been out there.
00:15:53.718 - 00:16:46.364, Speaker A: I mean, if you think about the core philosophy of RISC versus like a CISC approach, RISC Five is the spiritual successor of MIPS and that was the successor of something else. So people have really tried to boil these set of opcodes down in risk architectures to really kind of the bare minimum you need. So it also means when you translate it to it's hard to do this. When you translate it to a ZK system, you can achieve the necessary sort of efficiency in terms of column layout to actually get and obviously we use Starks. So this is another thing that people don't often talk about. The difference between Starks and Starks and Starks are just much better for this kind of VM type situation where you actually have a known set of registers and you're just running the same effectively circuit over and over and over again. Which if you could encode an EVM and a Stark, it might be really efficient.
00:16:46.364 - 00:17:06.250, Speaker A: But I don't think that you can. I think it's fundamentally too complex. So you'll end up having way too much requiring too much memory. You certainly could do it, but it doesn't work well. So having the smaller chip actually it's Fry based. I don't think it's actually encoded in Starkey yet.
00:17:08.140 - 00:17:09.690, Speaker D: Okay, I may be wrong.
00:17:10.140 - 00:17:13.720, Speaker B: Yeah. There's one other performance thing that's also, I think, really quite relevant.
00:17:15.020 - 00:17:17.548, Speaker A: Different EVMs. Yes, that one's not released yet.
00:17:17.634 - 00:17:54.648, Speaker B: So another thing that folks mentioned to us early on. So RISC Five, like any kind of conventional processor design, assumes that you have a traditional memory architecture together with a collection of general purpose registers. And I've heard it suggested that this approach is not ZK optimal because it involves working with registers. And as it happens, this thesis has also been tested out by traditional computer architectures. So if you know your computer history and you look back to the was an architecture called Spark, which has nothing to do with ZK. It's produced by Sun Microsystems, whose campus is now occupied by Facebook. Goes to show how good their ideas were.
00:17:54.648 - 00:18:26.428, Speaker B: And the Spark architecture had this view of oh my gosh, register handling must be so expensive, let's produce this exotic system where registers behave a lot more like memory. And what they found was in fact, there were no performance benefits to this because existing compilers are actually extremely efficient at using the registers and don't involve a lot of register changes and sure enough, in instrumentation. What we have found is that this traditional processor architecture, as far as I know, actually is not causing any significant overhead over a safe, pure memory architecture approach.
00:18:26.524 - 00:18:56.830, Speaker A: Yeah, there are other potentially interesting benefits of using a real sort of microprocessor, especially when you look at before you prove anything, you have to actually simulate the thing you're proving. You have to generate the witness. So if you're actually basing your ZK proofing system on an actual real architecture, there's probably a lot of hardware opportunities to actually accelerate the witness generation in hardware in advance. And so I think we'll start to see a lot of that really push the performance of the system over time.
00:18:58.720 - 00:19:14.252, Speaker C: Also another question, it sounds like Zkvm is more generic compared with Zkevm because Zkevm focus on EVM, right? So I mean, what's the cost of this generality? Like the overhead?
00:19:14.316 - 00:19:54.370, Speaker A: Well, I think it depends on how you construct your proof system. But if you think about the EVM is also extraordinarily general, but it doesn't really have like an LVM pipeline at the top and it's never really been optimized for running large programs. But every integer operation uses 256 to integers, right? So that actually has its own significant amount of cost versus just using risk five to 32. You don't necessarily like 32 bits is enough for a lot of operations anyway. So there's actually like efficiency to be had, I think, in general. So it's really hard to compare outside of the context of a very specific example.
00:20:00.260 - 00:20:20.870, Speaker E: I kind of have a question about Lasso and Joel implementation, and that's going to work well with risk five, but they talk a lot about kind of making smarts more developer friendly. How do you see the session with your design?
00:20:22.770 - 00:20:24.066, Speaker D: I'm happy to take that if you want.
00:20:24.088 - 00:20:25.060, Speaker A: Yeah, go for it.
00:20:25.590 - 00:21:12.898, Speaker D: In the short term, I don't think it's really that clear what the purpose story is. The details around Lasso is that the verification time is worse compared to Fry, so the gains in proverb, but there's some gaps in the literature I think that we need to see filled in before it makes sense to make decisions about whether we want to talk about implementing it. We're sort of definitely of the opinion that at some point we're going to have to make some changes to the but Lasso makes it easy for you to write circuits because you don't have to write polynomial constraints. You can instead use, lookups, we make it easy because you don't have to.
00:21:12.904 - 00:21:14.558, Speaker B: Write circuits, you just write rust.
00:21:14.654 - 00:21:35.640, Speaker D: So it's like lasso. That ease of developer experience argument for Lasso is like non unique. I guess it applies if your model is developers have to write circuits. Yeah, but I think we're all kind of moving beyond the developers have to write circuits as the.
00:21:37.450 - 00:22:12.322, Speaker A: Even with the stuff powder labs are doing or any other number of projects where people are building these lurk kind of higher level circuit description languages. You can imagine backending any of those to Lasso. So I imagine there's going to be and we have our own sort of circuit construction language we'll be releasing eventually as well. So I think that the additional simplicity is really only going to help people when they're adapting these languages to the ZK's proving systems. But yeah, as Paul mentions, we'll use the fastest proving technology that we can get our hands on.
00:22:12.456 - 00:22:48.910, Speaker D: Yeah, I'll add to that, I guess we are definitely designing for modularity and are certainly living in a world in which we think that fruit systems are going to continue to improve totally at some point. But yeah, Lasso, it seems like a more substantial pivot from our design than some of the other there's some low hanging fruit in terms of speeding up our fruit system. But frankly, in digging into the details, it seems like the log up approach in most practical use cases is offering as good performance as the Lasso approach.
00:22:49.730 - 00:23:18.730, Speaker A: I don't know if you saw that paper I just sent out that people are like, no. A cool thing about advances and proof systems is that usually if somebody goes off in a different direction, half of the tricks they took to get in that direction actually apply to a bunch of other proof system approaches. So usually you see a bit of back and forth between these different proof systems. So yeah, people just found some way to apply some of the Lasso techniques to fry construction. Who knows? Obviously we're fry not Maxis, but sands.
00:23:19.070 - 00:23:20.422, Speaker B: Certainly fry users.
00:23:20.486 - 00:23:22.074, Speaker A: Yeah, sure. Anybody else?
00:23:22.112 - 00:23:22.938, Speaker B: Yeah, sure.
00:23:23.024 - 00:24:15.926, Speaker A: Yeah. I'm just curious, since now I introduced to the public, what does it mean for the builders who want to build on top of it? And what's their relationship with you guys? Does it mean like transaction fee? Yeah, I mean, for now, basically, in order to get the small Snark out that you could actually verify on chain, you have to use Bonsai. So it has to go through an API key. But that will not be the case eventually. So we just started our security audits. We want to make sure that when we're putting something as powerful as general purpose ZK out into the ecosystem, that we have a strong confidence in it. The story for right now is that you can certainly play around with the code and know that it will be provable in ZK, or you can use Bonsai or you can run it locally.
00:24:15.926 - 00:24:26.218, Speaker A: It's just going to take a long time and it produces a very large proof that you have to compact down also using Bonsai. But in the future yeah, there'll be multiple ways to prove this, for sure. And hopefully a decentralized network.
00:24:26.314 - 00:24:32.382, Speaker E: I was going to ask the same question. Is there going to be something like, OK, staff the standardized ways for people.
00:24:32.436 - 00:24:53.782, Speaker A: To I hope so, yeah. I don't think we have solid plans there yet, but we'll definitely I mean, we have the core of Bonsai and we'll always be offering that to customers who want to pay to run a centralized ZK proving service. But we will build out this decentralized network that will continue to add new features over time, for sure.
00:24:53.836 - 00:25:02.186, Speaker E: Especially with this, you can actually commoditize running ZK roll up. Then that would just no longer become people will have to be opinionated ahead of time.
00:25:02.288 - 00:25:07.340, Speaker A: Yeah, definitely. Although as one of our investors, you're not supposed to be excited about commoditizing things.
00:25:12.030 - 00:25:56.470, Speaker B: And we think that the application landscape is only going to get richer from here. Right. Historically, all data on blockchain is public. And in a world where all the data is public, decentralized proving systems make a tremendous amount of sense. But once you have the ability for people to submit proofs about applications that act on private data to public blockchain, you'll start to see that there's significant interest, I assume, in the development of hybrid systems. Where the chain? Itself which maintains the public ledger will be built by a decentralized permissionless network and where applications that require manipulation of private user data from one or more users will certainly have to be running on some hardware that has some kind of trust obligation that's enforceable in traditional.
00:25:56.550 - 00:26:32.534, Speaker A: Yeah, and maybe people will use local proving for that. I think our prover now runs in browsers and lots of other people, especially geometry, just put out their article about accelerating with stuff with web GPU. So you are seeing a lot more proving capabilities, I think, start to come to people's laptops and phones. But eventually I still think people are going to like application developers that are heavily using ZK will probably want to choose a trusted party for many sort of cloud for the same reason people use AWS at all.
00:26:32.732 - 00:27:03.040, Speaker B: Yeah. If your application requires private data for multiple users, which would be the case. For instance, in an exchange where you have not DeFi, but we'll say, like really modern, complicated, I guess here we would say intent based. In this kind of context, there will be no individual user who has enough data to produce the proofs about their interactions. And so this is where you'll find centralized proving will probably have, I think, a very strong place still. Yeah.
00:27:06.790 - 00:27:22.374, Speaker C: So probably I have one last question. So regarding the coworking with optimism, I guess they got dedicated teams reaching out to them, obviously. Which part of deaf do you believe is most important for them to help them get on board or make know?
00:27:22.412 - 00:28:09.350, Speaker B: It's very interesting. So when optimism put out their request for proposals, their specific ask was for the ability to ZK approve the existing Op stack which is written in Go. So the very first thing that we did is I sat down with Jeremy and Stephen and others and we collected a tremendous amount of data based on the stack, what its precise execution needs were, and began estimating the complexity of going proofs of these programs, programs which were not designed to run inside ZK. And based on these data, we were already working on Zeph. We already knew what a ZK native solution would perform like. We estimated that Zeph is actually ten x cheaper to prove than the Op program fraud system from Canon. And so based on this we proposed to them.
00:28:09.350 - 00:28:30.490, Speaker B: Sort of a radical idea. We said, never mind these requirements you asked for. What you really want is something that's going to be cost and performance optimized. So we put forward this idea of using Zeph as the foundation for that. For them. What that means is that they'll be able to take their current dispute window of several days and collapse it to minutes. Right.
00:28:30.490 - 00:28:38.400, Speaker B: You can produce the proof within minutes that the block is simply correct and then there's just no need to even worry about any disputes. You have an absolute proof that it's in fact.
00:28:38.850 - 00:29:07.482, Speaker A: Yeah, you can actually even imagine, like, a fraud window that's determined by people. If you want your money out sooner, you effectively pay to get a proof of correctness generated sooner. And then you could submit that. I think the fraud proof, ZK proof hybrid models are going to be really interesting for any kind of especially these kind of high speed game clock chains. So I'm excited about the intersection of that and gaming in particular. Beautiful.
00:29:07.616 - 00:29:45.906, Speaker B: And even for much more established change like ethereum. As Brian mentioned, our proofs today cost around $20 per block to produce. And if you believe that blocks are free, then $20 sounds like a lot. But in fact, blocks are not free to produce. Ethereum today has over 700,000 validators. If you say that each of those represents a computer or if even only half of them or a quarter of them do. This is a significant amount of redundant compute capacity, which does in fact add to the security of the consensus aspect, but which otherwise is completely superfluous for block construction.
00:29:45.906 - 00:29:55.366, Speaker B: And so, empirically, the cost of producing ethereum blocks today outside of ZK is significantly greater than $20 per block just.
00:29:55.388 - 00:30:23.438, Speaker A: Because is it cheap enough already? Kind of question. And yeah, so effectively, Zeth to get back to the Op question, though, zeth is kind of the basis of we had been kicking Zeth around and then as an idea. And then when it really came time to submit the Op proposal, we decided to actually effectively productionize it and then modify it to work with the Op stack. So we expect this kind of Zath work to be relevant in the Op context soon. Beautiful.
00:30:23.524 - 00:30:24.320, Speaker B: Thank you.
00:30:25.970 - 00:30:36.210, Speaker C: So I guess if we have no more questions, we'll take a break around ten minutes. And then next up will be Boya from Menta talking about DK Shaggle.
00:30:36.290 - 00:30:37.060, Speaker A: Awesome. Thank you.
