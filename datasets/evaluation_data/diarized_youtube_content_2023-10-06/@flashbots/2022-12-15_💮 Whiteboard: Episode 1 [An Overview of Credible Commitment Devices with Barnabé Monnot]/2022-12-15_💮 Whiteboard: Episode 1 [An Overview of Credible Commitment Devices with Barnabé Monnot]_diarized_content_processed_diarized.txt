00:00:04.730 - 00:01:02.598, Speaker A: Great. Welcome. This is white force with Ching. It is supposed to be a show with Whiteboards and, you know, invite yes with Ching with Jing. And we are going to talk about how to specifically unlock the magic of crypto or unlock the magic of credible commission devices, which I think is one of the biggest value proposition of crypto. And specifically in the process of discussing the value propositions, we will see that MEB or traditionally named maximal extractable value will arise as whether surprising or unsurprisingly as a result of the application or our pursuit, the path dependency of our pursuit to use crypto to unlock the true social welfare applications. So for this, this is the first episode we're going to talk about solicitability, how the show, how show is going to be laid out and specifically through the game theory perspective.
00:01:02.598 - 00:01:08.674, Speaker A: And today we have Barnale, would you mind giving a little bit introduction? For sure.
00:01:08.712 - 00:01:20.790, Speaker B: I'm Barnale. I'm a researcher at the Ethereum Foundation and the Robust Incentives Group, which is a research team dedicated to mechanism design and game series for the Ethereum Protocol.
00:01:21.530 - 00:01:21.990, Speaker C: Yeah.
00:01:22.060 - 00:02:14.722, Speaker A: So let's start with discussing the big picture, right, the value proposition of crypto. So one of the things I think particularly interesting in game theory is the idea of price of energy and specifically how agents, when you assume that all of them have rationality meaning that maximize their own utility, then they inevitably run into some bad scenario. And that is what we want to solve. That is why, I guess many people in the crypto community are obsessed with words like coordination. Correct. And from what I have understood, so many game series previously all worked on various ways of establishing how do humans coordinate and specifically how do rational agents coordinate. What do you think are some of the good examples that you encountered?
00:02:14.866 - 00:02:42.000, Speaker B: Well, humans coordinate all the time. I think one of the most powerful coordination tools that we have, for instance, is language. Like, we use the same words to describe the same thing, money. We use the same referential to describe the same amount of value bytes data on the Internet. We use the same protocols to communicate with one another. Language is a protocol. So it's very clear that once you find the correct way for people to meet one another.
00:02:45.090 - 00:03:31.200, Speaker A: Here'S the real website you generate. Yes, I can see that language is like you're allowing humans in the past to coordinate with humans in the future. And we don't have time traveling machines, but even in the absence of time traveling machines, we devise this device called language. And then, yeah, money is another great example. I have this theory about crypto has the biggest value prosthetic being a credible commitment device. So if you imagine, look at all those previously mentioned credible or commitment devices, we see that human beings move from farter, which is like, okay, so here you have A and you have B and then they want to trace something. Right.
00:03:31.200 - 00:03:45.150, Speaker A: We think about the most general case. Two people have some preferences. You want people to find a way to satisfy your preferences. Okay. So they come together into a location, let's say like Central Park.
00:03:45.230 - 00:03:45.570, Speaker C: Yeah.
00:03:45.640 - 00:03:46.770, Speaker A: Central park.
00:03:50.330 - 00:03:50.742, Speaker C: Yeah.
00:03:50.796 - 00:03:54.450, Speaker A: Because it's central. Okay, so they come here and then they barter.
00:03:54.530 - 00:03:54.774, Speaker C: Right.
00:03:54.812 - 00:04:21.262, Speaker A: I give you something at the spot, and we both can physically extort each other. Oh, if you don't give me this thing after I give it to you, I will punch you the face, or something like that. Previous to Money with Money, then, I think money primarily has two sides. The first one is like, want people to coordinate. And we know that in a repeated game, people coordinate much more.
00:04:21.316 - 00:04:22.014, Speaker C: Right.
00:04:22.212 - 00:04:48.310, Speaker A: So then money is a way to market make welfare in the routine game setting, I'm willing to give up the so, for example, A, I'm going to give up something, for example, this box. I give up this box. But in return, I do not require you to give me a box prime in return as a spot. I just want some money. And then this money what's the money sign?
00:04:48.380 - 00:04:48.998, Speaker C: Okay. Yeah.
00:04:49.084 - 00:05:20.100, Speaker A: And then I get some money. And then this money is supposed to in my future, expanded, repeated games allow me to gain more welfare, allow me to gain an advantage and starter. However, another view to view money is that everybody has some utility dependent on money. And my utility minus $10 should be equal to the other people's utility decrease in that person losing another $10. However, we see that that might actually not be true.
00:05:20.550 - 00:05:23.154, Speaker B: What's your perspective on money?
00:05:23.272 - 00:05:23.746, Speaker A: Yeah.
00:05:23.848 - 00:05:29.010, Speaker B: Well, I would say first that money is bigger than barter.
00:05:29.090 - 00:05:29.334, Speaker C: Right.
00:05:29.372 - 00:05:37.250, Speaker B: Like, there's this popular narrative that barter doesn't scale. And maybe I want a cow and you want honey.
00:05:37.410 - 00:05:37.878, Speaker A: Yes.
00:05:37.964 - 00:06:09.382, Speaker B: And we confined at the exact moment in time the things that we both want. And so there's this popular narrative that money comes because now you can solve this coincidence of wants. Actually, people have found out that money comes up much more naturally in the system of credit. For instance, you're trying to give a debt or like an IOU to someone, and you denominate that in some kind of unit of account. So I can give you a cow today, and you give me the money today, and we can have some sort of record that we both enter into with debt transactions.
00:06:09.466 - 00:06:09.810, Speaker C: Yes.
00:06:09.880 - 00:06:26.002, Speaker A: However, I think that we need to kind of make a difference over there because money is like a one dimensional scatter that's like not even some kind of real number, but what you have been talking about is like a record. So that's kind of a memory.
00:06:26.066 - 00:06:26.342, Speaker C: Right.
00:06:26.396 - 00:06:44.334, Speaker A: So, yeah, a much richer form of expression like we see in credit cards. Correct. Even you don't have any money. So that transaction, that increase in social welfare shouldn't have happened. But then there's market makers and that market maker is some kind of like credit system. Yes.
00:06:44.532 - 00:07:16.040, Speaker B: And once a third party wants to get involved and once I say well, I have this IOU from Shield is that worth anything to you? We need these kinds of debt to be fungible because that's how we scale the system. And so then we are trying to move from NFCs or sole bound credit tokens to something that's fungible and that eventually looks like money. So money is really an emergent property of that inter mesh that one has with another I would call like multidimensional money.
00:07:18.410 - 00:07:56.786, Speaker A: I would say like smart contracts for here we are getting finally the value of crypto, right? So what is even better than those record credits and multidimensional money? We know you have the previous thing. You have money. But with enforcements, right? So the technology of enforcement allows you to for example, previously, without enforcement, people have to calculate the risk and account risk into the pricing. And then this reduces some kind of systematic risk for decreasing utility. However, with crypto we can actually avoid that.
00:07:56.888 - 00:07:57.202, Speaker C: Yes.
00:07:57.256 - 00:07:58.498, Speaker A: So there's always a risk that you.
00:07:58.504 - 00:08:13.274, Speaker B: Don'T pay me back in which case the third party who got the debt that I got from you is completely screwed. So if there is a risk enforcement system which exists somewhere, it's much easier for everyone to enter into this coordination game.
00:08:13.392 - 00:09:04.230, Speaker A: Definitely. And also since you are mentioning that we know that most of the financial crisis coming from the task derives from this kind of over leveraged financial product or both fungible and tradable. So you require those two things about your credit commitment device holdings position. So then once we have those, then of course we can scale which is another way of saying that we can bring more people into this coordination game. However, we know those is cause financial cris. So maybe it is better if we have some undesirable coordination. So that is another name for, as we know, collusion.
00:09:04.390 - 00:09:05.100, Speaker C: Yes.
00:09:05.470 - 00:09:19.120, Speaker A: So it has like two sides and we really want to avoid it. As far as I know you have like an interesting used diagram. Yes, that'd be great.
00:09:20.690 - 00:09:25.566, Speaker B: So we're coming back I guess a little bit on the idea of price of anarchy.
00:09:25.678 - 00:10:48.410, Speaker A: Yes, and just as an explanation, the price of anarchy is this notion where if you chart the plot and you see the best welfare state suppose this is like the welfare of state S and then this is larger than any other possible state, okay? And then this is supposed to be a real number, whatever. Let's suppose that right now and then we know that the original we have a set of equilibrium states, right? Agents play the game. The aim happens to be end up in a set of equilibrium states. Suppose we are playing this game G and then another state, let's say WG is in the set of equilibrium states, then we know that the price of energy denoted by price of energy equals to the welfare of the maximum state divided by the worst state in the equilibrium. So here we're like abbreviating it a little bit, I guess saying it's arbitrary game. But of course this is the generic definition.
00:10:48.910 - 00:11:25.878, Speaker B: So what we mean is that if we have a dictator in the game who can assign actions to every player in the game, that dictator can get the best overall welfare, the maximum of all the payments to all. The players, the players themselves, if they play against one another very often, they want to do things that are good for them, and they start deviating. So you can think for instance of the tragedy of the commons where everybody has an incentive to pollute a little more. At the end of the day, everybody is worse off because they all try to maximize their own profit. And then the price of anarchy can be quite large between a world where the air is very clean and a.
00:11:25.884 - 00:11:29.960, Speaker A: World where the air is very polluted. The fishing is very cheap now.
00:11:30.330 - 00:11:31.126, Speaker C: Yes.
00:11:31.308 - 00:12:33.710, Speaker B: And so this price of anarchy is really it came up, I guess, starting end of the 90s when people were thinking about the internet and congestion specifically on the internet. So trying to understand I want to send Shin some bytes of data he wants to send to someone else that this big network of bytes. We don't have time to compute the optimal routing of all these bytes through the network. So we have to make simplifications, we have to be opportunistic and maybe a bit greedy. I want to send it as fast as possible to you specifically and then we are going to get into situations where the network is not as optimized as it could be. If there was this, I think Christos Pavadmitrio called it micromanager who is optimizing the internet. And then we get in that state and this is really the price of uncoordination like how much we lose from not being able to achieve this near data of payoff.
00:12:33.710 - 00:12:35.486, Speaker B: Should I plot this?
00:12:35.588 - 00:12:37.598, Speaker A: Yes, usually. Okay, great.
00:12:37.764 - 00:12:50.066, Speaker B: So when I was doing my PhD physics I was trying to understand how exactly we can get back to the utopia. And I was thinking of it this.
00:12:50.088 - 00:12:50.660, Speaker C: Way.
00:12:53.670 - 00:13:38.098, Speaker B: Which is that on the one hand we have efficiency and on the other hand we have decentralization. We can be very efficient if we have a dictator, the monarch model, Singapore in this case, we're here, everything is perfect, everybody's needs are maximized. People are prevented from achieving better payoff for themselves or their individual greediness. But society as a whole is feeling better. And on this end, if we let everybody decide for themselves and make their own decisions, they are really naturally going to decrease the efficiency of the system.
00:13:38.184 - 00:13:38.820, Speaker A: Right.
00:13:39.830 - 00:13:42.194, Speaker B: Let's say language. If we're all talking like a different.
00:13:42.232 - 00:13:43.202, Speaker A: Language to one another.
00:13:43.256 - 00:13:49.830, Speaker B: The body can really understand. There's no one who's there saying no, you should all be using English so that we can talk to one.
00:13:49.900 - 00:14:06.214, Speaker A: Exactly. Because the most efficient language for myself is going to be optimized from my own personal experience. And everybody's experience is different than if we go the full energy route. Of course people are going to speak different languages and that inhibits this coordination.
00:14:06.342 - 00:14:07.020, Speaker C: Correct.
00:14:08.030 - 00:14:09.222, Speaker B: If it's hopeless.
00:14:09.286 - 00:14:09.900, Speaker A: Right.
00:14:10.430 - 00:14:34.850, Speaker B: Do we only have a choice between we Catholic, be anarchy in the world? No. As mechanism designers, what we really want is to still achieve efficiency while being somewhat decentralized. So we want to move in that direction.
00:14:36.470 - 00:14:36.930, Speaker A: Yes.
00:14:37.000 - 00:14:55.930, Speaker B: What I mean here is that well, to some extent there's a bit of centralization because we bind ourselves to follow what the mechanism tells us to do. But the mechanism is also able to recover the efficient allocation of the outcome of the game for all the players.
00:14:56.670 - 00:14:57.178, Speaker C: Yeah.
00:14:57.264 - 00:15:14.638, Speaker A: So this graph kind of reminds me like efficiency as defined by the total welfare achievable, which is utilitarian model, would be the sufficient air for these utilities. And the decentralization, I think, is kind of another way of saying credibility of mechanism.
00:15:14.734 - 00:15:15.042, Speaker C: Right.
00:15:15.096 - 00:15:38.098, Speaker A: So if we think about it, what decentralization or these things mean is that why we don't want monarchies now? Because if the monarch is benevolent, that's kind of better. Then if the monarch can do whatever he wants, he or she wants and is not restricted by constitution or a set of mechanisms.
00:15:38.194 - 00:15:38.502, Speaker C: Yeah.
00:15:38.556 - 00:15:45.750, Speaker A: So it feels like there is some fundamental uncertainty that human beings are not possible to gouge in reality.
00:15:45.830 - 00:15:46.170, Speaker C: Right.
00:15:46.240 - 00:16:34.102, Speaker A: You don't know if in the future that the king is going to turn bad or something. These things are simply unpredictable. And because of this inability to predict fundamental uncertainty, we try to embed this worry into our mechanism, which is that we model the fundamental uncertainty as a form of strategic uncertainty. Right. We say that, oh, we know that the monarch let's take the worst possible case. We assume that the monarch or the dictator is rational and it can have a well defined strategy space. And then we think in this strategy space, he can choose whatever strategy he wants, denoted by his own rationality.
00:16:34.102 - 00:16:52.330, Speaker A: And then this transforms the fundamental uncertainty, which is like the formula of the world we don't even know how to model into a nice clean game that we can even start reasoning about. I think that is kind of like the power of blockchain. Blockchain is about credibility and programmability.
00:16:52.490 - 00:16:52.958, Speaker C: Yes.
00:16:53.044 - 00:17:26.806, Speaker B: And in this price of anarchy framework, we never really consider the incentives of the monarch. We just assume that there is this benevolent dictator who's not going to mess up the players. But when you think about it, the monarch would always scare the players to say, unless I get this amount of the social welfare, I will throw you back guys to the anarchy. Now, when we think about blockchain, we think about replacing centralized institutions that pretend or sometimes help us embody the monarch rule.
00:17:26.918 - 00:17:28.394, Speaker A: It would be so much better if.
00:17:28.432 - 00:17:39.806, Speaker B: Instead we had ways of ensuring that the monarch doesn't act against our own interests or abuses the position by extorting the price of anarchy from people.
00:17:39.908 - 00:18:25.866, Speaker A: Yeah, exactly. And that's what's interesting also here is even if you're in a monarch model people have different beliefs. We have common knowledges. Each people have a different knowledge or a different belief. And even though if you're indeed credible and people think you are not credible then they will still act in an anarchy way by their own interest. So like incentive alignments and credibility signaling is of course a very important part. And by crypto we all know that it is secured by at least what Ethereum is doing in crypto is to ensuring the existence of a decentralized enough party.
00:18:25.866 - 00:18:39.806, Speaker A: So if enough disinterested parties disinterested, meaning that they have a high pollution cost if enough disinterested parties hold their commitment for you then you have less strategic uncertainty.
00:18:39.918 - 00:18:40.580, Speaker C: Right.
00:18:41.430 - 00:19:16.254, Speaker B: And something else maybe that needs to be said is very often to achieve this optimal outcome with the mechanisms the players have to do payments to one another and in particular they have to do payments to the mechanism which then decides to allocate things properly. And so having the ability to believe in the mechanism and this credibility of the commitment of the mechanism also maybe allows you to ensure that these payments of the mechanism they can be further redistributed or they can be used in socially beneficial ways.
00:19:16.452 - 00:19:21.822, Speaker A: The public goods founding and route public goods is good. Definitely.
00:19:21.956 - 00:19:22.640, Speaker C: Yeah.
00:19:23.270 - 00:19:49.166, Speaker A: So then we know we want to encode this mechanism. And what are some of the examples in crypto that we have actually enabled this to happen? We have talked about how this is the biggest value for decision. What has been the institution. Does Bitcoin using having people pre commit to a schedule of releasing tokens for releasing?
00:19:49.298 - 00:20:29.302, Speaker B: Yeah, I think the ability to commit to some kind of emission schedule, either schedule or emission process because Ethereum doesn't really have a schedule but we know that it has a function that will take as an input the economic activity of the chain and output, the issuance and the burn of the currency. I think this is very powerful because it means that everybody has their entice to this and it provides some sort of meeting point for people to make their decision based on that mechanism. So at least you fix one part of the mechanism and that lets people be more free to do what they want something with them.
00:20:29.356 - 00:21:48.650, Speaker A: And it is common knowledge and it is common knowledge yes, it is common knowledge that Bitcoin schedule has much higher probability than the traditional system that we use. Okay, so Bitcoin and Ethereum and all kinds of tokens I think another example might be that we see that people have attention, right? Your attention is a resource. We see that happens in modern day Facebook, Twitter, TikTok, that the big corporations have your resource, which is your attention. Your attention generated some revenue, and maybe you don't want this revenue to be easily generated, but then you can only choose to, oh, I allow you to use my data, or I don't allow you to use my data. So it's like a binary situation that we exactly see here, right? You have a monarch or you have an anarchy situation. You don't have a credible general commitment that people can do. This is also something I think is interesting for crypto to explore more with other examples such as privacy, privacy apps, as we talked about, kind of an over approximation in our seeking of credibility versus efficiency.
00:21:48.650 - 00:22:05.642, Speaker A: I think, for example, Morio, all those coins are a credible commitment such that we know the action space, the fundamental uncertainty or the strategic uncertainty of the monarch is reduced. Yes, these are all great examples.
00:22:05.726 - 00:22:07.366, Speaker B: I don't have more to provide.
00:22:07.468 - 00:22:39.322, Speaker A: It's very let's go into the theory land, right? Let's do the real. This is going to get more mathematical. Let's talk about the simplest example for prisoners given a go for it. Yes. I hope everybody has heard about it. Okay, so imagine two prisoners, Alice and Bob. Okay, so Alice and Bob, they were caught by a police.
00:22:39.322 - 00:23:36.546, Speaker A: And then they can either both remain silent or they can tell rat the other person out. So suppose this is a payoff matrix of the original principle. Dilemma here is being silent and this is being betrayed. Okay? So we know that if they both betray, then the payoff is going to be the original payoff, right. When they each going to get sentenced or punished based on some of the behavior. And if they both be silent, the police doesn't know they actually committed anything. So let's suppose it's two two.
00:23:36.546 - 00:23:49.846, Speaker A: And then if one betrays the other being silent, it's going to be 30 and zero three. And then we can verify that this is the equilibrium state for everybody.
00:23:50.028 - 00:24:05.006, Speaker B: Yeah, because if Alice decides to be silent instead of betraying when she gets zero and say if Bob decides to be silent instead of betraying, bob gets zero. So that's the equivalent.
00:24:05.138 - 00:24:05.820, Speaker C: Yes.
00:24:06.270 - 00:24:09.306, Speaker A: Okay, so knowing that this is the.
00:24:09.328 - 00:24:11.866, Speaker B: Worst outcome possible with the social welfare.
00:24:11.898 - 00:25:04.298, Speaker A: Of two dots, let's say it's like two. Okay, so then how can we improve it? Suppose now we have this amazing piece of technology, crypto. Let's draw it as a box. Suppose that before the two prisoners go to prison, they actually signal to each other, knowing that they exist in this box and that they can commit to their actions in the future. Then it is possible for Alice to say oh, if Bob commits to being silent and then I commit to being silent and if Bob commits to be betrayed or any otherwise or does not commit at all, then I commit to betrayed.
00:25:04.394 - 00:25:04.654, Speaker C: Right?
00:25:04.692 - 00:25:23.870, Speaker A: So you can kind of see that we can write it as a smart contract. So if other if Bob committed to actually silent if Bob committed to silent.
00:25:25.370 - 00:25:26.120, Speaker C: It.
00:25:29.290 - 00:25:45.146, Speaker A: The icon is silent. Okay so this should be an out in any other cases icon need to betray. Okay, so then publishing this T sub.
00:25:45.248 - 00:25:46.490, Speaker B: Smart contract.
00:25:49.790 - 00:26:25.522, Speaker A: Makes Bob seeing this smart contract. And Bob knows that the only rational move would be to commit to silence. So Bob's decision tree actually looks like so you can verify. So Alice has two options whether to do a commitment or not do a commitment. If Alice not do a commitment, then both parties know it's going to end up one one at the original payoff if Alice does a commitment. And then now we transfer the move to Bob.
00:26:25.586 - 00:26:26.006, Speaker C: Right?
00:26:26.108 - 00:26:47.434, Speaker A: So then Bob has bob can choose to either commit to Alice's contract or not commit to Alice's contract. So Bob knows if Bob does not commit. So this is the L scenario. Addis commits to betray. So knowing that Add is commit to betray, bob can only choose between zero and one else. So he's going to choose one. So we know that it doesn't commit.
00:26:47.434 - 00:27:24.566, Speaker A: Then this is one one. If he actually commits, then knowing that I has chose to be two, and he also committed to be in silence. So then they end up tracing. We know that this is a dominant strategy for those players to play. Great. So then here this sounds almost too good to be true right? Yeah what are some of the pitfalls you see right it's almost as if Alice isn't being maximally rational.
00:27:24.758 - 00:27:31.114, Speaker B: Yes I mean Alice because what gives her the right to be the first player in making the decision here?
00:27:31.232 - 00:27:34.734, Speaker A: Yes alice is the monarch and so.
00:27:34.772 - 00:27:43.730, Speaker B: Alice could say I want a commitment from Bob but on top of this I want a payment from Bob whatever extra payoff he can get I want to get it for myself.
00:27:43.880 - 00:27:53.700, Speaker A: Exactly. Yeah so it's going to be if Bob committed to being silent and.
00:27:57.270 - 00:27:57.838, Speaker C: Transfer.
00:27:57.944 - 00:28:38.834, Speaker A: Me let's say one dollars like one payoff after the game then I commit to being silent or else I commit to betrayed so then now if we go into the decision tree again this is not going to change. Let's say I commits to this, and then Bob knows that if he does not commit this one, this is all clear. And then if bob actually commits, then Bob is first going to give Alex alex less than minus one. Right. Bob's payoff will be this is the original. And after the game, Bob gains two. Payoffs supposed to be like two minus one, which is one.
00:28:38.834 - 00:28:54.262, Speaker A: And then for Alice, since it got plus one from Bob. So Alice's sales is going to be three one, right? Yeah. So then this is what's going to happen. And Bob is kind of indifferent between committing or not committing at all.
00:28:54.316 - 00:29:00.154, Speaker B: But of course Alex could always say $0.99. Exactly. And that would be enough.
00:29:00.272 - 00:29:12.030, Speaker A: Yeah, would be enough. So it can track almost all the social welfare or the major the box is useless in this scenario.
00:29:13.090 - 00:29:33.474, Speaker B: Well, yeah, useless in a sense. But also there is greater social welfare in that society. It's just that Alice is able to capture all of Italian. I like that situation better. Strictly better, actually.
00:29:33.512 - 00:30:03.022, Speaker A: We can always argue that because it is better to have the value extracted rather than give the value to the uncoordination. Exactly. Okay, so then I suppose we all know that crypto is permissionless. This has always been the case. So it's not just that Alice can publish this smart contract. Bob can also make these critical commitments. So then we know that then the payoff is only going to be one three or three one.
00:30:03.156 - 00:30:03.886, Speaker C: Right.
00:30:04.068 - 00:30:27.970, Speaker A: So those are the feasible payoffs. And in fact we can see that the solution space seems to got larger from this original game. The original can only be one one. Even though those two feel unfair, they're still larger. And greater social welfare. Actually, there seems to be a principle in traditional game theory about this, right?
00:30:28.120 - 00:30:28.446, Speaker C: Yes.
00:30:28.488 - 00:30:34.530, Speaker A: In order to increase social welfare, we have people play repeated repeated games.
00:30:34.610 - 00:30:35.990, Speaker B: Should I talk a bit about this?
00:30:36.060 - 00:30:36.438, Speaker C: Yes.
00:30:36.524 - 00:31:16.222, Speaker B: So in repeated games we allow players to not only play this game once, but to play it over time. And when you play it over time so when you are a game theorist, what you do is you specify the strategies that the agents are taking. If you play the game one time, I only need to specify one strategy for you to play. But if I play over time, I can specify a strategy that eventually evolves as time goes on. For instance, my strategy when I play over time can be the minute Jing plays betrayed, I betray until the end of time. So we call that the green trigger. It only needs to be one failure of Jin's behavior.
00:31:16.222 - 00:31:34.486, Speaker B: And I will just do the worst thing possible for the both of us to educate suicide one another and do the bad payoff forever. That means that for you it's never rational to play betrayed. You want to constantly play being silent because otherwise I will click the button and we both betray.
00:31:34.518 - 00:31:48.350, Speaker A: It's kind of as if it is common knowledge that one of the players committed have a credible commitment committed to that in the repeated game setting I have this retaliation strategy, correct?
00:31:48.420 - 00:31:48.702, Speaker C: Yes.
00:31:48.756 - 00:31:59.810, Speaker B: And so to understand the space of the failure that we can achieve with these critical commitments, we use what is called the folk theorem, which is a description of a feasible space.
00:31:59.880 - 00:32:07.102, Speaker A: I can write folk theorem? Yes, folk theorem.
00:32:07.166 - 00:32:22.330, Speaker B: The reason it's called folk is not someone whose name folk. It's just that everybody knew that this was true and the proof is not so complicated. And so, yeah, it was just never rewritten up as something I don't remember who wrote the folks here.
00:32:22.400 - 00:32:23.900, Speaker A: Yeah, maybe, actually.
00:32:26.350 - 00:32:46.418, Speaker B: Okay, so to visualize what the folks here tell us, we're going to plot the payoffs of the players on a two dimensional axis. So first we have the bad payoff, which is here, eleven. Then we have the good payoff, which is here, two.
00:32:46.504 - 00:32:47.090, Speaker C: Two.
00:32:47.240 - 00:33:22.666, Speaker B: And then we have these other two pairs which happen when players play asymmetrical actions. So in one case, Alice has a pair of three and Bob has zero. In the other case, Bob has a pair of zero. And so, okay, so what is achievable in that space of pairs? I'm going to draw what's called the convex hull, which is all the pairs that we can achieve from randomizing our.
00:33:22.688 - 00:33:23.454, Speaker C: Actions, in a sense.
00:33:23.492 - 00:33:38.234, Speaker B: So I could say when Shin plays Betray, I play Betray for five rounds, and then I revert to playing silence. And by mixing up the proportions of being silent or betraying, we can move around with space and achieve any payoff that's containing.
00:33:38.282 - 00:33:42.898, Speaker A: So anything containing this shape, anything in.
00:33:42.904 - 00:33:54.662, Speaker B: That shaded region is possible. But there is something that we call individual rationality, which is that we can always do one right, we can always.
00:33:54.716 - 00:33:58.038, Speaker A: Be trained, we can always be bad to someone.
00:33:58.204 - 00:34:56.154, Speaker B: And so one is what we call our individually rational. And so we know that Alice is never going to pick a point which is in that part of the hull, and Bob is never going to pick a point which is in that part of the hull. And so really, the space of equilibrium, or achievable equilibrium, is what I now shaded in green. And what you notice is this point, which is the socially optimal outcome, is part of our green shaded region. And so all of these payoffs here, they form what we call the Pareto frontier, which is that anytime we move on this frontier, one of the player gets felt, maybe another player gets a bit extra payoff, but one of the player gets hurt. And it's true from linear programming that the maximum socially social welfare is achieved at one of the corners of the hub, which is bad corner two.
00:34:56.192 - 00:34:57.082, Speaker C: Two. Yeah.
00:34:57.136 - 00:35:06.414, Speaker A: Here it will make sense. And we can see that we actually achieve a much larger region of payoff, which is strictly better.
00:35:06.612 - 00:35:13.934, Speaker B: In the one shot game, we are prisoners of the dilemma. Make that and we are stuck here.
00:35:13.972 - 00:35:15.570, Speaker A: But in the repeated game, we can.
00:35:15.640 - 00:35:19.374, Speaker B: Break out of our prison and be always cooperative.
00:35:19.422 - 00:36:08.834, Speaker A: And what's interesting is that we can kind of see a striking correspondence between this traditional way of game called game theory, think of improving social welfare with the actual what we can see over here in the critical commission world in. The world of crypto by the ability of having agents to do those critical commitments, which is candid from this one one payoff to actually different points of the hall, which is ideal here and here. Okay, so then we can have a folk serum for credible commitments. And in fact, it is indeed proven that we can explore a much larger set of tails and that strictly benefit here's. One question for you.
00:36:08.952 - 00:36:15.650, Speaker B: These two payoffs that you said were achievable with the commitment device, they are not dot about convex her, right?
00:36:15.800 - 00:36:16.500, Speaker A: Yeah.
00:36:17.350 - 00:36:27.834, Speaker B: Because they allowed here one three here and this is one three. So how do we get here?
00:36:27.872 - 00:36:28.218, Speaker C: Right?
00:36:28.304 - 00:36:28.938, Speaker A: Yeah.
00:36:29.104 - 00:36:30.540, Speaker B: That's a great question.
00:36:32.430 - 00:36:44.110, Speaker A: What's the power that crypto or CreditBook can use and devices have beyond repeating games? Why is it even better than all of the previous technologies?
00:36:44.690 - 00:36:48.400, Speaker B: Is it even better because people can extort one another?
00:36:49.490 - 00:37:16.214, Speaker A: That's the double two sides of the coin. You have collusion and you have coordination. But overall I think the sense of payoffs is kind of the value add of the enforcement technology in arbitrary expressions. But then we still have this unfair problem over here, which is something we kind of need to talk about.
00:37:16.252 - 00:37:16.502, Speaker C: Right.
00:37:16.556 - 00:38:13.370, Speaker A: Because either in here is like Bob being the monarch and here is Alex being the monarch. We kind of need to address this. So for reason about this in an easier way, let's just say that abstract the person who is the monarch or abstract away the person who can be first in the game as agent C, let's call we have Alice, we have Bob, and then we have the person who goes first. So then now we can kind of formulate this in a cockpit game theory. So we know that in cooperative games there is a worth or a value of each coalition, which means that the payoff that the coalition could achieve by the agents in the coalition. So in this sense, the payoff of only Alice playing this game should be one.
00:38:13.440 - 00:38:13.674, Speaker C: Right.
00:38:13.712 - 00:38:21.082, Speaker A: So if Alice places by itself, then without any coordination. Yeah, without any coordination. Only one.
00:38:21.136 - 00:38:23.834, Speaker B: So this is same for Bob for.
00:38:23.872 - 00:38:31.438, Speaker A: One and then the person, since there is no coordination happening, the person, there's no president of the first. So it's going to be zero.
00:38:31.524 - 00:38:32.014, Speaker C: Right.
00:38:32.132 - 00:38:46.950, Speaker A: Okay. So then now let's get into the interesting part. So the poses, Alice and Bob somehow form the collision, right? They form a collision. They are coordinating. Then we know that the payoff of this collision is going to be four.
00:38:47.020 - 00:38:47.542, Speaker C: Right.
00:38:47.676 - 00:38:59.080, Speaker A: So it's 1331 or two. Two payoff. So that if one of them colludes with coordinates with the person who goes first.
00:39:02.250 - 00:39:02.806, Speaker C: Yes.
00:39:02.908 - 00:39:16.862, Speaker A: Using smart contract or vertical integration, meaning that Alice, without going to crypto, alice is already the validator. So in this case, AC means that the work of this correlation should be three.
00:39:16.916 - 00:39:17.134, Speaker C: Right.
00:39:17.172 - 00:39:44.630, Speaker A: Because Alice already has three. So this is going to be the same for Bulb. And finally the grand collision. If ABC forms the grand collision, no Gal is still going to be four because there's only four value. Okay? So we can see that. Okay, so those are our coalition words. What's the actual feasible payoff? So we know that in Carsy games there is this notion called the core.
00:39:44.630 - 00:40:20.206, Speaker A: So the core solution of a Carsy game means that it is a set of payoff schedules to each player. So this is like suppose it's like payment to payment to A, this is payment to B, this is payment to C. So it means that the sum of all of those payments equals to the collisions worth. So in this case equals to four. And we also require that no sub collision could profitably deviate from this coordination.
00:40:20.238 - 00:40:20.434, Speaker C: Right?
00:40:20.472 - 00:41:11.970, Speaker A: So, for example, if we have ABC as the ground collision, then we see that this is not stable, right? Because AC could profitably deviate and then A will get one. And this is not exactly a structure. What's the exact cell structure that is feasible? I will give the calculation, but we can see that the feasible one is one. One two, meaning that Alice and Bob get exactly what they would have got if they did not use crypto or crypto commission at all. And no smart contract for them is active and then proceed abstracted coordinator or Monarch or validator in this case gets a tail top two. And this is not desirable at all. Even less desirable.
00:41:12.790 - 00:41:16.446, Speaker B: Can you give an example with front running or arbitrage?
00:41:16.558 - 00:41:47.790, Speaker A: Definitely. So what's interesting is that this is a theoretical result, right? We started only using those prisoners dilemma and the formulation of cosby game theory. And we see this is the only feasible job. And in reality this is kind of core manifest itself, right? Theoretical result unsurprisingly manifests itself. So in reality we have a front running situation, which is that Alice proposes a payoff schedule of three one.
00:41:47.940 - 00:41:48.398, Speaker C: Right?
00:41:48.484 - 00:42:20.914, Speaker A: So Alice alice and Bob are both searchers. Yes. So Alice gets a payoff of three, Bob gets a payoff of one. And then this is what happens if Bob commits being silent and then Alice will commit to being silent if else, and Alice commits to betray. So this is what we originally talk about. But then we know that the commitment is credible only if it is landed on chain.
00:42:20.962 - 00:42:21.174, Speaker C: Right.
00:42:21.212 - 00:42:24.774, Speaker A: The smart contract has to be published. Your bundle has to be published.
00:42:24.822 - 00:42:25.226, Speaker C: Yes.
00:42:25.328 - 00:42:35.358, Speaker A: Okay. So then we also know that they can create C. So in reality we see these uncles as one month. Yeah, we are both searcher, we are.
00:42:35.364 - 00:42:49.634, Speaker B: Both trying to lend the same bundle. And what we know now from practice of blockchains is that the only way for the bundle to be lended is we have to give away our whole payment exactly. To the block producer or to see.
00:42:49.672 - 00:42:52.702, Speaker A: The coordinator if there is a common value scenario.
00:42:52.766 - 00:42:53.042, Speaker C: Yes.
00:42:53.096 - 00:43:15.706, Speaker A: So in this sense, since the game structure is common knowledge, then it's guaranteed that there's common value that everybody wants and we end up see. So in reality it kind of manifests itself in the sense of a coinbase stock transfer payment, right. As most of bundle related services do.
00:43:15.888 - 00:43:16.620, Speaker C: Yes.
00:43:19.550 - 00:43:32.190, Speaker A: Okay, so we kind of see, we previously talked about the price of the energy example. So price of energy equals to the best welfare state which is four minus the equivalent welfare state.
00:43:32.260 - 00:43:32.494, Speaker C: Right?
00:43:32.532 - 00:44:22.030, Speaker A: So two. So we know that price of energy equals two. And in the collusion example we see surprisingly we have a pair of so then what the coordinator gets is also equals two. So does this mean that price of energy always is always going to be internalized by the coordinator? So I think this is on interesting pattern and in fact we can indeed generalize this into Empire scenario and improve that which I think is particularly interesting. And what about block space futures? I think because I think having thinking.
00:44:22.100 - 00:44:22.720, Speaker C: About.
00:44:25.270 - 00:44:42.178, Speaker A: How validators in different slots coordinate is an interesting problem. We know that we have this critical commitment and then in reality we see that we have different players and suppose they are like four players and in different slots.
00:44:42.354 - 00:44:53.622, Speaker B: Okay, so we're in a model where there's a bonded set of validators and they are sampled and for each slot there's one person who is chosen to produce the block for that specific slot.
00:44:53.766 - 00:45:14.510, Speaker A: Exactly. Okay, so we can abstract it as four players and they all play again. And then now let's suppose there are some payoff that they can extract such that the worst of the collision is going to equals to the size of the collision squared. So the more person are in the collision, the more payoff that they can collectively extract.
00:45:15.250 - 00:45:19.178, Speaker B: The semi is greater than the hole.
00:45:19.194 - 00:45:20.454, Speaker A: Is greater than the semi.
00:45:20.522 - 00:45:21.140, Speaker C: Yeah.
00:45:21.750 - 00:45:36.054, Speaker A: So this is like the supermodel the words of AB is larger. Larger or equal to the words of A plus the words of B.
00:45:36.092 - 00:45:36.680, Speaker C: Right.
00:45:38.270 - 00:45:53.390, Speaker A: Okay. So suppose all those players, they coordinate using credible commission devices, using permissionless, credible commission device. Then what is the equilibrium payoff?
00:45:53.970 - 00:45:56.542, Speaker B: That's a good question. What is it?
00:45:56.676 - 00:45:57.550, Speaker A: What is it?
00:45:57.620 - 00:45:58.430, Speaker B: 16.
00:45:59.970 - 00:46:26.534, Speaker A: We know that the worth of the brand COVID is 16. But we also know we have this price of energy situation. They can drop from each other to publish smart contracts. So I think a good solid experience to go through is suppose you are player one, then you can publish a smart contract saying oh, because we know the maximum deal those three players can get is nine. Right.
00:46:26.732 - 00:46:27.094, Speaker B: Okay.
00:46:27.132 - 00:46:34.090, Speaker A: So it seems that the lower bound of the tax, the coordinated tax should at least be 16 minus nine.
00:46:34.160 - 00:46:34.730, Speaker C: Right.
00:46:34.880 - 00:47:02.402, Speaker A: Okay. So this is like how much? This is seven. Okay, so this is at least but then we also know that it is also common knowledge. Meaning that a one noted two notice, three notes, four noted that if 2234 themselves tries to form a coalition and extract this nine sales, then they're also going to run into a smaller situation of the Prisoner Dilemma. The original four player case.
00:47:02.456 - 00:47:02.674, Speaker C: Right.
00:47:02.712 - 00:47:11.910, Speaker A: So in doing those two can publishes my contract and then extract nine minus four equals five.
00:47:12.060 - 00:47:12.422, Speaker C: Okay?
00:47:12.476 - 00:47:53.694, Speaker A: So two can publish and then say oh, I want five and then those two players and you're in even smaller situation. So we know there's a recursive case here which you can model nicely. We say that it is dominant strategy for every dominant meaning like it is better for your utilities in any situation and it is dominant strategy for every player to publish a smart contract or private commitment. Saying that if all the other players commit to cooperate with me, achieve the maximum welfare state. So in this sense we say that it is the welfare. Suppose there are two collisions, right. So collision A and collision B are from symbol.
00:47:53.694 - 00:48:15.226, Speaker A: So the welfare of AB this collision, this is the maximum sort of welfare state minus the welfare of B. So suppose the worst that collision A can do to collision B is by quitting the brand collision. So then this is the total of the off. This is what you would have got if I torture you.
00:48:15.248 - 00:48:21.426, Speaker B: This is what A can demand of B. Yeah, benefit of having A in your coefficient.
00:48:21.558 - 00:49:05.798, Speaker A: Exactly. Suppose that they are extorting. So they also need to subtract their own original scale the pure gain from extortion. However, we can see kind of here is the situation. This does not consider the common knowledge of the use of permissionless credible commission devices. So what this means, this coordinator tax situation is that there is some externality to collision formation. So I think this is something not captured at all in traditional foreign theory and something we see now because you have this piece of technology constructed.
00:49:05.798 - 00:49:27.010, Speaker A: So then what you are really getting you can actually extort the externality of the formation of the collision B. So it's like suppose this is your collision, the extortion profits of collision A, extortion collision B. So then you can give an example.
00:49:27.080 - 00:49:32.914, Speaker B: With these playoffs maybe let's say, okay, A is one two and B is three four.
00:49:33.032 - 00:50:26.288, Speaker A: Yeah. So this is one two, this is three four. So then one two can collectively achieve like four if we assume there is no externality four with no externality. But in reality there is some externality and three four. You can see this is same as the Prisoner Dilemma situation, right? Three four have externality of. So here we have like kappa which we use to denote the calculate. This is externally which is the sub collision of B can extort from the other side of the collision that forms B.
00:50:26.288 - 00:50:56.792, Speaker A: And we know that this is recursive. So we consider for example if instead of one, two and four three we have like one and two, three, four, we see that we incorporate that scenario and one can let's keep the computations but one can verify that this is indeed the dominant strategy. And then this once we calculate this is equals to the price of anarchy scenario assuming there's no other cost other than this extortion cost in the externality of correlation formation.
00:50:56.856 - 00:50:57.084, Speaker C: Right.
00:50:57.122 - 00:51:27.780, Speaker A: And then the proof is kind of simple. We see that this is going to reduce to sub cases of the welfare state. And then if we actually unpack those three tails together, we see that the welfare of AB as a collision is basically the welfare if a and the welfare if A and B are working together and then the pelvic for A plus the welfare of A and B working together for B.
00:51:27.850 - 00:51:28.324, Speaker C: Right.
00:51:28.442 - 00:51:36.280, Speaker A: Okay. So then we make this term into this term and minus the welfare of B together wafer A together.
00:51:36.430 - 00:51:37.992, Speaker B: So this is supposed to be B.
00:51:38.046 - 00:52:14.052, Speaker A: Only working together, a only working together. Well, then we see this term equals the summation of the externality that A is causing to B and B is causing to A. And since in this situation the payoff function is super modular, we know that each agent has a positive externality to each other. In cause pride nation there's some weapons begin. So then we see that this recursive situation forms into sub cases. So then we know that in the end the grand pollution's kappa, the extortion payout is going to be equals to price of energy the summation of all of the agents externality to each other.
00:52:14.186 - 00:52:16.004, Speaker B: Yeah, maybe another way to think about.
00:52:16.042 - 00:52:17.364, Speaker A: It if you are a bit more.
00:52:17.402 - 00:52:30.504, Speaker B: Messy is the distribution rule. When you don't have interaction between the terms it looks a bit like the sum of everything minus the sum of the partial terms and that's it.
00:52:30.542 - 00:52:30.696, Speaker C: Right.
00:52:30.718 - 00:52:52.560, Speaker B: So which are not intersecting, which is the case if you have intersections or if you have positive externalities or if you have supermodularity from people being in other ones coalition then these externalities they won't be zero and you have to account for this otherwise you're double accounting in a sense, right?
00:52:52.630 - 00:52:53.250, Speaker C: Yeah.
00:52:54.500 - 00:52:56.256, Speaker A: Classic mathematical example.
00:52:56.358 - 00:52:57.010, Speaker C: Yes.
00:52:58.020 - 00:53:09.536, Speaker A: We can see that in general case kappa equals price of anarchy. What's more interesting is that in reality we don't really see people paying all of their tax to the coordinator.
00:53:09.568 - 00:53:09.764, Speaker C: Right.
00:53:09.802 - 00:53:14.890, Speaker A: It is not the case that in yeah, not the case that in any scenario they fix the tax.
00:53:16.860 - 00:53:19.540, Speaker B: What is it in our game? The price of anarchy.
00:53:19.620 - 00:53:20.680, Speaker A: Oh, in this game?
00:53:20.750 - 00:53:21.304, Speaker C: Yes. Okay.
00:53:21.342 - 00:53:50.064, Speaker A: So the price of anarchy would be if everybody will play for themselves, 1234 will play for themselves. So we know the value of one equals one is similar for others. So the summation the worker is going to be equals to four and then the best possible workflow state is going to be 16 when it's like four -16, which means that the maximum that anybody is going to extract is twelve.
00:53:50.182 - 00:53:50.848, Speaker C: Yeah.
00:53:51.014 - 00:54:11.428, Speaker B: Every time they're in equation, they want a permission smart contract that says they extort the rest of the coalition and then they extract these extra benefits they get from the coalition. But since everybody is doing that, the only stable thing is that there's no block producer here. So we're in a setting where these are the block producers. So who's above the block producer?
00:54:11.604 - 00:54:12.890, Speaker A: The protocol itself.
00:54:13.680 - 00:54:18.364, Speaker B: The protocol is able to sell these blocks based futures. You should expect the protocol to get.
00:54:18.402 - 00:54:25.068, Speaker A: The payoff of twelve from the block based future salary. They will extract this money.
00:54:25.154 - 00:54:25.548, Speaker C: Correct.
00:54:25.634 - 00:54:33.564, Speaker A: Or you suppose that if they actually publish the collision and coordination smart contract in their own slots.
00:54:33.612 - 00:54:34.450, Speaker C: Right. Because.
00:54:36.840 - 00:55:11.212, Speaker A: The example you were using is that suppose yeah. So suppose that they live on mechanism M. So this is the credible commitment device platform and then there's like M Prime, another one. So M Prime is going to be extract that much, but then all of the people on M has no incentive to use M Prime because they're already the value person only can self publish. So if that's actually the case, then the sales is going to be something a bit different.
00:55:11.266 - 00:55:11.484, Speaker C: Right.
00:55:11.522 - 00:55:37.284, Speaker A: It's going to be one publishes a smart contract knowing that it can manipulate its own ordering. So the one is able to extort 16 minus nine and then four by the two, it can extort nine minus four. And then we request so the final schedule is going to be 7531.
00:55:37.402 - 00:55:37.780, Speaker C: Right.
00:55:37.850 - 00:55:50.724, Speaker A: And then we can check this sum to the next payoff. Yeah. So this kind of seems there are two situations.
00:55:50.772 - 00:55:51.032, Speaker C: Right.
00:55:51.086 - 00:55:53.032, Speaker A: What's a better name for Kappa?
00:55:53.176 - 00:56:01.788, Speaker B: What's a better name for Kappa? Something that looks like an extinity. Something that is crossing one another.
00:56:01.954 - 00:56:04.750, Speaker A: Exactly. Kai, maybe.
00:56:05.440 - 00:56:09.244, Speaker B: Or do you want notification yeah, we.
00:56:09.282 - 00:56:28.516, Speaker A: Want Alec in them. We know that this almost feels like if you draw the health matrix, we know that if they can both to choose to front row or not front run, so then we know that if they both not front run, which means that they don't use the platform, the.
00:56:28.538 - 00:56:29.596, Speaker B: Platform does not exist.
00:56:29.648 - 00:56:29.864, Speaker A: One.
00:56:29.902 - 00:56:30.344, Speaker C: One.
00:56:30.462 - 00:56:38.168, Speaker A: This is one one and then this is like something like three one or one three.
00:56:38.254 - 00:56:50.092, Speaker B: So to whom does the value flow from the searchers trying to get their bundles in and the minor yeah, hence minor extractable value.
00:56:50.226 - 00:56:50.910, Speaker C: Yes.
00:56:52.160 - 00:56:53.512, Speaker A: Whoever is monarch.
00:56:53.576 - 00:56:54.190, Speaker C: Right.
00:56:55.280 - 00:56:56.216, Speaker A: Or mediator.
00:56:56.248 - 00:56:56.396, Speaker C: Right.
00:56:56.418 - 00:57:15.572, Speaker A: Because you have a mediator because we originally wanted this thing to use for coordination. Coordination is mediation, mediator is try the word all makes sense. Suddenly it becomes monarch. Right. internalizes all of the things. So this is the dominant strategy. It's erect President dilemma almost met.
00:57:15.572 - 00:58:37.432, Speaker A: And we can recurse this all the way up. So we see that this is one form of mev that erodes the fundamental value of crypto. We are unable to unlock any of its potential, which is sad. What can we do? First, we know that mev at least equals the price of energy situation. So this is only even if we only consider every agent is rational and they all use the permissionless coordination device, we know that if we solve mev, meaning that we redistribute mev or like dividing a better mechanism for EC extraction, we see that we actually achieve the optimal complex situation, which is great. Later in this series we will talk about that much, much more. And together with this real world applications of the solders and some real complications, we see that it is not necessarily the case that this is only used for coordination and there's no externality, actual externality aside from the MEB externality to collisions forming.
00:58:37.432 - 00:58:53.110, Speaker A: Maybe you divide a bad mechanism. So in that sense you have like a bad mechanism m prime, but M is really what you want. So all of the agents are using M prime to approximate M. So then there is some extra preservative over here, right?
00:58:53.480 - 00:58:54.500, Speaker B: This is the case.
00:58:54.570 - 00:59:18.910, Speaker A: We see that in most fair ordering or opaque privacy solutions such as threshold enclosure. So this is the price we don't want. Yeah. So 90 problems before. In fact, I want to kind of conclude this episode using manpower model very highly similar to the one Barnaby was talking about.
00:59:23.200 - 00:59:23.950, Speaker C: Yeah.
00:59:29.380 - 00:59:34.530, Speaker A: So you have the two axes, efficiency and credibility, right? So.
00:59:45.060 - 00:59:46.460, Speaker B: The axis are fixed.
00:59:46.540 - 00:59:46.832, Speaker C: Yes.
00:59:46.886 - 01:00:42.016, Speaker A: Yeah, I flipped it a little bit. I use the green light to draw. Okay, so we know that what's the mechanism? The mechanism that achieves greatest efficiency in the credibility? This is a monarch situation and then this maximum credibility and zero efficiency is the anarchy scenario. Right, which is similar to what if everybody has a commitment and then you order their commitment randomly or just by some arbitrary order. Okay, so then those two states are bad and then this is the state we want, right? A state with maximum credibility and then also maximum efficiency. So now with the formulation of Meav and quartz gain theory, we see that we can kind of actually give a quantitative model to the graph that you were using. So we see that we can define credibility as the amount of mev that the coordinator could extort.
01:00:42.016 - 01:01:01.100, Speaker A: We see that this equals to the cut up the extraction amount by the coordinator to the rest of the equalization. Let's say the grand provision is P. So it's like all other players. Yes, exactly. So then we say that, oh, this is how we quantify credibility and the efficiency is of course the summation of the utility.
01:01:02.480 - 01:01:03.230, Speaker C: Right.
01:01:05.220 - 01:01:12.384, Speaker A: Okay, so then we say that we also know that kappa maximally could equal to the efficiency, right?
01:01:12.422 - 01:01:12.864, Speaker C: Yes.
01:01:12.982 - 01:01:39.892, Speaker A: If I am a seller of block space, I'm a controller of whatever blockchain and I say from this block onward, I do not accept any transactions. So I would have zero payoff income maximum for the social welfare. So we see that this line represents the social welfare minus cutout, minus the mev. So we know that this is a space of feasible mechanisms.
01:01:39.956 - 01:01:41.290, Speaker C: Yes. Okay.
01:01:42.140 - 01:02:28.536, Speaker A: So today we are kind of if we have maximum public mempo and people playing repeating games, people have at least some credibility. So we might stay somewhere at some point like this, right? And then this is where we want to move forward. And specifically you can use many technologies. For example, if you add opaque privacy, if you overshoot, you're going to go here. This is what many of the existing solutions in the space we're looking at, which is south Altimo, right? Closer to there. So then, for example, you add expressivity you allow agents to express strategies about strategies and hours. Then you move up efficiency, you move it and you move to here.
01:02:28.536 - 01:02:44.460, Speaker A: And then maybe through expression of efficiency, for example, in the credit part example, we see that the technology of people able to record information in whatever database allows us to use credit rather than money.
01:02:44.530 - 01:02:44.776, Speaker C: Right?
01:02:44.818 - 01:03:01.830, Speaker A: So then that's an example of the increase in efficiency actually leads to an increase of credibility. So then there are also these technologies that allows you to do this. So then what we need to do is to find the shortest path, the combination of vectors that allows us to do it here.
01:03:03.800 - 01:03:08.416, Speaker B: Are we always moving that direction and we have to go somewhere?
01:03:08.528 - 01:03:14.230, Speaker A: Yes, sometimes there are obvious. Maybe there is.
01:03:16.280 - 01:03:19.120, Speaker B: Great. Is this the end of our episode?
01:03:19.200 - 01:03:20.836, Speaker A: Yes, this is the end.
01:03:21.018 - 01:03:21.410, Speaker C: The out.
