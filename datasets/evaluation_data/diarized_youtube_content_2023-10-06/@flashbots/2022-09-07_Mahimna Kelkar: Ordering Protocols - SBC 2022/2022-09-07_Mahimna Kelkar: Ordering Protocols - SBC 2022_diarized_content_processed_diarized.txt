00:00:01.770 - 00:00:27.206, Speaker A: Okay. All right, so today I'm going to talk about decentralized ordering protocols, PBFs and more. I'm Mahimna Kalkar. I'm a PhD student at Cornell University. So before I begin the talk, I just want to put out a little disclaimer there. I am an academic first, and this talk is much less academic than the ones I typically give. So I like to give this disclaimer whenever I give this type of talk.
00:00:27.206 - 00:01:22.150, Speaker A: So this is going to be based more on ideas and conjectures than rigorous results. It also includes a lot of scattered thoughts and hopefully a lot of thought provoking questions that you can take away from this. All right, so let's start. So what are sort of the goals of PBS? So the core idea is to sort of logically separate roles for block proposers and block builders. So the block proposers are part of the consensus layer of the protocol and the builders are part of the ordering layer of the protocol. So the goal is to logically separate these layers. So the main philosophy is that if mev becomes a centralizing force, then we don't want the centralization to affect the consensus part, right? So it's better to have a building part that's centralized than having the proposers or validators or the consensus layer be centralized.
00:01:22.150 - 00:02:56.978, Speaker A: So I just wanted to kind of talk you through a couple equilibriums that can happen with PBS and kind of try to understand them better. So here's sort of a first equilibrium that can happen, right? So if most of the mev strategies by value are known, sort of publicly known, and they're also easy to deploy, so this is one possible equilibrium. Of course they can be shaken up once in a while if there's protocol upgrades or new juicy smart contracts, but eventually it'll get back to equilibrium. So what happens in this kind of scenario is that mev can be primarily extracted by validators and likely the building role is no longer required to exist. There's some problems that arise in this kind of equilibrium is like some kind of consensus destabilizing or dos attacks is if I'm the proposer of the next block then I can try to dos or reorg the block from the previous slot in order to extract the mev because it's cheap to extract and I'm not giving up any resources to do that. There's also a lot of incentive for validators or proposers to collude across slots. So if you control two consecutive proposer slots, and if you have instances of mev where the mev across two slots, so maybe reordering across two slots is larger than the mev you get from each slot individually added up, then you can collude with other proposers to get more benefit yourselves.
00:02:56.978 - 00:04:00.780, Speaker A: So this is kind of the first equilibrium that can happen with proposal builder separation. Here's another equilibrium. So what happens when most mev strategies by value again become easy, but you need some kind of infrastructure that requires significant capital in order to extract. So you can say that something like HFT is in a similar stage where most of the strategies for high frequency trading are well known, but you need significant infrastructure and latency for example. So in this kind of equilibrium, you can basically say that large validating pools can run their own mev algorithms, small validators can still exist and they'll use a whole building market to propose their blocks. So if something like this happens, then you can see that it's easy for the biggest validators to become the biggest proposer biggest builders, and vice versa. Because again, you'd need significant capital to extract these mev opportunities, but they're not necessarily hard to do.
00:04:00.780 - 00:05:00.326, Speaker A: And it can also plausibly lead to validator centralization for executing mev strategies because validators can pull together their resources to build this infrastructure that they need for extraction. So here's sort of the most interesting equilibrium I would say. So this is the third one. So what happens when complex and highly profitable mev strategies are regularly found indefinitely? So this seems similar to kind of non HFT traditional finance, but not exactly the same because we have some kind of transparency for blockchains. This seems to be the path that mev is going so far, but it's unclear if this will be sustainable long term as well. So in a situation like this, you can imagine building become hyper specialized and this was alluded to in a few previous talks as well. And Validators can be just thought of as simple proposers.
00:05:00.326 - 00:06:00.590, Speaker A: And this was kind of the original design that PBS was meant for. But the point here is that even when new strategies are found, it's likely that the fraction of new mev that can be created by these strategies will be small compared to the overall mev pool. So if you have one builder that controls most of the block building and is significantly more powerful than others, then you can consider a scenario where the builder is kind of forcing proposers to exclusively get blocks from this builder. And I'll talk a little bit about this in so called consecutive or two block mev in the next slide as well. So my argument is that builder centralization has significant negative consequences. So the first sort of consequences is it's very likely that large builders will become large validators as well. And partly because becoming a validator has low additional overhead over already becoming a builder.
00:06:00.590 - 00:07:11.910, Speaker A: The second and more interesting thing is if you have two or more consecutive proposal slots, they can lead to better mev strategies if two block mev is greater than the sum of one block mevs. And so it's sort of profitable for a large centralized builder to also become a large validator to extract this mev. The other thing that can happen is builder centralization can create collusion incentives for validators or even validator centralization. So again, if you have one builder that's significantly more powerful in that it can find consistently more mev than others, then it can incentivize or force proposers to completely ignore other block builders and so that it's rational for the proposers to do so because they get more mev from this particular block builder. So it can form these exclusive partnerships with validators that PBS is trying to solve the most. Another thing that it can do is it can get exclusive order flow from wallets. So all of the transactions kind of gravitate towards this builder, the builder builds all the ordering responsible for them and then it also colludes with the proposers to exclude from any other builder.
00:07:11.910 - 00:08:10.586, Speaker A: So that's sort of the nightmare scenario I would say for PBS in the case that mev opportunities still continue to be found and the builder becomes centralized. So the problem here is actually if you look at it from a formal perspective, we still need a lot of rigorous analysis on PBS. We currently only think that separating these two roles of builders and proposers is possible, or it can actually happen. And we don't actually have a good understanding yet of the centralization points. And this is again me speaking as an academic, so we don't have an understanding of under what situations a single entity can play both the role of a proposer and a builder, and what happens when it does. But more importantly, we also don't have a good understanding of how this affects users and I'll nicely segue into this in a few slides later. So the likely solution is to decentralize block building or ordering and this has also been alluded to in a couple of talks before.
00:08:10.586 - 00:09:13.850, Speaker A: And one kind of possible solution I would put out there is these so called decentralized ordering protocols that I've been working on. So the idea is to decentralize the transaction ordering to many nodes instead of a single node and you can get some kind of strong time based ordering properties if a majority of the nodes turn out to be honest. So these notions of receive or batch order fairness, which roughly say that if a transaction TX one was received before TX two by many nodes in the network or a majority of the network, then the first transaction should be ordered earlier. And the crucial point here is that even if you don't make any honesty assumption, the ordering is still decentralized and it does not actually break safety, even if you don't have any honesty assumptions. And all of the fair ordering protocols or all of these decentralized ordering protocols like Equitas or themis, they all work this way. So just a quick two slide kind of summary of how Iquitos operates, but please see the full talk for more details. Is the idea is quite intuitive.
00:09:13.850 - 00:10:26.558, Speaker A: You essentially build a transaction graph where vertices in these graph are transactions and the edges denote ordering preferences and then you can compute the condensation graph of these to collapse the strongly connected components because it turns out that you can get some cyclic orderings due to a connection with social choice theory. And then finally you can output the topological sorting of the graph in order to get the final total ordering. And in the new protocol, themis, what we do is the leader, when it's building this graph, it can categorize transactions into three types. The first is solid transactions that are received many times which will be included in the block proposal, blank transactions which are not received enough times and they will be excluded. And other cases are shaded transactions and they'll only be included if they have some path in the graph to a solid transaction. And importantly, the missing edge between these shaded transactions can be added by subsequent honest leaders and this only depends on network delay. So yeah, again, you should see my more technical talk on this at SPC, the main conference for more details here's.
00:10:26.558 - 00:11:34.358, Speaker A: Also a couple paper links for these ones. Another kind of sort of problem that's being stated about these kind of fair ordering protocols is so called latency wars. So I like to claim here that despite popular belief, there's actually easy ways to prevent latency wars on more time based ordering protocols. So how do we do this? So you can actually change the granularity of fairness that you want to consider in the protocol. So you can bucketize the time by either the time or the number of transactions and still get time based fairness across buckets. So we can call this some kind of fuzzy or bucketized notion of first in first ordering and you can randomize ordering within each small bucket, but you still get significantly better, even spam protection, let's say, than randomizing full block. And basically another advantage in latency terms is you don't have any more advantage of having a faster network than the granularity version.
00:11:34.358 - 00:12:36.414, Speaker A: So this is actually in the themis paper as well. Another sort of thing that I would point out in comparison to latency wars is it's very easy to think about like latency wars and directly point to something like HFT where latency wars are bad. But it turns out that in a lot of markets the way HFT works is very different. So this kind of reductive analogy doesn't necessarily work. For example, in some markets, instead of the network switching doing sort of a first and first come for ordering transactions in the centralized marketplace, basically there's some markets that locally randomize transactions that are sent in some duration. So HFT looks a lot different in these kind of markets because having a significant latency advantage no longer helps. So the point I'd like to make is that you can't just think of latency wars as like being reduced to HFT equals bad.
00:12:36.414 - 00:13:55.442, Speaker A: There's actually ways that you can fix this and we do have the chance to think about this more deeply when we integrate this into protocol design. So I want to talk about a couple sort of more dispersed thoughts and segue into them a little bit. So the first thing I want to talk about is actually long range mev extraction and why it's terrible for the ecosystem and should actually be completely abolished. So if you think about sort of full block mev auctions, especially happening under proof of stake, then you can think that they lead to significantly larger mev or significantly more long range mev than is actually required. For example, if you control consecutive proposers, you can order manipulate across two or three block time periods, which is like 20 or 30 seconds even, which gives significant bad UX to the users who can have their transactions be reordered across like ten or 20 seconds. And this is very easy to detect and it's clear to everyone that a user transaction was received before if it was reordered like 10 seconds into the future basically. So it's hard to agree on the true ordering, quote unquote for transactions that are close to each other which could be sort of shuffled just due to network delay.
00:13:55.442 - 00:15:05.486, Speaker A: But it's very easy to detect transaction reordering for transactions that are clearly far apart and we should definitely, definitely not glorify order manipulation across such long time periods. And even today it should be easy to detect when something like this happens in employee slashing by something like eigen layer, for example. The other kind of scattered thought I wanted to talk a little bit about is incentive compatibility. This has been a pretty hot topic in this conference as well. So my main point here is actually mechanism design needs to take into account actual users and not just protocol participants. So incentive compatibility for builders or proposers or validators doesn't make sense if there's no users, right? Negative externalities that you're thinking about should be considered for regular users as well. For example, are some users being harmed more than others? So if you think about, for example, sandwich attacks, reducing gas cost for everyone across the board, but they harm specific users and make the cost better for everyone.
00:15:05.486 - 00:16:25.090, Speaker A: So is this sort of an infrastructure that we want to create where one user gets harmed significantly more than others? Right? So that's sort of one thing to consider. The other thing I'll say is actually maximal extraction or mev auctions, they're not incentive compatible for users so you can argue that they're incentive compatible for protocol participants, right? I can maybe even formally study this also, although I'm not sure of any formal study on this, but you can potentially say that they're not incentive compatible for users. On the other hand, you can say that something like first come, first serve is likely incentive compatible for users because it can give users best price execution, right? So at the end of the day you don't want your users to leave and suboptimal profit will always be preferred by protocol participants. If the alternative is your users moving somewhere else. And of course the thing is, incentive compatibility, even on its own is a difficult problem and if you bring users into the mix, it probably becomes even more significantly difficult. An additional point is even sometimes incentive compatibility may not be enough. You can think of killing the competition by temporarily taking losses, something like Amazon lowering their prices in order to kill the competition and become more centralized.
00:16:25.090 - 00:18:00.680, Speaker A: So these are some things that we have to sort of think about even when we think about incentive compatibility. So this was sort of alluded to by Phil's Talk as well. But we need to balance incentives for protocol participants and incentives for users. So my controversial take here, potentially controversial, hopefully not, is that incentives for users should actually be significantly more valuable than incentives for protocol participants because bad incentives for users ends up with just users leaving the system, regardless of how good the rest of the system is, right? And understanding the real long term cost of some of the solutions is very key, especially if this transaction order manipulation, for example, an easy example to say is what do you prefer? Right? No fees but maximally extractive versus no order manipulation but higher fees. And a classic example of this is Robinhood versus any other real brokerage where Robinhood was selling order flow but giving free transaction fees to its users. And actually the SEC found recently, I think a year and a half ago, that if the users were getting significantly lower price by using Robinhood with zero transaction fees than if they just paid the transaction fees at a different exchange in the first place. So this is something that we want to think about from the perspective of users as well and that sort of comes under this umbrella of incentive compatibility.
00:18:00.680 - 00:19:00.344, Speaker A: There's a couple ways that I thought about combining sort of decentralized or fair ordering protocols with PBS. The first thing that should be done almost immediately is some kind of time based ordering for transactions that are clearly far apart. So we don't want the terrible UX of reordering transactions that are 1020 seconds apart, right? So we can still let short term mev exist via auctions, but this would be significantly better UX for the users while keeping everything else the same. Another sort of potential long term idea that still needs a lot of formal analysis is to split blocks into buckets. And so you can think about each bucket as having a top part and a bottom part where the top part has some kind of mev auctions. So these are transactions that are within network delays of each other and have mev and the bottom part of each bucket does random ordering. So these are transactions that are within network delay and don't expose mev.
00:19:00.344 - 00:19:54.012, Speaker A: So we can just order them randomly and then you can think about more first come, first serve style ordering across transactions in different buckets. So you cannot reorder transactions across different buckets and basically provide better UX for the users of not reordering across long time intervals. And you can also think about dynamically changing the size of each bucket and the parts within. So let me end with a couple takeaways that if you listen to nothing else in this talk, I would like you to get about. So the first thing is you really have to think about incentive compatibility not just from the protocol entities, but also from the users. If you think about MBV auctions, you can think about incentive compatibility from protocol participants but not users. If you think about something like first come, first serve, it's incentive compatible from users but not protocol participants.
00:19:54.012 - 00:21:10.068, Speaker A: So are there ways to sort of solve in either direction or some ways to combine both of them? Right, the other very key, very crucial point is that we need to think about long term optimization of our goals and not just in the short term. So we need to understand long term effects of the decisions that we're taking right now. Otherwise what ends up happening is you get stuck in a local extrema rather than a global extrema and you cannot escape this local extrema very easily. So for example, when a short term optimal design gets entrenched into the system, it's extremely difficult for people to switch to a better, more sustainable design that's more long term focused in the future. And this is something that we need to think about when adding things to protocols in a permissionless network especially. The other kind of interesting takeaway that I want you to go away with is reasoning by analogy or bias sometimes versus reasoning by first principles. It's often easy to make reductive analogies to something, to argue a point and in fact, they're highly effective sound bites.
00:21:10.068 - 00:21:49.380, Speaker A: You can get people's attention very easily by making reductive analogies. They're useful for propagating information. But the real takeaway here is when we want to actually deploy some of these systems, we need to take a step back and more precisely, more formally understand what's going on and in ways that do not reduce to just reductive analogies. So it's kind of another key takeaway that I want and I'll sort of end there. If you want to reach out, here's the email and Twitter. I hope this talk has left you with some questions. Probably more questions than answers.
00:21:49.380 - 00:22:41.310, Speaker A: Hopefully some thought provoking insights as well. And happy to take any questions. I can't promise that I'll be able to answer all of them, but I can try. And here's while I think about questions, here's sort of a bonus slide which says without context, sometimes the path that looks worse at the start can actually lead to the paradise with the unicorn. Um, one quick question on the courser resolution fare ordering. Yeah. How do you order transactions inside each bucket? We still have a single threaded execution engine.
00:22:41.310 - 00:23:19.160, Speaker A: Yeah. So basically they're bucket size so you still have a total ordering within them. So the way I was thinking about it was you can randomize the ordering or you could even do something like mev auctions within each bucket. But you want to have the UX of not reordering across long time intervals and especially with something like proof of stake where you know, if you're a proposer in advance, you don't want the kind of bad UX of someone waiting two blocks and using mev from two blocks. Right, so reordering across two or more blocks.
00:23:22.470 - 00:23:54.190, Speaker B: Hey, thank you for the presentation. Two questions. The first one, do you have any clues for researchers in the room on how PBS should be formalized or the types of tools that can be used in the formal weapons? The second question is you mentioned there are markets that have slightly different market structures in traditional finance where that lead to different outcomes for help higher frequency trading players interact with these markets. Could you give examples of these markets?
00:23:55.090 - 00:24:59.304, Speaker A: Yeah, the first question, it's sort of a challenging problem, right, formalizing PBS and understanding all the nuances of rational in general. It's kind of hard to formalize rational behavior and incentive compatibility, especially in this kind of permissionless setting. But I can imagine sort of starting small with some basic model and then kind of building up what insights you get from this and then kind of throwing more things into the model. For your second question, I have been told that something like the Indian market, the way the network switches work is somewhat in the sense of randomizing the ordering across some time interval. So you can't do very low latency HFT there. Thank you for the talk. If I understood correctly, you said that long range reordering can be detected at slashed.
00:24:59.304 - 00:26:11.332, Speaker A: I wonder if you can expand on that and if there is any relationship to detecting censorship here. Yeah, so there's actually a number of ways you can do that. For example, you don't even have to because the way things propagate on the mempool, you can actually take advantage of even light clients that are just observing the mempool and not really processing transactions. And it's significantly easier, for example, to have an honest majority assumption among all the clients rather than just like a portion of the validators or something. You can also have more fancy techniques of audit based mechanisms to make it like incentive compatible for builders to not reorder across blocks. So you can think of like one transaction piggybacks on top of another so that you can come up with an actual proof that there was an ordering violation. Can you describe the randomization inside each bucket that you proposed in the middle for basically microscale shuffling so you prevent HFT player from predicting how things goes? Yeah.
00:26:11.332 - 00:26:42.472, Speaker A: So basically the idea was in each bucket you can think of the top of the bucket as some kind of mev extraction, but only within transactions that arise in this sort of small network delay. Right. So they could in general be randomized as well. And then the later part, because it doesn't have MEB, you can just order it however you want. So that's what I was thinking. For the random part, do you need true randomness? Typically. So I mean, that's sort of an orthogonal question.
00:26:42.472 - 00:27:03.700, Speaker A: Typically you can use something like VDFS even to provide the randomness. You really only need like one honest participant to provide true randomness. Right. Thank you. Okay, thank.
