00:00:37.570 - 00:01:37.500, Speaker A: So hello everyone, welcome to the presentation. Today I will talk about buying time, latency versus speeding in fair transaction ordering. This is a joint tour together with Mahim Nakalkar who is PhD students at Cornell University, jan Christoph Schlegel who is senior lecturer at City University of London and recently he joined Flashbots and Edward Felton who is co founder and chief scientist at Off Chain Labs. And I'm from off chain labs doing research there. So, motivation is transaction ordering policy, which is one of the biggest questions in blockchains, especially with the rise of DeFi. And we know from the traditional finance that they are maintaining order books and they are matching buy and sell orders and their policies first come, first serve. And this is the same policy for all roll up protocols that I'm aware of.
00:01:37.500 - 00:02:33.230, Speaker A: And in case of traditional finance, it is enforced by law that this is the only policy. We know that at Ethereum it's a big problem that there is a lot of front running and back running sandwich attacks happening because there is one block writer for each block which can choose the transactions from the man pool and also the ordering of it. And this is then exploited by them and transactions include beats or tips. But in case of roll ups, we don't have this and we are proposing to change that. So there are a lot of advantages to first come, first serve. First of all, it's the simplest probably, and it's easy to explain to anyone. It also sounds fair.
00:02:33.230 - 00:03:12.280, Speaker A: Whoever is fastest gets executed fastest and it minimizes the latency. There is nothing faster you can do just receive the transaction and execute it or put it in the execution. But it comes with some disadvantages also. And the biggest disadvantage is that it creates the latency competition. So transaction senders are trying hard to be fast. And in case of traditional finance, we know that there is this whole industry high frequency trading and it constitutes more than half of the exchange volume. So it's a huge business.
00:03:12.280 - 00:04:08.810, Speaker A: And in case of blockchains, it was acknowledged only very recently in 2020 in Flashbots paper. And then there is a company Flashbots around that too. So we think that it's only the front running that is a bad activity and want to prevent it. But back running is good activity because it corrects the price in the DeFi on automated market makers and we can support it. Or whoever wants to get that arbitrage opportunity should be able to, or whoever has the highest value for it. Okay, so this latency racing also exists in roll ups because all of them use first come, first serve policy. And we notice that parties spend resources to get closer to the sequencer.
00:04:08.810 - 00:05:11.274, Speaker A: And the problem is that parties with more resources have always an advantage, so they always win all the races. And this is problem because they are not necessarily the ones that value the transaction fast inclusion the most. So it can be someone who has less resources but better algorithm or better liquidity in different pools and can extract more value. So it's also inefficient. And from the protocol perspective, it's complete waste because they spend resources to pay, I don't know, Amazon servers or some other servers or improve their Internet, but no resources goes to protocol. And one example that was observed on Arbitrum is that parties create nodes, a lot of nodes, and connect it to sequencer for the feed, just to get feed faster than others. Because Sequencer uses the fair feed policy, which is random.
00:05:11.274 - 00:05:43.946, Speaker A: So whoever has more nodes has higher probability to get the feed faster. And these too many nodes, unnecessary nodes, slow down the sequencer. So this is also a problem for the roll ups. And of course it would. Be much better if these resources, or at least part of it, captured by the protocol. And this could be used later to subsidize the regular transaction fees or used for the maintenance and improvement of the infrastructure. So there.
00:05:43.946 - 00:06:21.160, Speaker A: Are a lot of properties that we ask for the new proposal. So first of them are more informal. So we want to have good properties that first come, first serve has, which is low latency, of course. And some level of transparency and we want to reduce the waste that is caused by latency raising. More formally, we want to maintain a dark mempool. So transaction details are not visible until they are already scheduled. And once they are scheduled, then there is no problem.
00:06:21.160 - 00:06:46.330, Speaker A: So we want low latency. So we don't want to make transactions. Especially regular transactions wait a lot. So once the transaction is submitted and received by sequencer, it should only take some short. Time bound to be executed. So one more property that is maybe more exotic. It's independence of irrelevant transactions.
00:06:46.330 - 00:07:44.830, Speaker A: And this means that we don't want two different races to interact with each other. So any complicated algorithm that you may come up with. We can prove that this will have this problem, that it won't be independent of irrelevant transactions. So different traces will affect each other. Also we are looking for the algorithm that is easy to decentralize so it's stable after decentralization so it should not be something very complicated and actually it turns out that third and six are coming together in the dark ManPool solution which we achieve by maintaining a committee of sequencers and threshold encryption. So for that we need to have some number of sequencers and this needs to be typically low number. So between seven and 16 we are thinking also we need to have this threshold encryption decryption.
00:07:44.830 - 00:08:36.130, Speaker A: So every sequencer only holds some share of the secret key and we need some threshold of them to come together to decrypt the transaction contents. So we want it to be Byzantine fault tolerant. Namely, assume that sequencers can be arbitrarily malicious and network to be asynchronous. So the only assumption is that once the transaction is sent, it will be delivered at some point. But we don't make any assumptions about how long it will take. Of course, higher N gives us more security because we then need more people to come together to decrypt transactions but it's also getting much slower so we cannot increase the number arbitrarily. Also, there are other considerations why we don't want too many sequences.
00:08:36.130 - 00:09:57.290, Speaker A: So, informal description of the algorithm is that we want to mix the timestamp. So arrival time with bits, with a simple logic, higher the bid faster, the transaction needs to be scheduled and also lower the timestamp which would also be executed faster. Okay? If we mix bits with the timestamp, this will motivate the senders instead of spending resources on the latency improvement, which is quite expensive, instead of that to bid also we want to guarantee that no transaction can be outbid by any other transaction if G time passed. So you cannot buy more advantage than G and G we think to be approximately half second. So that's a maximum what we add to the latency. And we believe that human users will not notice much difference but of course, bots and programs will see a huge difference. Okay? So our algorithm is in my opinion, very simple and it also has some fairness guarantees.
00:09:57.290 - 00:10:38.242, Speaker A: It has all economic properties of first price, all pay auction. So all pay auction means that you cannot take your transaction back if you don't like the position of it. So once you bid, you need to pay for it even if you lose the spot you wanted. So your transaction is not the first in the race. And yeah, it's incentive compatible in that sense that if you bid more, your transaction just gets earlier. And in case of back running, that's all you care. Because in front running, you also care that transaction that you try to exploit comes in between your two transactions.
00:10:38.242 - 00:11:14.580, Speaker A: But here it's only about back running. So you just want to be as fast as possible. Okay? So we don't want to discourage people to send transactions earlier. So if you send earlier, you should get some advantage. But if buying time is not too high, then you prefer to maybe wait slightly and then buy more time. So, priority time. And we think that it gives chance to players who don't have high budget to have low latency because it costs a lot to sometimes at least win the race, especially when their valuation is high.
00:11:14.580 - 00:12:23.510, Speaker A: Okay? So with our algorithm, we also avoid a situation where there is a transaction that beats low and right after it, there is a transaction that beats high and the high transaction is executed later. And this you cannot avoid as soon as you have block based approach. Because if you have a block based approach, some transaction just makes it in the previous block and then there might be very high bit transaction that comes quickly after but it makes to the next time block. But with our approach we have this continuous time so we don't get such situations. Of course we care about not sophisticated transaction senders because they constitute more than 95% of the transaction senders or maybe even more so when they send transaction and bid nothing their transaction will be executed in at most G delay or more precisely after G delay. Okay. Now, more formally about the algorithm.
00:12:23.510 - 00:13:02.200, Speaker A: We have a stream of transactions. So it's continuous time. We don't have time barrier there. So transactions just come one by one and we maintain some score of them. But to calculate the score, we look at the timestamp that sequencer writes on the transaction. So the time that sequencer received it and bid, that is denoted by bi. So then we calculate the score si and post the transaction for the execution that has the highest score and no transaction has a potential to outbid it so that's also important.
00:13:02.200 - 00:14:29.700, Speaker A: So now what is the score function? We have a function for the priority time which is g times bid divided by bid plus constant. So for this talk assume that constant is one but this is set by a system and can be updated as the system goes. So if you think about CS one ethereum so that would mean that one ethereum buys you 250 milliseconds extra which I don't think you can buy by improving the latency. So I guess with latency improvement maximum you can get maybe 100 milliseconds then the score is the priority times minus t or if you look at the dual problem it's timestamp minus the priority time but because we call it score we want to maximize it. So this is the negated value of updated timestamp. Okay, so first result is that the only algorithm that satisfies independence of irrelevant races is the one that looks at the score function. So no other algorithm or any other algorithm you come up we will come up with an example that it doesn't satisfy this property.
00:14:29.700 - 00:15:10.090, Speaker A: But of course, I didn't show why that particular score function is the one. Because we could be just looking at the timestamp. That would be first come, first serve. We could be just looking at beats. That would be a bit weird because there is some time, so there must be some at least waiting time. Well, of course we can have block based approach where we look at all the transactions that come in some time interval and sort there by bids or we can be looking at any other score function. But regarding the choice of the function that we have so first we have normalization, so we have few properties.
00:15:10.090 - 00:15:57.680, Speaker A: So if you bid nothing, you don't get any priority. This is very intuitive. Second was that property that no matter how much you bid you cannot buy more than G time. So that's the second property, we want it to be increasing. So more you bid, more priority you get. And okay, so the last property is the concavity of the score function. So this is more technical or it implies the convexity of the cost function, which tells that at least in our modeling, if you want to have equilibrium where higher valuations bid value, you need to have some convexity of the cost function.
00:15:57.680 - 00:16:47.630, Speaker A: Okay, if you take these properties into account, I think that the function that we have is the simplest one. But if you have some other suggestion for a simpler function, I would be very happy to hear from you. Okay, so algorithm I briefly already discussed, but here it's more explicit. So we are posting the transaction with the highest cost as long as there is no potential for other transaction to outbid it. Complexities are very good here, space complexity is linear. So we just look in some time interval to the transaction so we don't construct any additional table. And runtime is n log n.
00:16:47.630 - 00:17:47.490, Speaker A: So as fast as it can be. Okay, fastest would be linear, but we need to at each step find the transaction that has the highest score. So for that we need this additional factor logarithm. Okay, now let me go through some economic analysis. Suppose there is some arbitrage opportunity and players, let's say we have two players, they need to decide on the technology, latency technology and later maybe about the bid. Suppose there is this cost function C, that depends on the time and of course lower you want the time, so faster you want the transaction, more it costs. And these two users have some valuations for the arbitrage and in principle they are different, but we assume they come from some distributions of valuations.
00:17:47.490 - 00:18:35.522, Speaker A: Okay, so now we take the cost function that is one over t, but actually it doesn't really depend on the functional form here. What matters is that if you want to be right after the opportunity arises, so your time is zero, it costs infinity. So you cannot be, physically speaking, right after the opportunity is there, but you can get arbitrarily close, it just costs arbitrarily much. And the valuations are the same distribution for both players. But here also we can look at slightly different valuations also. Yeah, big assumption here is that we assume the independence of valuations. There are two models that we look into.
00:18:35.522 - 00:19:07.914, Speaker A: In the first one, players invest in the latency before they know the arbitrage opportunity valuation. And this is most realistic one. First you set up your system and your infrastructure and then you learn about valuations from time to time. But yeah, so here we should think that after some time you need to change it too. So it's not for forever. So it's not one time cost you incur and that's it. So technology changes, maybe sequencer moves somewhere else.
00:19:07.914 - 00:20:08.270, Speaker A: So you need to change it only lasts for some time period. Okay? And in the other we look at the model where you can invest in both the latency and also bits after you learn about the opportunity. Okay, so this may be less realistic, but there are some cases where for example, there is this twelve second block time for the ethereum. And you saw some transaction that you know, if it will be executed, you have some arbitrage opportunity. Therefore, you can condition with some service provider that if this transaction is executed or included in the next block, then I want my transaction to be very quick. So that would correspond to that case. Okay? But in both cases bidding is interim activity so you bid once you know the valuation.
00:20:08.270 - 00:21:18.258, Speaker A: So this is a very simple game. First we look only to the latency investment game where more you invest, better your time is. So your strategy is how much you invest and if you invested more than the other player you are the one that wins. Then the expected payoffs are written here and this game has only one so it has maybe many equilibrium solutions but in any equilibrium solution the expected payoffs of both players are zero so they completely exhaust each other. Thank you. Okay now a slightly more interesting case is if one has lower budget than the other, situation changes again we have maybe multiplicity of equilibrium but in each equilibrium the weaker player gets expected payoff zero and the stronger player gets positive payoff and this doesn't change. So this is quite robust.
00:21:18.258 - 00:22:26.490, Speaker A: So the one that has more resources always wins basically. Okay? So in this equilibrium it does not always win but it makes the weak player win only very seldom and the expected payoff is zero while its payoff is positive. But this changes with the bidding. Okay? So now we add this to the model biding. Suppose in the first round they invested some levels x one and x two then we know how much it costs to produce some score sigma so that's the score that I was talking about. And now the players solve an optimization problem so they try to maximize their expected utility and this is done by first order condition. And in the end we get a system of differential equations.
00:22:26.490 - 00:23:31.066, Speaker A: So the functions map valuation into bidding or the other way around the signals into valuations. And we are not able to solve these system of differential equations analytically, but we can derive some properties. First case is very simple if both of them invested the same in the first round then we can even explicitly solve the functions and in particular we show that there is a completely separating equilibrium which means that as the valuation increases, bid increases. So we know exactly what was the valuation depending on the bid and they bid in every case and gets much more complicated if it's asymmetric. So if one player invested more in the latency in the first round. Then there is some threshold below which none of them beat. So there is a pooling.
00:23:31.066 - 00:24:13.470, Speaker A: There is no separation here. But of course the high low latency player wins or high investment player wins. But as soon as we are above this threshold, they start bidding and from there on we have full separation equilibrium. Okay, so this is more formal result. We can even find what is this threshold? It depends on the difference in the latency and on G. And yeah, so it's a lot of discussion here, but I don't have time for that. Main takeaway is that if we take G large enough, it approximates the good case where we have completely separating equilibrium.
00:24:13.470 - 00:24:57.126, Speaker A: So in the future work we are thinking to add more players, which should not be a big problem. At least the insights that we obtain should generalize. Also we are thinking to have more general cost function that should not change qualitative results, but of course it will change quantitative. Adding dependent valuations is also not very difficult. But also we want to estimate the valuations from the data instead of assuming theoretically that they come from some distribution. And actually there is a lot of data about valuations. We try to compare it to the other alternatives.
00:24:57.126 - 00:25:36.854, Speaker A: And the first alternative is of course block based auction. So we just take some time interval and sort the bits in that time interval. So we want to have some easy algorithm to update the parameters, especially C. So if C is small enough, then we need to increase it because it's too cheap to buy the data. And if it's too high, we need to lower it because it's too expensive to buy the time G. Maybe not in the beginning. We think that half second is the good trade, gives a good trade off.
00:25:36.854 - 00:26:01.100, Speaker A: And also we are interested in implementing and experimenting the proposal. If our assumptions were correct or what can we improve about that? So thank you, I'm out of time. We milk around a bit so you can ask me questions. Thank you.
