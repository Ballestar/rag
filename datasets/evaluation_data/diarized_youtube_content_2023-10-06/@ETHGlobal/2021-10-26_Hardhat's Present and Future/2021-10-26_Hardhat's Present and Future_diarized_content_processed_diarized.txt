00:00:05.450 - 00:00:19.440, Speaker A: And with that, we are ready for our next talk. And up next is Patricio. And Patricia is going to be talking about Hard Hats present and future. Super excited to see what he has to share and announce today. Welcome.
00:00:20.690 - 00:00:41.850, Speaker B: Hey, thanks, Karthik, for the introduction. So let me just set this up. Okay. Can you see my screen? Yep. Right. Okay. Thanks everyone to be somewhere watching at this presentation.
00:00:41.850 - 00:01:31.818, Speaker B: So I'm Patricio, co founder and CTO of Domic Labs, and you probably know us from our product, our development tool called Harhat. In this presentation, I'm going to talk about how Hard works internally and its future. So first, a quick recap about what Harhat is. Harhat is an ethereum development environment which is composed by two major components. One is Hardhat Runner and the other one is hard network. Hardhat Runner is a task runner that lets you automate all the common tasks that you have to do while developing Ethereum applications. There are similar tools to Hardhat Runner in every development ecosystem or platform.
00:01:31.818 - 00:02:50.210, Speaker B: For example, Webpack and Parcel are some of the most common ones when doing web development, or Gradle when doing Android development. And actually, Hardhat Runner has been heavily inspired by this. By default, Hardhat Runner comes with a set of predefined tasks that let you do things like compiling your contracts, testing them, deploying them, things like that. But there's also a rich ecosystem of plugins that let you integrate other tools, add features or new tasks to Hardhat Runner, or customize the existing ones. Harhat Network is our development ethereum network, and what this means is that it simulates an Ethereum network just like Main, robstone or Coban, where it's totally local. Instead of having many nodes communicating with each other, you can think of Harka network as a single node ephemeral Ethereum network. When you run Hashan Runner, like running your tests, a new instance of Harka network gets initialized.
00:02:50.210 - 00:04:08.220, Speaker B: Your smart contracts are run within Harka network, and then after your test passed or failed, this instance of Harka network gets destroyed. And in this presentation, we are going to focus on how Hard Network works, the challenges of building and maintaining it, and how it's going to evolve in the future. So first, how does hard network work? Hard Network is a node JS module that it's embedded into Hardhat Runner and exposes the same JSON RPC interface that a real Ethereum node does. This means that we kind of lie to the different libraries and applications that integrate with Harka Network and pretend to be a real node, but instead do everything locally within node JS. So, for example, when you connect MetaMask or other wallet to Hardhat Network, it doesn't need to know that it's dealing with Hardhat. Instead, it just acts as if it were working with Ethereum Mainet. The same happens with Web Three or Ethers or any other library or application.
00:04:08.220 - 00:05:26.230, Speaker B: This gave us the advantage of being able to integrate with everything quite easily. So instead of communicating things to a network, hacker network resolves everything locally. And what this means is that if you send a transaction, we don't forward it to a network of different nodes, but just create a local block in our own blockchain and keep it for us. We don't forward those blocks either and not communicate with anyone else. And to do this we use Ethereum Jsbm which is a TypeScript EVM implementation and it lets us execute everything locally. But what makes hackath network different or special is not being able to simulate a network, but rather that we always look or try to provide as much context and information about what's going on with your smart contracts because that's what let developers debug and test their smart contracts and be more effective on building them. And this is where automation challenges appear.
00:05:26.230 - 00:06:59.650, Speaker B: And the way this works is by relying on runtime observation. And this is because in Ethereum, as we all know, you have to pay for gas when you try to execute something and in particular the person paying for that gas is the final user. So keeping the amount of gas that you pay to a minimum is super important because it makes your application more or less competitive. And this leads to many things, but in particular one of them is Salt C generating a minimal amount of bytecode when compiling your smart contracts. Instead of embedding functionality to help you debug your smart contracts or head tools do that, it has to offer a very minimal buy code without any kind of runtime support so that the amount of gas that the final users are going to pay when running that smart contract is lower. So things like for example, generating stack prices are not part of the program but rather left to the tooling. In contrast, in other development platforms this will be embedded within the actual binary which makes things easier because the compiler has the whole controller information to be able to do this effectively.
00:06:59.650 - 00:08:50.790, Speaker B: The way we approach this is instead by, as I mentioned, random observation. But what does that mean? It means that when we execute anything within Harka network we trace this execution and by tracing here I mean collecting information or the history of what went through when executing the contract at the ABM. And this trace, you can think of it as a sequence of EVM opcodes or operations that have been run during the execution of a VM smart contract. And while this is very useful at that level, it isn't very approachable because you have to be very deep into the EVM to be able to understand things at that level. But instead what we do is take that EVM and very low level representation of the execution of your contract and try to recreate more friendly information for the user. For example, this is how we implement Console log when we take a trace of the ABM we recognize certain patterns that mean that someone tried to call to the console log library or function and when we detect that we decode the call arguments and print them to the console or to the terminal of the user. And this is in general the approach that you have to take if you are willing to build advanced tooling for ethereum.
00:08:50.790 - 00:10:38.780, Speaker B: Another thing that we do is generating stack traces and in this case it's similar like we take the trace NBM trace but also fetch the compiler sample that has a lot of information about your contracts and combine those to recreate a call stack in Solidity. Which means that we know when a function was called or a return from a function was executed. But this is not always complete or perfect because there are certain ways that a contact can fail or certain paths of execution which are not actually reflected or come from your code, but rather from internal Solidity functionality or auto generated code. That information is not very useful for the final user or developer and this is particularly true for older versions of Solidity. So what we do is rely on a set of heuristics that we beat through time to improve this information and give richer things to the user, like auto generated error messages. For example, when you send a transaction to a contract and the function that you are trying to invoke doesn't exist in that contract, we can detect that and give a clear error message explaining what it did while depending on the version of Solidity and your settings. Without this set of heuristics, all you would have got is a revert without reason therefore message.
00:10:38.780 - 00:12:06.962, Speaker B: And this is in fact a ton of work to build and has some limitations that I'm going to explore next. The first one is that relying on heuristics is extremely fragile. They are built super ad hoc for every version of Solidity. In fact the way that we build them is by looking at hundreds of EVM traces and after some time you start to recognize patterns between them and then encoding those patterns within hard network. So we can kind of translate our intuitive pattern matching that we do when looking at the CBN traces and encode it as JavaScript code within Hackam network. But the problem is that as they are super ad hoc they are fragile because whenever a new version of Solidity comes out, they can break. Because maybe Solidity changed something that broke our pattern, it doesn't repeat anymore and the change may make sense, it's not their responsibility for our heuristics to be stable through time.
00:12:06.962 - 00:13:23.550, Speaker B: But the problem with that is that nothing guarantees us that we will be able to fix them or that we will be able to create new ones if we need them for a particular feature. Another problem with them is that they are not always accurate because these are heuristics and for the ones in the audience that don't know what that means. You can think of them as fussy matching things in this context at least. They are like pattern recognition logic, but not very precise. And the reason that they are not very precise is because they can't be too precise or we are going to have a lot of voice negatives and they can't be too loose because otherwise we are going to have voice positives but we'll never be able to be 100% accurate because the compiler doesn't give us the precise information to do that. Another problem with this approach is that the tooling that does this is always getting more and more complex. The reason for this is that Solidity versions tend to stick forever within the ecosystem.
00:13:23.550 - 00:14:40.502, Speaker B: Let's suppose that you build the contract now you probably are going to use zero eight four or zero 85, something along those lines and tomorrow zero point and you finish your contact, you deploy it and tomorrow 0.9 comes out and has some cool features that let us tool developers improve your development experience. The chances of you porting rebuilding and redeploying your contract just to get better tooling are very low because that's going to cost a lot of money, at least for audits. And also it's going to take a huge coordination effort. And while the advantages for you may be huge, it's not great for the community. They won't get a lot of advantages, so that's not going to happen. But even if you are building with the latest version of Solidity, chances are that you are interacting with a contract that was deployed using another version of Solidity and you still want to understand what and why things go wrong while you are testing your context or debugging them.
00:14:40.502 - 00:15:35.560, Speaker B: So that means that we still need to support every version of Solidity probably forever. Another disadvantage is that creating these tools is very expensive and complex and this leads to two things. The first one is that there are fewer tools than maybe there would be if these were cheaper. And the other disadvantage is that you get very inconsistent features across the different tools. For example, if you want to use console log today, there's only a few tools that support it like hardhat. If you want to step the barrier there is just a few that have it, that have them. If you want to use a good integrated fasting system, there is just a few of them.
00:15:35.560 - 00:17:23.500, Speaker B: And the problem with this is not that we want to have every feature that everyone has, but that these are very basic features that in other development platforms are a given and they should be a given for every tool in Ethereum, but they are not and that's something that we are aiming to improve instead. I think tooling should be different, tools should be differentiating themselves with more specific functionality instead of the basics. And the way that we are planning to tackle these problems is with two new projects that for now we call Slang and ResNet. Slang is a new Solidity compiler that is going to be built specially for tooling and ResNet is a library that provides a development network with all this rich functionality that Hardhat has. So as I just mentioned, ResNet is going to be a library for other tools to build on top. ResNet won't be a replacement for Hardhat network within Hardhat but instead it's going to be its new core and the interface of Harkat network will still be the same. It's being built as a platform, as a runtime observation engine that will let Kraken network be more stable, faster and mature, but also will let other tools get the same functionality for free.
00:17:23.500 - 00:19:18.366, Speaker B: It's written in Rust so it can be run everywhere and distribute just as a normal library like a C library. And also it's going to be compileable to WASM so that it can be used in the browser and things like Premix or Ethereum Studio can get all these features for free. It's going to provide all the functionality that hacksA network has out of the box and instead of relying in Heuristics, it's going to depend on Slang's output to have an accurate understanding of Solidity, of what's going on everywhere. So we are going to be able to be precise on all the things that we do, but we'll also be able to do more stuff because we are going to have the information to do those things. And once the basic functionality is mature and we have the common base for a development network, we are going to work on more advanced tools, more advanced features that Hackhand network doesn't provide today like a step debugger, a gas profiler, code coverage analysis and more. Slang is a Solid compiler, as I mentioned that it's going to be focused on developer tools. This means that we won't be competing with Solc but instead we will be focusing on development time of your smart contracts, but like orienting the compiler to be able to integrate with other tools and provide all the information that those need.
00:19:18.366 - 00:21:22.482, Speaker B: But building a compiler is a huge challenge and we think that Susie does a good job on building a compiler, that it's good for running your smart context on Mainet and we pretend to keep using it for that. So Hardhat and other tools that want to use Slang will build both with Slang and Solt C and use each of those builds for different things and of course deploying with the Solc build. Slang is also going to have an integrated language server which is a service that lets text editors and ides integrate with the compiler to gather features that advanced editors have to support the language for free. Things like some to definition refactors, some to references, things like that kind of require a compiler to be built or at least part of the compiler. And we think that having them integrated within the compiler is a way to have them to build them reliably and keeping them up to date with the evolution of the language. So we are also providing that out of the box, it's also going to have APIs for different tools to be built on top like exposing the parser or the type check system and those things are going to allow things like reliable, fast and up to date linters formatters and other things. One of the key things of slang as I mentioned is that it's going to generate all the metadata that ResNet requires not only to be able to avoid relying on Heuristics, but also to let us iterate breathnet faster.
00:21:22.482 - 00:23:10.234, Speaker B: So if we want to do anything that is not possible with bresnet we can just tweak slang a bit and generate more metadata. And the cool thing here is that as we are not going to be deploying these bytecodes generated, by compiling these smart contracts, we can tweak what we generate so that we can enable everything we want within ResNet. But maybe some of you are wondering on how reliable are things going to be if we compile with one compiler during development and with another one when deploying. And that would be a great question because if we use different compilers for these things this wouldn't be as reliable as just using suitcase. But one of the cool things about ethereum is that it's a completely deterministic and isolated platform or execution environment that lets us do pretty cool things. If you have two builds of the same smart contract but that were built with different compilers, but both of them did a good job like translating solidity to EVM bytecode, they should execute and give the same results. There's of course going to be small differences in gas costs and stuff like that, but in general the modifications to the ethereum state should be the same.
00:23:10.234 - 00:24:48.406, Speaker B: If I execute an ERC 20 transfer built in a smart contract built with slang or built with soil C, the end result should be the same. So we are going to take advantage of this and build breathnet in a way that can execute both versions of a smart contract at the same time. So instead of relying just on slang during development, we are going to be building with both and executing with both slang and solc. We are going to trace both of them, compare the results and use slang to gather rich metadata of the execution and soil C to commit the changes that that smart contact execution made to the ethereum state. And in general this should work great and let us have the best of both worlds reliable test results because they are the results that the Salt C version of the contract generated and that's going to be the same ones that this contract would generate on Mainet. And at the same time we are also going to have as much metadata about the execution as we need. There's of course, a chance that these results don't match and this would probably be because some bug or immature aspect of slang.
00:24:48.406 - 00:26:02.298, Speaker B: But that wouldn't be terrible either. Because at worst, what would happen is that the development experience would degrade a bit on a certain transaction or smart contract, but we'd still be able to rely on our test results, so we won't be compromising on correctness or security of our smart contracts. So the current status of these projects is different for each of them. ResNet is already under development, it's being built from the inside out of Cartham network. We are going to replace one module at a time and ship them as part of Karthan network. Probably we'll ship both at the same time initially and do something similar to what I already mentioned of executing two things side by side and comparing results to ensure the correctness of redneck before shipping it to production. And Slang is not being developed yet, we are building a team of compiler experts and doing some research and planning around it.
00:26:02.298 - 00:27:39.520, Speaker B: But both should ship within the next year and will be shipped initially as part of Carhat and as they mature they will be exported as libraries and independent tools for other libraries and tools to be able to rely on them and get all this functionality for free. And finally, our goal here, both for reinhardt restnet and Slang is for them to become the building blocks of the tooling of Ethereum, like all the major tooling. And our objective is to lower the cost that I mentioned of building new tools and maintaining the existing ones so that this leads to new and more mature tools for the Ethereum ecosystem and eventually for all of its users and protocols that build on top of it. And finally, if anything of this looks interesting to you, we are hiring both for Breathnet and Slang, also for Hardhat and other projects that I didn't mention here. So feel free to go to this link nomiclabs iohiring and take a look at our shop listings there and if you have any question you can join our Discord which is in our website hardcat.org and that's all.
00:27:43.250 - 00:28:16.998, Speaker A: Awesome. Patricia, that was a greater view and I mean that's such a massive overtaking that you're going to go into over the next few years. You talked a lot of this is still in the works and especially for Slang. My only kind of question is how do you think about that? Especially when Solidity itself is also evolving in parallel. What are some kind of things that you are thinking that should be kept in mind and how do you kind of match the direction and the speed of the EVM ecosystem evolving?
00:28:17.174 - 00:29:15.440, Speaker B: Yes, I think that's a good question, especially about Solidity still evolving. I think that's a common fact for every language. It's going to evolve fast and tools are going to struggle to keep up with it and we are already struggling to keep up with it and these projects were made massive, are going to make keeping up with it much easier for us and other tools. But also as the ecosystem matures and gets bigger and has more tools and more stakeholders depending on solidity. I guess the natural progression is for the evolution of the language to split apart from the evolution of the compiler and have some kind of governance over the language. But that's still to be defined in the future.
00:29:16.210 - 00:29:39.766, Speaker A: Absolutely. Obviously finding this out in real time with everybody else is the challenge. Anybody to know what this look like? But this is great. I think you already talked about how people can get involved so check out hardat.org and if you're interested in being part of this future, sure. So thanks again, Patricio, this is great.
00:29:39.788 - 00:29:40.374, Speaker B: Thank you.
00:29:40.492 - 00:29:43.380, Speaker A: I wish you all the best and can't wait to use playing in Red.
