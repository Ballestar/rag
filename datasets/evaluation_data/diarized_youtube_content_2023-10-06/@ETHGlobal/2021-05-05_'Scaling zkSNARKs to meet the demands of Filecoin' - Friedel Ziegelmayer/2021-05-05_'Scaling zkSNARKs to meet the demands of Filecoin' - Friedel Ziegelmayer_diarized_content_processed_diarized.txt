00:00:00.090 - 00:00:18.000, Speaker A: And I want to welcome Friedel from Protocol Labs and he'll be talking about how they're scaling Zkissnarks to actually maintain the demands of FalcoIn. So apologies again for the technical side of this, but welcome to the summit and I'll let you take it from here.
00:00:19.250 - 00:00:55.786, Speaker B: Thank you. Thank you for the introduction. Let me share my screen so we can get started. All right, thank you everyone for having me. Yeah, so I'm free to let go by Dignified Choir or Dig for short on the Internet. And I work on Protocol Labs, and I've been working on bringing ZK snarks to Filecoin for the last three years. Probably, if you don't know Protocol Labs is we build things like IPFS and Filecoin.
00:00:55.786 - 00:01:52.534, Speaker B: And if you don't know Filecoin is filecoin is the decentralized storage network that happens to rely very heavily to actually work on using ZK snarks. And I'm going to talk a little bit about the challenges that we had making that actually work. All right, so I've been working mostly on the proofs side of things for Filecoin. And if I'm talking about proofs, what I mean here is the part in Filecoin that verifies that storage is actually maintained as is claimed, and it turns out that is a non trivial problem to solve in Filecoin. I'm going to go quickly over this. There's much more information on the LDA proofs of work in detail, so you can find that out on the Internet or ask later. But I'm just going to go very high level so we can cover some ground.
00:01:52.534 - 00:02:47.422, Speaker B: So we have three big proofs that we need to take care of in Filecoin, and all of these use ZK snarks to actually make it on chain. So if we look at the first one is called Porep, called short for proof of replication. This is a proof that gets generated every time some storage gets onboarded onto the File client network. So storage gets onboarded in sectors which are just chunks of data. They are currently either 32GB or 64GB. And if I'm giving numbers, I'm usually referring to two gigabyte sectors because that's currently the most commonly used size. The second proof is what is called winning post, short for winning proof spacetime.
00:02:47.422 - 00:03:44.002, Speaker B: This is a proof that gets generated on every block. So if a miner mines an actual block, they have to generate this proof, proving that some challenges actually are still valid on their current storage that they're providing. And then there is window Post, which is short for window proof spacetime. And this gets generated every 24 hours for all sectors or partitions. And so basically it's a rolling challenge against the storage that a minor provide storage minor provides in order to secure that that storage is still available. Now, all of these proofs by themselves wouldn't actually need to use ZK snarks. You could just simply generate mocha proofs in most of these cases and just post them and be happy and live happily ever after.
00:03:44.002 - 00:04:57.718, Speaker B: But unfortunately, the amount of proof, merkel proofs that we would need to generate would make any blockchain explode in terms of size. And so we use ZK Snarks, really to compress the amount of information that needs to be posted on chain and to keep up with that demand. Now. Turns out when we've looked at the design of these proofs and what the amount of things that need to be proved here in ZK snarks, there were some challenges looking at what we had when we started as a reference. When we started building this, the new hot stuff that was out was the library called Bellman from the folks at Zcash and they were preparing the Sapling update, switching over to this new library, implementing graph 16 and rust as a very great baseline. But unfortunately, the things that we were trying to do were a little bit more complex. And so we really took us the last years in getting to a place where we can actually do the things that we run today on Falcon mainnet.
00:04:57.718 - 00:05:33.986, Speaker B: So the first challenge that we ran into, and this is like demand one that Falcon gave us, is, hey, we would like to have a billion constraints. A billion constraints is a lot of constraints in a graph 16. I want CS based. And for reference. So I looked this up earlier. So the Zcash Sapling spend circuit, which is like what was there back then, was like, this was really cool. And a live deployment of ZK Snarks uses roughly 100,000 constraints.
00:05:33.986 - 00:06:29.740, Speaker B: So we are orders of magnitude beyond what was available and known to actually be done. Now, how did we get a billion constraints put into ZKS next? We looked at a lot of things. And what we ended up doing is so we had two steps, really, that we needed to deal with. So the first one was we needed to look at just the setup. So for graphics, you need to run a trusted setup called Powers of Tell. And this defines how many constraints can there actually be in any given circuit that you ever run based on this trusted setup. So, as you can see here, Zcash ran back then a trusted setup based on two to the power of 21 maximum constraints, which is roughly 2 million.
00:06:29.740 - 00:07:55.638, Speaker B: Now, we ran a trusted setup a couple of years later to solve our scaling issues, which allows us to construct circuits based on two to the power of 27, which has run roughly 134,000,000 constraints. Now, this is already a great increase, but if you look at the math, 134,000,000 is still not a billion. So how do we get to a billion? Well, unfortunately, we couldn't grow the trusted setup we could have, but that would have introduced other issues. So distrusted setup generates a lot of output files, and some of these have to share later down the line. If we had increased even more, then we would have run into issues there that we could have not distributed those files easily. Also, the larger you go in this size, the more complex the trusted setup computations become and so you reduce the amount of participants that really can deal with this, which is really not something you want to do in a multiparty computation where you rely on having a lot of trusted parties participate. So we were stuck somewhat at this two to the power of 27 number and had to come up with something else.
00:07:55.638 - 00:08:56.470, Speaker B: So the next thing that we did to solve this was well, it is not that exciting, we split the snack. Luckily for us, the parab circuit really is a repetition of the same thing over and over again, proving that a certain challenge is still valid. And so it is very repeatable what we did. We constructed this proof instead of just having a single ZK snark, we generate ten ZK snarks each with roughly 100 million constraints. And so if we take that times ten, we end up with 100 million constraints. Cool. Unfortunately, this means now you're not putting 192 bytes on chain, you're putting 1922 bytes on chain, which is not great, but way better than if you would put all of those local proofs directly on chain.
00:08:56.470 - 00:09:53.740, Speaker B: So this was how we got to a billion constraints. Now, if you have with a billion constraints, kind of obviously what follows is that the proving part of your snarks is going to go a little slower because you're suddenly generating very large circuits and trying to prove them. And the demand that Filecoin gave us was really like look, this has to be done in reasonable times and in some cases specifically winning post in very fast times. So winning post has less constraints, but because it needs to be generated per block and we have a block time of 30 seconds, you need to generate this post in under 10 seconds. So you need to go really fast. This is crucial for every minor. There's no excuse if this is not going fast enough.
00:09:53.740 - 00:11:16.466, Speaker B: So the first thing and really the most critical thing to enable fast proving that we did is take all the proving from the CPUs to the GPU. So not all the proving, but the most expensive parts. So when you generate a ZK schnarc using Roth 16 turns out the most expensive computations that you have to do are FFTs and multi exponentiations. So what we did is implement those first in OpenCL and later in CUDA to really optimize the performance of those. And if you look here at the graph, the amount of speed up that you get is quite considerable, especially when you look at the 100 million constraint size, which is the one that really counts for us for per apps and the 10 million ballpark for winning post. And so we're seeing between four and six times speed up between the optimized CPU version. So as an explanation here, bellman is the original library that Zcash gave us and to the world and that we've built on.
00:11:16.466 - 00:12:15.720, Speaker B: And then Bellperson is the fork of that library that we optimized over the past three years. So you have the CPU version which is slightly faster, but it is by no means as fast as the GPU based versions here in OpenCL and the CUDA version and the OpenCL version is the one that is currently deployed on Mainnet. The CUDA version has been in development for a little bit and we hope to ship it soon. It'll give another nice boost to everyone running on filecoin. For reference, these times are recorded using machine running RTX 3090 but you get similar results with weaker GPUs. There's some limitations around the amount of Ram you have to have to load things in, but generally you can use this. Well, that helped a lot.
00:12:15.720 - 00:13:51.480, Speaker B: Let's take a look what else? All right, so the other demand is really fast verification, right? So because we have a lot of Snarks that go on chain block validation needs to handle Snark validation and so you need to really make sure this goes as fast as possible because otherwise you will just simply stop the chain which would be very bad. So the first big thing here that we have is Blast and this was a collaboration with the folks at Supernational. We've built this really based on the requirements from the Ethereum community and us. Originally Blast is focused on implementing BLS twelve 381 based signatures, BLS signatures. But because when we designed this we really requested that the operations were available to also do everything we need to actually do Graph 16 based of this. Because what you really need there is a fast implementation of B list twelve 381. This is a library implemented where all the critical routines are implemented in assembly and the higher level parts in C and we've created bindings and rust to that and so we can use it as a drop in replacement for the pure rust implementation of B Lust twelve 381.
00:13:51.480 - 00:15:05.386, Speaker B: It's been audited by NCC and currently there is a formal verification underway for the full library. As you can see here, this saves again very heavily factor of three on just doing a simple pairing which is one of the biggest bottlenecks. When you do verification times both G one and G two BLS twelve multiplications also considerably faster and for most of other curve operations this is similar and so you end up with considerably overall speed ups. Not necessarily the three X everywhere, but considerably faster. But this is not enough, this was not enough. And so another thing that we did was implement the Graph 16 Bash verification. Bash verification, the first time I've read about it is in the Zkesh spec and as far as I know they hadn't actually implemented it back then, but we found it and luckily got it implemented.
00:15:05.386 - 00:16:14.446, Speaker B: And this is really great because it allows you to so there are three Miller loops per proof if you do a verification, but this allows you to save two of those three and do them only per batch and not per proof. And so you can see here, as soon as you start batching a lot of proofs together, the growth and verification times really goes down, which is great. And so we currently don't fully employ this as much as we would like to currently. So we do this for props because as I mentioned, we have ten poor apps per ten Snarks per poor app. Sorry. And so we do a full batch verification there and gain the considerable speed ups. Doing batch verification for more proofs is a little bit more tricky because you need to make sure on the execution path that you can actually do this batching.
00:16:14.446 - 00:17:10.950, Speaker B: And the batching only gives you everything is valid or invalid. So if something is invalid, you need to go back and do more complex checks, which is a little tricky to deploy. But this, as you can see, it gives us really good speed ups from factors of six on just eight verifications, which really counts if you look at the amount of verifications that we have to do on mainnet. All right, so we got that in. Seems like we've got already a lot of things in, but there are more demands. Fuck. One wants to make more Snarks, really the network and really there's more demand and the chain is getting fuller and fuller.
00:17:10.950 - 00:18:15.062, Speaker B: And so there's the request to make the proof smaller. So, as I mentioned, for poor apps, unfortunately, we have not just 192 bytes from a single Snark, we have ten Snarks. And the amount of Snarks and poor ups that go unchained is considerably high. I'll show the numbers later. And so you really end up with an issue of having currently, if you look at the chain stats, we still need less bandwidth for Snarks. And so one of the latest things that we've been working on is called Snarkpack. This was just put up on Eprint and is a collaboration with a couple of folks and based on some work from Mary Mallor and others that allows taking a graph 16 based Snarks and to take them and aggregate them, there are a couple of restrictions.
00:18:15.062 - 00:19:10.718, Speaker B: They have to be based on a power of so the number of proofs you can aggregate have to be a power of two. But is really great because this is really what allows us to take size and verification times to the next level. There's a more detailed there was just a more detailed work at the ZK Proof Workshop early this week from my colleagues on this, if you want to know more about this. But just if we look at the numbers here, they're really good. We can see that from going. So the light blue line is what we have as the batch verification, and up to 256, we really go, we're not faster yet. If you look at the verification times, the difference is not big, but it's not worth it yet.
00:19:10.718 - 00:20:11.326, Speaker B: But as soon as you cross the 256 mark, the verification time stays much, much lower. And you have a difference at the end with 8000, when you aggregate 8000 snarks of a difference from 33 milliseconds versus 435 milliseconds, which is a roughly difference of 13 x, which is very considerable. If you look at the size I don't have a graph here, but if you look at the size differences, you have a similar cutover point at aggregation of 256, and the cut over there grows very quickly. And if you aggregate at the 8000 level, you're looking at 40,000 bytes versus 1.5 million bytes. And so the difference is about 37 x. So really considerable size improvements here.
00:20:11.326 - 00:21:38.646, Speaker B: If you can use this, this is unfortunately not yet deployed. There is a FIP Open FIP 13 if you want to look at that for more details on how this exactly is planned to land on Falcon main net, but it should enable a lot of improvements on onboarding storage, which is where it's used for the moment on improving poor apps. Yeah. All right, so if we look at this a little bit back, did we get what we needed? So we've launched Mainnet back in October, and it's been running pretty stable so far, so that's great. And people have been making some snarks. So let's take a look at how many snarks and what are we looking at? So currently the main net roughly produces 30,000 winning posts per day, which is the smallest one. We have 36,000 window posts.
00:21:38.646 - 00:22:34.510, Speaker B: So Window post has the same number of similar number of constraints as a single GK snack and poorer. So this is roughly 130,000,000. So just scratching at that top level of how many constraints we can put in there, and this was specifically actually designed such that we're trying to get as much information into a single ZK Snark there as we can get into on chain, of course, and then roughly 5.5 million porks per day. So this is 550,000 PO day that we're currently averaging on Falcon main net. And so this is 5.5 million Snarks there, and each of them again, roughly 130,000 constraints.
00:22:34.510 - 00:23:17.754, Speaker B: So that's a lot of constraints. So I have a fun number here that we calculated. Where is my constraints per second? Sorry. All right. Yes. So for March 21, we average roughly 4700 Snarks per second. And so this is the main net, roughly produces one snark every 0.2
00:23:17.754 - 00:24:01.320, Speaker B: milliseconds. If anybody was curious, we think this is a big number of snarks. We think this is a lot of constraints. But one thing that I would love to ask folks is please do reach out to us, to me afterwards or in the comments. I really want to know who's deploying Snarks and how many Snarks are there deployed and I think it's really cool to see what we can push these systems to today. So I would love to hear some numbers from folks. So far I have not heard anybody claim that they make this much mini Snarks, but please prove me wrong.
00:24:01.320 - 00:24:57.500, Speaker B: Yeah, this is covered over roughly 2000 miners and this proves currently 4.7 exabytes of storage. And so that's a lot of sectors, right? So most of these are 32 gigabyte sectors. So that is a lot of storage that we have there, which is a great proof of that. This Snark scaling actually worked and produced what we wanted because it actually runs now. There was always, as always with these systems if you think about them in theory and then actually deploy them, there's still always the fear that might not quite work as you plan to work. All right, so yeah, that's all.
00:24:57.500 - 00:25:24.100, Speaker B: If you have things, please get in touch, help us create great things and please send me all your Snarks of all your systems. I'm really curious what other systems are currently doing, what other tricks are people using to scale their Snarks and make them faster? Yeah, and if we have time for questions, I'm happy to answer questions.
00:25:26.790 - 00:25:59.680, Speaker A: Absolutely. So thank you so much for that awesome talk. I learned a lot of new things personally and kind of looking at all the other chats we have going on where we're streaming this. A lot of excitement about Snarkpack. So I think one question that's more of a follow up from something you said would be great if you're able to share a link to the workshop on Snarkpack so others can catch up and learn more about it. The only other formal question that we have is just generally speaking, what is the impact of Snark verification during the sync on your end?
00:26:00.450 - 00:26:57.170, Speaker B: I don't have exact numbers unfortunately because this varies. I need to reach out to the folks that make these stats. I do know from a very active feedback from those folks it is usually always not as fast as they would like it to be. So it has high impact and especially it has impact when there is an issue with it. If somebody is actually running into issues then you get really quickly problems with verification. In terms of related to block times, not the biggest issue is more winning post for the miners because the actual generating that on time is even more critical and it's just more resource intensive and so that is more critical in terms of block times currently on the network. But verification always needs to be faster.
00:26:57.170 - 00:27:06.340, Speaker B: But quote unquote lucky for us there are also other parts of the system that need time. So we're not the only one slowing things out.
00:27:06.950 - 00:27:40.720, Speaker A: Awesome. Well, I think we are slightly over time. So I'm going to have to wrap this up. But I am personally super excited to learn more about Snarkpack, and I think to follow up on the ask you made, which is getting you in touch with all the Snark apps, we'll be happy to follow this up on the hackathon site. There's a lot of people working on this stuff, and I also think you may be interested in the next talk, which ends up covering how to speed up a lot of those things by using noir and plookup or plokup. So thanks again, Dignified Choir, and really appreciate you giving us the talk today.
00:27:41.730 - 00:27:44.300, Speaker B: Thank you so much. Thank you for listening. Awesome.
