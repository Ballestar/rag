00:00:00.090 - 00:00:18.798, Speaker A: On stage. Ilya from Neoprotocol, he has just finished the Neoprotocol guys have just finished their hack, the Rainbow hackathon that was a huge success. But Ilya is going to chat to us about Sharding for scalability and development experience. Ilya, can you hear us?
00:00:18.964 - 00:00:56.446, Speaker B: Yeah. Thanks, Simona, for introduction. Well, it's great to be here and welcome everyone. Hope your hacking has been starting well, let me kind of talk through some of the Sharding and I'm sure with all the news around these two, there's a lot of questions. So I'll try to kind of COVID some of the topics and surface some of the interesting questions. But if you have any other questions that I didn't cover, please ping them in the chat and we'll respond at the end. So first I wanted to just mention near itself.
00:00:56.446 - 00:02:01.250, Speaker B: So near is a proof of stake layer, one that's specifically built with experience in mind, which covers user experience and developer experience. And scaling, if you think of it, is part of this kind of experience because as your users, as your user base grows, as your application gets more usage, you should not be dealing with scaling your application and also your users should not be dealing with high fees and figuring out what to do with all that. This stuff should be completely transparent and just work. And so kind of the general idea is to build the experience that matches kind of the web two, right, the existing internet, and in such a way that you can bring in non crypto people and they can just use applications. And it's been pretty important to obviously interoperate with Ethereum because this is where kind of most of the existing developers and users are. But we also want to onboard lots of developers who are coming new. So let's talk about scalability.
00:02:01.250 - 00:03:12.326, Speaker B: So let's start with how you cannot scale, right? And so there's been a lot of kind of new consensus algorithms where people would be providing an idea how to scale the blockchains with some amazing consensus algorithm and being able to do stuff that is faster. There's been a lot of also like changing from blocks to trees to dags to some other data structures which are presumably allowed to do something faster or switching to different proof of, right? And to be clear, obviously proof of work has inherent limitation. And that limitation actually is in the fact that you first need to organize a block and then you need to mine it and that mining time is actually adds on top of your kind of execution time. And even then actually that is possible to pipeline. So in theory it's possible to optimize that and actually have throughput faster with proof of work. But it's still not the bottleneck. The actual bottleneck is not data structures, it's not consensus, it's not a proof of, it's actually just executing.
00:03:12.326 - 00:03:56.950, Speaker B: And the reality is we want to run a smart contract platform, right? We want to run generic code and this code needs time to execute. And so this is actually just syncing the ethereum blockchain with fully downloaded snapshot of all the blocks. And so there's no network, there's no consensus, there is no data structure. So just purely EVM execution. And as you see, there is a limit how much gas you can push through in a second. And so EVM itself on somewhat default hardware, right, maxes out at 200, 300 transactions per second of normal smart contracts. This is like real smart contracts on main net.
00:03:56.950 - 00:04:42.002, Speaker B: And this is a bottleneck, right? It doesn't matter if you switch to proof of something like proof of authority, it will be the same way. It doesn't matter if you have the magical pipelining and stuff like this. So the only other option, like the way to scale is either significantly optimize the transaction processing, so this way includes changing from EVM to another kind of execution environment. So WebAssembly is faster, but it has its own challenges. There is kind of Berkeley network, the Berkeley system that Solana, for example, uses, which is faster. You can put execution on, for example, GPUs and stuff like this. The problem is all of that really works if you have a homogeneous load.
00:04:42.002 - 00:05:18.686, Speaker B: Because if you only need to do payments, for example, you can scale pretty dramatically because you can parallelize that. You can have lots of processing power on one machine going really quickly and doing things in parallel. But at the end, actually, one of the core limitations is not an interpreter. It's actually hard drives, right? Just being able to randomly access hard drives is hard. And again, you can put more hard drives, but there's limit. And so the other option is just split the work, right? I mean that's an option that the whole internet is using, right? Google is not running on one machine. Facebook is not running on one machine.
00:05:18.686 - 00:06:02.830, Speaker B: There's lots of machines running in parallel, processing things in parallel. And so this is what's called Sharding, right? I mean, this is not a new concept, right? It's been around for probably two decades, if not more. And the idea we need to scale up, we cannot have network pretty much everybody in the network doing exactly the same work. They need to paralyze. The problem is this is very hard. And this is hard because there's a lot of kind of technical challenges on how to make sure that everybody's doing the right thing, how to make sure that the chain is correct when you don't actually run all of those things. So I'll just cover kind of like a little bit of a difference, but between the kind of default Sharding approach and also what we've been working on.
00:06:02.830 - 00:07:41.434, Speaker B: But the general idea, right, is you parallelize the work, which means you kind of need to do a bunch of stuff in parallel, which the kind of naive approach or like the approach everybody starts to take, including us. Is to parallelize the chains itself, run multiple chains in parallel and then in some way to kind of link them together and use this as a way to communicate between them. And so this is approach that originally like East Two took it's approach originally we took it's approach that polkadot implemented it's approach that kind of if you think of Cosmos as a sharded chain, which it's not and it's a little bit different, but in general the concept is the same. This is by scaling by a lot of chains. But one of the core issues with this, and we'll cover a lot of other interesting issues is just like it's inherently complex and has a lot of kind of repercussions. And so the approach we're taking is really paralyzing the blocks and I also draw some parallels with other approaches as well. So, what are the kind of technical challenges with sharding? Well, one of the big challenges is if you think of any proof of stake system, usually if you have a one third malicious and network split, you can actually take over the network, right? So either stall it if you have one third plus one or of stake, or if you have network control, you can actually do some fork of the network and one sort of stake usually you can incentivize growing the stake.
00:07:41.434 - 00:08:59.494, Speaker B: It's still sadly in current pro stake systems we still have a pretty big concentration, but hopefully over time this will grow. And you also need to kind of track like you're pretty much saying that because we assume that there is less than one third of malicious actors bin on the network, then pretty much by induction, right, the whole chain is valid because every other block, every next set of validators is selected by the validators that were correct. And you need to actually have some intersection between when the validators are rotating because they need to actually download the state. So that's how this works. Sorry. The problem is that if you think when we're doing sharding right, what happens is that in each shard you need data subject of flight theory, malicious. And so this leads to a problem that if you sample kind of for a long term we just sampled it and just run with that this validators can get corrupted, right? Let's say you have ten shards, 100 shards.
00:08:59.494 - 00:09:50.774, Speaker B: Sampling into that will pretty much lead to probability. If one third or even 20% is malicious, sampling into one of the shards can be higher. But also what's more important is that people can get adaptively corrupted right over time. So you need to actually rotate validators across the stack and make sure that they pretty much cannot know which shards they validating too much in advance. So that's where this thing come in is you pretty much rotate validators. But you need to give them some heads up to download the state of the next shard. Because compared to a normal single chain, right, where all the nodes have the full state of the whole network, now you only have a partial state.
00:09:50.774 - 00:10:38.742, Speaker B: And if you are responsible for validating next shard, you need to go and download the state of that shard before you are able to validate. Otherwise you'll be lagging and I'll mention data availability in a second. So the other problem is what happens if somebody actually did attack, right? Let's say there is more than one third, like two third of malicious validators in one of the shards because in general the numbers are smaller of stake. And the problem is that the other shard cannot detect this very easily, right? It's not validating that shard. It's not actually validating state transition. It only kind of keeps track of that shard. The kind of the attack would be to send, let's say invalid state.
00:10:38.742 - 00:12:10.514, Speaker B: Like for example, say, well, all the money are mine on this shard and then send them to another shard to exchange and exit, right? And so maybe network everybody will detect this, but it will be too late and people already stole the money. So the general approach to this is called fisherman and this is where somebody can pretty much tell the other shard that hey, something is wrong in this shard and they provide a fraud proof, right? And so this is similar to what we've just heard in kind of plasma's approach as well, where if something is wrong on a plasma, right, then somebody can provide a fraud proof to the ethereum and exit pretty much from this. The problem here is, and similar to plasma is data availability, right? So somebody needs to actually have a fraud proof generated in the first place to be able to prove that something went wrong, right? And so if this malicious actors did not produce a block, right, if they did not post a block to the network, then this, let's say, honest actor in that shard has no way to really create a fraud proof and prove it to the other chain. The other side is obviously finality, right? So if you send a cross shard transaction and there's a fisherman, right, the fisherman needs some time. So in case of plasma, it's seven days. Big part of it obviously is because of gas issues and transaction sensoring and bunch of other stuff. But this change period needs to exist.
00:12:10.514 - 00:13:09.170, Speaker B: I mean it can be shorter, but this is the finality of this cross shot transaction. We cannot execute it until we make sure that there's no fraud proof coming in and there's no way to not prove there's not going to be a fraud proof, right? So there's no way to speed it up. And so this creates a lot of issues on just kind of creating a fast cross shard transactions between chains. The other thing is if you for example do speed up, then you need to actually roll back a bunch of stuff, right? You need to fork off. Like if you allow transaction to go in and you just kind of revert it, you need to revert kind of cascading effect of all the shards that get affected. And this also can be very complicated. And then the data availability part is what I mentioned, right? If validators did not post invalid block, right, there's no way to prove that there is an invalid state transition.
00:13:09.170 - 00:14:11.882, Speaker B: And next, validators cannot do anything about it. So important is obviously to have a mechanism to provide data availability to the network. And so one of the main solutions that pretty much everybody uses so it's us, Polkadot and other folks is to pretty much use erasure codes. And so this is idea that when somebody produces a block, or in our case part of the block, they need to send out parts, they need to send out erasure coded parts of this block to other participants. And only if those other participants saw this parts, they will include this block as a valid. Like they don't need to validate it, they don't need to make sure the state transition is correct, they just need to know that there's data availability. So what you do, you weave in into consensus, right? You actually include this into consensus that you can only accept those things that you saw parts of.
00:14:11.882 - 00:15:48.922, Speaker B: And if you have enough of these parts, so enough people are tested to having these parts, then it means that actually from these people you will be able to reconstruct this block that was transmitted, right? So even the original block producers, or in our case chunk producers disappear. You'll be able to reconstruct just from these parts the full information. And so just for context, razor coded means that it's a special encoding that even if you only have like in this case you have six parts, right, to split the data into six parts and any two parts is enough to reconstruct the original data. So it's kind of a three x duplication of information that's like any of the two parts is sufficient. And so this mechanism is kind of at the core of our approach. And Polkadot as well uses this to pretty much make sure the data is available across. So I mentioned before, so we transitioned from this idea of having multiple chains running in parallel to having actually one chain and sharding blocks, right? This actually allowed us to do a lot of things, including because we have now one blockchain, this allows to really simplify the whole challenging things, right? Because if there is an issues that have happened at some block, we can just roll back the whole chain that contains all the malicious tradition, right? So we don't need to find a bunch of chains and figure out where they went wrong and roll them back.
00:15:48.922 - 00:17:03.346, Speaker B: You just have this kind of one finality for all of them. And so this provides kind of a better experience and it's also just easier to implement. That being one of the core things because obviously this stuff is very complicated and it's very hard to test. So the other thing is obviously it provides a better cross track transaction functionality and so we can move into this. I think a few things important to mention here, which if you think of it, what roll ups are and what plasmas are in a way is kind of a specific versions of this as well, right? It's all a way to run some kind of computation outside and then settle into the main chain where main chain is providing data availability and it's providing the kind of finality security, the challenging system, right? So in this case, in our case, we're using kind of our full chain to do this. But you can imagine this full chain would be Ethereum and then all the shards are just running as a parallel chains and that's kind of what plasmas and optimistic roll ups are. And then they have all the same problems that I mentioned.
00:17:03.346 - 00:18:40.286, Speaker B: They have a seven day challenging periods to do this. If it's optimistic roll ups for ZK roll ups, right, they can improve the full state transition so they don't have this problem. And this is kind of how it all links, right? In reality, all of those methods really just like different ways of handling these problems that I mentioned. Now, the important piece is how to communicate between, right? So if you have a MultiChain environment or multi shard environment, or multi roll up environment, it actually becomes really complicated how to connect them together, right? And specifically there's a lot of kind of issues around rollbacks, right? So if the other chain rolled back, what do you do? Right? If Ethereum rolled back, if you are optimistic roll up, you actually need to roll back as well, right? Because you're tracking Ethereum. If you're a separate chain and there's a rollback, what do you do there's gas costs issues, right? It's because if you're trying to send a message from one chain to another chain, if the gas is different there, it's in a different token, right? Like, who's going to pay for it? How are you going to calculate when you're sending in transactions that you prepaid the right amounts and stuff like this? I need to attach another token or something else. The critical state recovery is around roll ups is around rollbacks, right? If something happened, if a roll up producer stopped, what do you do? It's a mass exit. You roll back some stuff, you prove things, you make sure that you have all the data availability.
00:18:40.286 - 00:20:08.234, Speaker B: But what happens with all the transactions that are in flight? What happens with all the cross shard? You were calling from here to here, what happened with them? And it's important to figure out what's the order of execution that will happen on the other side if that happens, right? If like optimistic roll up producer stopped everybody's mass exiting but you actually had a proper exits as well. And they'll not get executed at the proper time because they all like waiting for seven day. It's like what's going to be happening in all of those cases. And then the important part is composability, right? If you have like two roll ups or two shards or two chains, right? If the communication between them is complicated, it has gas, it has all those seven day waiting periods or even like a minute waiting periods, like composing contracts, composing applications become super complicated and especially if you want to also receive a callback and hear back about what happened on Zelda Sharp. So with near approach we have pretty much been focusing on fixing all those issues and making the experience really smooth on how this works. And think of it because we have one blockchain from a developer perspective, you actually don't need to think about kind of shards and sharding in general. Like if you go to our Explorer right now, you actually will not see we are running one shard, we have another test network with four shards, but you will not see any difference because the sharding is completely hidden from the developer.
00:20:08.234 - 00:21:16.606, Speaker B: And the kind of main thing that we propagating is that all the contracts working asynchronously so the communication between them will not happen within one atomic transaction, it will happen within next block, right? So you're pretty much creating a message and you're sending it to another contract and another contract will execute it in the next block. And this is similar to message passing in Erlang or any other similar language. And first of all, it removes any kind of reentercy problems. Second, if there's any rollbacks, this can be handled cleanly because all those receipts are kind of staying in the block and then you roll back, you restart it and you have all those receipts just coming in and you need to process them. They are mandatory to process before any other transactions coming in that are like all the receipts. Like these messages passed from one contract to another kind of coming in in the beginning of the next block. It solves gas costs because we kind of pretty much like one of the core ideas is dynamically balancing contracts between shards.
00:21:16.606 - 00:22:05.630, Speaker B: And so this would allow to kind of even out the gas costs over time. So it will not be like immediate. Within a few hours you may have spikes, but over long term if there is some contracts that are more used than others, it will pretty much even them out and split them between different shards evenly maintaining kind of the balance. But importantly, it kind of has this compatibility property, right? Because now you can realize that the contract will be delivered, right? And you'll get a callback. The only thing that you need to handle is the fact that it's not atomic, right? So you do need to handle kind of state reversals somewhat manually. There's like few tools to do that on developer side. But you do need to kind of handle a failure of the execution of sales contract.
00:22:05.630 - 00:22:48.526, Speaker B: The benefit is now you can handle the failure of execution of sales contracts, which you actually couldn't on Ethereum. So you can actually call contracts, see if it fails or not, and execute some logic based on that. So think of it try catch. So this callback kind of allows you to attach obviously near as a resource, it allows you to attach information and receive it. So, here's an example how this works. So this is in Rust for WebAssembly contracts, you can call is whitelisted based on some interface you provide. It the kind of contract that you want to call some arguments, no deposit in this case and somewhat of gas.
00:22:48.526 - 00:23:43.086, Speaker B: And then you have way to attach a callback to this execution on one of the methods on the contract itself. So you pretty much will call it, it will execute something and then return and actually that contract can call something else and then process something, get callback, get callback, right? So you can actually have a pretty long chain of calls that processing. Now, this is very useful. You can actually have a very interesting multi shard contracts now built across this. You can also shard your existing contract. So for the users, like for example, for a token, we actually have some proposals for sharded tokens because tokens actually don't need to have all the accounts in one shard, right? You can actually shard them as well. And there's like few other applications that kind of need the scale of multiple shards, so going beyond even one shard.
00:23:43.086 - 00:24:32.690, Speaker B: But there's also a lot of the DeFi specifically has some issues with Async calls. And this will be very clear because one of the kind of benefits of DeFi and I usually call DeFi as just a programmable exchange, right? So if you think of it, you deposit money into exchange and it gives you a bunch of tools, I'm talking about like Nasdaq or one of those exchanges. They have a bunch of tools already pre built on that exchange. You can trade, you can do stuff. And within that exchange, everything is atomic, everything very fast, nobody can front run you, hopefully. And then you can go to another. And so what ethereum with DeFi provides in a way is ability to deploy new contracts, new code into this one kind of one exchange, one place.
00:24:32.690 - 00:25:30.082, Speaker B: And so this is provided by ability to execute kind of atomic transactions, atomic across multiple calls. So it does become complicated to do that for Async calls. And one of the main issues is that because if you think of it, if you have two contracts that have Async and you have some kind of logic flying between them, people can try to front run you to go after this contract earlier on, right? And so there's ways to mitigate that in this kind of multi async environment. But obviously as the system grows, as you have more and more pieces working together, this becomes more and more complicated. So there is a value in sync calls. And so we are actually just launching the EVM, what we call the vaults. So in a way it's like one shard of a mean, it can obviously scale but goes up to one shard.
00:25:30.082 - 00:26:06.734, Speaker B: And within that EVM we have synchronous calls, right? So within that EVM it behaves like Ethereum. So you can think of it as just launching ethereum shards inside nier. Actually originally it was done as just a smart contract. So we literally compiled EVM code and just launched it as a smart contract on near and had like original initial testing. But now we pre compiled it and kind of speed up. So that gives us whatever the raw performance you can get from the EVM, you get that. The interesting thing is you can have many of them, so you can have multiple EVMs in parallel running and have async calls between them.
00:26:06.734 - 00:27:03.390, Speaker B: So you can think of it like can have multiple exchanges or some other contracts maybe that are not exchanges, that dealing with in game items or whatever that don't care about specific DeFi stuff. And so you can still async communicate with them within one block. So you have all the same functionality and you can communicate with also other contracts. And in a way this actually works as like you can call it realistic roll ups, right? Because what optimistic roll ups is you execute something off chain and then you post it on chain as a data and then you don't execute it, but then you wait for anybody pretty much optimistically there, but then somebody wants to challenge. You need to wait for a period here. You actually execute this state transition of EVM right away and it's valid, right? So I call it realistic roll ups. I mean, even better was if there's EVM that's running off chain and that just post all the data and gets valid on chain.
00:27:03.390 - 00:27:34.458, Speaker B: But because we have 1 second blocks, you don't even need to do that. It pretty much gets the same performance. So each EVM vault will be as fast as optimism or any other kind of EVM engine running as a separate thing. But it will work within this whole environment and this kind of going forward. There's some ideas how to scale them in this app, but this is kind of future work. So before kind of ending, I have one last thing I want to mention. So Simone mentioned about our rainbow hackathon.
00:27:34.458 - 00:28:35.438, Speaker B: And so this idea is in a way from perspective of Ethereum, near is kind of a multi charted environment in which you can run contracts. And with EVMs it's like many EVMs and there's no kind of like with this bridge. We're actually connecting these two environments and from Ethereum side, EVM is from Ethereum side. Near is a side chain. Like many side chains connected together with EVM and with WebAssembly from near side, Ethereum is just another shard, right? Because you can actually call into it and get responses as callbacks and kind of have all the same functionality just with a little bit of lag on finality and all the timing. And so this is pretty much facilitated by like client smart contracts in both sides that are providing you pretty much this trustless, decentralized way of connecting to chains. And it's done in very generic way where you have multiple levels of smart contracts that pretty much provide connectivity.
00:28:35.438 - 00:29:18.622, Speaker B: And as a developer, you can link in any way and prove any fact about Zaza chain. Just to finish, like one of the main things we've been doing is really focusing on education in the space. So we actually have two series, one on YouTube and one as a podcast. One on YouTube is called Whiteboard Series and it's about kind of talking about protocols and talking about kind of how they work on the whiteboard in depth. It has Ethereum two Cosmos polka dot optimism matic. I think there is like over 40 protocols now. And then the Open Web Collective is building.
00:29:18.622 - 00:29:58.854, Speaker B: The Open Web Podcast is the podcast about entrepreneurs and builders and investors actually coming together and sharing information, sharing kind of insights on how to build businesses and how to build in this open web. So keep building. Join us at Near Chat. I mean, if you are interested in kind of expanding your business, we have a protocol agnostic Open Web Collective, which is really an incubator and accelerator for companies in open Web. And if you just want to learn more about near, there's open Chat and I'm open for questions. Awesome.
00:29:58.972 - 00:30:04.300, Speaker A: Thank you so much. Elliot. I think my favorite piece was realistic roll ups versus optimistic. That.
