00:01:39.740 - 00:02:42.228, Speaker A: All right, welcome, everybody. My name is Kartik. I'm one of the co founders of 8th Global and want to welcome everybody to day five of Hackath judging. So Hackathfest is the hackathon that we're partnering up with Protocol Labs on as 8th Global, and we're super excited to kick off a whole week of judging and showcasing all of our amazing projects from this event. So for those of you who are going to be watching this later on as a video, over the past month, we've had 470 hackers from 50 different countries working across 19 different time zones, and they've been working on playing with what's possible with the world of Ethereum and Protocol Labs, Filecoin and IPFS. And as of last Thursday, we've had 132 projects that have come out of this event, and we're using this entire week to showcase every project in having them be known to the rest of the world. So before we move on to the demos for today, I want to quickly go over the logistics of how the event itself was set up and how today is going to work.
00:02:42.228 - 00:03:34.500, Speaker A: We're going to have about seven teams today. They'll be presenting to our judges, and each team will have four minutes to demo, and we'll have a four minute Q A session. And to minimize any technical difficulties, we've asked all teams to pre record their demos, and they'll be playing them as they come. And kind of briefly, everything that we're going to see today was either a set of individual or a few members, up to five members that worked on a project. And everything you're going to see today was worked on over the course of the last four weeks. And the only criteria we had for all these projects was that everything we're going to see today was not only just built over the course of the hackathon, but also must incorporate the tools and technologies from your Protocol Labs and the Ethereum ecosystem. So we're super excited to see a lot of interesting mashups that bring the best of decentralized storage and smart contracts.
00:03:34.500 - 00:04:17.100, Speaker A: And as for how judging itself is going to work, our judges are going to be rating our projects on how technical, original, practical, and usable they are. And we understand and realize that those four categories are not enough for everything. So we also have a general catch all that we like to call the wow factor to incorporate anything that we may have missed on our criteria. So before I really go into the demos, I want to make sure I emphasize that this is not a competition. This event is here for all of our hackers to learn and see what's actually possible. They're very much here to share their excitement. And the judges are here primarily to give feedback and offer their suggestions and comments on how you can take this thing further and make this even better than what you originally intended.
00:04:17.100 - 00:05:06.610, Speaker A: And not everybody's trying to become a business. So while we'll see certain things that are ready to be commercialized after this event, the goal for Whole Hack FS is to very much promote experimentation. And this is kind of the theme and our idea for this event. So with that, I want to really kick off our judging day five, we're going to see these seven teams present today. And doing the hard job are three judges. So I want to welcome Andrew Hill from Textile Prejudice from Protocol Labs and Kyle Tutt from Pinata, who will be here with us for the next hour, talking to the teams and giving their comments on all these projects. So, without further ado, I'd like to call up our first demo for the day, and that is Team Climate Data Pool and welcome them to share their demo and take it from here.
00:05:06.610 - 00:06:12.250, Speaker A: As I note, Kit will be talking over the video. So whenever he's ready, we'll kick this off.
00:06:12.700 - 00:06:13.208, Speaker B: Hi.
00:06:13.294 - 00:06:13.930, Speaker C: Welcome.
00:06:14.540 - 00:06:52.500, Speaker B: Today we're doing a demo of climate data pool. This hecfs project is a proof of concept for a decentralized global data storage. Our use case stems from the Paris Climate Accord. In that agreement, all the nations of the world united to tackle climate change. Climate Data Pool intends to unite the reporting of the nations on a blockchain. Each signatory country is responsible for submitting reports on a scheduled basis. In our contact with the United Nations, we learned that having one single canonical file as the data source would be a real advantage.
00:06:52.500 - 00:07:22.696, Speaker B: Well, that's easy with IPFS. We wrote two user stories for the hackathon. One is Sam, the civil servant based in Singapore, who's responsible for uploading reports. The other story involves Riley, a climate researcher who could be based anywhere. Let's start with Sam. When Sam logs in, he lands in the overview screen of his country's document storage. You can see from the URL that we're using Fleek hosting.
00:07:22.696 - 00:07:53.770, Speaker B: So all of this is on IPFS. We're using fleek extensively. Maybe you recognize its user interface. We're imagining that Fleek offers a white label service which can be branded. So we forked the UI into a document management app in storage jargon. Each country and Climate Data Pool has its own bucket with its own environment. On the left is a country selector and a menu with overview files listing upload and team.
00:07:53.770 - 00:08:44.600, Speaker B: The UI presents a number of tasks that Sam could engage and shows a short list of recently uploaded documents. Sam has to upload a report, so he goes to the Upload screen. He selects the file. This is a document entitled Energy Usage analysis of Decentralized Data Storage. He gets the title on his clipboard, paste it in. Then he selects the document type and he clicks upload voila. The feedback tells Sam his upload succeeded.
00:08:44.600 - 00:09:19.490, Speaker B: Now let's look at what's going on underwater. We wrote scripts which use a Fleek SDK. I'm going to run one called Get Lists JS. This will show us a list of files in Singapore's bucket. And there's the new file. A listing like this would be used to populate the front end interface. Now we're going to switch roles to Riley, the researcher.
00:09:19.490 - 00:10:06.990, Speaker B: When searching, she'll see a screen similar to the overview that Sam sees, but with limited menu items on the left and more metadata about the documents. Instead of managed actions, the screen will have download buttons. Raleigh searches again, and there's the new file. Raleigh clicks the download button and the document loads. At this point, she's done and we're done with our demo. There's a team page with information about us. Thanks to ETA's Global and the sponsors for running a superbly organized and inspiring hackathon.
00:10:10.390 - 00:10:17.780, Speaker A: Awesome. Well, thank you so much for that demo. And with that, I'd like to kick off our Q and A with our judges. So, any comments and feedback? Welcome.
00:10:19.930 - 00:10:20.680, Speaker B: Thanks.
00:10:24.490 - 00:10:25.640, Speaker C: I have a question.
00:10:26.650 - 00:10:27.510, Speaker B: Shoot.
00:10:29.930 - 00:10:36.300, Speaker C: Were you using the directed graph to do any interdocument linking, and can you explain that if so?
00:10:38.030 - 00:10:49.440, Speaker B: No, we're not using a Dag. That's something we considered. But the document linking at this point is only envisioned. We haven't actually implemented anything.
00:10:55.730 - 00:11:06.020, Speaker C: You said you had done some conversations with the UN or the Paris Climate Accord. Could you tell me a little about that research?
00:11:06.630 - 00:11:41.230, Speaker B: Sure. We participated in It challenges that the UN runs on a regular basis. And in one of them, in 2017, we proposed storing the climate data documents from the Paris Accord on a blockchain, and we used IPFS. Those were early days, so we were doing command line stuff, and we've since then taken that concept further and we've used it for this hackathon to take advantage of the wonderful developing ecosystem.
00:11:42.930 - 00:12:08.280, Speaker C: So during the challenge, we also got to meet the judges, and they are the ones who told us about the requirement and things about the canonical files and all that. So Kit actually went to New York and met the judges, and they actually spoke about their requirements and stuff.
00:12:12.990 - 00:12:19.260, Speaker D: I have a question as well, which is, if you had had more time to work on this project, what would you have implemented next?
00:12:21.230 - 00:12:45.970, Speaker B: Well, we originally wanted to implement the underwater stuff so that it was all real. The scripts that we wrote, they actually work, but they're not hooked up to the web user interface. And that was the complicated aspect that we had to triage out of the proof of concept.
00:12:52.250 - 00:13:00.200, Speaker A: Great. I think one of our judges just had some AV issues, so we'll wait a few seconds for him to come back on. Kyle, are you back?
00:13:03.860 - 00:13:10.580, Speaker C: I'll just fill really quick that I think this is an awesome area to be working, so I'm really excited to see your submission.
00:13:11.720 - 00:13:12.470, Speaker B: Thanks.
00:13:17.580 - 00:13:24.650, Speaker C: This is a great cause to use IPFS, so it's great to see.
00:13:25.980 - 00:13:34.190, Speaker B: Yeah, IPFS is really perfect when you want to have a canonical file, which is really important for the researchers. It's just built in.
00:13:35.920 - 00:14:04.870, Speaker D: I also really like that you took the approach of kind of fleshing out the UI because it does help kind of share the vision for what this sort of thing could be. And I can totally imagine showing this to someone who works in that realm or at the UN and having someone really buy into how they could use this in their workflow because you have a really cool demo of it. So I think it's a great first step and excited to see where you take it next.
00:14:05.660 - 00:14:11.930, Speaker B: Thanks. Yeah, we enjoyed it. Abby, maybe you want to talk about our next step.
00:14:12.700 - 00:15:00.810, Speaker C: Yeah, basically one other next step. After the UI and backend linking, we also wanted to build a small middleware component which will serve both the Web Two and Web Three interfaces. So if you have a front end that supports only Web Two, we'll still support it. But we want to move to a Web Three thing. So we want to kind of enable people to move from Web Two to Web Three through our system. So we are investing middleware component that will probably do that.
00:15:03.800 - 00:15:24.030, Speaker A: Super cool. Well, congrats on demoing and building this amazing project. We hope you continue working on it, and I want to thank you for coming on first. And with that, I'd like to move on to our second demo for the day, and that is Team Incentivize. So, Adam Matimat, feel free to kick off with your demo.
00:15:25.120 - 00:15:32.430, Speaker C: You. Bye. Hi, everybody. Guess we'll just start the video.
00:15:38.400 - 00:16:25.790, Speaker E: Hi, we're Team Azentifies, and we're doing Incentivized machine learning, where you can provide rewards for models trained on your data and earn rewards for training models. A lot of small companies have very valuable user data, but they can't extract any value from it because ML engineers are very hard to hire. There is a need for a data marketplace where someone with limited expertise can publish a data set and a problem definition. Then others can come and attempt to create models that solve those problems and be paid based on their model performance. The basic way that our system works is that a user who holds the data but doesn't have ML experience can submit a proposal for model training. That proposal comes with some reward bounty. Other users then submit their trained models against that proposal's training set.
00:16:25.790 - 00:16:50.532, Speaker E: The accuracy is self reported for each model, and a reward is proportionally dispensed at the end of the proposal according to each model's self reported accuracy. If other users believe that accuracy is incorrect on a model, they can dispute it, which will trigger a verifiable resolution process that reaches out to a compute oracle. Now, let's hop over to a demo.
00:16:50.586 - 00:17:08.604, Speaker C: To show you how it works. First, let's say I'm a project manager and I want to submit proposal. We want to figure out the lifetime value of our users. We have the data. And we want people to write models that will figure that out. So this is our regression type. They could be worth zero or $100 over their lifetime or more.
00:17:08.604 - 00:17:26.450, Speaker C: And this is important. So tight timeline. Let's say we give people a month to submit solutions. Also a high bounty because this is an important problem to our company. Now let's upload the training and validation data. So feature data. And this is going to IPFS, obviously.
00:17:26.450 - 00:17:48.004, Speaker C: And you could upload this through our CLI script as well, as well as the evaluation script that will load the actual model that you submit and run it. So this is going to ask me for my stake. Just ten ether. Cool. Now let's switch over to a different user who wants to submit a solution. Cool. And we see this proposal.
00:17:48.004 - 00:17:53.384, Speaker C: It's worth ten. Awesome. So we're going to upload an onyx.
00:17:53.432 - 00:17:54.350, Speaker A: Model here.
00:18:00.720 - 00:18:20.288, Speaker C: As well. You can upload a preprocessor script. This will allow the oracle to preprocess the data. And you want to guarantee verifiability as well as an accuracy score. And it's self reported, and people can dispute that, let's say 90. So let's submit that it's going to ask for gas confirm. Cool.
00:18:20.288 - 00:18:45.652, Speaker C: So now let's go select another user, a third user. And we can see that this user is going to get the entirety of the reward because nobody else has submitted a solution. However, I think that this reward, maybe this person was lying. I ran their model and it didn't run. So I'm going to actually dispute that. What that's going to do is if I win the dispute, I get all of the money from here, here. Confirm that, and I won the dispute.
00:18:45.652 - 00:18:50.190, Speaker C: So I will receive the funds from this. This person was lying. So there you go. Thanks for watching.
00:18:50.640 - 00:19:38.220, Speaker E: The basic architecture of our system is entirely based around a front end and a smart contract. The front end stores a lot of its data in the smart contract and also uses IPFS to store larger data set information such as training and test data and the proposal metadata. There is a CLI script we provide which users can use to upload their training and test data using Powergate. This ensures that the data is available and persisted globally. We use Fleek to host the front end. We believe that ML engineers should have the ability to upload preprocessor scripts which can output transformed data, improving the training accuracy of models. Any model that is trained using an uploaded preprocessor script will share its reward with the original author of the preprocessor.
00:19:38.220 - 00:19:54.640, Speaker E: We also want to finish implementing the use of a computational oracle in order to verifiably resolve disputes against the self reported accuracy of the trained models. Thank you for watching our presentation.
00:19:58.790 - 00:20:02.340, Speaker A: Awesome. I'll let the judges ask questions.
00:20:10.020 - 00:20:49.836, Speaker C: Okay, I'll jump in again. So I saw a few really cool kind of moving pieces. A little hard to tell which pieces were done. I think maybe one cool way to wrap that up. What do you think the hardest piece that you built was? What do you think the hard part that we should focus on is? Oh, you mean in terms of developer experience? Yeah. Where did you find difficult pieces to put together that you feel like you had some achievement there to share with us? Well, I think that all of us were coming into this without any experience at all in doing any kind of Web Three development. So all of this was new to us.
00:20:49.836 - 00:21:49.680, Speaker C: But the biggest thing, I guess the biggest kind of departure from what we've done before was writing smart contracts just because of sort of the way you have to shift your mental model of how things work. And I'm primarily like a JavaScript developer, so it was a pretty steep learning curve in trying to figure out how Solidity works and all the sort of drawbacks it has and the limitations it has still because it's kind of like, I guess a little bit immature, sort of. So there was a little bit of working around to do there. So that was probably like the biggest thing that was pretty trying to associate sort of to be able to have a sort of data model where you can associate a list of solutions with a certain proposal and to be able to have different users be able to see those lists of solutions and then have the calculation that shows, okay, there's a reward that's distributed by this amount among users is kind of like almost making a database within solidity kind of hackily.
00:21:54.740 - 00:22:07.830, Speaker D: Could you say a little bit more about how you landed on the particular scheme that you're using for reward distribution? Like maybe what other models you investigated there and why you landed on the one that you ended up with?
00:22:08.600 - 00:23:09.096, Speaker C: Yeah, so the kind of primary goal behind how we dispense the rewards is that we didn't want anybody to have completely wasted work because training a model is usually a pretty expensive computational operation. You got to have a giant cluster and spend a bunch of money on running those computers. So we wanted to make sure that you would at least have some guarantee that you would get something if you submitted a model. And it might be that if a lot of people were interested in that reward, then the reward would kind of get split proportionally in a way that might be unfavorable to you, but it also means that everybody that did some work will still get some compensation. And I guess the hope is that in general, you'll get more back than you might have put in, in terms of your time and your resources. So that's kind of why we landed on that approach. Guess the question I have is, do you know of anything similar to this out there? Or I guess what was your experience that led you to come up with this idea? Yeah.
00:23:09.096 - 00:23:59.320, Speaker C: So the most similar thing that we looked at, and this was actually suggested in one of the head feedback sessions, was this service called Numerai. And they basically run this thing where every week they have sort of like a training competition against stock market data. And it's basically like a prediction problem of stock market data goes in, give us sort of predictions coming out and then it's kind of similar to our thing where you get rewarded and paid based on how good your prediction is. But in that case it's kind of like one specific problem. And also you don't give them a model to run against any arbitrary data. You give them sort of like the predictions in a CSV file. So it kind of benefits only numerai itself rather than the general public because in our sort of system, you can just download the model directly off the interface and you can just run it against anything.
00:23:59.320 - 00:24:03.390, Speaker C: Right. So it kind of creates a bit more like a public good, I guess.
00:24:09.130 - 00:24:29.370, Speaker D: Well, I think this is a really cool project and it's pretty impressive what you all were able to accomplish in just a month. And especially being new to Bugs Three, it's really cool. It seems like you've touched a lot of broad ranging technologies including decentralized compute and powergate, so IPFS and file clinic and everything. So really excited to see what you have already accomplished.
00:24:29.790 - 00:24:49.058, Speaker C: Thank you. Totally plus one that yeah, I already have many thoughts on how we would use it and think it's right along the right path. So great job. Great. Thank you very much. Thank you.
00:24:49.224 - 00:25:09.660, Speaker A: Awesome. Well, thanks so much for demoing and also doing this thing across multiple time zones. I feel like this team is all over the place. So with that, we'll move on to our third demo for today and I'd like to welcome Robin from Multiverse. So Robin, please feel free to kick off with your demo.
00:25:13.470 - 00:25:39.282, Speaker F: So we have a crypto domain thanks to unstoppable domain extension direct to the Flake where we are hosting the website A. Multiverse is a decentralized social network. We can upload text, photo links and chat regarding post. Text is stored in three box storage photos through textile buckets likes and comments through three box thread. Post visibility meaning who can see the post. All the read and write requests goes.
00:25:39.336 - 00:25:40.430, Speaker A: Through the cache layer.
00:25:40.510 - 00:26:47.398, Speaker F: All the write requests made within 30 seconds are batched up and sent as one single request to three box. Each post is encrypted using a symmetric key for public post. The key is shared in the post metadata. For post that is meant for friends, this key is encrypted using the encryption key of that user. That user and all the friends or his or her friends have access to that key. So that's the need for the friend request so that the encryption key can be shared let's create an account now we have a couple of metamarks and three box sign in need to give a couple of permissions. Now we have to select a username and our profile is created let's send a friend request we copy the address, put it in the search bar and define the user and let's send a friend request and it instantly appears in the friends account we can accept or reject.
00:26:47.398 - 00:27:17.806, Speaker F: Let's say we reject and it's already added to a block list. We can see a notification. That request has been denied. Let's remove the unblock right now and the friend request is back again. We accept. We get notification that byproduct accepted your friend request let's upload a pic and profile. Pick is also updated.
00:27:17.806 - 00:27:41.770, Speaker F: We can also share a profile with various social media websites like Twitter, Facebook and so on. So for chat both the people has to be friends. Let's open the chat window. We can see the friend. Let's send a message, a notification pop up saying that a new message is coming. We can see the message. Let's type a reply.
00:27:41.770 - 00:28:08.820, Speaker F: And it instantly appears. Now the chat is working through three box persistent thread and the thread is private between these two users. We can also have other chat session with other users. Now I'll show you how to create a post. Click on the button. Now you can have a rich text editor. You can select the text we want.
00:28:08.820 - 00:28:40.798, Speaker F: You can select the post visibility. Meaning who can see the post? Let's make it private. Now let's upload images. Images will be stored in textile buckets and the post metadata, which is encrypted, will be stored in three box storage. So a post is created. You can see the time and the status of private. Let's see in this account the same profile but they can't access the private post.
00:28:40.798 - 00:29:02.250, Speaker F: We have a like action. Do a like. The like instantly appears on the other account. We can see who liked the post. Another like. We can write a comment. You can see the comment.
00:29:02.250 - 00:29:09.360, Speaker F: Also we have photo albums. Thanks for watching.
00:29:13.170 - 00:29:19.200, Speaker A: Awesome. Thank you so much for that really cool demo. Robin and I'll let our judges ask any questions.
00:29:23.810 - 00:29:46.006, Speaker D: Hi, Robin. That's a super cool demo. I wasn't sure if I heard it correctly, but it sounded like you said that the images are stored in textile buckets, but the post metadata is stored somewhere else. It sounded like it said three box. Is that correct? And if so, could you share a little bit more about how you architected the system to have different okay, so.
00:29:46.108 - 00:30:35.430, Speaker F: Each post has a symmetric key which are used to encrypt all the data. Post data metadata means post key where the images are stored. Images are stored in text type packet that is also encrypted and then we have timestamp the post visibility and actual post content itself. And the metadata of this will be stored in either public rebox storage of the user or private storage based on the visibility if it is private visibility to be stored in private storage. If it is public or friend will be stored in public. But if it is friend visibility, then the whole metadata, it's already encrypted with the key. So if it is front, I encrypt it again with the encryption key of the poster and store it in public rebuff storage.
00:30:44.120 - 00:31:12.816, Speaker C: Sorry. Yeah. Super floored by how much of the user interface you completed. It's pretty amazing. So I'm curious, I just kind of want to back out from that a little bit and maybe add on to where Puja was headed. When you first started thinking about building this, how did you conceptualize this architecture, this data model? Were there a lot of iterations in there, or did you kind of know how to piece this together? What was that process like?
00:31:12.998 - 00:31:35.076, Speaker F: There were a lot of iteration. The design I actually thought of keep changing, evolving. There are a lot of components I actually want to build, and halfway through, I realized I never have time to finish it. So a lot of things that happened at the one month plus hackathon, so a lot of things change. Yeah.
00:31:35.258 - 00:31:58.156, Speaker C: My question is, do you think there's more of an advantage for the users to hold and control their data, or do you think more of the advantages on the basically obscuring it from other people? Where do you think kind of the best advantages for this? Over? Traditional social media sites actually want people.
00:31:58.178 - 00:32:26.676, Speaker F: To have control of their own data so they can decide where they want to store the data, what data they want to store, so people can actually use that data for some other malicious purposes or ads. We hear a lot of data leak these days, and if you control your data, then if something happened, it's your responsibility. I just want to try and see whether such a model can actually be built or not.
00:32:26.778 - 00:32:36.010, Speaker C: That's great. Yeah, I think it was great.
00:32:38.860 - 00:32:47.988, Speaker A: Well, thank you so much, Robin. I want to just also congratulate you on doing all of this thing by yourself, one man show. So this is really impressive.
00:32:48.164 - 00:32:48.616, Speaker F: Thank you.
00:32:48.638 - 00:32:57.310, Speaker A: And with that, I'd like to move on to our next demo, and that is mapping the network and Cost of. Feel free to share your screen and present the video.
00:32:58.820 - 00:33:29.850, Speaker C: All right. Hey, guys. My name is Costco, and I'll be presenting this little analytics tool and call it Toothless Analytics because I definitely didn't get to work on it too much because I literally lost it too during the hackathon. But it breaks down in these four main steps, so just remember them because the demo kind of goes by really quickly. Data generation, storage on Pinata dashboarding and then sharing it. All right, hopefully this works. Can you guys hear that?
00:33:31.260 - 00:33:35.710, Speaker A: We cannot cost them. I think you have to share again and make sure the audio check.
00:33:37.120 - 00:34:21.736, Speaker C: Yeah, got that again. Thank you. Karthik Go. And this is a demo for FalcoIn Visualizations. So what we do is we get peering information from the filecoin network using this node JS script that uses Textile's Power Gate and actually uses the hosted node to fest that network. And you can see this is kind of the data that comes in. It's written to a JSON file, and then the JSON file is uploaded to both GitHub.
00:34:21.736 - 00:34:40.610, Speaker C: And we also use pinata. And Pinata is actually really fast, so that will come through in real time. And boom, we have that uploaded. So, next step is actually creating these dashboards, actually having a place where you can play with the data and figure out how you want to display it. So we did that here.
00:34:42.340 - 00:34:55.556, Speaker A: Pausing for a second. I think the video is being downgraded to 360 or some other format, so we're not able to read anything. Let me know if it's the same video.
00:34:55.658 - 00:34:57.956, Speaker C: And this is the demo. Pardon?
00:34:58.068 - 00:34:59.848, Speaker A: It's still the same quality, I think.
00:35:00.014 - 00:35:01.768, Speaker C: Okay, give me one.
00:35:01.854 - 00:35:06.632, Speaker A: You're able to share the video with me? I can play it on my side. We'll make sure that the quality is high.
00:35:06.766 - 00:35:13.870, Speaker C: Sure. Grab the link for you.
00:35:14.240 - 00:36:03.532, Speaker A: Just put it on the zoom chat.
00:36:03.616 - 00:36:04.250, Speaker C: Yes.
00:36:09.260 - 00:36:12.010, Speaker A: Great. I got a copy. And let me just light it up.
00:36:16.230 - 00:36:16.980, Speaker C: Go.
00:36:18.890 - 00:36:21.800, Speaker A: All right, so just give me 1 second, and I'll play it here.
00:36:25.690 - 00:37:15.416, Speaker C: And here's a slide if anyone just wants to look at the notes. This is a demo for Hoin Visualizations. So what we do is we get peering information from the filecoin network using this node JS script that uses Textile's Power Gate and actually uses the host node to test that network. And you can see this is kind of the data that comes in. It's written to a JSON file, and then the JSON file is uploaded to both GitHub and we also use Pinata. And Pinata is actually really fast, so that should be shared with everyone. See, that loads.
00:37:15.416 - 00:37:46.010, Speaker C: TADA. Get these nice little map visualizations. And we do load it from Piata. It just directly goes to the gateway for it, since that way I don't need to upload my API key on a static site or anything. And so created a few short visualizations. There's another one on a different notebook which should have a picture in the repo. And so once these have been created, they can actually get exported as static sites, which is super cool.
00:37:46.010 - 00:38:15.616, Speaker C: We can open this in the Vega editor. And while that loads, let me actually pull up the site over here. There we go. So that's JSON and we should be able to find the site give me 1 second. Here we go. And generates the static site. And the static site source basically just takes the spec and loads it.
00:38:15.616 - 00:38:17.170, Speaker C: That's all there is to it.
00:38:24.520 - 00:38:35.480, Speaker A: I think maybe. It'll be helpful, Costa, if you can just also just go over some of the reasons behind why you wanted to work on this thing so we can kind of get the remaining context because now we have to merge the slideshow.
00:38:36.940 - 00:39:24.516, Speaker C: Yeah, basically kind of we go through these steps, right? And the why is being able to share these dashboards and being able to demo. Kind of an example is in the States with the COVID data. It's not actually immutable. And on top of that, the public can't actually use that data when it's just like uploaded, right. Actually making the dashboards, actually making the data consumable in an easy way is kind of important. And so what I tried to do was basically create a pipeline that would automatically generate these websites given data and you could use these dashboards and just go into a jupyter notebook, play with it, and then figure out what kind of dashboards worked. And yeah, I slipped and fell, so lost some teeth.
00:39:24.516 - 00:39:43.390, Speaker C: So didn't really get a chance to really flesh out the full pipeline. Mainly got to the website, the notebook, and so I ran that for a week and this is all online. This is actually in the repo and you guys can kind of look at it if you want as well.
00:39:48.020 - 00:39:50.850, Speaker A: Awesome. We'll let our judges take it from here.
00:39:53.540 - 00:40:04.052, Speaker D: This is really cool. In the demo, are the visualizations, are they somehow connected to a node and getting data live or was that data.
00:40:04.106 - 00:40:10.890, Speaker C: Sort of no, I didn't get a chance to finish that part. But they are interactive. You can hover over the little images and stuff.
00:40:11.900 - 00:40:12.650, Speaker D: Awesome.
00:40:14.380 - 00:41:23.612, Speaker C: So it seems like you wanted to go kind of into other data types. Like you mentioned the COVID use case, trying to pipe some of these things through there. What other ones did you have in mind? Where do you want to land with this? Well, so I wanted to actually have a generalized solution and there's actually a lot of stuff out there in the framework that lets you pretty much pipe and build, in essence, a kind of like lightweight analytics solution on top of this technology. And it can just be a static site, right, and then this sort of like data generation that lets you just upload the data in a mutable fashion onto IPFS through something. And I wanted to look at COVID data, I wanted to look at actually more data, like deals data using the hosted node network, but I didn't get a chance to do that. And like, user data on Ethereum and Etsu. Yeah, I don't really have a question, but comment wise, a lot of the use cases that we see being built on Pinata, I feel like this could add a lot of value to them just with their data sets that they have.
00:41:23.612 - 00:41:28.880, Speaker C: So great job on that front. Thank you for pinata.
00:41:34.340 - 00:41:42.388, Speaker A: Well, if we don't have any other questions, we can move on to our next demo. But thank you so much, Kosov, and I hope you keep building.
00:41:42.554 - 00:41:44.550, Speaker C: Thank you, Karthik. Good luck, guys.
00:41:45.480 - 00:41:53.850, Speaker A: See ya. And with that, I'd like to call up our next team, and that is Team Not Pied Piper. So I'll let them kick off with their demo.
00:41:59.730 - 00:42:46.702, Speaker C: Hey, guys. Our pack is called not pipe. Piper. And what we tried to do with Not Pypiper is basically we wanted to allow users to upload videos and other large files onto these public distributed networks like IPFS, while allowing them to still have control over who can see their files. And I noticed that encryption schemes like PGP did not work too well with very large files. So we came up with this entirely new encryption scheme that is not even based on cryptography, but rather based on neural networks. We're using cyclogan to basically create a sort of mask that is applied to a video.
00:42:46.702 - 00:43:29.050, Speaker C: And this mask makes the video unrecognizable to another AI. And it can be used to obfuscate videos and store them anywhere. And basically, without this mask, you won't be able to restore the video to its original form, so it can be thought of as a kind of key, although it has nothing to do with cryptography. And this works well with large files. So the way it works is a user will generate an encrypted version of their video as well as a mask, and they can upload this video to IPFS. They can store the mask on IPFS as well. And this mask can only be accessed by authorized people, including the user himself.
00:43:29.050 - 00:44:47.514, Speaker C: And then the user will fetch this mask from IPFS whenever they want to watch this video, and they'll get the encrypted video, and they can just use the mask to decrypt the video. So let's take a look at how it works. We have this video here of a couple and some apples, and we are going to create an encrypted version of this video, and then we're also going to create the mask. So now we have our obfuscated video where the couple has become zebras. As you can see, their identities are protected. So from a privacy standpoint, this is pretty useful. Okay, so now we want to generate our mask and upload this mask to IPFS.
00:44:47.514 - 00:45:37.558, Speaker C: So let's go ahead and do that upload to IPFS, get our hash, and now we can fetch it from IPFS at any time. And this is the IPFS mainnet. Great. So we're going to use this mask now to restore this obfuscated video back into the original video. And there you have it. That's it. Hey, guys, our hack is called, not Pipe.
00:45:37.558 - 00:45:38.330, Speaker C: Pipe.
00:45:39.250 - 00:45:45.280, Speaker A: Amazing. Sure, everybody has a lot of questions, so I'll let them go here.
00:45:50.050 - 00:46:09.380, Speaker D: This is so interesting. I guess I have two questions. One is, I've never seen this form of encryption before, so I guess I'm curious, what's the context around it? And what do you envision? Are the main use cases for it?
00:46:10.150 - 00:46:56.454, Speaker C: Yeah, sure. So I guess really the idea was to think about if our public key cryptography didn't exist or how we saw it's not necessarily good enough for encrypting large video files. How would we kind of reimagine it from scratch, right. As two people who don't really know anything about cryptography, using CycleGAN was very interesting from the perspective of protecting user privacy because it doesn't just randomly alter the video, it actually tries to alter the video to the extent that another AI cannot recognize it. Right. So in theory, if it's good enough, it's rendering it unrecognizable to another person. Right.
00:46:56.454 - 00:47:51.880, Speaker C: So that was the basis of it from a privacy perspective. And we just wanted to kind of do this alternative version of cryptography. And we think for anything that's stored in the public domain or something that maybe you don't trust, like PGP, or you think public key cryptography is going to be hacked by quantum computers. This is a way for which your content can still be out there in the public domain. Or even if it's stored somewhere encrypted, people can find it and they still won't be able to reconstruct the original video. It is extremely computationally expensive to reconstruct this video to its original form without these stagnographic math because they're actually much more complex in their structure than just like a cryptographic key.
00:47:53.210 - 00:47:56.490, Speaker D: Yeah, I think it's extremely creative. So really cool.
00:47:56.560 - 00:48:46.934, Speaker C: Thanks. Yeah, I love how you came at it with thinking about a way without doing encryption. I think that's a super unique mind frame to come from. So that's super impressive. And so one of the things that I'm thinking about is there's a couple NFT kind of projects out there that are trying to use multiple NFTs to kind of overlay images or videos over top of each other so that you can change them. So I'm just kind of thinking how you could actually leverage that kind of with NFTs. If you have the NFT, then you have access to the AI decryption key.
00:48:46.934 - 00:49:37.286, Speaker C: I guess if you super, super interesting and I think there's going to be a lot of opportunities to keep working on this. Thanks. Yeah, I don't really have many questions. I guess it's really far. It's very, very cool idea. So I guess that is my question is what led you to this idea, specifically that algorithm thinking that you could apply it and then hide in the open the way the IPFS network works? How did you kind of arrive to that? Yeah, so it started with the idea of just like steganography. And steganography is basically where you have an image and you have applied some kind of mask to it such that unless you have this mask, you can't get back the original image.
00:49:37.286 - 00:50:38.170, Speaker C: This is like a very kind of old concept of hiding things in plain sight and not being able to reconstruct the original thing without, I guess, knowing what pattern to use. So we started with this idea, and Sid is kind of an ML wizard, right? So he was like, hey, what if we use CycleGAN? It's specifically designed to create things like realistic humans. It's designed to fool other AIS, right? So this, we thought, would be the most modern kind of incarnation of this very old concept of steganography. And we thought that it maybe deserved a second look compared to all of these cryptography based approaches to securing files. So, yeah, it just started with this kind of old idea of steganography.
00:50:39.710 - 00:50:47.070, Speaker A: One quick question for me. How does this decryption happen? Does it require pre processing or post processing, the whole thing? Or can you do that frame by frame?
00:50:48.050 - 00:51:06.114, Speaker C: Yeah, so decryption it is frame by frame, right? Siddharth, you can correct me if I'm wrong, but basically what happens is, yeah, so you got to split it up frame by frame, and there's different matrices, parse matrices that are applied to every single frame.
00:51:06.242 - 00:51:29.020, Speaker A: This is really cool because kind of just jumping off of Kyle's Eddie on NFD side, you can actually do the key decryption client side with just MP4 filters on top of kind of a canvas. So you can actually just decrypt client side if it's frame by frame. And I'll just be even cooler because you can store unencrypted video. It'll just be faster, and they can be decrypted per user who has access. Really cool idea.
00:51:30.590 - 00:51:31.340, Speaker C: Nice.
00:51:32.670 - 00:51:45.380, Speaker A: Awesome. Well, with that, I will move on to our second last demo for the day, and that is Team Fractal. So I think Jacob and the team is already here. I'll let them turn the video on and share their video.
00:52:00.970 - 00:52:35.794, Speaker G: Hey, everyone. We're team Fractal. Fractal is a decentralized Ethereum accounting tool that preserves user privacy and provides insights into interactions with DeFi protocols. DeFi is poised to become the new paradigm in online finance. Usage of lending and trading protocols has exploded in recent times as retail users are discovering the new world of peer to peer finance. Yet the accounting tooling in the space is very undeveloped. Despite the open and permissionless nature of Ethereum, transaction data is still proprietary and expensive for retail users to access.
00:52:35.794 - 00:53:11.100, Speaker G: Currently, there exists no platform to categorize and compile transaction data while preserving user. Anonymity centralized account based solutions exist. However, they are a point of failure in preserving your identity. We believe that profiling users is the wrong approach to accounting on Ethereum. The emergence of DFI is only set to exacerbate this problem. Fractal is a privacy preserving Ethereum accounting tool that gives insights into Ethereum, ERC 20 and DFI protocol transactions. Fractal's reports turn these basic transactions into enriched, categorized transactions that provides its users direct value.
00:53:11.100 - 00:53:51.898, Speaker G: Most importantly, Fractal respects users privacy by enabling them to remain pseudonymous while aggregating and sorting their Ethereum transactions. All user data is encrypted and stored on IPFS data that only the user can unlock. All of Fractal's infrastructure is deployed on fleek. We created and deployed our own IPFS node for hosting encrypted user data with Asymmetric encryption, giving users a completely secure experience. It is only the users themselves who have access to the keys to unlock the IPFS hosted data. Once their computations are complete, users are able to download their tax reports from IPFS using Felix's Space Statement or download directly to their home computer. Getting started with Fractal is easy and fast.
00:53:51.898 - 00:54:15.134, Speaker G: Here's how it works fractal supports both new and returning users. Returning users can reuse their IPFS Content Hash to render their previously generated Fractal reports. Let's check out the report generation flow. First, users firstly enter their Ethereum addresses that they wish to aggregate. There is no limit to the number of addresses a user can enter. Then users can select DeFi protocols they would like to gain insights into. Fractal currently supports Uniswap and Compound.
00:54:15.134 - 00:54:48.614, Speaker G: With dYdX and Aave coming soon. Protocols are onboarded by building in house algorithms using GraphQL on the Ethereum blockchain. Users then select a date range they would like the report to cover. Once these parameters are finalized, the user is presented with the cost of the service. To uphold transparency, Fractal displays the formula that we use to calculate this fee. Once accepted, the user is shown their IPFS Content hash, which can be used later to access the report. Users are prompted to pay, and upon confirmation, the report will begin generation.
00:54:48.614 - 00:55:24.022, Speaker G: This process involves our back end trawling over the entire Ethereum data set to provide clear, categorized transactions in USD for all Ethereum and ERC 20 transactions. Our Graph algorithms enrich the data further to provide protocol specific statistics. When the report is generated, it is encrypted and hosted on IPFS. Our front end then fetches the data using the user's key and compiles the content of the Dashboard with WebAssembly. The Dashboard provides a user a holistic view of their addresses with the date period selected, which includes their total transactions made, USD in and out, and their gas fees spent. These statistics are then broken down into a month on month basis. Since Uniswap and Compound were selected, we can visualize all of their transactions that were made within them.
00:55:24.022 - 00:56:00.130, Speaker G: Like here, we can see transfers between general Ethereum addresses. Moving on, we can see liquidity transfers uniswaps. So here we can see the total amount length borrowed, as well as repaid between compound protocol. Here we can see quite a big swap made on Uniswap between wrapped ETH last month. Once finalized, users can download a complete CSV or month by month CSV of all their traction in historical order. Reverting back to the alternative flow, we can use the content hash we previously stored to view the report we paid for. Again, this can also be used for handing your report over to accountants or other trusted advisors.
00:56:00.130 - 00:56:11.250, Speaker G: Fractal is for all users of Ethereum. Simply enter the addresses associated with your business or personal interactions and the tool will compile and categorize your transaction data while preserving your anonymity.
00:56:13.870 - 00:56:14.620, Speaker C: It.
00:56:22.750 - 00:56:28.080, Speaker A: Awesome. I'll let our judges ask any questions.
00:56:29.330 - 00:56:44.660, Speaker D: Yeah, I think it's like really polished demo. It's really awesome. I'm not as familiar with the Ethereum ecosystem, so I'm kind of curious, from your perspective, what was the need that drove wanting to create this sort of tool?
00:56:45.750 - 00:57:23.710, Speaker G: After reaching out to several liquidity miners no providers, as well as general Ethereum and DFI users, it was very clear that 80% of the problem is that the data is actually not there. And then beyond that, most people would rather use their own accountant rather than a profiling online tool that will identify them and then aggregate and store their transactions. So there was a huge demand that we found through discourse for having this decentralized private accounting tool that will show them aggregation and categorization of all their DeFi and ethereum interactions.
00:57:28.210 - 00:57:40.738, Speaker C: Can you explain a bit more about what are the privacy preserving steps? How are people remaining synonymous in this? Is it just because it runs primarily locally or is there something more layered in there?
00:57:40.904 - 00:58:15.470, Speaker G: Yeah, so I guess the encryption was one of the big privacy preserving steps. And on top of that, rather than having traditional accounting tools, you create an account. You then have your ID mapped to all these addresses or exchanges that you own. We kind of obfuscate all of that. All we need is an address. You pay using crypto so we have a payment smart contract, and then once it's generated, there's no data stored on our end. We just upload it to IPFS and then it's essentially all yours after that point.
00:58:15.470 - 00:58:21.330, Speaker G: So that's the main steps, taking all the data away from us and giving it back to the user.
00:58:25.830 - 00:58:30.210, Speaker C: Maybe I missed it, but how are you thinking of sharing it with the accountant?
00:58:31.270 - 00:59:08.000, Speaker G: Yeah, I guess the encryption, we had a few different issues that's kind of still a work in progress, but the original idea was using public private crypto on the client side and then generating a symmetric key on the back end so then the client would have that symmetric key decrypted. So then I guess essentially you'd have to share both the symmetric key and the IPFS content hash. So that's where that kind of like plug in your IPFS hash flow comes into play. But yeah, it needs to be ironed out a little bit for that encryption step.
00:59:10.470 - 00:59:28.040, Speaker C: Great, thank you. Did the nice weather and birds chirping help you put together such a nice UI? Is there some connection there?
00:59:29.610 - 00:59:31.910, Speaker G: Let's just put a good backdrop on.
00:59:32.060 - 00:59:41.020, Speaker C: This is just every day. Yeah, really cool stuff.
00:59:43.950 - 01:00:13.240, Speaker A: Amazing. No, this was really cool. So if there are no more questions, we'll end this here. And I've been notified that the last team is having Internet issues. So what we'll do is we'll just give them a minute or two to see if they can rejoin. If not, we'll just end our session here today, and we'll reschedule this team to go in the next slide. So maybe this is a good opportunity for our judges to gather their thoughts, take a break, stretch a little, and we'll just try to come back in two minutes.
01:00:14.730 - 01:00:15.720, Speaker C: That's it.
01:02:19.440 - 01:02:52.650, Speaker A: Looks like we'll have to move the last team to a different judging slot, so we'll just end it here. All of that said, I want to thank all of our judges for giving us the hour today and sharing their amazing feedback and comments. So if all of the judges can quickly check their email after this session ends, I'll be appreciated. But thanks again for giving us your time, and thanks to all of the teams from all over the world demoing today and showing what they did to all of us. And we'll resume judging tomorrow. Thanks, everybody.
01:02:53.900 - 01:02:57.750, Speaker C: Thank you. Thanks. Thank you, everyone.
