00:00:00.170 - 00:00:23.066, Speaker A: The radio. I'm here to introduce our next speaker, Alexe Akanov. I think I did that right. Please correct me, Alexe. He's the founder and lead of the Turbogas Client. He leads research critical for the future of Ethereum. And today he's going to talk to us a little bit about how modular development will enable more people to be involved in the ETH One core development.
00:00:23.066 - 00:00:26.120, Speaker A: So without further ado, please take it away.
00:00:28.810 - 00:00:53.334, Speaker B: Hello Emily. Thank you for introduction. Yes, so let's get straight into this. So I prepared a little presentation. It will be shorter than the last time, but I need to just click a couple of buttons. 1 second. Sorry, I am bit disoriented.
00:00:53.334 - 00:01:37.690, Speaker B: Yeah, that's here. Okay, so what I'm going to be talking about here is that let's see just 1 second. Okay. So I've started watching Ethereum core development somewhere maybe in the beginning of 2018. And it took me a while to really understand what's going on. And I remember we had some kind of gathering or summit in Stanford in the beginning of 2019 and you kind of walking around the campus. Stanford campus was a great place.
00:01:37.690 - 00:03:00.406, Speaker B: And somebody just asked me the question without any kind of without any sort of irony or sarcasm, so how does a particular change get into Ethereum? How does it work? And then I stopped and thought about it and of course the prepared answer would be like oh, there's this process, there's EIPS and you need to do this and that, you need to go through the stages. But actually I tried to describe what happens in reality so not just as a process tells us, but what actually happens. And it sort of started led me onto this path of thinking about it a bit more. And I kept coming back to the same realization that we are seriously bottlenecked on the core developers. And it was obvious to pretty much everybody that we have a very smaller number of people that are involved into the most critical part of this process. And there doesn't matter how many people will be throwing in the AIPS and kind of coming up with the interesting kind of ideas, but eventually all this deluge of stuff needs to get through a very small number of people. And the reason for that is because these people are what you would call the owners of the code.
00:03:00.406 - 00:04:11.630, Speaker B: And I think it's understandable that if you own this code, not in the sense that it belongs to you, but in the sense that you are responsible and accountable, well, you feel that you're responsible for the quality. So if somebody gives you the change and you didn't verify it yourself, you are basically feeling that, well, that might break. I need at least to test it myself. So I can tell you, for example, that in our project I do quite a lot of testing. You might not expect that, but I actually spend a lot of my time testing what other people have written because I feel that I have an ownership and I need to make sure that any serious problems don't just pass through. And I also understand that other people might not completely get all the nuances and I need to basically do all the testing. And so that applies to other code developers as well, so who take their job seriously, and they do need to look at everything and they need to try to understand everything.
00:04:11.630 - 00:05:20.426, Speaker B: And then sometimes, or oftentimes that causes some frustration from the people who propose things. I was trying to figure out how are we going to solve this? There must be a way to widen this bottleneck, to get more people in, but without sacrificing the quality. And back in 2019, so there was this idea about working groups and things like this and yeah, wrote a blog post about it, but then after a while I realized it didn't quite work. And the learnings from there was that the working groups that we created back then, they didn't own any code. So they could basically do research and they could prepare some changes, this and that, but because they didn't own the code, all these changes still had to go through the developers who did own the code. And they have to apply their own rigorous checking, testing process and things before changes go in. And then another realization is that most of the ethereum core developers is not implementing EIPS.
00:05:20.426 - 00:06:09.698, Speaker B: It's something very different. Again, it's a lot of testing, it's a lot of people defuzzing, it's a lot of figuring out the performance improvements, which most of them don't have nothing to do with the IPS. So that's another thing I realized. So that view that the core developers are the people who implement the IPS is actually not adequate. And there's a lot of other nuances scroll into 2020. And some of you might have seen, we had about three o core dev calls which I put on the slide. Like when they happen, you can go back and watch them or listen to them.
00:06:09.698 - 00:07:16.022, Speaker B: And so we spent these three calls where we decided we're not going to talk about EIPS, but we're going to be talking about other issues. And three main issues that I put highlight here is the burnout of the core developers, which I think is the consequence of these sort of responsibilities that I talked about in the first slide, and the pressure that they feel on themselves. And the pressure also combined with the pressure coming from other people who want something to be changed, but that kind of double pressure from one from within and one from the outside, and not everybody can deal with that. So then we talked about diversity of different implementations and we also talked about the barrier to entry. So this is the secondary issues. We recognize that it's very difficult for anybody new to come in and just, let's say, produce a new implementation. So I kept thinking about those things.
00:07:16.022 - 00:08:22.560, Speaker B: I mean, we did discover quite an interesting thought and then I went ahead and I tried to find the solution. So my solution that I already proposed on these calls, which some people were skeptical about, but some people were supportive, is essentially introducing modularity. And it's actually something that I and my team can do, can action rather than simply propose and try to push for. So we can actually do that. Before we talk about modularity, let's just quickly go into the so on these calls. One of the theme was that we do need a diversity of limitations for this reason that if we have basically the dominant implementation, that there might be some bugs happening. And that bug essentially becomes of the rules of the protocol, which, from my point of view, sometimes it's okay, it's permissible, because that's already actually been done a couple of times.
00:08:22.560 - 00:09:11.726, Speaker B: But if we look at the different implementations, disagreeing about the state, what people call consensus failures, this is presented as the scariest thing, one of the scariest thing that can happen on the ethereum network, consensus failure. So if we look, I mean, that is my own observation because I do look to the past, but I didn't take the statistical analysis of this. It does happen quite rarely, but it does happen both on main net, on a testnet. And so my observation is that most consensus failures are actually happen in one specific place of the ethereum implementation. And that's what I put here in the diagram. So what I call the interblock state. So it's not actually EVM itself most of the time.
00:09:11.726 - 00:10:11.918, Speaker B: EVM isn't generating consensus failure. It's sort of a layer around it which deals with the things like caching, the things that retrieved from the database, it's the refund logic, it's the self destruction logic and things like this. So they are not strictly, I mean, depending what you think EVM is. But for example, if you look at EVMC, which is the very popular interface for written and C, for example, EVMC is if you look at my diagram, you would see that EVMC is actually interface between that EVM block and that like a pink block of the interblock set. So this is the EVMC which is this boundary. And then within this boundary, for example, we're going to be talking about EVM one, which is one of the implementation. It implements this, but it does not implement that pink thing which needs to be wrapped around it.
00:10:11.918 - 00:10:42.360, Speaker B: And I pose it that this is where most of the consensus failures happen. Not in here, not in the peer to peer things, not in the mercalization, not in the state reader or anything. It's actually mostly there. And that would be important for my next slide. So what we already did so as I said, we started to take action on this. I mean, we've been doing this for quite a long time. So what we actually did since May, we have started.
00:10:42.360 - 00:11:13.986, Speaker B: So we have Turbuleges, of course, and that is the derivative of Go Ethereum. And we've replaced quite a lot of things in Go Ethereum. But I think we still have a virtual machine that's pretty much unchanged, which is inherited from Go Ethereum. We have changed the interblock state quite a lot. So it's very different. So that's why I'm saying TG here. But we also already produced the alternative implementation of these piece where the most consensus problems happen.
00:11:13.986 - 00:12:13.010, Speaker B: And this is completely clean room and implementation. So there is no code lifting from anywhere. We did not reimplement the EVM, we just took the EVM one, which has been written by other people. And currently it works actually it works better than this one. So currently the written benchmark was that we could run this bit through all the Ethereum blocks in about 36 hours and we can run this through the same blocks in 21 and a half hours, which is pretty good improvement. But what I was going to say in this diagram is that if you look at those things that they have completely different implementations of the part which generates most consensus failures. So for this kind of purpose, these could be deemed as a different implementations and they would provide the diversity if you run Turbogeth in these two different configuration.
00:12:13.010 - 00:13:13.766, Speaker B: So, by the way, we did not finish the integration of the Soakworm thing into Turbogeth, but I think we will finish this quite soon. So next thing we did is that we started on the path of the modularization and taking out components. And that was one of the first things we did. We did it for the purpose of performance because we noticed that the RPC requests, they do tend to be quite CPU intensive. I think it's mostly because of the transformations of data between formats and stuff like this. So what we did, we separated them into different processes and we have created the sort of very simple interface, very low level interface between the RPC daemon and the node. And that also has an interesting consequence that there could be multiple RPC daemons, for example, implementing different variations of the JSON RPC standards.
00:13:13.766 - 00:13:49.094, Speaker B: They could reside on the same machine or different machines. So currently it works with the TCP IP here, but actually we realized we can also make it work with a shared memory. So if you can put them on the same machine and you need the really high performance, you can do that. It doesn't quite work yet, we just need to fix it. But it could work in principle. But this is where it all kind of started. So this is, by the way, for the communication between the protocols, our current standard.
00:13:49.094 - 00:15:12.334, Speaker B: We just basically do it for all the components. If we want to separate them, we use gRPC, which is basically Http two based sort of framework which is based on protobuff and you can generate bindings for pretty much any language in existence. So for example, this is like a protocol definition for our RPC demon as you can see that what it does is basically it's like a database. You can open a transaction and then inside the transaction you can open a course or to a table and then you just do basically extract go first, go to the first record, you can seek something in the database and you can retrieve it. And it doesn't have all the semantics of prefetch and everything like this, but the point is that the interface is very universal and simple so you can implement pretty much any RPC request using that because what you are given is the remote database access. And so that basically was the first thing and then that gave us idea so that we can actually then next thing whenever we had this issues where we wanted to do some variations, we said we can split these things as a component. So the work currently ongoing on the Consensus Engine split and this is actually quite an important one.
00:15:12.334 - 00:16:32.860, Speaker B: So currently, although people say that the Consensus Engine is pluggable, but actually it's not quite pluggable because it basically lives inside the code of the interlementation and anything that lives inside the code of implementation has to be verified, as I mentioned before, by the code owner. It has to be checked and everything. But if imagine instead you had the Consensus Engines implemented as a component with a well defined interface, then the people who developed the Consensus Engines could own that code. And so that could be already a start of separation of the code ownership. And so let's say if that was a working group that worked on the Consensus engines, that working group not only just proposed something, but they could actually own the code, they can do their own releases as long as there is a compatibility through the interface. And then it's one of the examples where the core developers essentially give up the responsibility and the power they need to come at the same time so that they don't own this code anymore, but they own their implementation of the interface. So the switch to different Consensus Engine can happen already with the participation of a different group.
00:16:32.860 - 00:17:28.282, Speaker B: And so currently we're still figuring out the first version of the interface because in this interface we're trying to support three things already, the already existing ones that we know about. We trying to support Et hash which is quite simple. So in Et Hash you would have, let's say these things that are marked green like the core wants to verify a certain header, it just sends it to the consensus and there's a bit of chatter going on between them but in the end it gets the result whether it's verified or not. So the consensus can ask for additional information, for example for some parent blocks for parent headers and stuff like this. But then another use case is when the core wants to do fork choice. And again the fork choice rule is the job of Consensus Engine. So it will ask out of these set of headers which one is the best.
00:17:28.282 - 00:18:15.734, Speaker B: And the Consensus Engine, we say this will say this one and again it can ask for some additional information. For example if these things are actually on different forks, it will ask for the predecessors of these headers to arrive at the common ancestor and then it will be able to figure out for example what's the highest difficulty or whatever consensus algorithm says. And then for the ceiling it's like it's for the miners or for validators there's another request. So we're looking at the Et hash, we're looking at the click could be implemented with this protocol as well. And we're also looking at Aura. But Aura is the most complicated one but we actually are probably going to try to implement anyway. So next practical example is the PTP sentry.
00:18:15.734 - 00:19:10.442, Speaker B: This is also work in progress and it's a bit further ahead than Consensus Engine. We actually have initial implementations running and one thing to notice here is that we already have two implementations of sentry. One which is we basically separated the code, the peer to peer code that existed in Tuberget which was inherited from Go Ethereum. And so literally like today I was testing it as a sentry and then we have another sentry written in Rust which is basically completely clean room implementation and they implement the very similar protocol. And so I'm not going to go into details but this is the protocol we're currently working around. So what we're trying to do is that we're trying to sort of make these two sentries are essentially pluggable. Or even another interesting thing you can do is that you can actually have multiple centuries connected to the same node.
00:19:10.442 - 00:20:19.646, Speaker B: And with all of the other this actually opens up some more interesting flexible options because you can have two different centuries of different implementations connected just in case one of them has a bug, then you still actually connect it through another one. So here basically the idea is that the diversity could come in many forms. It doesn't have to be the entire reimplementation of the client but it could be a reimplementation of a specific bit or you could actually have multiple implementation in one installation like with an example of centuries. And the last bit that I'm going to show is the transaction pool split which we haven't started yet, but we are going to start it very soon with some help from other people. And some of you might have watched today's talk about account obstruction. So for example, the work on account obstruction requires a lot of changes inside the transaction pool logic and how I propose we're going to do it. So first of all, we're going to lift it out of the core component.
00:20:19.646 - 00:21:09.940, Speaker B: And as you can see here, it could be done with the help of P to P sentry. So essentially a transaction pool becomes this kind of component suspended on the two interfaces. So on one hand, it's interfacing with the Peter to P sentry for the transaction traffic, and on the other hand, it does require some access to the state, but it can use the same interface that Rpcdemon is using for the state because it can basically query the database. And what is interesting here is that I predict that there will be multiple different implementations of transaction pool. Some for the purposes of the counterbstruction experiments, some of the purposes of some sort of mev experiments and so forth. We also have a project to try to implement it in a different language. I think we're going to try to do it in Python, for example.
00:21:09.940 - 00:22:09.634, Speaker B: How cool is that? You can implement transaction pool in Python connected to our Sentry, which is written in Ruston to the Trooper Guest Node, which is written in Go. And it's all going to work. So, yeah, as a conclusion to underscore the things I just said, that we need to think about implementation diversity in, I would say more diverse way. So it could actually come in different forms. It could be not just basically just reproduce the whole thing in a different way, but maybe reproduce the parts in a different way and also kind of recombine them. Another thing is that working group can and should own the code and that would make them, because they are the owners, they will be responsible for code quality and this is how we're going to widen the bottleneck. And another thought is that for everybody, innovation always happens elsewhere.
00:22:09.634 - 00:22:23.900, Speaker B: We are not the smartest people on the planet, right? There will always be people who are smarter than us that will come and help us to do things better. Yeah, that's it.
00:22:28.590 - 00:22:44.930, Speaker A: Awesome. Thank you so much, Alexe, for sharing that with us. If you've got time to hang out for a little bit, I do have a couple questions that came in from the chat. The first question is how big is the performance overhead for decoupling via JSON RPC?
00:22:45.830 - 00:23:44.546, Speaker B: We actually haven't measured that, to be honest. We did have a project to measure this some time ago, but it was long time ago and we only saw the overhead on the things that were really intense kind of chatter with the database. I think we were testing something like Get storage range at and it was some considerable overhead. However, if we look at the current usage of ARPC requests infuria, for example, they had a blog post recently that one of the most popular requests is East Call, which is essentially executing transactions onto the state, which is current state or something. And so this is where I expect a lot of benefits from taking that out of the node because you will run EVM in the RPC daemon, not in a node. And it will just once in a while go into node to ask for a state. So, yeah, the answer is I don't know.
00:23:44.546 - 00:23:57.880, Speaker B: But as I said as well, we also introducing the mode where you can do shared memory with the node. And actually, I think in this case, the performance might not be even different.
00:23:58.830 - 00:24:14.750, Speaker A: Awesome. Got a couple more questions. Next one, I'm going to mispronounce this. I know it. Is it possible to use C plus plus silkworm EVMone already? This particular user would love to add it to their fuzzing efforts.
00:24:15.570 - 00:24:41.202, Speaker B: Yes. So the silkworm is open source, and it's got Apache two license, so you can search for it. I think there are also links from the Turbogast repository. Definitely open source Apache license. I mean, it's just out of the oven, so it's not like we haven't did a lot of testing with it. But if they want to experiment, be my guest.
00:24:41.346 - 00:24:58.170, Speaker A: Right? Cool. All right, let me check on the chat. Got another one. Okay. Will this modularization stay in Turbogas because there is more freedom to design or have other clients expressed an interest in adopting this approach?
00:25:01.890 - 00:26:04.270, Speaker B: Yes and no. Because I think at the moment this is very new kind of new direction, and some people are still skeptical about this. And my sort of way to go about it is that we will essentially just do it. We will not try to sort of like, get the to try to basically do consensus on this before we're actually just going to prototype and we will see if this brings some benefits. And if it does, I'm sure other people will join and people will recognize that if somebody will try to come in and reimplement Troopergat, for example, or any other client, it will be much easier for them to start with the one component and then get to another component and another one. So the job that I was doing for the last almost three years would be much easier if I just had some small part to start with rather than just looking at this whole thing. I'm sure there will be a lot of cooperation.
00:26:04.770 - 00:26:27.990, Speaker A: Got it. All right, I'm going to hang on just for one half a minute to see if we have any more questions because the chat does seem to be pretty active. Let me see. Okay, I think that's it. Thank you, Alexey, for joining us, giving us an awesome talk. I'm sure I'll see you around.
00:26:28.140 - 00:26:29.170, Speaker B: Okay, bye.
