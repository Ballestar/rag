00:00:00.650 - 00:00:49.020, Speaker A: All right. Good morning everyone. Welcome to the workshop on using the graph to index data off of Ethereum. I guess to start, I was going to just do a quick explanation of what the graph is, but can I get a show of hands on who knows what the graph does and in general what the goals of the graph are? Great. Okay, cool. So go quickly, just an explanation of the graph. We are a data access layer and tools for you to index the data off of Ethereum chains that you're interested in for your applications, which is typically for a specific smart contract that you are either created or you want to look at data for.
00:00:49.020 - 00:01:39.802, Speaker A: And what we're going to do today is we're going to go through how to start up a subgraph, which is what we call our data sets on the graph. And we're going to initialize that from a contract and deploy it to our hosted service and start that syncing so you guys can see that process and go through it with us today and then build your own subgraphs this weekend or later on if you want to. Before I get into graph in it, is there anything just as far as the general idea of the graph and our goals that you guys want to ask questions about or talk about before we dive into the specifics? What's the strongest use case you've seen for the graph? What do you think?
00:01:39.936 - 00:02:22.294, Speaker B: Immediately, one of the very useful things is just being able to integrate it into like a front end app. So one of the first ideas that we came up with was basically people were querying directly to Ethereum nodes and it was loading super slow. So what the graph does it'll sync from block zero to 9 million or whatever it's at now. Index all the data you want and then you have this back end where you're hitting this endpoint and you design the schema yourself. It's a GraphQL schema, so if you have the dashboard you have in mind that you want to build, you can actually just build it so the schema will come out exactly how your front end developer wants to grab it. And it makes it really simple basically to grab data off Ethereum.
00:02:22.342 - 00:02:27.130, Speaker A: So making better react components is probably one part of being able to query.
00:02:28.270 - 00:02:31.498, Speaker B: Yeah, being able to compose all the.
00:02:31.504 - 00:02:33.980, Speaker A: Data more easily and just not have to worry about.
00:02:35.090 - 00:02:45.220, Speaker B: Yeah. So if you design the schema correctly, it could just be one API endpoint instead of grabbing a few, putting them together and then displaying it. Yeah.
00:02:46.310 - 00:03:41.214, Speaker A: Cool. So I'm just going to open up a terminal here and so there's a bunch of different ways to start up a subgraph. Okay. And I'm going to go through the process of starting it from a contract. And what we do there is you enter in the contract and we pull that down from Etherscan and we scaffold out a subgraph for you that will run in a very simple way to start. And then what you can do from there is build out your schema based on the application usage that you want. And so to do that, you first install our graph node module.
00:03:41.262 - 00:03:42.020, Speaker B: Oh, yeah.
00:03:43.670 - 00:04:16.028, Speaker A: Is it too small there? Good, cool. So I already have our graph CLI installed. It's pretty easy to install. It's a node module and I'm not going to go through that. But we'll just start out by running graph init and it kind of takes you through the process. So you just put in the subgraph name, which has to be your username, and then the subgraph name that you want it to have. What we're going to be doing?
00:04:16.114 - 00:04:16.556, Speaker B: Yeah.
00:04:16.658 - 00:04:51.850, Speaker A: Is this a new blank directory? Yeah. So this is just a blank directory. I haven't set anything up, but my intention here is that I'm going to make a directory inside this with my subgraph in it. If I wanted to, I could make that elsewhere in the machine. But just to make it easy, I'm going to keep it in this directory. Before I keep going through this, I will go over here and get the contract that I'm interested in. So what we're going to do today is the gravity contract, which is pretty simple, so it's easy to understand what it's doing.
00:04:51.850 - 00:05:48.170, Speaker A: And it is just a tool to have to store gravitars on chain. So you can build your own avatars, post them there, and then with a subgraph you'll be able to query the different avatars that people have made, list of users, stuff like that. So I'm going to go in here and get the contract address, go back to my terminal and it's going to ask me what I want to name my directory. I'll just name it gravity. It's going to be on mainnet and I'm going to paste in my contract address here and it's fetching that Abi from Etherscan, creating my directory, scaffolding out my subgraph and installing all the dependencies with yarn. So looks like that's going to take a second to install. Cool.
00:05:48.170 - 00:06:44.072, Speaker A: And then we give you some next steps for how to then deploy your subgraph. But before we do that, I'm going to go ahead and open that up and see what we've generated there. So I'm going to go over to my IDE and we have this gravity folder that was just generated and in it we have the main pieces of a subgraph. I'm going to try to make this bigger for you all. It's probably pretty hard to see. I'm not sure how to make that one bigger, but I can talk through that a bit. So the things that are generated are GraphQL schema, a subgraph manifest, a package JSON that has your dependencies in it, and we also auto generate mappings.
00:06:44.072 - 00:07:42.060, Speaker A: And what mappings do is they transform your data to the entity structure that you want it to be in in the database in the end and in the schema here. This one's really nice because it's pretty simple. We just have one entity here with counts and owners. And I use this example because it shows one of the potential pitfalls of using graphinet, which is in this case, the contract has an ID in the contract. So our auto generate created two IDs here, which is probably hard for you guys to see in the screen, but this is something that you guys might come across if you guys are initializing your projects. So all you have to do here is just remove one of those IDs so you only have one of them. And then I'm going to go back to the terminal and I'm going to kind of go through some of the steps that will be next.
00:07:42.060 - 00:08:34.780, Speaker A: So we will run graph build. I'm not in that directory yet, so I'm going to go into our directory gravity and we generate a package JSON file for you. And in that we have some NPM scripts for building your subgraph, deploying your subgraph and running code gen. What CodeGen does is it takes your schema and your APIs and we generate classes for you and then you can use those classes in writing your mappings. So we auto generate the entity types with the fields on them and getters and setters for those. And you can then write pretty quickly, write nice transformations for your data. So I'll show you guys what I mean by that.
00:08:34.780 - 00:09:40.580, Speaker A: If the abi isn't on Etherscan and you have access to it, you can just download the abi yourself and you will in the subgraph manifest which was auto generated. In this case, it has a reference to where the abi is located under the abis portion of the manifest. So you'll just put your abi in the directory somewhere and then reference that. And then you'll run code gen and it'll build your TypeScript classes that you can then use in your mappings. And the nice thing about that is then you get some nice autocomplete and type checking for you. So I'm going to go back to my terminal and I'm going to build our subgraph. And you can see I have another error here related to our ID.
00:09:40.580 - 00:09:50.354, Speaker A: And the error requires a specific code.
00:09:50.392 - 00:09:52.082, Speaker B: Gen again because you fixed it.
00:09:52.136 - 00:10:35.490, Speaker A: No. So this is a type big int. Our IDs need to be strings and the data coming from this contract event is a big int. So I'm just going to go in here, I find that error and I'm going to convert that to a string before saving it to my entity and that should fix that. So I'm going to run yard and build again. So now I need to run yard code gen because I had changed my schema and now that's going to update all my classes.
00:10:40.470 - 00:10:43.220, Speaker B: How was the first email built?
00:10:48.810 - 00:10:56.540, Speaker A: Yeah, so it's not easy to develop a subgraph if you don't already have some understanding of how the contract works.
00:11:02.430 - 00:11:43.178, Speaker B: So init is pretty basic. It goes over the contract and looks at the events and creates identities based on the event signatures. So depending how the events, it's very basic, right? It goes over and just creates it. Most likely if you're developing a complex smart contract, you're going to look at the events yourself and start building them. You're going to make the schema file yourself. Basically you're going to say, I want the CDPs, and that might combine four different events in the maker contracts and you kind of just map it out yourself, put it down there. So graph init is more based for anybody who's getting started with a graph and like their first project and looking at maybe a contract that's simple online and it'll generate it for you.
00:11:43.178 - 00:11:52.700, Speaker B: But yeah, if you're developing a subgraph to be used in production, you probably might not even use graph in it. You might just create the schema from scratch yourself.
00:11:55.970 - 00:13:15.980, Speaker A: Okay, thanks for the good questions, guys. Yeah, so now that I've built the subgraph, I can either run a node locally and deploy my subgraph to that node to index all of my data and then build my GraphQL server, or I can deploy it to our hosted service. So today I'm going to use our hosted service, which I suggest everyone doing because then it's pretty heavy on your own machine. So I'm going to go sign in at the graph and I'm already signed in here and you guys can just sign in with your GitHub accounts. And then I'm going to go to my dashboard and I'm going to create a new subgraph that I'm going to call Gravity. I'm just going to really quickly put a description in here, Gravitar for Ethereum, and then create my subgraph. And once I've created the subgraph through our UI on the hosted service, then I can deploy my data definitions to our hosted service.
00:13:15.980 - 00:14:24.000, Speaker A: So I'm going to go back to my terminal and we have all these scripts already built for you in our package, JSON, so I can just run yarn deploy. Actually, let's go make sure I don't think that we have the name my GitHub username in there. Ford n gravity. Okay, yeah, it looks good. So I'm going to deploy that, and what it's doing is it's uploading all those files to IPFS and then using the unique IPFS hash, sending that to our hosted service, which will then pull it down, take the data definitions and start indexing that's so that we have storage to keep all our subgraphs around. So we use IPFS as distributed storage to be sure that we're not losing that base then, because that's basically our data definition. And from that you could always rebuild the subgraph data.
00:14:24.530 - 00:14:41.570, Speaker B: Yeah, so like long term, when it's a decentralized network, what it allows for is the subgraph files that allow anybody running a graph node around the world if they can just grab them from a decentralized data source like IPFS or Filecoin, all you do is grab that file and then it'll start indexing.
00:14:43.270 - 00:15:38.720, Speaker A: So I got this error here that I have an invalid account name or access token, which is because I did not use my access token for deploying, so it didn't authenticate. So I'm going to go back here and I have my access token shown on my dashboard. I'm going to copy that over and I'm going to add that into the deploy script, which is just access token. And I'm going to try that again uploading to IPFS built and it's deployed. So then let's go back to the hosted service here. And there we go. So we have the gravity subgraph has deployed and it's actually already finished syncing.
00:15:38.720 - 00:16:22.800, Speaker A: And I believe that's because yeah, so that subgraph has already been deployed to our service and we dedupe using the IPFS hash. So if there's an already identical subgraph that's been indexed, we won't redo all that work. We're just going to reference the data that's already there. So that's why, you see, I already have 73 entities that have been stored and I have this bar that showed I've synced all the way to the latest block, 8.9 million. And then you can see the simple schema that I had generated here and we can actually try to query it. So I queried for the first five entities and their ID count and owner.
00:16:22.800 - 00:17:30.456, Speaker A: And so you can use this page to try queries out and explore data sets that we have already on the graph or your own. And then the other really important thing about this details page is the logs tab, which you can use to see progress of your subgraph syncing and debug. By looking at errors there in the mappings, you can log out messages or debug statements. So that can be really helpful as you guys try to figure out and debug your subgraphs. So that kind of is a really quick walkthrough of using a really simple subgraph, deploying it to the hosted service, initializing it, and what we want to do now is go into a slightly more complex subgraph, the Uniswap subgraph, which I think everyone should be familiar with Uniswap here. So we'll kind of switch over to that and use that to explain how our mappings work and how a more complex schema might look.
00:17:30.638 - 00:18:06.032, Speaker B: Yeah, let's do it. So how many of you guys know Uniswap? Okay, other people are we built can you, can you stop? Yeah. So we built a subgraph for Uniswap, actually. So this kind of relates to like, we didn't use graph init here, as you can see with this schema. I'll zoom in here. Too nice for this one because we're familiar with it. We set it up.
00:18:06.032 - 00:18:24.088, Speaker B: You could use graphenet as a scaffold and then start deleting stuff as well. We could also totally help with you guys, like anything with that this weekend. Yeah, go ahead. For sure, no problem. One more. Okay. There we go.
00:18:24.254 - 00:18:24.680, Speaker A: Nice.
00:18:24.750 - 00:18:51.760, Speaker B: Perfect. So as you can see, this schema is massive. There's a lot of data here. What we're doing with Uniswap actually is you can query data that exists on the smart contract, like Live. Like right now you could query the specific amount of liquidity that is available in Uniswap. But what we also did is we tracked historical data as well. So every time there's a transaction, we're making an entity which is a piece of data.
00:18:51.760 - 00:19:37.404, Speaker B: So as you can see, this is a pretty long schema, 200 lines. Took us a few weeks to do, but what it ends up allowing you to do is get a subgraph like this. So as you can see, this is the Uniswap subgraph. I'll try to zoom in here as well. So Uniswap is a decentralized token exchange, basically where you just swap tokens. And one of the queries that you're able to do on the subgraph for it is right here we're organizing exchanges by trade volume. And as you can see, if you look over here, we got Dai, which makes sense if you ever look at Uniswap, dai is the most traded token.
00:19:37.404 - 00:20:19.980, Speaker B: So you start to get all these values. Like we're recording the price of Dai each time, the price in USD, the price in Ether, the balance of the contract and the liquidity of the contract. And we're organizing it by token trade volume, which right here is basically around $600,000 a month getting traded. So right now, actually, Uniswap's main website for info is actually being powered by the graph right now. So this website right here, which is ran by Uniswap because we've worked with them, this is all being produced by the graph. So whenever it goes from block zero to 9 million, it records all those historical data points and allows you to build historical data. And it also has the values down here.
00:20:19.980 - 00:21:16.444, Speaker B: So the values down here for synthetic ETH and Maker and stuff, these are more like Live values as well as 24 hours values. So what we're doing is we pick a day like the zero hour of each day and we pick 24 hours and combine those values and then make day entities. So you can also, instead of querying all time, you can query one week and it shows you like more condensed. See, that doesn't really show you the much, but when you go to three months, you get a little more finer detail and you also get over encompassing values for the whole entire Uniswap protocol. Which is interesting because each Uniswap contract is a different or each exchange is a different contract. So we actually allow you to index those dynamically. So we follow the Uniswap factory contract and then index the 800 exchanges every time a new contract gets created from the factory.
00:21:16.444 - 00:21:32.228, Speaker B: We index that. Start recording that data. We index it. Start recording that data. Yeah, go ahead. Yeah, so essentially it's written in the mappings and that's where it gets very complex. And that's why it took us a few weeks.
00:21:32.228 - 00:22:04.850, Speaker B: But right in here, basically every time an event happens, we check what time it was and if it's within a day and we're keeping track of that day somewhere as well in the mapping. And then every time it crosses that day mark, we cut off adding to that entity because we're adding how much trade volume happened in a day. And then you start a new one and you keep adding them and keep adding them. Yeah, go ahead. Primarily around triggered events. Can you give us an instance or is there anything where you're pulling additional information?
00:22:10.180 - 00:22:27.270, Speaker A: Yeah, so you can bind to the contract in your mapping and call any of your viewable call functions within your mapping, return data from that and then save that into an entity, which is a nice paradigm to use.
00:22:29.340 - 00:22:48.620, Speaker B: Yeah, so, yeah, you can trigger on events as well as function signatures because some functions don't have events, but we added that ability and yeah, you can basically query any ethereum contract at that block that it happened. You just bind to it. You have to have the Abi so it knows and then it just calls it directly.
00:22:49.280 - 00:23:12.500, Speaker A: And we have a few different types of triggers. So there's event triggers, there's call triggers which will trigger anytime an external source calls into that function. So then you'll index and get an event for each each time that call happened. Or you can use block triggers. So your mapping will just run on every single block.
00:23:13.880 - 00:23:14.630, Speaker B: Yeah.
00:23:19.000 - 00:24:10.600, Speaker A: Transactions that maybe don't involve transaction. Yeah, so we're working on an ERC 720 subgraph right now that pretty much just tracks all transfer events without filtering on a contract and that is taking forever to sync. It's a lot of data. So we're kind of actively trying to optimize and make that something that can run in like a week and that's something we're working on right now. But you don't have to filter on a contract. So you can filter on just a signature and then get all contracts that are applying that function signature.
00:24:12.860 - 00:24:39.840, Speaker B: So, yeah, we're offering five prizes of 1000 die for the hackathon. Essentially, if you're building your own smart contracts, you can use the graph and use graph init and start getting the data from there. And we can help you guys with that. You can also just go to existing contracts that exist, like today. Like you could do uniswap, you could do something else. And we have some ideas we could share with you guys as well. And we're just looking for people to either build a subgraph of existing smart contracts or your own smart contracts.
00:24:39.840 - 00:24:56.260, Speaker B: We can help you guys with that. Likely you'd start off with Graph init, but come on down and we can show you changing the schema or anything like that. Go ahead. Like within one of the event handlers.
00:24:57.720 - 00:25:35.510, Speaker A: So we generate a URL every time you make a subgraph, and on that subgraph details page, you have a WebSocket URL and an Http URL that you can then use to query directly in your application. So we recommend using something like Apollo, which is nice for working with GraphQL. And then pretty quickly using our URL, you can write some GraphQL queries and have a front end right away. Yep. So you can subscribe over WebSockets and you'll get the full query response every time new data comes in.
00:25:38.200 - 00:25:40.150, Speaker B: How do you store the data on the.
00:25:41.960 - 00:26:00.764, Speaker A: It'S stored in a postgres database, and we actually just recently updated our structure of how it's stored, so we're using fully relational tables now, and our queries have gone a lot faster just in the last couple of weeks, so we're pretty excited about that.
00:26:00.962 - 00:26:07.550, Speaker B: Yeah. Any more questions? No, that's it. All right, thank you guys very much.
