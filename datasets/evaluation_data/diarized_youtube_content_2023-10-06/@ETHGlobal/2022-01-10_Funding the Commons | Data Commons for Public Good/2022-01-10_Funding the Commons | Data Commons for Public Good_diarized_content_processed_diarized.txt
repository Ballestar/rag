00:00:05.690 - 00:00:46.276, Speaker A: Dawn is professor of Computer Science at UC Berkeley, specializing in deep learning, blockchains and security. Dawn's going to talk to us about building a data commons for the public good, maximizing social welfare and economic efficiency while protect users data rights, and enabling fair distribution of value. Welcome Don. Yes, we can see you.
00:00:46.298 - 00:00:47.430, Speaker B: Can you hear me now?
00:00:48.120 - 00:00:49.556, Speaker A: Yes, see and hear you.
00:00:49.658 - 00:02:31.070, Speaker B: Okay, great, thank you. So let me share my screen. Can you see my screen?
00:02:34.880 - 00:02:39.612, Speaker A: Yes, I can see your screen. The presentation has great, great.
00:02:39.666 - 00:03:21.112, Speaker B: Yeah. Thanks everyone for being here. My name is Dongsong. I'm a professor in computer science at UC Berkeley and also the founder at Oasis Labs. Today I'll talk about data commons for public goods. So one thing I just want to preface this talk with that. I think everyone knows that this workshop is organized, put together fairly quickly, and essentially I think it was I was invited as a speaker and essentially just with I think a couple of days notice.
00:03:21.112 - 00:04:21.808, Speaker B: And initially I thought I wouldn't have enough time to prepare the slides and so on, but I thought the topic of the workshop is really exciting and fits really well with some of the work that we have been doing. So I agreed to give the talk, but there have been emergencies that came up. I didn't quite have the time to really finish prepare the slides for everything that I really want to talk about. But I hope that with the time that we have, I can help give some high level ideas and I think this workshop is fairly exploratory as well. So it's a great opportunity to get a conversation started and I'm very happy to talk to interested audience in more detail after the talk. Thank you. Okay, so as we all know that data is a key driver for modern economy and the lifeblood of AI and the machine learning.
00:04:21.808 - 00:05:23.020, Speaker B: And also huge amount of data is being collected and more and more data is being collected every day. And there is estimated that a significant percentage of the EU's GDP comes from personalized data and the overall global data economy is growing exponentially. But however, a lot of this data also is really sensitive and hence handling the sensitive data has posted unprecedented challenges for both the individuals and institutions. For individuals, as we know that individuals have already lost control over how their data is used. Oftentimes their data has been sold without users awareness and approval. And a lot of this data was sold under the premise that the data has been anonymized. But however, there's volume of research showing that anonymization is insufficient to protect users'data privacy.
00:05:23.020 - 00:06:36.650, Speaker B: So, for example, here is a really interesting case study that New York Times did where they showed that they were able to obtain anonymized mobile phone location data sets, and from this anonymized mobile phone location data set, they were able to track the location of a Secret Service agent with former President Trump, and hence knowing the trajectories and whereabouts of former President Trump. And also the even bigger issue is that there's all this data being collected and they contain really valuable information. But however, most of this valuable data has been locked in data silos. So today actually the biggest issue is that it's very difficult to gain access to this valuable data. It's very difficult to actually utilize this valuable data for public good, for example, for medical research. It's very difficult for medical researchers to gain access to medical data that's needed for their medical research. And similarly also in finance and in IoT many different segments, this has been a huge issue.
00:06:36.650 - 00:07:55.380, Speaker B: And if these issues are not addressed, they will only get more and more severe as we move further forward into the digital era to the extent that it could significantly hinder societal progress and even undermine human values and fundamental rights. And hence there is an urgent need for a framework for a responsible data economy that's different to how today's data economy works and make the needed changes. And here what do I mean by a responsible data economy? What are the main goals and principles? So first we need to establish and enforce data rights. Data rights form as the foundation of data economy and prevent misuse and abuse of data. And also we need to enable fair distribution of value created from data such that users should be able to gain sufficient benefit from their data. And the ultimate goal is to enable efficient data use to maximize social welfare and economic efficiency. Unlike today, when data stays in data silos, it's now being utilized to maximize social welfare and economic efficiency.
00:07:55.380 - 00:08:42.684, Speaker B: And there are a number of unique challenges and complexity due to the unique nature and properties of data. For example, there's natural tension between utility and privacy. And also data has this property of being nonrival. So unlike a physical object, for example, if I'm holding my laptop, nobody else can hold the same laptop at the same time. So only I get to use my laptop. But whereas with data I can hold a copy of the data and somebody else can hold a copy of the same data and both parties will be able to use the data. And also there's data dependency and data externality and so on.
00:08:42.684 - 00:09:46.696, Speaker B: And hence, to address these issues, we cannot simply copy concepts and methods in the analog world to the digital world. And hence, to enable a framework for a responsible data economy, it needs a combination of technical and nontechnical solutions. In particular includes three components technical solutions, incentive models and legal frameworks. So today's talk mostly focus on, I would say, some particular elements in this overall framework. I actually have longer talks about this whole framework, how to build a responsible data economy. But today we are going to focus in particular on the data commons aspects and the relevant parts with the Data Commons for public good. So before we talk more about the data commons for public good, another related aspect that I wanted to just briefly mention is about a little in more detail the personal data side.
00:09:46.696 - 00:11:52.240, Speaker B: So as we know that the personal data also today is largely locked into third parties, are being fellowed into third parties. However, now actually it's a great time for making a change both in terms of data sovereignty and helping users to take better control of their data and better utilization of their data. So there is a number of privacy regulations including for example GDPR that requires to the users the right to data portability. So essentially it requires third party services to provide users with the capability to be able to essentially download and move their data in an easy fashion outside of the particular third party service. With this one can users could actually with the right tools and capabilities could actually for the first time to download their data from the different third party services and put together into what I call this Personal Data vault. And with this also including for example their Facebook, Google data and even for example their Fitbit and other medical records and so on, which has similar portability requirements also. And then with the data vault, essentially one can also build these collectives that I will talk about in a second, these data commons that can actually help pull data together but still enable users to maintain control of their data, specify how they want their data to be utilized, and then enable data to be utilized in a privacy perceiving way and a compliant way and help users to gain more benefit.
00:11:52.240 - 00:13:34.716, Speaker B: So just very quickly, in order to achieve this again, I'll talk about the data commons concepts in more detail in just a minute. But before talking about that, I do want to set up some stage in terms of the technical requirements in order to achieve that. So the high level goal of the data commons is that users or other data producers, even institutions, for example, could provide data. But instead of having the data all kept in its own data silos, by utilizing new technical advancements, we can enable this new secure, distributed computing fabric that enables data to essentially be, for example, governed by a structure called data governance which specifies how data could be used and have the techno mean to ensure the privacy and compliance of the data usage and hence enable this data value to be extracted from this data commons for public good. And also users in the end can also receive benefits from that as well. So given the limits, the time limits, I won't be able to go into the details on the technical side of the different component technologies that's needed to enable this. I just wanted to quickly mention essentially there has been huge advancements in what I call Responsible Data Technologies.
00:13:34.716 - 00:15:26.230, Speaker B: And these responsible technologies help enable data to be utilized in a privacy preserving way, including secure computing, which helps protect the computation process from leaking sensitive information, and differential privacy, which help ensure that the computation output won't leak sensitive information about individuals. And federal learning help users to maintain the data on their own device or machines by enable machine learning models, analytics to be done in a distributed manner. And finally with distributed ledger, it can help provide immutable log to ensure to essentially want to provide the immutable log of users rights to data and how the data should be utilized, and also ensure artificial log for the actual data usage as well. And I wanted to just quickly put this together and give you a quick glimpse of what can happen when we put all these component technologies together and how this can enable a new structure for data commons. So in this case, we have the data providers. Again, these can be individuals as well as institutions in general we call data providers that has data, and the data provider in this case can provide the data in the encrypted form with the policy attached to it to specify how the data provider wants the data to be utilized. For example, the data should only be used in a privacy precision way, for example, to train differentially private machine learning models and also should be used by medical researchers from nonprofits and so on, for example.
00:15:26.230 - 00:16:56.290, Speaker B: And this information can be committed, for example, to a public blockchain. And then you can imagine that essentially there are many of these, we also call them data capsules. So this can be in distributed catalog and the data analyst or data consumer in this case could search through these data catalogs and as it finds data that it wants to use, it can submit a program that it wants to run on the data. And then there's a component that essentially we call it, in this case the access controller, that will then actually run static analyzer to check the required data use policy against the program, and also outputs a register policy, which we don't have time to go into detail. But essentially this static analyzer will check whether this analytics program satisfies the required policy and if yes, it can send a proof of policy compliance to this distributed key manager. And also we have, for example, a trusted execution environment. This can be done in different ways, either utilizing a secure hardware, we can also use, for example, methods as MPC and so on.
00:16:56.290 - 00:18:05.316, Speaker B: In this example, let's say it uses secure hardware, this trust execution environment. So essentially the data will be encrypted, data will be sent into the stressed execution environment, and the key manager will send the jury key inside as well. And the encrypted data can be decrypted and the program can be run which returns the result. But the result will also stay encrypted together with a residual policy and then for example, if the program fully satisfies the desired policies, in the end actually the result will be actually in the clear and the data analysts could see it depending on the residual policy and so on. So depending on the residual policy, the analysts can download the results to query the trend model around additional programs to satisfy the policy and so on. And all this will be also logged on the public blockchain to maintain the audit trail as well. And the different writers can also inspect the blockchain to see how their data has been utilized.
00:18:05.316 - 00:19:32.630, Speaker B: And again today we have the technical means to essentially accomplish all these different aspects. And when we put this together, essentially we can enable what I call here a data commons for decentralized data science. So essentially the data owners, the data producers, they can register their data sets with the policies specified to this distributed data catalogs that can be maintained also on the public ledger. And then the data consumers analysts, they can search the data catalog, identify the relevant data and then write their data analytics and machine learning program and submit it to the platform. And then the platform then can provide the distributed secure computing while ensuring that the program is compliant with the desired policies and then with others. Then essentially this can enable decentralized data science that can help reduce friction of data usage, remove the data sellers and also provide strong security and privacy guarantees. And also in this case as the data consumers and data analysts use the data, it can also provide rewards back to the data producers and also other entities in the overall ecosystem by providing for example, utilizing data tokenization methods as well.
00:19:32.630 - 00:21:02.400, Speaker B: So essentially, for example, the data commons can utilize essentially having pools of data tokens and the data consumer data analysts will need to for example purchase data tokens in order to utilize the data as an example and enhance the original data. Owners and data producers can get rewarded that way. Overall, in ten years I strongly believe that data trust and data commons will become predominant ways of utilizing diverse sources of data, enabling ownership economy where users benefit from their data as owners and partners. And in ten years data stewards, fiduciaries and trustees will be a new class of entities important in the data ecosystem, managing and protecting users data and growing its value. And in ten years huge economic value will be created through these new forms of data traps and data commons alt as a magnitude higher than today's data marketplace. And I know I'm running out of time, so one thing I just wanted to quickly mention is that I think this data comments also allow really interesting new structures and new types of services. So for example, one of the biggest challenges for providing value back to users for their data is that in the end, each user may be only getting a very small amount of value back to their data may not be sufficiently interesting to them.
00:21:02.400 - 00:22:28.812, Speaker B: But however, with data commons when this data is being pulled together then the value created by this data commons can be really significant and in this case with decentralized governance. For example, in healthcare, in medical domain actually such data commons with medical data could actually be a provide means for providing new forms of medical insurance to users who are contributing data, their medical data for the data commons and as a reward, as value received back to them, they actually get benefits from this medical, essentially health insurance and so on. So essentially this gives a really interesting new ways how to empower users and also for public good to set up these data commons also and also I didn't have time to talk about some of the other projects that we are also working in the process. For example, the data commons is now just limited to fully to the type of sensitive data that I mentioned. Essentially we hope that in the future the data commons from users data, it can also train these machine learning models that can be another form of public goods and another product from these data commons. And even in the future we hope to be able to create knowledge graphs that are being created by users together and as a form of data commons as as well. Thank you.
00:22:28.812 - 00:22:30.830, Speaker B: I know I'm running out of time.
00:22:34.480 - 00:22:44.280, Speaker A: That was great. Thanks to dawn for giving us an overview of the landscape of responsible data technologies and some really new great exciting opportunities in that sphere.
