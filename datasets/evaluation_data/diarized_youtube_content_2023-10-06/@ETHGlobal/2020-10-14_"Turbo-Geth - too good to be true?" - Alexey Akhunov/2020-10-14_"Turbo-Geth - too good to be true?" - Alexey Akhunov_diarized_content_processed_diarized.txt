00:00:00.570 - 00:00:19.710, Speaker A: And next up we have Alexi talking about triple get and he'll just be covering sort of all the latest improvements and performance updates that have been added to the triple get project. And he is in this room, so I'll let him introduce himself and kick off with the presentation. Welcome, Alexi.
00:00:21.410 - 00:01:00.858, Speaker B: Hello. Hi Karthik. Thank you very much for the introduction and yeah, so I will let me just start the screen share. I will sort of say a couple of words about let me just figure it out. Yeah. Okay, so first little bit about myself. Basically, I've been a programmer for pretty much all my professional life and for almost last three years I was working on this particular software just called Turbogeth.
00:01:00.858 - 00:02:13.030, Speaker B: And what turbulet is, is basically it started as a fork, as a fork of go Ethereum and sort of experimental software with certain goals to basically make it a bit more performant if possible. But now it's becoming more and more and more different and we're going to see in our talk how and why. And the title basically is too good to be true, is addressing something that I haven't thought about until very recently, is that whenever we present some kind of performance numbers for Turbogeth, most of the time the first question I get, where's the trade off? Like what's the catch? Because people think, well, if we do something better, then we have to do something worse, basically. That's what I want to address. So here's the picture about the, in the databases. First of all, I look at Ethereum as a database. Essentially it's a distributed database with certain properties.
00:02:13.030 - 00:03:17.270, Speaker B: But in the databases, essentially you have usually like three points between which you're making trade offs. So update efficiency, space efficiency and access efficiency. And sometimes, of course, the space efficiency helps all others and you can kind of move around and trade one for another. Usually you might have seen those triangles as well in different areas. And so people normally think about moving in this little plane which is the circle underneath and so this is where you're going to get trade off. Like, oh, if I get more efficient in access, I must be losing something in some other places. But what actually happens in technology all the time? And if you look at it, there's always technologies which are strictly improving, right? We have seen this many times, for example, move from SSD to HDD.
00:03:17.270 - 00:04:17.098, Speaker B: First of all, the SSD were more expensive, but now SSDs for considerable volumes, they're not so expensive anymore and they're pretty much better at everything. So nobody's asking what is a trade off between SSD and HDD, at least not anymore. So this happens all the time in technology. And usually the way I see it is that the previous technology is a bit further apart away from these efficient frontier where you can do trade offs. And so usually you want to get closer to the efficient frontier and that normally requires some kind of paradigm shift and the paradigm shift usually also kind of goes against the common intuition. So we're going to look at this. So what kind of the paradigms did we have to shift here? Which basically I posit that we did get closer to the efficient frontier.
00:04:17.098 - 00:05:09.674, Speaker B: So there are a little bit of trade offs but not many. So next time that's why every time people ask me the questions what is a trade off? This is what I'm bringing up. So we're going to be talking about the paradigm shift in the data model where we replace trees with a flat data. We're not going to be talking about history, the data model for history because it's another complex topic and I'm not going to cover it today. But we are going to be talking about other things like concurrency and architecture. So with the concurrency is essentially whether you do homogeneous concurrency essentially when you're trying to run many things at once but they're pretty similar or do you want to run like lots and lots and lots of different things at once. That's what I call the heterogeneous concurrency.
00:05:09.674 - 00:06:16.530, Speaker B: And in terms of architecture here basically the shift is from monolithic system with a lot of cross cutting concerns done in the name of optimization. We're moving towards the modular system with the separate concerns which I posit as well is more optimal. So we'd start with the data model. I thought about this for a very long time. Like where does this come from, where does this data model come from that everybody's using? And some time ago I saw that it actually probably comes from the yellow paper and again it comes from a sort of common intuition that yellow paper essentially calls this particular way of storing data that most implementations do right now, a sensible implementation or a reasonable implementation. And it says that it's mimizing the function C, which is essentially function C is mapping the nodes in that sort of state try into their sort of hashes or something like that. But actually it should probably be the inverse.
00:06:16.530 - 00:07:55.922, Speaker B: So although this particular paragraphs, these paragraphs are not normative, they already create this sort of they basically based on somebody's intuition or some sort of common kind of expectation and therefore everybody assumes, I guess everybody assumes this is how we should implement this and this is where it goes. So back in 2017 when I just started this project, I started with Profiling. Go Ethereum and I bumped into this thing which was 20% of my profile and this is exactly accessing this memoized inverse of a function C. So essentially given the hash you need to figure out what is the node, the tree node is. In the previous slide we were talking about recommendation to memoize this function and here we are actually using this memorization and that was from my point of view, that was a performance bottleneck and this is where all that started and this is the picture you probably saw many times, some of you. So on the left is basically is my very simplistic representation of the state try where you have some elements which are v one, v two, v three, so forth and they are built up into this kind of sophisticated, I would say state try. And the arrows, the black arrows in that tree on the left are those function like memorizations of this inverse function.
00:07:55.922 - 00:08:55.510, Speaker B: So given the root hash I can get this root element and so forth and I can navigate through. So for example, if we wanted to find element v five so we need to basically access this memoization 1234 times. So that's basically four access to the database if you use this memorization as a data model. And what I noticed straight away is that I couldn't parallelize this because there is a data dependency there. So you don't do the second arrow until you resolve the first one. So there is necessarily data dependency and no possible parallelization. And on the right you can see how this minimalization kind of is laid out in the database if it's a key value store here's another point is that essentially the deeper the tree, the worse the problem is becoming.
00:08:55.510 - 00:10:32.280, Speaker B: But why do we actually have to do this at all? And it turns out this is all about computing the state root hash which we need to either if you are a minor you have to place it in header or if you are just a verifier of the block you have to compare what you've computed with what you got in the header. So all of this complexity is simply to verify the state with hash and this is actually one of the interesting innovation of ethereum compared to bitcoin which essentially commits every block we commit to the state root. And this is actually quite a good interesting feature but it has a certain cost and apparently so my opposite that the cost of implementing it in the old way is actually very high and it's unnecessary high. What does the EVM think about it? So if we look at it from the EVM point of view so if you looked at what is the EVM opcodes are they actually have no idea about the try. So they don't actually look at the try, they look at the account addresses and they want to fetch balances bytecodes or storage and stuff like this and also some instructions for storage loading. Again, they look at the addresses, they look at storage location. There is no mercurialization here at all.
00:10:32.280 - 00:12:02.770, Speaker B: Basically what the hypothesis was that okay, having the state stored in a radix tree as a primary structure it tries to satisfy this sort of computation of the state root which I think is quite the minimal thing but it sacrifices the efficiency of EVM access. As you saw before, it requires the sequential data dependent access. So is the sacrifice worth it? So essentially we sacrifice EVM efficiency and to be able to compute these state root hashes quickly and that's what yellow paper calls sensible or reasonable implementation. So yeah, that's what I've been working on for last two years is that trying to figure out if this is sensible or reasonable or could it be that this is a false trade off. And now I am convinced that this is a false trade off because it is possible to do both things efficiently compute the state route and have efficient EVM access. But how do we do that? So we know if we have a flat data structure. I don't need to explain you how to provide efficient access from EVM because it's clear like if you go back to the slide, if this is exactly how we store the data EVM, simply accesses it in this form.
00:12:02.770 - 00:13:00.262, Speaker B: So I don't need to tell you how this works, but I do want to tell you how the other part works. So given the flat structure, how do we calculate the state root? So yeah, if we know the structure of the tree then we can do that and of course you can try to build up the tree and then compute the root. But obviously that is going to be quite memory intensive. What you could do better is that you can start some kind of stack and then you need to calculate the tree, let's say from the left and then you only keep the stack of 1234 or five hashes and then every time you move on you are accumulating sort of the hashes on certain level. So you can basically compute this whole thing with a very limited memory. So with about five accumulators for hashes and so forth. But that's still quite large.
00:13:00.262 - 00:14:25.710, Speaker B: I mean, I can tell you that for example, it says NSSD. It might take you, I don't know, half an hour or something like this or sometimes an hour to get depending on the speed to essentially iterate through entire state and compute the state root. So although it's sort of relatively quick but it's not as quick as you can't do it for every single block, right? It's too slow. And so the problem then becomes is that, okay, what if we had a root calculated and maybe we cached part of that state tree in memory but because we don't store it in database, once we flash the memory it's gone. And then we needed to mod in our block, we needed to modify this green thing and then sort of like how do we actually calculate the new state root? How do we do that? So originally this is how I was doing it. I would essentially load the entire region of the flat structure, the entire one and then I would calculate the route and that kind of worked as long as my kind of cache of the state was large enough and in the beginning it was terribly slow, like on a startup. But then it was also if the block started to hit some cold spaces in the state, it was quite slow.
00:14:25.710 - 00:14:58.230, Speaker B: But I sort of thought, okay, I'm going to solve this problem later. So I was running with it. I was saying, okay, I know that it's really crap right in the moment, but I know we're going to solve this because I had this idea how to solve it. So this next slide shows how we're going to discard this data because we just used it to compute the state root of that little subtree and then we discard it. Okay, so the solution that I was thinking about all that time, but I just didn't have time to implement it. But now it's all implemented, by the way. So now it's all done and dusted.
00:14:58.230 - 00:15:36.406, Speaker B: So we going to store what I call the intermediate hashes. So you might think that, oh, but this is actually going back the direction of these trees. Actually, no, because in the tree that yellow paper suggests to do, we are mapping the hashes into the nodes. But here the mapping is kind of opposite not opposite, but slightly in reverse. So we're mapping the prefixes in the trees to the hashes. So essentially the function that this particular structure serves is simply to have those hashes and we store them in the database. Yeah.
00:15:36.406 - 00:16:30.060, Speaker B: And the storage is also I would say that it has nice properties because it's got to the data locality and things like this and it's also pretty small. So now if we need to solve exactly the same problem, if we have this green thing that we modified and we don't have the pieces of the trees around it in our cache, so we can use this structure, new structure that we introduced to help us to bridge the gaps. And so that's how we do it. We load only those things that I pointed out with the arrows from the database instead of the entire region. And here we go. And we can basically compute our new state route. So this has been implemented this year quite recently, I think, about April or something, or May, and it has been optimized multiple times.
00:16:30.060 - 00:17:16.754, Speaker B: So now I show you what is the cost of that, right? And also I'm going to show you how our storage looks like. So the additional structure that I was talking about just now is called intermediate hashes. It's currently, for the reasonably recent state, is about two something gigabytes. So actually it's not a bad price to pay for this, right? And it is the structure which essentially stores all the intermediate hashes for all the levels of the street and stuff like that. And so you can also notice that there are two structures for the state. That's another innovation which we introduced quite recently. So we stored the state twice.
00:17:16.754 - 00:17:49.314, Speaker B: But why? So first we store the state as a plane structure which is currently 9.9gb, 9.9gb maybe a bit more right now. And then we have a hash state. So some of you may know that in order to keep the tree balanced we need to apply some hashes to the keys before we put in the structure. So we separate these two things. So EVM actually works with the plain state and then in order to produce the state root again, only then we use the hash state.
00:17:49.314 - 00:18:40.734, Speaker B: So we convert it to the hash state and then we compute the hash there. But we have managed to get rid of the there was another table here which is called pre images. So because we have the two states we actually don't need the pre images and there are some other nice properties about it. For example, it allows us to withstand certain Dos attacks easier than other implementations. As I said, I'm not going to touch on the history storage but it's also pretty efficient. And so all in all the efficiency in storage actually translates into efficiency of access as well because things are simply smaller and then they're actually more kind of standard so you can look them up quicker. So now we go into the second paradigm which is the concurrency.
00:18:40.734 - 00:19:52.540, Speaker B: And so here is something which kind of was discovered again this year is that on the top you basically could see how the things were processed before in Go ethereum and in Trubageth before that time. And you can see that when we download the blocks there are let's say some sort of stages of processing. For example, we do recover signature sorry, we recover senders from the signature so forth and we do certain things like run EVM through it and compute some storage route or whatever it is. And so usually this happens sort of in parallel with this kind of staggered concurrency and lots and lots of things happening in parallel. And so what we did is that we decided to split them up like this and of course intuition would tell you that this is actually slower. If you just look at this graph diagram it's like oh no, this is definitely going to be slower. Actually it probably was a bit slower in the beginning but then because we managed to split them into more homogeneous pieces we were able to actually make the individual pieces faster and in the end everything was much faster, maybe like ten times.
00:19:52.540 - 00:20:49.462, Speaker B: Again, this is another paradigm shift which you go into the counterintuitive direction and then you arrive at a kind of next level result. And so this is the stage sync. So what basically happens here is that we are trying to process especially when we do the initial sync, we try to process things in large batches, as large as possible. So for example, if I were to sync right now, my first batch would be more than 11 million blocks. So I first put this 11 million blocks through the first stage and then another million in the whole batch through the second stage and so forth. And so because we do that and so in the red I just put here some text, is that because we're putting most of these stages are actually essentially data transformation. They take the data from one table and database, they do something with it and put it in another table of database.
00:20:49.462 - 00:21:43.294, Speaker B: That's why I'm calling Ethereum a database, but with the databases that we use is that it's actually easier, it's actually much more faster to put that stuff in the database if it's pre sorted. So, in other words, database are pretty you should not use them for sorting. So there are much better sorting algorithms. What that basically graph means that we can use that property to speed up the sync quite a lot. But if you, let's say, process one block at a time, then the time is not going to be so impressive. But it would be very impressive if you start processing millions block million a block at a time. And this also explains why our implementation actually could work with HDD sync, which hard drives actually, one is currently just on a finishing line right now.
00:21:43.294 - 00:22:26.838, Speaker B: I started almost 30 years ago, sorry, not years, 30 days ago. Sorry. And today is finally catching up with the tip of the chain. But when it catches up, it's not going to be able to process every single block through the stages. What it will do, which I tested before, is that you see this intersection here, for example, for my AGD, it will be about 15 blocks. It means that it cannot process one single block quicker than 13 seconds, but it can process 15 blocks approximately at the same speed as these 15 blocks are produced on the main net. And so this is where we find the intersection.
00:22:26.838 - 00:23:14.474, Speaker B: So my HD sync would be always having a lagger of about 15 blocks, but for some application it's probably all right. But as I say, that because SSDs are so much easier now, so much faster. And then because we don't require you to have five terabytes, so you can actually buy a pretty cheap SSD and run it, so you don't need HDD. So it's pretty much for academic purposes at the moment. And I think by the time our size grows to two terabytes, you're probably going to be able to afford SSD at the same price as it's now for 1 TB or something like that. Anyway, so then we go to the third paradigm shift, is the architecture. So I put the note here about core dev calls in July 2020.
00:23:14.474 - 00:24:34.580, Speaker B: Some of you might have listened to those calls. So for three calls in a row, we decided to not discuss the IPS for Berlin, but talk about other issues, about kind of wider issues and we talked about the developer burnout, we talked about development process and things like this about the client. So maybe some people think it was completely useless but actually I thought it was really good and I learned basically I found it very informative and I made some conclusions out of it. One thing we debated on the call is that whether the modularization and splitting things in component would actually help. And so there were opinions on the call with saying that if you start splitting things up you will lose the optimizations because there was assumption that optimizations have to be cross cutting and they basically tie everything together and then eventually it becomes a big spaghetti. But my experience is actually the opposite and there was one person on the call actually agreed with me and so this is where we are on the journey to make this happen. And so as you can see here is that one thing is already done, two things in progress, one thing will be started and actually on the right here is that the stage sync is also part of that.
00:24:34.580 - 00:26:00.062, Speaker B: So it's also kind of componentization. One of the interesting things about this architecture is that it hopefully will allow us to bring more developers into the project and into the core developer development in general. It does actually also lead to certain sort of abdication of power from the core developers but it's a bit larger topic and I would probably discuss it in somewhere else in kind of more long form whatever chats and podcasts and stuff like that. Then the last slide, which is a bit of a you probably heard about or seen me talking about ten x improvement in ethereum, and today I just realized that oh, why is it can't we go from ethereum one x to ethereum ten x or some x maybe two x if we actually take it seriously? Then there are, from my point of view, three main challenges here. So it's a nice goal to have of course to do ten x and layer one. But there are three main challenges which I sort of described here. I'm probably not going to unpack all of them for you, but they're dos attacks and there are certain dos attacks we know about and some of them are because of the state access but some of them are computational, some of them are related to precompiles and that's thanks to vitalik to pointing out to me.
00:26:00.062 - 00:26:30.860, Speaker B: So we're currently actually assessing those things. I mean I know that some groups are doing the same thing but we're assessing it in the context of Turbogeth. We are finding more attacks and we're trying to figure out how to protect against them and so we just recently started this effort and then there are other two things that people are worried about but I think they are also solvable. So on this point I think I'm going to end my presentation and I don't know if you have any time left. Sorry, guys, I think I might have.
00:26:33.310 - 00:27:02.580, Speaker A: Thank you so much, Alexi. We do have a few minutes for questions, and we actually have a lot of questions coming in. So what I'll do is I'll just kind of ask a few here and if we end up running out of time, I'll just ask you to join the live chat on Live eonline.org, and you can just answer them directly there. All right, so handful of clarifications, a handful of broader, bigger questions. The first one being, does double hashing attempt to address any long term state degradation related issues?
00:27:04.790 - 00:27:12.998, Speaker B: I'm not sure about the double hashing, but I don't know. I'm not sure what it means. What is meant by double hashing, to be honest. Sorry.
00:27:13.164 - 00:27:34.510, Speaker A: I think this comment will reach the audience in about 30 seconds, so I'll just have them see and clarify this. But in the meantime, I'll just move on to the next question. And that is you kind of talked about that it's taken about three years to kind of get where you are. Purely out of curiosity, what was like the most surprising breakthrough in that time from kind of your perspective?
00:27:35.010 - 00:28:24.110, Speaker B: I think the stage sync was the most surprising thing because I kind of thought I discovered very early on that data model would be beneficial. It's just that with the data model, essentially the reason why it took so long is because it's sort of like in engineering, you solve one problem, but then you get another two problems to solve, and then you solve those two and then there's another three. And so I wasn't sure whether this sort of I will be able to tie all the loose ends in the end. And then I only figured out like last year, that we will be able to tie all the loose ends and it will work, but generally I thought it would work, but with a stage sync, for example, or now with a competent component. That was quite surprising to me that such simple things actually make such a huge impact.
00:28:25.810 - 00:28:35.810, Speaker A: That's awesome. I guess maybe a follow up on that. How many people are currently kind of helping you with being a contributor to Turbogap?
00:28:36.310 - 00:29:28.770, Speaker B: Right. So actually the team has grown quite quickly and so specifically this year, so we currently have about 13 people who are basically working on it, like ten full time, three part time. And most of the time I'm losing count because I'm actually inviting people all the time. And the reason I'm inviting people more all the time, because I do want to test this idea that if we do things in components, we can have many more contributors, which are sort of they're not just waiting for the pull request to be merged but they are there to develop their components maybe on multiple different implementations of the same component and so forth. So I'm actually opt for testing the theory that we could have a much wider contribution.
00:29:29.270 - 00:29:49.990, Speaker A: No, it's awesome. It's a wonderful problem to have when you're scrolling too fast and you get to track who's fully contributing. So that's wonderful. Another question that sort of came from the answer you just gave, and that is, what was the actual original intuition that sort of made you even try stage think in the first place? Was it just experimentation?
00:29:52.490 - 00:31:10.290, Speaker B: Yeah, that's a good question because what I was so basically I was profiling Tour Brigade all the time at that some point I got to the place where the rights to the disk were the bottleneck at some point and obviously, as I normally do that, I started to split. Luckily, the database we had allowed us to look at, I've called the right chore how much data you write for each table rather than on the entire database. And I noticed certain tables were very intensive in terms of writing and so I saw that those were the ones that they had a very sort of randomized keys. So for example, the transaction lookup one where if you have a transaction hash, you can figure out which block this transaction in. And obviously the keys being a transaction hashes are pretty randomized. And so we were inserting those randomized hashes within the middle of the table and I was scratching my head thinking maybe if we try to presort them, if we just take the whole bunch of them, like the 900 million of them presort from them by hash and insert them that way, is it going to be faster? And it turned out to be faster. And this whole thing generated essentially the stage sync.
00:31:10.290 - 00:31:35.500, Speaker B: It required a bit of a bravery because in order to push it through, we had to delay the release and we had to rip everything apart and then change the architecture entirely in about two months. It was actually kind of risky thing to do because some people said, no, you shouldn't do that. It's just like release and then change it. But no, if I release, I'm never going to change this. I have to do it right now. And I think it sort of paid off.
00:31:36.110 - 00:32:08.630, Speaker A: Absolutely. High risk, high reward, and glad that you did. Last question for today before we move on to our next talk. And that is it's a pretty big core value in our space that people should be able to run and kind of individually run invalidate nodes, especially from the Genesis block. So what are your kind of thoughts on this as a norm? And is one of the motivations behind Triberget to actually make this world a lot more possible or is it purely.
00:32:09.210 - 00:32:44.250, Speaker B: It wasn't started for this reason. I mean, I simply wanted to create a better technology. I don't know if it's going to be the norm because maybe if we do like, let's say it's ten X, maybe we will. Make it impossible again to essentially we improve the technology, but we use it not to let everybody run a node on their Raspberry Pi, but maybe we're going to use it to expand the boundaries of the system instead. Right. So I don't know. It's not for me to kind of make this sort of into ideology.
00:32:44.250 - 00:32:56.050, Speaker B: I think I just want to make a better technology and we'll see what it's going to do. Is it going to make people run Raspberry PiS or is it going to make times bigger?
00:32:56.130 - 00:33:18.850, Speaker A: Right, that's wonderful. I think you're taking the stance of being a technology maximalist and that's awesome. So there's a few more questions that are in the chat, so I'd encourage you to just kind of reply in the chat directly. And I want to thank you again for doing this talk and answering all of our questions and excitement on the chat. So thank you so much, Alexey.
00:33:18.930 - 00:33:19.538, Speaker B: Thank you. Bye.
