00:00:00.650 - 00:00:45.962, Speaker A: Hi everyone. Can you hear me? Okay, cool. So I'm going to be talking about scalability solutions specifically from Snarks today and hopefully I can say something interesting. So when we're talking about okay, sorry. Can you hear me now? Yes. Okay. So when we're talking about trying to scale blockchains using Snarks, one of the classic ways you would do this is you would have lots of transactions, which typically if you were to just use them normally, every single person on the network would have to verify that every single transaction was correct.
00:00:45.962 - 00:01:41.338, Speaker A: They would need to have all of the data from those transactions in order to do so. And this just makes things not scale very well. If you have a proving system, what you can do instead is you can have some third party who takes all of the transactions, they do know all the data and they do have all of the information which they need to verify. But rather than post all of this information to the blockchain, what they are instead going to do is they're going to aggregate the information together and only provide a much smaller piece of information which you need in order to verify all of the state changes in one go. You can't trust this person. Of course they might cheat, but what the Snark lets you do is it lets you actually check that they haven't cheated. You get to just check a short proof that says if the aggregator has behaved honestly, then this proof will pass and if they haven't, then it will fail.
00:01:41.338 - 00:02:43.110, Speaker A: So there's no trust involved at all. And definitely people have been already excited by the possibility this would have, in terms of scaling vitalik at some point wrote a blog post where he was saying that you could get it down to 500 transactions a second. Might be a little bit of an overestimate, but still. And since then, people have been working on something which is called ZK roll up, which is doing precisely this. It's saying if we have Snarks, are we able to compress down this data? Okay, this isn't so much related to Ethereum, but it is a project that I am really excited about. It's called coder. And what this one is doing is it's saying not only can we verify that the state change is correct, but we can also at the same time verify that the previous state change was correct.
00:02:43.110 - 00:03:22.188, Speaker A: So you can have a snark of a snark from the previous state change and then a snark again of the next state change when the next sort of validator comes along, or aggregator. Sorry. And what this means is you can get into a situation where in order to verify the whole block sorry, the whole blockchain, you can just verify one snark. I mean, you need to know what the current state is. So you will have all of the addresses that you need to sort of check and the balances corresponding to them. But actually checking the history is something you can do just like that. EY is currently working on a scaling solution.
00:03:22.188 - 00:04:40.990, Speaker A: Also, Loopbring, I'm aware, has recently gone live with a solution, and Matterlabs is working on a solution as well. And this isn't something which you can only do with Snarks. There are other solutions, specifically Starks, which have different trade offs. They are much bigger proofs and they don't scale so well for smaller statements. But equally you get some quantum guarantees with them and you don't have any trusted setup issues. Have I missed any scalability solutions here? Is there anyone in the audience that is really offended because I haven't mentioned their project? So if putting this sort of more in a snark way, rather than talking about blockchain and transaction and all of this, what we instead have is we have a computation, we have a description of a computation which is known to the verifiers and it's known to the prover. And the prover is going to compute the computation, they're going to output the output of the computation, and they're going to also output a proof that this is correct.
00:04:40.990 - 00:05:30.200, Speaker A: And this is worth doing if you have lots of verifiers, because then every verifier can take that same proof and that same output and not have to compute it for themselves. The property we need more than anything else in these proofs and the hardest thing in order to achieve is soundness. We really need it to be the case that if the result is wrong, then no prover is able to convince any verifier. Another thing that you can achieve as well is a property called zero knowledge. This is actually much easier than the soundness property. And for zero knowledge, what you would have is you would have that the verifier learns nothing about the input of the computation, they just learn that the output is correct. So this is very useful for privacy preserving solutions.
00:05:30.200 - 00:06:26.770, Speaker A: And yeah, soundless is harder than zero knowledge because if you're working with very, very small proofs, there's only so much information you can really reveal. Okay, the downsides of things that are making this quite actually hard to achieve at the moment is mostly prover computation. So we have a situation where if we're trying to compute the proof in order to give it to all the verifiers, sometimes actually computing that proof would be more expensive than all of the verifiers checking the proof, which is not great, we want to get that much faster. And this is more schematic talking about how you'd insert snacks into the roll up. You have your state change. You say, this is what my state change is. You still have to specify that.
00:06:26.770 - 00:07:27.246, Speaker A: But any information that you might need in order to check the state change, typically things like signatures you don't actually include, you just keep those in the snark and you verify the Snark. So the easiest way to do this. And the one that people are starting with is just with schnor signatures. And these are the most commonly used signatures, I would say anywhere for just sort of saying this transaction is correct. So what we want to do is we want to make it so that rather than everybody checks the Snore signature, the proof checks it. But there is a challenge in this approach. The main one being that the hash functions which we're using inside Snore, sha, two, five, six, are really expensive to compute inside a Snark.
00:07:27.246 - 00:08:34.760, Speaker A: Like, you're talking hundreds of thousands of gates, which ends up making your prover just explode. There are cheaper hashes such as Mimsi and Poseidon, but the problem with these hashes is currently we have asked people that work on hashes is it okay to use these? And they've said, no, this might change maybe in a few years, once they've had more time to look at them. They'll say, Actually, we haven't broken them, maybe they're fine. But for now the answer is it's a bit risky. And there have actually been a couple of attacks already on Mimsi, none over the prime ordered fields in which they're being used. So they're more sort of breaking things that are Overfields of size, that are described by something of two to the N rather than defined over a large prime. We don't yet know whether the attacks might extend to prime fields, but they might.
00:08:34.760 - 00:09:40.500, Speaker A: So one thing which would be quite useful for rolling up Shin or signatures would be to use a universal Snark. And in a universal Snark, what we mean is that the same setup. So how many people in this room have heard of trusted setup? Lots of people. Not quite everyone. In a trusted setup Snark, you have a situation where you have many participants who take part in something called a multiparty computation in order to produce the reference string, the parameters which are needed to verify that your proofs are correct. And if you have it that a single party who has taken part in this MPC is honest, then the whole thing should be secure. With a universal Snark, though sorry, with a typical trusted setup Snark, though, the issue here is that you might have many different applications for which you want to be running your Snarks over.
00:09:40.500 - 00:10:22.738, Speaker A: And if you have to have a different setup for every application, then that becomes very difficult to coordinate, it becomes difficult to check that the outputs are correct, it becomes difficult to get people involved. And just in general, it is less flexible, it's less easy to fix bugs. If you have a universal Snark, you can use the same Snark, sorry, the same setup across many different applications, but you do still have that initial trusted setup layer where things could go wrong. I would add here that if you're building any kind of implementation of any cryptography, things can go wrong. We're not perfect. We're human. Mistakes get made.
00:10:22.738 - 00:11:16.872, Speaker A: So it is worth considering, if a bug does happen, if there is a mistake, how are people going to tell you about it? How are you going to respond to it? Is it something that you will be made aware of just in terms of best practice? This is what you should be thinking. There have already been a couple of setups for Vasa online and which you can use. Aztec has done one, and that's over BN two, five, four, which is the curb that Ethereum supports in its pre compiles. So if you do want to use these setups, then you don't need to do your own. You can use those ones. I have recently been trying to implement Marlin, which is one of these universal Snarks in Solidity, which I'm hoping that at some point people will be able to use. I mean, so far, it's just toy code.
00:11:16.872 - 00:12:06.064, Speaker A: It's not nowhere near production ready. And I have the Solidity code, and I can input a proof and it's verifying, and it seems to be okay. However, I am running into some problems, and my problem is that I'm trying to implement the prover in Python because I'm not somebody that wants to learn Rust. And in Python, there's a library called Pyecc, which is very easy to use. You just have simple operations, add, multiply pairing. But that multiply operation is taking me zero one seconds, and I need to do something on the scale of a million of these, which is really, really not doable. Like, I don't mind if it's slow, because this is just a reference implementation.
00:12:06.064 - 00:12:50.528, Speaker A: I'm assuming somebody that does know Rust will take it away and make it faster. But if it's taking zero one seconds, I can't even run it in order to check that it works. So if anyone knows how to maybe have some sort of back end C code which will make that one operation faster, this would be really helpful. Okay. The classic way to do this, which I found out with some quick googling, is to use Siphon. The thing that's unclear to me here is how do you represent uint two, five, six types? Because these are happening all over when we're talking snacks. And maybe you can represent them in Siphon, but it is not easy to figure out how.
00:12:50.528 - 00:13:55.176, Speaker A: For me, maybe it's easy for you, and if it is, please tell me. Okay, so we can not only roll up signatures, we can also roll up Snarks. I mentioned earlier that coder was attempting to do this. So many smart contracts, especially ones that are privacy preserving, are not just using, here's the state change, here's my signature. They're using some kind of Snark in order to say, I have done this computation correctly and please don't reveal my identity. So in this situation, you want to be able to approve a Snark that many Snark proofs have verified correctly, and there are some great ways to do this, which rely on curves within the curves, the three that I'm mentioning. Well, there's MNT curves, and these are good because you can have like limitless recursion.
00:13:55.176 - 00:14:30.360, Speaker A: You can have a Snark of a Snark of a Snark of a Snark of a Snark. There is a recent paper called Halo, which was by the Zcash team. This doesn't work over pairings. This works over just normal discrete log groups. And it's quite exciting. And there's also Zexi, which just allows you to do one layer of recursion, but which is a bit faster than MNT curves. The downside with these MNT curves really is that in order to get the same level of security, you end up having to have much larger groups, which means large approvers, larger verifiers, larger everything.
00:14:30.360 - 00:15:07.500, Speaker A: Again, unlimited recursion is really cool, so sometimes worth it. Like I said before, Halo doesn't use pairing groups. And one thing they've also innovated is this idea that you can have two cycles, so one parent group and one discrete log group, and you can sort of switch between them. That's not what they're doing. They're just going from discrete log groups. Zexley was more designed around privacy. So what they're trying to do is they're trying to hide specifically which operation is being carried out.
00:15:07.500 - 00:16:05.708, Speaker A: Hence why they only need one layer of recursion. So this makes it more expensive than curves, which require no recursion at all, but certainly cheaper than curves. With the MNT curves on ethereum, we really can't use either of sexy or MNT curves because there's no pre compiles. And the fact that we don't have precompiles just means that it would be far too expensive for the Verifier to run. Yeah, we can't do it. The other downside to this, even if we did have the curves working, is that when we're trying to represent the Snarks, what we're doing is we are taking our Snark Verifier and we are converting it into a list of sort of constraints. And this conversion process is actually really expensive.
00:16:05.708 - 00:16:50.020, Speaker A: You end up adding on, I'm saying more than 10,000 overhead. Actually, it's much, much more than that. That's an underestimate. And the result of this is the thing I was saying before. Our provers are expensive enough that people are talking about, oh, maybe if we have a massively parallel computer or maybe if we have an ASIC, we can get this to work. But currently it's just lagging behind the speed that we need. One alternative solution, which we've introduced recently, is you can use something we're calling an inner product argument argument which works in the target group in order to represent Snark Verifiers.
00:16:50.020 - 00:17:57.000, Speaker A: So this overhead is not as good as six, but it might be as good as ten. It's really sort of the prover is fast, it's not perfect. In particular, one thing that we cannot cover is we cannot cover Snark provers that are universal anymore because every universal scheme that we have, which is efficient enough, has a random oracle. And random oracles are something which is just not supported by this scheme. But with a trusted setup such as snarks that are using Grot 16, we can get it to work and save prover time and still get pretty good verifier time as logarithmic. The downside to this one is that we need target group operations. So target group operations are something which you can which are described how to implement in a guide to pairing based cryptography.
00:17:57.000 - 00:19:44.400, Speaker A: They're fairly standard, but currently, because other schemes haven't really been using them, they're not included in the pre compiles. So this means that if we can get a pre compile through, which says here's some target group operations for any curve, so it doesn't matter what curve, as long as there is one, then we can use this technique. But if we can't get that pre compiled through, then we can't use that technique. Additional considerations that we should be considering if we're doing sort of roll up styled projects, first one, if there's any one aggregator, how are you going to avoid censorship? There might be good answers to that, but need considering. If there are many aggregators and your snark proving is expensive and the aggregator has to pay the gas costs, then you need to incentivize participation in some way. And if you're going to incentivize participation, then you might very well be having the aggregator being paid by the people that have taken given the aggregated transactions. And then you have to think, is this transaction also going to be privacy preserving, if that's what you're using your aggregator for? Or is it the case that by paying the aggregator you reveal your identity? Are there any more considerations that I haven't considered? Okay, I realize that I've gone hugely under time here, so I have lots of time for questions if there are questions.
00:19:44.400 - 00:19:50.050, Speaker A: Cool. Yes.
00:19:50.920 - 00:20:25.724, Speaker B: Are there fundamental theoretical reasons why a more start friendly hash should be hard to design? Or is this more about we usually know that hash functions are good because they've just been around for longer, but you might think like, oh, well, this is simpler in some ways, and it's simpler in the sense that it uses certain operations more versus less, like Mimsy or Poseidon. But yeah. Are there theoretical reasons why you would expect it to be hard in the long run to design a hash function that would be friendlier or is this a timing thing?
00:20:25.922 - 00:21:18.290, Speaker A: I think possibly a bit of both. The real answer is that we don't know yet. But if you're thinking about doing things like sha two five six, this is running largely over binary fields. So every operation which your person that is computing the hash is doing is really, really cheap. Whereas if you're trying to do something over a prime ordered field which is suitable for building snarks, then every single operation that you're trying to do is going to be really expensive. And this means that the number of operations that you can reasonably expect people to do is also much less. So any hash function which is going to be secure in this setting for similar computation costs, you're going to have to argue that that prime ordered field is actually giving you some form of security.
00:21:18.290 - 00:21:31.140, Speaker A: And currently maybe that's true or maybe there's some subtle Grubner basis attack that people can use which would actually negate the benefits of having the large prime.
00:21:33.000 - 00:22:05.704, Speaker B: I guess my assumption had been that it was unfortunate that shot 256 didn't work well, but maybe expected. Like if you take the full space of good hash functions I don't have a good reason for this, but it feels like most of them probably aren't compatible just out of chance. But how much of this is like chance versus, like you said, the inherent there might be some inherent structural stuff with prime using prime fields that actually just makes it fundamentally hard to build a good hash.
00:22:05.752 - 00:22:32.920, Speaker A: And I know you don't I'm not an expert here, but yeah, I mean with the Sha functions, the reason they are so expensive inside the Snarks is because inside a Snark your binary operations cost exactly the same amount as a prime ordered operation. Which means that even though it's really cheap to compute natively because there might be tons and tons of operations but they're all just binary operations, you move that to Snark world and that's not true anymore.
00:22:37.420 - 00:22:49.048, Speaker C: Both systems, like in one system one kind of operation is cheap and the other one, the other kind of operation. Our hash packs have all been built for this system. So now it's obvious that they won't.
00:22:49.064 - 00:22:49.630, Speaker A: Be.
00:22:51.940 - 00:23:11.670, Speaker B: Right and that's where it's not surprising that the current ones weren't good. But now that we're optimizing for this new system naively, you'd assume that once we have time we'll be able to figure it out. And that's what I was wondering if it was more a factor of that or if there are these other inherent factors that make it harder also to.
00:23:12.460 - 00:23:14.010, Speaker A: I would say we don't know yet.
00:23:14.540 - 00:23:16.570, Speaker C: Sure that we can get something better.
00:23:20.460 - 00:23:22.090, Speaker B: Makes sense. Thank you.
00:23:25.010 - 00:23:26.226, Speaker A: Yeah, thank you.
00:23:26.248 - 00:23:29.154, Speaker C: That was super interesting though most of it went over my head.
00:23:29.192 - 00:23:30.020, Speaker A: Oh, sorry.
00:23:31.190 - 00:23:42.194, Speaker C: So I'm aware that in order to support e two Light clients in E one, the client teams are working on adding a VLS three to one pre compiler.
00:23:42.242 - 00:23:42.840, Speaker A: Yeah.
00:23:46.730 - 00:24:06.458, Speaker C: I suppose. I just had a question around, more broadly what the process is for gaining community support around pre compiles within clients and whether there has been any kind of movement or discussion around getting some of the pre compiles that would be needed to unlock some of these other techniques.
00:24:06.554 - 00:24:09.040, Speaker A: You would be far better to answer this than I am.
00:24:10.850 - 00:25:12.956, Speaker D: So yeah, there has been some discussion. There was the 1962, which was sort of general operations of the wall parasite which complexity the general concern is just that we're introducing things that produce consensus faults into clients. So lots of the clients themselves or the client implementers aren't going to natively implement this or it's out of their wheelhouse. This is sort of far more mathematical and implementing crypto from just developing client software. So I think you have to sort of outsource that and trust it. And so there's a bit of nervousness like this. And ultimately, if we implement a few different libraries and the libraries have bugs in them, then we have consensus faults that we will have lotwalks in Ethereum and that could be very interesting, particularly if we have clients optimized for mining using different crypto libraries than normal ones that use.
00:25:12.978 - 00:25:15.196, Speaker B: And I think that really scary things.
00:25:15.218 - 00:25:34.790, Speaker D: So I understand that people are generally keen to move over to e two and are receptive to implementing these things. So it's sort of how do we incorporate, how do we do the slicing ways that's important. I think we can get these photograph parents in.
00:25:40.150 - 00:26:14.378, Speaker B: The way I frame it real quickly is the threshold of confidence for introducing a pre compile should be extremely high that's irrespective of the domain. It turns out that this is a domain that it's not quite in line with the general expertise of the people working at the client. Understandably? And so that further compounds a challenge. If there was an oracle that gave you perfect implementations that you were totally confident in I would assume that it would happen pretty straightforwardly. So it's just this practical security confidence consideration.
00:26:14.474 - 00:26:24.560, Speaker C: I seem to have a memory that there was an intention to use like wadding bundles for this for consistent implementations across all client code bases. Is that right?
00:26:24.950 - 00:26:51.320, Speaker D: There was some leaning towards this at some point but it's never great to rely on one single implementation for everything. And even the wasn't implementations have not been audited properly or verified yet. So that's not great. They are space to have properly verified or really well designed secure implementations which would be great for the numbers.
00:26:53.750 - 00:26:57.650, Speaker C: Feel like I want to ask how you begin to verify implementation.
00:27:00.310 - 00:27:04.902, Speaker D: It's kind of scary. You don't do the whole thing, you just do certain you basically improve optimization for improvement.
00:27:04.966 - 00:27:06.700, Speaker C: That's about as good as you get.
00:27:17.390 - 00:27:18.890, Speaker A: Thank you very much for listening.
