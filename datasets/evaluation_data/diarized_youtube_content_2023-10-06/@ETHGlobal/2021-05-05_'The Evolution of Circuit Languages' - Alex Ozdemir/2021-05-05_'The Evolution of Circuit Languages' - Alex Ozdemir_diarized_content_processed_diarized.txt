00:00:00.810 - 00:00:01.360, Speaker A: Cool.
00:00:02.530 - 00:00:03.422, Speaker B: Yeah. Yeah.
00:00:03.476 - 00:00:24.854, Speaker A: Okay. Very nice. I want to introduce you then. Let's kick it off. OK, so we're now going to hear from Alex Osdamir, who's a PhD student at Stanford. He's interested in programming languages and cryptography and we're going to be hearing from him about the evolution of circuit languages. So please take it away.
00:00:25.012 - 00:01:02.670, Speaker B: Yeah. Thanks, Anna. Yeah. So, as Anna said, I'm kind of like, I guess my background is cryptographic, but also in programming languages. And one thing that's been really exciting for me is that what we've seen over the course of the past, maybe eight, nine years or so, is a real explosion of languages for building circuits for use in zero knowledge proofs. And the result is a sort of extremely rich landscape, a rich ecosystem of different languages that are serving different needs and work in different ways. And what I'm going to try to do in this talk is tell a cohesive story about all languages that we've created.
00:01:02.670 - 00:01:51.110, Speaker B: So let's start by thinking about why languages matter. Essentially, the reason that languages for circuits are important is that zero knowledge proofs, as you know, ZK Snarks, they're very powerful and that they allow you to do really cool things, but they're also limited. And so what do we mean by the power? Well, the power is that we have some sort of problem with the pen now. It's exciting. Okay, we're all good. So the power of these proof systems is that they allow you to prove a very rich language of statements. So what kind of statements do they support? Well, to be frank, you guys probably know this better than I, but I'll list a few examples.
00:01:51.110 - 00:02:23.910, Speaker B: Common statements include I have the money. Maybe this is what the Zcash circuit is proving. You're proving that you have some previously unspent coin. Perhaps I know the credential. So if you're trying to take some action to modify some sort of distributed state, you might need the credential to do that. You could use a zero knowledge proof to prove that you have that credential, that you know, it perhaps in some kind of distributed voting application. You might want to prove that we have the votes.
00:02:23.910 - 00:02:50.990, Speaker B: That is, some agenda that you're pushing has enough votes to actually affect the global state. And these statements are useful primitives in implementing various kinds of application logic. And this is why zero knowledge proofs are so powerful. But the thing is, they're also limited. And the reason that they're limited is that you can't prove any of these things. These statements are not what zero knowledge proofs give you. We often think that they are, but they're not.
00:02:50.990 - 00:03:59.920, Speaker B: What do zero knowledge proofs actually give you? Well, what they actually give you is, I know the solution to some equations and oftentimes we view the equations as a circle. So what do I mean? I mean that if what you have is something like x times y equals z. If you're trying to prove that x times y equals Z and, you know, values of x and y and Z, which make this equation true, you can view this also as a circuit where you've got x, you've got Y and you've got Z and you're multiplying x and Y and you're checking for equality with Z. You can prove that this circuit evaluates to true. You can prove that these equations are satisfied. And so what we have is we have a compilation problem, essentially, because the ideas that we have around the statements that we would like to make are very high level. So we have these ideas, but in order to get to writing proofs about them, we need to first turn those ideas into a circuit.
00:03:59.920 - 00:04:26.644, Speaker B: So we have this problem to solve. Once we solve this problem, then we can use our favorite zero knowledge proof system to actually produce a proof. This is where the terms Grot 16 come into play. Grot 16 is a proof system implementation. Planck, maybe stark. These are all proof systems. They're all essentially for turning circuits into proofs.
00:04:26.644 - 00:05:43.360, Speaker B: But the question that this talk focuses on and the question that circuit languages address, is how do we do that first step, how do we go from the idea I have the money, to a circuit that encodes that idea? And what we would like to have in order to solve that problem is a language that we can use to describe these circuits in some kind of higher level fashion that's easier for us to understand and easier for us to express ourselves in. So circuit languages solve this problem, this problem of expression, and they've been undergoing quite an explosion of growth recently. I would even call it a Cambrian explosion. In the same way that life very quickly went from single cell organisms to dolphins, circuit languages are quickly getting more and more complex and more and more numerous. In my mind, the story starts in something like 2013. This was when the first things that I would call circuit languages for proofs started to arise and it's been continuing through the present and has only been accelerating. So what I'm going to do in the rest of this talk is I'm going to walk us through this timeline from 2013 to now and look at the different ideas that have come to play in various kinds of circuit languages.
00:05:43.360 - 00:06:18.460, Speaker B: And I just want to give a warning up front that the timeline that I'm going to give you is a little bit of a lie. A lot of the works that I describe were actually developed concurrently, but we're going to sort of put them in a particular order to try to help us understand the different ideas that they're working with. So let's get into that. So what's the beginning of the story for us? Well, the beginning of the story is what I would call part one, macro Assemblers. So the very first circuit languages. Perhaps we'll say that the beginning of our story is the Libsnark code base release, which was in 2014. On June 2 in particular, I went and looked at the commit logs.
00:06:18.460 - 00:07:02.936, Speaker B: And Libsnart is dominantly an implementation of proof systems. So it's dominantly an implementation of a zero knowledge proof. But inside of it, there was some machinery for building the circuits that you were trying to write the proofs about. And that machinery was called Gadgetlib. And Gadgetlib was essentially a C library for building circuits. And the way that it was designed is that it provided gadgets. So sort of like C plus plus classes that wrap references to the circuit for modeling ideas like booleans, like machine integers, that is, fixed width integers that wrap around and overflow, and then also gadgets for various kinds of cryptography.
00:07:02.936 - 00:07:44.424, Speaker B: So implementations of hash functions, implementations of signature verification, that kind of thing. Essentially what Gadgetlib is is it's a representation of this idea that you can use C abstractions for circuits. So Gadgetlib is saying, we don't need a whole new language. All we need C plus plus is a great language, it's got great abstractions. We're just going to try to use those abstractions to wrap various concepts that we want to embed in our circuit like a boolean. And we're going to come up with C plus plus functions and methods for combining booleans in the circuit. And this is a good idea.
00:07:44.424 - 00:08:45.330, Speaker B: It makes people very productive. But some people look at this, they look at this idea, use C plus plus abstractions, and they start to get nervous because C plus plus is kind of a crufty language. It's very old, perhaps venerable, and it's not known for having particularly solid abstractions. So some people look at this and they ask, what if you don't like C plus plus? What if you're worried about the kind of pervasive unsafety that C has led to in the development of system software? All this blockchain stuff is very security critical. It's very important that the circuit encodes the right thing. And perhaps we're nervous about using a language whose abstractions have traditionally allowed programmers to express the wrong thing many times in a great number of damaging ways. So this concern that C, while the idea of having a macro assembler a library that helped you assemble circuits was useful, they were worried that C wasn't the right language for them.
00:08:45.330 - 00:09:27.164, Speaker B: And this led the development of two other projects that we'll talk about. Now. The first of these projects was Bellman. So Bellman was developed starting in 2014, and it was built by the folks at Zcash and dominantly that the idea was, they built it in order to construct the Sapling version of the Zcash cryptocurrency. Their goal at a high level was just to take Libsnark and just move it into Rust, move some kind of Gadget description language into Rust and build out the kinds of high performance gadgets that they needed for sampling as they went. And this went well. And Bellman is something that's still used.
00:09:27.164 - 00:10:04.964, Speaker B: Matterlab has continued to add to it and use it. I've used it for various projects myself, various academic projects. It's a great macro assembler, but in the same way that the folks at Zcash thought, hey, Gadgetlib is great, but I want it in Rust, other organizations thought, Gadgetlib is great, Bellman is great, but I want a library that's in my other favorite language. So as an example of this, we have another macro assembler. Snarky. So this was built in OCaml. It was built by the folks at Order One Labs and Development, or at least the first public release was in 2018.
00:10:04.964 - 00:10:44.756, Speaker B: It continues to be developed today. It essentially is another iteration of the same idea. We want to build some library in a different language for constructing circuits. Now we're just going to do it in OCAM. So this continues to be useful, but it's not the end of the story, because one issue that you run into when you're building a library in some other language to model circuits is that this language wasn't really designed for circuits in the first place. It might have ideas latent in it that aren't useful for building circuits, and there might be things that are important in circuits that are hard to express through this language. And you're kind of limited because you're working in this language like Rust or like OCaml, you're working inside some host language.
00:10:44.756 - 00:11:53.804, Speaker B: You're just writing a library, so you don't have as much control as you like. And so this drove a push to construct languages explicitly for circuits. Not libraries that were writing on the back of some existing language, but a language for circuits. And the first step in this direction, I would say, was the construction of so called hardware Description Languages, or HDLs, for arithmetic circuits. And in order to understand this idea of using HDLs for circuits, we need to understand HDLs, and that means that we need to take a detour into the world of digital design. So what is digital design about? Well, it's about answering the question, how are computers made? Or really, how are computers designed? So if you open up your computer, inside it, you'll find a collection of chips, perhaps Pentium, perhaps something in the Sandy Bridge family. This is one of Intel's, I guess, now old generations of their architecture, perhaps the recent M One chip that Apple has built.
00:11:53.804 - 00:12:38.264, Speaker B: And some people are excited about, maybe too excited about, I don't know. So what digital design is about is it's about building these computer chips. And this is relevant to the design of arithmetic circuits, because what computer chips are, is they are essentially digital circuits. If you open up one of these, then what you're going to find is you're going to find an, andigate you're going to find or Gates, you're going to find these registers maybe connected to clocks. You're going to find all these different components wired together in such a way that you can do things like run assembly instructions. That's what CPUs do, that's what computers do. And the natural question is how do people these things, they have billions and billions of gates.
00:12:38.264 - 00:13:40.864, Speaker B: How do people design these chips that have so many gates? And the answer is they use hardware description languages. These are built with HDLs and there's a lot of HDLs, verilog, VHDL, Bluespec, maybe more recently Chisel, more or less. People are still basically using Verilog system Verilog. But what these languages are, is these are languages that were designed not to program a computer, not to wire up different components, but instead to connect gates to one another. And so one question that people asked is hey, we got these great languages, these HDLs, for building digital circuits. Can we use those to build the kinds of arithmetic circuits that we need in Snarks? And of course the answer is yes, you can totally do that. So the first project, the most prominent project in this area is Circom, which was mentioned in the previous talk.
00:13:40.864 - 00:14:41.280, Speaker B: This was a HDL essentially for arithmetic circuits that was developed by Jordy Balina, also starting roughly in 2018. That's when it was first public. And it has a number of big ideas in it that are taken directly from the world of HDLs. And these are the ideas of constructing equations in the digital world. This would have been constructing gates and organizing those equations into modules and then connecting the modules to one another. So all these ideas, these are ideas from the world of hardware description languages and Circom just brings those to arithmetic circuits. So now with Circom, you can write a circuit in the same fashion that you can write an arithmetic circuit for zero knowledge in the same way that you would have written a digital circuit for processor.
00:14:41.280 - 00:15:31.200, Speaker B: This is pretty cool because people have been using the hardware description languages for a long time and they're reasonably good at it. But this isn't the end of the road. And the reason it's not the end of the road is that hardware description languages are nice, but they're not programming languages. So programming languages are a little bit different programming languages, they don't have this idea of gates being wired together and organized into modules. No, instead programming languages have stateful semantics. They have this idea of variables that you can change over time and if statements that you can either execute or not execute depending on some condition. You might have loops, you might have function calls with returns from those function calls and within loops you might have breaks and you might have continues.
00:15:31.200 - 00:16:42.964, Speaker B: You have, broadly speaking, this notion of a sequence of instructions that gets executed or not executed depending on the inputs of the program and the systems that we have for expressing ideas in terms of variables in terms of loose, in terms of if statements. We call these programming languages. It's not hardware description languages, programming languages and programming languages for those who have used both HDLs and Pls, programming languages people like more, they're easier for us to reason about. And so what we would really like is we would really like to be able to write down our circuit in a programming language or write down our idea in a programming language and have it be churned into a circuit for us. And this is a challenging thing to do because there's a core problem we have to overcome, which is that programming languages are fundamentally designed for the Ram register computational model. So programming languages are designed with this kind of processing and unit in mind that executes instructions over registers, over data, and sometimes takes that data and either puts it in or gets it from some large memory or Ram. So this is the computational model of Turing machines.
00:16:42.964 - 00:17:45.070, Speaker B: This is the computational model that our computers actually implement. This computational model is what programming languages are designed to abstract. But what we need in order to use a zero knowledge proof is we need a circuit, which is a computation written in the circuit model where there is no state, there is no notion of time, there is just a whole bunch of values that get combined and recombined into a single answer. The circuit model is much simpler in some sense, and HDLs, they're designed to target the circuit model. But like I said, what we'd really like, because we'd like the Ram register model more is we'd like to be able to write programs and then use some kind of compiler to turn our Ram register programs into circuits. We'd like to take that program, that loop and unroll it into a big circuit that we can do cryptography on. And so the question of how you do this conversion or this compilation is a challenging one and this is what needed to be overcome in order to have true programming languages for zero.
00:17:45.070 - 00:18:40.380, Speaker B: So this project is a challenging one and it's something that people have been working on for a long time. I have here a few of the academic projects that have been working on this listed pinocchio Gapeto, Bikini. These were the original people who tried to tackle this problem. And the way they viewed it is they wanted to take some existing programming language, they chose C and compile it or some subset of it into a circuit. And it should be obvious immediately that they can hope to do this completely. Why? Because C is like a super nasty language with really complicated ideas in it that are possible to implement in the Ram register model, but rather incompatible with the circuit model. What kinds of ideas am I talking about? I'm talking about, for example, function pointers.
00:18:40.380 - 00:19:17.296, Speaker B: So in C you can have a pointer to a function. This is a pointer to some code that's not known at compile time and you can call that function during the execution with the program and it's just not clear what that would mean in a circuit. I guess it would mean like I look at some data and depending on what the results are, I run a different subsurface. This starts to get really hard to reason about. A similarly challenging idea is the idea of dynamic memory. So I have some huge chunk of memory and I want to access it at different locations depending on the inputs to my program. That's also pretty hard to express in a circuit.
00:19:17.296 - 00:20:05.876, Speaker B: But nonetheless, the researchers who are working on these projects, they thought that it would be interesting to try and so they charged down and compiled the largest subset of C that they could manage into circuits and produced some compilers that were really interesting. They developed a lot of great techniques. But fundamentally I think that these projects were just too ambitious because despite their successes, there were still huge chunks of the C language that they don't handle or they handle incorrectly. And programming in the presence of a compiler that doesn't handle the input language fully is really challenging because you worry that you might write something down that the compiler is going to misunderstand. And so fundamentally, I think a less ambitious approach was needed. This was just too hard. And so this brings us into part four of the evolution of circuit languages which I would call DSLs.
00:20:05.876 - 00:21:01.820, Speaker B: And so the idea here is that this premise that we want to compile programming languages to circuits isn't fundamentally broken, it's just too hard. And so what we want to do is we want to make it easier by co designing the language, the programming language with the compiler that's supposed to turn it into a circuit so we can design the language in a way that makes this problem easier. And there are a number of great works in this space. The oldest of them is Socrates, which has sort of like a Python esque language with built in support for field elements and various restrictions that make it possible to compile perfectly into a circuit. And since Socrates there have been a number of other projects. So there's Noir which is being developed by Aztec, there is Leo which is developed by Alio and there is Zinc which is developed by Matter Labs. And this is not an exhaustive list, there are also other examples.
00:21:01.820 - 00:21:44.090, Speaker B: But broadly speaking, all of these projects what they're trying to do is they're trying to design some programming language specifically for zero knowledge. That they, because that language is restricted and because that language has been specially designed can subsequently compile into a circuit far more easily than compiling something like C. Looking at this landscape, I would point out that Zocrates sort of has the best polish. It's the least buggy. So if you're doing something this second, I would probably recommend Zocrates. But these other languages are much more ambitious in scope and they really do have great potential and they're extremely hot off the presses right now. I think that Noir was released literally like a month or two ago.
00:21:44.090 - 00:22:14.100, Speaker B: But there's a lot of potential here. And so I would keep an eye on them. I expect in the long run, they will be quite great tools for expressing ideas that you want proven zero knowledge. So that's kind of the end of the main line of the development of circuit languages. I do want to take a moment to sort of do another detour into Stark's. So Stark is a different proof system. It's a new proof, a new kind of proof.
00:22:14.100 - 00:23:09.824, Speaker B: It is the proponents of it would say more compatible with Ram register programming, more compatible with the idea of state and iteration. And the reason that people say that it's more compatible with this is it's built on repeated circuit application. So rather than building some big tree that is your circuit, some huge monolithic thing like the Zcash spend circuit, what you do with Stark is instead you come up with a small circuit C that gets repeatedly applied. And so what you try to do is you try to view the computation, the statement you're trying to check. Instead of just being a standalone circuit, you want to express it instead as the repetition of a smaller circuit. And so this is obviously a very different programming model. You're looking for common structure.
00:23:09.824 - 00:23:52.900, Speaker B: And so that means that there are different programming needs that need to be met. And the people at Starkware have built this language, Cairo, that's trying to capture programs in this alternative model. So that's kind of interesting as well. Okay, so I want to take a moment and just reflect on what we've covered in this talk. The first thing I want you to take away is that there has been an explosion of languages over the past eight years or so. Languages have gone from being simple things like Gadgetlib to far more complex things like Zocrates and Zinc and Cairo. The second thing that I want you to take away is that in the design of these languages, there are serious compilation challenges.
00:23:52.900 - 00:24:38.784, Speaker B: This is harder than building a C compiler. This is harder than building L of M because it requires you, for example, to move between the Ram register model and the circuit model. It requires you to get rid of powerful and hard to express notions like variables. And because this problem is so challenging, I think the third thing that you should take away from this talk is that we really need to continue to explore. We don't know the best language yet, we don't know the best way of writing languages yet. But even as we explore, we don't want to waste effort because this problem is so hard, if we try to start it from scratch every time we build a new language. That's going to be a lot of work.
00:24:38.784 - 00:26:28.630, Speaker B: And I think more concretely, what will actually happen is that the languages will be bad. They'll be missing huge chunks of things that would be useful because it just takes too much time to build everything again from scratch every time. And so the final reflection I have is I think it's worth asking, can we reuse compilation infrastructure? So Zocrates has a great compiler, but if you want a different language, if you want to build on Zocrates, is it possible for you to, for example, reuse the infrastructure that underlies Zocrades? Or that probably isn't doable, but is it possible for you to build on some kind of shared infrastructure that can support building new and exciting languages for Snarks? And I would argue that the answer is yes, this is possible. In the same way that the computer science community has constructed LLVM, which is this superpowered Ram register compilation infrastructure that allows people to compile Rust and C plus plus to Arm and X 86. We could build a circuit compiler infrastructure which allows you to go from a language like Socrates, or a language like Zinc, or a language like Noir. You can imagine all these languages building on some kind of shared infrastructure that helps them on the back end, not just produce arithmetic circuits, say, written as R One CS, which is a particular arithmetic circuit format, but also produce the kinds of arithmetic circuits that you need for other proof systems like Plunk. This is like what we would want, I would argue, if we want to continue to explore different languages without reinventing the wheel every time we need some infrastructure that our different languages can share.
00:26:28.630 - 00:27:09.200, Speaker B: And I think that working on this is a really interesting thing to do. And I've been spending some time on it with a few friends of mine. We've been building what we call Cersei for circuit compiler. We think that it might be this infrastructure. If you want to learn more about it, you can go back and you can watch the ZK study called Videos. We did a presentation on it at some point. Or you could go to this URL where we have a paper that talks about these challenges of common infrastructure and talks about our take on how you might build it.
00:27:09.200 - 00:27:47.500, Speaker B: Okay, so again, once more to recap. Over the past decade or so, we've seen a real Cambrian explosion of circuit languages. These languages are solving a hard computational challenge, moving from the Ram register model with variables and loops to the circuit model where you don't have those concepts and the jury is really still out. We're not sure what the best language will be, we're not sure what the right abstractions are. We're still trying to figure it out. And I think that one thing that we should try to do is as we continue to explore this space. We should try to build as much shared infrastructure as possible so that we don't have to reinvent the wheel every time that we construct a new circuit language.
00:27:47.500 - 00:27:53.810, Speaker B: So thanks for your attention. I'm happy to take any questions now at this point. Cool.
00:27:54.180 - 00:28:07.024, Speaker A: Thank you so much for this talk, and it's been awesome to see it visually. It's very cool. Question from the audience why do ZK proofs need to run on circuits and not as programs?
00:28:07.152 - 00:28:43.216, Speaker B: In a nutshell, it's a really great question. And there's two answers to so the first answer is that to run a ZK proof, what you do is you do a lot of cryptography, you do a lot of math. And one thing that makes doing math easier is you know what you're working with. And so the problem with a program is you don't know how long it's going to run. This is an object that changes over time. That's something that's hard to do math on. So the reason that we use circuits instead of other kinds of programming models is that circuits have fixed size and they're easier to build proofs for.
00:28:43.216 - 00:29:01.590, Speaker B: If we could build proofs for your CPU for assembly, we would. And this takes me into the second answer, which is that the people who work on Starks say, actually, you can. And I personally, the jury is kind of out. We'll see how well that works. I think that's not going to pan out, but I could be wrong. But if it did work, it would be very important.
00:29:04.440 - 00:29:10.424, Speaker A: One question is actually do you have the slides? Will you be sharing these slides shortly after?
00:29:10.462 - 00:29:13.930, Speaker B: Yeah, I'm happy to. I can send them to, I guess, the ETH people.
00:29:16.540 - 00:29:31.680, Speaker A: Maybe. Another question here is I think you've highlighted something that needs to be fixed and improved. Is creating that sort of center point are there any other things in the general language ecosystem that you think need work where people could potentially direct their attention?
00:29:32.660 - 00:29:50.180, Speaker B: Yes, I think there's an extremely long list of things. What are these things? Support for a richer set of types. Support for large integers. Support for various kinds of memory embedded in the circuit. Complete support for the primitive things. Having integers of any width you want. Having field elements.
00:29:50.180 - 00:30:12.140, Speaker B: Being able to embed other fields like baby jubjub inside your current circuit. There's a huge list of things. And while this is my wish list, this is what I wish every language supported. I recognize that as long as we sort of try to do everything independently, it's unrealistic to get all these goodies unless we try to pool efforts. Cool.
00:30:12.290 - 00:30:26.416, Speaker A: Another question. Does research from Snark provers and Stark provers build on each that. Is there some connection point there, intellectually, for sure.
00:30:26.518 - 00:30:54.228, Speaker B: So when people started figuring out how to build these small circuits that get repeated in Starks, you bet. They looked at how we had been building circuits already to. Figure out how to do that. And also there's been cross pollination in the other direction as well. So at least some people who think about Planck actually sort of view it as exposing a similar abstraction. It's not an exact match, but they draw on Stark ideas as well. At an intellectual level, 100%, there's been cross pollination, and there will continue to be cross pollination.
00:30:54.228 - 00:31:15.490, Speaker B: At an infrastructural level, I think it's also possible, but it's an interesting question. So this is not something my friends and I have explored so far, building common infrastructure that can be shared between Starks, not just like common ideas, but actually a common program that you could run to Starks and other things. But I think it would be an interesting thing to explore. If you're interested in that, come talk to me.
00:31:16.340 - 00:31:26.420, Speaker A: Here's a question from our next panelist, actually, Barry. How can Cersei take advantage of Planck's specific optimizations?
00:31:27.480 - 00:32:01.500, Speaker B: Yeah, this is a great question. So what I actually want to do is scroll down a little bit and look at this picture we have of the infrastructure here. And what I would say is on the back end here, I've sort of just drawn the different kinds of representations you might produce as separate things. But actually what you really are going to have here is you're going to have sort of a branching tree. So what you really want is you wanted to design the back end of this infrastructure so that it starts by doing the things that are common to all proof systems. So, for example, all proof systems work in arithmetic over finite fields. So step one is sort of make everything a finite field.
00:32:01.500 - 00:32:34.390, Speaker B: You want to do that whether you're targeting Clonk or whether you're targeting R, One, CS, or some other format. And then after that, you want to apply increasingly proof system specific optimization. So, yeah, at some point you want to run Plonk specific optimizations, and there is space for that in this infrastructure. I guess what I would say is like common compiler infrastructure, the idea isn't that every single thing gets used by every single target application. It's that you try to use as much as possible of what already exists for your target application. And if you need to add more specific things, that's okay. Too cool.
00:32:35.000 - 00:32:39.690, Speaker A: All right, so I think we're at time. Thank you so much, Alex, for this great talk.
