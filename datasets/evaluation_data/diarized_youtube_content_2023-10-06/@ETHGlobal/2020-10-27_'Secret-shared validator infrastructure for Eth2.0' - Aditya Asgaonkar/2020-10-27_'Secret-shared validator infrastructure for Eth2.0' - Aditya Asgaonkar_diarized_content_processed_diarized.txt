00:00:00.170 - 00:00:31.030, Speaker A: Speaker is adysia. He is a researcher at the Ethereum Foundation. He specializes in consensus research and spends a lot of his time focused on ETH Two. He's going to give us a talk today about secret shared validator infrastructure for E Two. It's about achieving resilience against no failures and securing against key theft as an E Two staker here. So without further ado, I'm going to go over, hand it over to Aditya.
00:00:37.550 - 00:00:59.360, Speaker B: Hey, there. Thanks for the introduction, Emily. Let me share my screen real quick. All right. Hello, everyone. I guess we have two more minutes until we start, so yeah, we just wait till then.
00:01:11.350 - 00:01:13.858, Speaker A: You are fully good to go if you just want to log.
00:01:14.024 - 00:01:28.210, Speaker B: Yeah, sure. Yes. Hello, everyone. I'm Aditya. I work on consensus. Things at the EF, mostly focused on Casper and ETH Two. Today I'm going to be sharing a presentation on secret shared validator infrastructure.
00:01:28.210 - 00:02:06.642, Speaker B: So let's dive right into it. Staking on ETH two. Ethereum Two is a proof of stake network, which means that validators put up a security deposit in order to enter as a validator. And the deposit is insurance so that they perform their duties correctly. And the duties consist of producing blocks and attestations at the time specified by the ETH Two protocol. And the main reason for the existence of the security deposit is having strong disincentives for validators doing bad things. So a couple of bad things exist.
00:02:06.642 - 00:03:01.058, Speaker B: So inaction on the part of the validator leads to penalties, and malicious actions on the part of the validator lead to slashings. Both of these are reductions in the security deposit that the validator put up. So more specifically, penalties result from offline behavior such as failing to produce a block or an attestation when required, and slashings result from misbehavior, which is provable malicious actions such as making two conflicting blocks or conflicting attestations in the same slot or violating some of the Casper FFG consensus rules. So I'm sure a lot of our listeners here are very interested in becoming ETH Two stakers. And there's a couple of risks involved with them. Some of them are very serious. The two most serious risks are key theft and node failure.
00:03:01.058 - 00:03:55.634, Speaker B: So key theft is when the validator key that you're using on the ETH Two network is stolen. And there are a couple of keys that are involved in the process. The Staking key with which you sign blocks and messages, and the withdrawal key which you use to withdraw your security deposit at the end of your term. And the other major failure is node failure, where your E Two node that is running your validator just goes offline or it produces some unexpected behavior. And of course, our goal with resilient validator infrastructure is preventing against both of these types of failures or theft. So how do we go about that exactly? Let's look at key theft first. So obviously key theft, as I said before, is when your validator key has been stolen.
00:03:55.634 - 00:04:35.118, Speaker B: So maybe the machine that you're using to run your validator, it obviously contains your entire private key. If your machine is compromised, then your key can be stolen. If you're not running your machine yourself, you're running it in the cloud. If your cloud architecture is compromised, that can happen easily if you're using a staking provider that holds your keys. That can lead to situations like this. But there are standard ways to prevent against key theft. The most usual way to do this is using threshold signatures.
00:04:35.118 - 00:05:42.374, Speaker B: What this means is you take the existing singular private key, which is your entire private key, and you break it into multiple smaller parts and you break it up such that signatures from any of the smaller parts can be combined into producing a signature for the complete key. So, as you can see in the figures at the bottom, the key n is split into three parts n one, n two and n three. And signatures on the same data from N one, n two and N Three can be combined to produce a signature as if it has come from the entire private key. N. So if we split our keys in such a way and kind of something. It's sort of like Horcruxes in Harry Potter where you just put one each in different places and then you're secured against key theft unless someone goes ahead and attacks all of them at once. The second failure was node failure.
00:05:42.374 - 00:06:37.750, Speaker B: And there's two major subparts in this. The first one is crash faults, which is your node going offline. And this can be caused by a number of factors, most of which are out of our control, such as power outages, network outages, hardware failures or software crashes. And the usual way to prevent against this is using redundancy. So instead of running a single e two validator so for a single e two validator, instead of running just one instance of the ETH two client, we run multiple instances, multiple redundant instances, so that if some of them fail you still have the others as a backup. The other kind of fault is Byzantine faults where your node isn't exactly going offline, but it is producing unexpected behavior which can cause slashable events. So this can be caused by software bugs in the ethereum client.
00:06:37.750 - 00:07:49.438, Speaker B: It can be caused by network attacks where an attacker has taken control of the network around your node and the attacker is sending messages in such a way that you are influenced to produce bad messages. And we can prevent against this by having a consensus instance running among the multiple redundant e two validators that you are running, rather the multiple redundant e two nodes that you are running for the same validator. This is so that no node is making unilateral decisions. Only if a majority of the nodes sign off on something, only then do all of the nodes produce a certain message or a block. So the most resilient ETH two staking architecture is going to be a combination of all of the things that we discussed. So namely threshold signatures, redundancy among nodes and consensus among these redundant node instances. So what does this mean exactly? It means that we are going to run multiple each two nodes for the same validator.
00:07:49.438 - 00:09:12.358, Speaker B: Each of these redundant nodes is only going to have custody of a part of the private key, but not the entire private key for the each two validator. And the third one is that there is a consensus instance running among all of these redundant nodes. And nodes only sign messages if the consensus running among these nodes instructs them to do so. Essentially, the last point here makes these redundant instances replicated so that all of them always have the same state and transition in the same way. So if this is the current e two architecture where we have a single validator v and the entire private key is existing at this validator instance, what I'm suggesting, or rather what the resilient architecture would be, is something like this where we split up the key in multiple places. So here we have split it up four ways and we put the parts of this key in redundant e two nodes. So v one through v four are the redundant nodes and we have a consensus instance running among these nodes so that none of them is unilaterally taking decisions, right? But in practice it's a bit more complicated than that.
00:09:12.358 - 00:09:52.794, Speaker B: And in order to understand that, we should discuss a little bit more about the ETH two client architecture. So an ETH two node right now consists of two major parts. The first one is the beacon node and this is the part that takes care of peer to peer networking, chain tracking, folk choice management, et cetera. This is the part that is directly exposed to the network. So the beacon node is responsible for gossiping messages, verifying that the messages it has received is valid, and so on. And the beacon node can be run by users who are not stakers. So just like Geth today, you don't have to be a miner to run Geth just like that.
00:09:52.794 - 00:10:43.882, Speaker B: You don't have to be a staker to run the beacon node, you can just run it in order to get information about the beacon chain. So that's the beacon node. The second part is the validator client. This is a rather lighter piece of software. The main responsibilities of the validator client is handling the validator private keys and signing blocks and attestations when it is the correct time. And the validator client is only connected to the Staker's beacon node in order to get information about the network that is unsigned attestations and blocks that the validator client will then sign and publish to the network. So the validator client is in no way connected to the ETH two network directly so this is what the current architecture looks like.
00:10:43.882 - 00:11:43.760, Speaker B: We have a beacon node and a validator client connected to it and the validator client has the entire key. So a resilient architecture would be splitting up the validator key into multiple pieces so key one through key four and also assigning them to redundant validator client instances. So again, v. One two through v. Four are the validator client instances. And all of these are put into a consensus group so that if there is a bug at, say, v three, that causes it to sign something, bad that basically won't go through because all of the other validator client instances have to come to consensus about what to sign before any of the clients actually sign anything. So this results in a replication of these redundant instances which is a really important part.
00:11:43.760 - 00:12:31.680, Speaker B: So this is better than the previous architecture, but this is still not the best we can achieve because obviously there is a single point of failure which is the beacon node itself. So the beacon node is how all of these validator clients are getting information about the E two network. They have no way of connecting to the E two network otherwise. So if the beacon node fails, the validator clients will not know what's going on on the E two network. They have no information about the chain, they don't know how to produce blocks without a beacon node. So even though this is better than the current architecture, it's not the best we can achieve. So in practice, the best architecture that gives us the highest resilience looks something like this.
00:12:31.680 - 00:13:48.806, Speaker B: So we have multiple beacon nodes, we have multiple validator clients, and we have some secret shared validator middleware that is running in between interfacing the beacon nodes to the validator clients. So this middleware consists of these SSV clients, these secret shared validator instances which are responsible for basically managing the timing, managing instantiating the consensus instances for each block or each slot and so on. And anything that goes from the secret shared validator middleware to the validator client should be put through consensus instance. So that all of these validator clients are in the same replicated state. They sign off on exactly the same things. There's no situation where some of them sign on one fork and the others sign on another fork and then we have a deadlock that should not happen and hence this consensus filter basically. So there's a few more parts that are required to make this work.
00:13:48.806 - 00:14:37.078, Speaker B: The most notable is this signature combination component. So each of these validator clients, since they only have a part of the key, they produce threshold signatures. And all of these threshold signatures have to be combined in order to make something that is tangible to the ETH two network. And this entire architecture is going to be a single ETH Two validator. And specifically for these numbers right here where we have three out of four signature combination, and we have four redundant instances. We can tolerate one failure, so one node can go completely offline, and we can still have all the good properties about our network, about our validator. So there are a few design choices that were made on the way.
00:14:37.078 - 00:15:36.994, Speaker B: The most notable is the consensus algorithm itself. The requirements for this were that it has optimal resilience, that is, it tolerates the maximum number of faults that are possible, which is one third. We want this because in order to achieve the same fault tolerance, we want to run the minimum number of nodes possible and reduce our staking costs that way. The second requirement is fast leader change. So the consensus algorithms have a leader which proposes what the consensus value should be, and if the leader fails, we have to change the leader in order to achieve consensus. And we want to do this in a really fast, daily responsive way, because the E Two network expects validators to produce messages at a certain time. And obviously we'll run the consensus algorithm at some time before the expected time to produce blocks or attestations.
00:15:36.994 - 00:16:31.580, Speaker B: But if the leader fails, we want to be able to conduct the failover and have the new leader propose a block or an attestation as soon as possible so that we can still produce a block or an attestation before our expected time. And the almost perfect candidate for this is Istanbul BFT, which has all the good properties that we want. And you can find more information about that on this link in this paper. So, some additional information. There is a proof of concept made by Dankrad and Alon and you can find the proof of concept on this GitHub repo. We had a validator running using this POC on Medasha testnet for a while. I don't think it's active right now, but you can go ahead and explore that if you're interested.
00:16:31.580 - 00:17:02.690, Speaker B: Some more additional information. The key people involved in the secret Shared Validator effort are from the Ethereum Foundation tankrad and myself, from consensus Mara and Colin and from blocks IO Alon, who's been really helpful with the proof of concept. Thank you. That was all from my side and my contact information is on your screen. Hit me up if you're interested in Secret Shared Validators, have any questions or want to get involved.
00:17:04.470 - 00:17:24.902, Speaker A: All right, thank you so much. Ditya, if you don't mind sticking around, we do have a couple questions came up in the chat. And also a reminder to everyone. Once again, if you have questions for Aditya or for any of our speakers, go to Live Ethonline.org for starters. Great talk. I really enjoyed the Harry Potter reference.
00:17:24.902 - 00:17:41.946, Speaker A: There's like not enough of those in the Ethereum world. But the first question we have know this sounds like a really good step forward for validation security. Are there any trade offs to it of additional bandwidth, more complexity for node operators?
00:17:42.138 - 00:18:31.470, Speaker B: Right. So I think the most important one is the complexity in operating this node. The E two clients are already a complex piece of software and even technically competent users have issues running E Two clients. So running this kind of a complicated setup across multiple machines is going to be a challenge. And unless there is tooling to make this easier, I definitely don't recommend normal users to do this. But this is a giant leap forward in terms of resilience at the validator level. I hope that the major staking providers or exchanges who are doing staking use this kind of an architecture so that they don't experience some of the issues that we have seen in the test nets.
00:18:31.470 - 00:18:38.900, Speaker B: But this is definitely a complex piece of engineering and it'll take time to refine this.
00:18:39.350 - 00:18:51.330, Speaker A: Cool. Next question. So what are other improvements that are left before this can be used at scale for mainnet validators? Like besides launching mainnet?
00:18:51.490 - 00:19:11.690, Speaker B: Right. So I think all of the research problems are taken care of. Basically what I described, redundancy and replication, those are the two main pillars on which this entire architecture is built. The research is all done. It's basically time to implement this for a production ready setup.
00:19:12.350 - 00:19:24.580, Speaker A: So I guess my final question really is how can people get involved? I know you sort of touched that on your slides, but if you want to reiterate, that would be great. And more importantly, what are some of the things that you would like help with?
00:19:25.030 - 00:20:14.350, Speaker B: Right, so as I said before, there's a few key people involved in this effort. The contact information is on the screen right now. You can reach out to Dankrad or myself through our emails and we'll certainly be able to figure out the best way for people to help us depending on their skills. Right now we are looking for people to maybe get involved in implementing a production ready software in order to run this. That would be a great help for us. Staking providers are encouraged to maybe contribute resources or time into making this because this will definitely be helpful for them. But obviously anyone else who's interested in making this work should hit us up.
00:20:14.500 - 00:20:30.630, Speaker A: Yeah, that's good to hear. Actually, we have another question that came up in the chat if you're down. Okay. The question is, do you expect this to be used more so by individuals or entities for heightened security or do you think it'll be more used to enable trustless pools?
00:20:31.290 - 00:21:23.126, Speaker B: So solely this is not enough for trustless pools. There is an entire set of other things that are required for trustless pools. This addresses some of the issues in implementing trustless pools, but those issues are, I would say, not fundamental. That said, I do expect some solutions to appear using this. I know Alon from Blocks IO is working to convert this into a trustless staking setup. As far as individual validators go, I think the main trade off is that in order to achieve the fault tolerance associated with one node failure, now, instead of running one node, you have to run four nodes. So four is the minimum number that you have to run.
00:21:23.126 - 00:21:47.470, Speaker B: Just running two is not going to get any fault tolerance. So because of the heightened costs of this staking setup, I don't think single validators or individuals are really the target users here. Maybe whales who are staking on this or staking providers who have huge stakes and a lot to lose. It makes sense for them to bear these costs.
00:21:48.930 - 00:22:00.080, Speaker A: All right. Makes sense. It looks like that's it for questions. Thank you so much for joining us. Great talk. It was good seeing you. I feel like I've really missed seeing people in person these days.
00:22:00.610 - 00:22:01.694, Speaker B: Yes, for sure. Yeah.
00:22:01.732 - 00:22:02.830, Speaker A: Thanks for joining.
00:22:03.250 - 00:22:05.218, Speaker B: All right, thanks, everyone. Bye.
