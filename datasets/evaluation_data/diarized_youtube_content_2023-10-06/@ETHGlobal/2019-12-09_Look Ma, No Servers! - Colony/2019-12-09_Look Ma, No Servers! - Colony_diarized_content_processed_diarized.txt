00:00:00.250 - 00:00:54.974, Speaker A: Yeah, all right. Sorry for the clickbaity title, but I think it kind of made a lot of sense to me. So when I presented one of one of our data solutions to a colleague of mine, he said exactly that. He said, look, no servers, and always feel like, yeah, that's nice and you've done something great, but at the same time you also feel like something terrible is going to happen after that. So people are probably going to get hurt. So you might not want to just do it like this. So what I feel like what we're doing, or what we at Conley are doing a lot of times is we're trying out new stuff.
00:00:54.974 - 00:01:47.470, Speaker A: We are trying to be cutting edge, but we always put on training wheels for that. So we have a safety net to cover us when something goes wrong. Or in that case, we try to be decentralized, as decentralized as possible, but in a sense, in a way then we still wear a training wheels. So who am I? I work at Colony. I basically do all kinds of things over there except for writing smart contracts. So I basically do all kinds of front end stuff over there. And in the past couple of months I've been mostly busy with getting our data layer to work, which is based on IPFS and Orbitb.
00:01:47.470 - 00:02:53.090, Speaker A: So what we're trying to do is, as I said, we try to make things as decentralized as possible. That means. Currently, in our current state, I don't know if that's going to change anytime soon, but in our current state, we don't have a server back end, so everything happens in the front end and everything is connected to various services. And we're just using yeah, we try to make things as decentralized as possible and we connect to IPFS and use as many peer to peer applications as we can. So what I'm going to do is give a little bit of a primer to orbitdb the data layer that we're using. This is not the actual logo, but I thought it would be nice if that was the actual logo. This is the actual logo.
00:02:53.090 - 00:03:34.340, Speaker A: So Oberdb, they say it's a serverless distributed peer to peer database. It's based on IPFS and it has a lot of moving parts to it. So it's based on technologies that we all know or that we all have seen already. For me, it always consists of these kind of seven parts. They would probably argue otherwise because there's more to it, there's a lot of more packages, there's a lot of more things going on. But for me, this was always the main core parts. So we have data API, we have the operations log, this is kind of the core of the database which holds all the data.
00:03:34.340 - 00:04:40.790, Speaker A: I'm going to explain that a bit more in a second. We have the replication kind of logic that allows various stores to sync with each other in a peer to peer manner. The next point was crucial for us to pick this technology for our data layer because it has access control. That means that we can actually control who can write to a certain database and who not in a decentralized database, which is kind of cool and I haven't really seen that anywhere before. The next very important module is the identity module. So that's kind of crucial for the access control that we know who is writing to this store. And that obviously means some sort of key store, or in our case the Ethereum wallet to identify themselves to be able to sign the data that they're writing to the database.
00:04:40.790 - 00:05:33.400, Speaker A: It's all stored on IPFS itself as files that are being hashed while it's stored. And we have the communication layer. So how do two stores know that they want to be synchronized? How do two databases know that they want to be synchronized? And this is the communication layer of it, which is IPFS Pub sub. Are you all familiar with IPFS pub. Subm? Yeah. This is basically a set of technologies I'm going to talk about that later as well. A set of technologies that allow direct communication between peers in real time.
00:05:33.400 - 00:06:05.118, Speaker A: So this is a brief overview of how Orbit works. Every store in Orbitdb is just an oplog. It's basically just a log of operations. I don't know if you can read it there. So the first says Operation Put and then it's like foo bar. And then next operation is Put bass fizz or whatever you want to put on your store. And you go through all these operations.
00:06:05.118 - 00:06:40.958, Speaker A: And the important thing here is that it is an append only log. That means you cannot delete anything in between or something like that. You can just append. So what do you do when you want to delete something is just say flag this for deletion as another operation that comes with it. And in the end you have this long store of entries. The yellow little box symbolizing that this has an identity attached. So you always know whenever an operation is put on the log.
00:06:40.958 - 00:07:47.074, Speaker A: You always know who initiated that operation because it's signed in an Ethereum wallet. So you have this long log of operations and in the end you kind of want to reduce that to a final state, the state that the user in the end is going to see. And Obitb is using reducers for that. So we have a lot of different store types on Orbitb. One is the key value store, which is kind of the most accessible thing, I guess, for us, where you reduce all of the operations, which I did here, to just a single JavaScript object. So this is the object that you end up with from this oplock in the end. What I kind of wanted to symbolize there is that every new operation has to go through an access control access controller that gets arguments, the identity and the operation itself, which is kind of cool.
00:07:47.074 - 00:08:35.460, Speaker A: So the access controller can be anything. It can be anything asynchronous that means you can actually even make requests to the blockchain, which is what we are doing. So we are checking whether a certain person has access to a resource or can write to a resource by checking our smart contract suite. So we can say does this person have this role on our smart contract? And then they're allowed to write to this particular data source. We're doing all this I didn't say that. We're doing all this because we want people to have a way of writing and reading data without signing transactions all the time or accessing the blockchain all the time. So this is free, this doesn't cost any gas, you can just do it.
00:08:35.460 - 00:09:35.042, Speaker A: I put a link there to a talk for anyone who wants to dive into this is really accessible, actually, for anyone who wants to dive into Crtts more, which is the base of this kind of IPFS this uplog. And it allows a lot of cool things, which I am going to touch on later as well. Just keep in mind that CRDTs is the main technology or the main scientific method behind this log. So this is how one person adds to their personal store in the browser. So keep in mind that all of this is happening in the browser. So how do two stores sync with each other? It's really easy. You just have one store and then you have another and they try to keep themselves up to date.
00:09:35.042 - 00:10:21.380, Speaker A: And this is what Orbit handles for us and this is also what the CRDTs handle for us. It's just one implementation of this technology. And at the same time, whenever you replicate stores, all of the operations are then again piped through your access control. That means that no one can have a fraudulent store that is synchronized with yours. Every single package is again checked against the access controller. The way they communicate is what I already said. Obitb uses IPFS pub sub for these stores to talk to each other and basically say I have a new operation.
00:10:21.380 - 00:11:48.204, Speaker A: How many do you have? Or what's your newest operation? Hash, should we sync now? And then the whole operation is started. So one thing that you have to keep in mind all the time is that it's fundamentally different from any other data store that you're used to in a centralized world. You know, maybe MongoDB, you know, MySQL or any kind of SQL storage. In Orbitdb, everything is a database, everything is a store. That means how do you access stores? How do you know where something is stored on IPFS? How do you know who's got this information that I need? And in this scenario, or in general, Orbitb has store addresses for every single store and that you can just address it's basically like an IPFS hash, you can address it like an IPFS hash. So as long as you know the IPFS hash and a bit of more information, which is technically just the store name, you can access the store. And that means you have to kind of have a cascading layout for your stores.
00:11:48.204 - 00:13:05.576, Speaker A: So how do you know, for example, in Colony we have the main entity which is colonies and a colony can have tasks. And in this case we have a Colony store which then links maybe I can move the mouse pointer over here, which then links to a Task store which is a list of other stores. And every single task in Colony is also a store in Obgb. And this is our way of getting around this problem of how do you organize your data, how do you store data and how do you address your data? So what you need to know in the first place is just the Colony store address and then from there you can fetch all of the other stores that are being addressed. So now you might ask how can we get the Colony store address in the first place? And I'm going to talk about that later. So yeah, what does all mean? I kind of talked about that already a bit in a sense. We have lots and lots of databases which are stored in autotb and we have to kind of control them.
00:13:05.576 - 00:14:04.030, Speaker A: And we have always on a case by case basis, we have to decide which data type we want to use, which kind of cascade we want to use for our store and in the end how the whole structure is going to look like. You cannot use any of the stuff that you're used to from traditional centralized databases. You have no joints, you have no aggregation, you have no many to many relationships, which is particularly bad. So no GraphQL possible, you cannot have multiple indexes or something like that. So you have only one index per store and you have to kind of deal with that. Yeah, and in the end it's peer to peer and that's kind of what we have to live with, I guess. But we want to be peer to peer as much as possible and that's our trade off.
00:14:04.030 - 00:15:31.352, Speaker A: So now we have another problem with this and that is how do we keep Data alive? Because there's no central entity. How do we know which is the newest version of data? Which is the most up to date version of something? Let's say I add something to one store and another person adds something to their store and we synchronize and then I go offline and the other person adds something to their store as well, meanwhile and then they go offline, I mean offline, which basically means they close the browser. And how do I know that they just added something to their store? How am I supposed to know that? So they have the most up to date version, but they're offline and I cannot get it anywhere. So this is what we call pinning as well. You have an IPFS, you have the concept of pinning data. So what we thought would be a good idea, or like the only case around that we need some sort of additional client in our peer to peer network, which always has the most up to date version of something. And yes, absolutely, we can pin content.
00:15:31.352 - 00:16:18.260, Speaker A: That means that there is one entity which is not really a centralized thing, but it's just acting as another peer for our network of peer to peer clients, which always has the most up to date version. This is our solution. I just today pressed the red button on GitHub to open source. It not the one to delete it. You can check it out if you want. It's pretty simple. It's just what happens is when some peer in the DAP wants to add something to their store, they kind of notify our pinning service, basically saying, I have a new piece of data and I kind of want to pin it.
00:16:18.260 - 00:17:14.760, Speaker A: And then the service is opening the store that this data belongs to, and then they synchronize and then it closes the store again after a while when it's not needed anymore. So this is happening. And when another peer comes online, then this pinning service is going to tell them, hey, someone just gave me the most recent version of it and here it is. And then we can replicate again. So this is our, I would probably say these are our training wheels for the decentralization here. So this seems like a lot of work and it is, honestly, we worked on that for a really long time and I can definitely show you around at some point in our DAP, which is the client side of it, of this pinion thing. But there are some good parts here.
00:17:14.760 - 00:18:18.556, Speaker A: So what we get for free here is data reactivity. Like when two stores replicate it, we get notified by obitb. They basically say, hey, these just replicated, do you want me to show you news data in your DAP, in your react app? And then you get data synchronization, real time data for free, which is nice. We get the conflict resolution for free, which is something that is also really hard to do. Sometimes when you look at other real time applications, then you might not notice that it's really, really hard sometimes to really get this right. And with the CRTs, it's a solution to get this right and we get the peer to peer for free again. Does anyone have any questions until this point? Yeah, is IPFS free or do you have to use gas or anything? IPFS is free.
00:18:18.556 - 00:19:18.458, Speaker A: IPFS is so they have like a method of incentivizing keeping files alive, which is filecoin, they call it filecoin. And that means as long as you keep a file and keep it available for other people, you get rewarded, you get incentivized for that. But in a sense right now IPFS is just free and people do it just because they want to. So you can just use it right now for free in your DAP and it's fine. Next slide is just a lot to take in. Don't really look at it, it's just I made this, I wanted to include it in slides. The thing is here that again, training wheel stuff, the IPFS itself, I told you we have this pub sub thing where peers find each other.
00:19:18.458 - 00:20:05.602, Speaker A: But how do they do that? How do peers find each other in IPFS? And there's a couple of ways that IPFS supports, but not all of them work really great. I made this overview for myself mostly to know what can I use and how do we actually do that? I just linked it in the slides. There's definitely way more descriptions under that link. This is just for you to know. It's complicated. It's complicated. So this is a way of for me, these are the only valid ways of how two peers find each other in IPFS.
00:20:05.602 - 00:21:18.782, Speaker A: And that's like WebRTC because both of them are kind of peer to peer. So this is WebRTC, which is supported by all of the browsers, which basically serves as a star server, basically saying so peers connect to it and then they say, hey, I know about all of these other peers that are online as well and this is how they find each other. And then there's the WebSocket star server which does the same thing, but at the same time it's also tunneling the connection. So we have like one point of failure there. That's why I would probably say WebRTC is better, but sadly it's not because WebRTC is in a current implementation full of errors and flaky and doesn't really work well. So we use WebSockets with caution and knowingly that it's not really peer to peer. So before I told you that we need to find a way of knowing the address of the store in the first place.
00:21:18.782 - 00:22:04.814, Speaker A: Like how we colony. We have two main entry points for stores and there's users and there's colonies. So as soon as I know a user store, I can access all of the other stores that a user has or read them. And as soon as I know a Colony store address, I can access all of that data. But how do we get it in the first place? And how do we get this nice interaction in our DAP? Because people want usernames, right? And how do we do that in a decentralized way? Because normally we'd just say, yeah, I'm just going to store it on a database, map it to an address and that's fine. So if only there was a way of mapping human readable names to addresses. Oh yeah, there is.
00:22:04.814 - 00:22:51.566, Speaker A: And it's really easy and it exists. So if you're interested in that, you should just take a look at our ENS register and resolver. So what we are doing is we are assigning a username. So we're actually creating an ENS domain for every one of our users. When they sign up, they get assigned an ENS domain which is a subdomain of our top level domain. And we not only tie that to an Ethereum address, ENS also allows to map that to more than that. So what we do is we also map that to an Orbit Store address.
00:22:51.566 - 00:23:37.002, Speaker A: So every time you request a username in our DAP, you get assigned an address. I mean, you have an address already, but you get assigned an Orbit Store address which is then tied to your username. That means I can just look up a username and get all of their data by just typing in their username and you get this kind of really nice interaction. This is actually taken from our DAP where you can see that a username is already taken when they type it in. And yeah, this feels very natural to a lot of people when they see that. The only thing I wanted to touch on is forking dependencies. Orbitb was for a long time.
00:23:37.002 - 00:24:18.486, Speaker A: I mean, it still is kind of alpha status and we forked it because we needed a lot of features like access controllers that we built ourselves for Orbitb. Eventually they merged our PRS and we could move on. But I think we had to maintain our own fork for, I don't know, eight months or something. Which is not the nicest thing that you want to do. In the end, I would probably suggest, depending on how big the changes are, to use patch files. Otherwise you're stuck with a fork way around it. That just leaves one more slide.
00:24:18.486 - 00:24:31.260, Speaker A: I want to thank you for your attention here and yeah, I hope at some point we can get rid of our training wheels and really drive, like free handedly. Thank you.
