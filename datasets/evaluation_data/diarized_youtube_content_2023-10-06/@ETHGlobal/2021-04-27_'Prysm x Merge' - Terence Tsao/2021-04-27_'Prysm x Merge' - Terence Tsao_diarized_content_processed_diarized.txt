00:00:00.570 - 00:00:20.800, Speaker A: All right, looks like we are almost good to go. So yeah. Next up, we have Terrence from Prismatic Labs who will talk about how Prism plans to interope with other Ease One clients. And I think he's here already.
00:00:22.370 - 00:00:22.782, Speaker B: Hi.
00:00:22.836 - 00:00:24.998, Speaker C: Yeah, can you hear me? Hi.
00:00:25.044 - 00:00:25.350, Speaker B: Hey.
00:00:25.420 - 00:00:26.600, Speaker A: Yes, I can.
00:00:27.610 - 00:00:36.454, Speaker C: Thank you for having me. It's good to be here. So, let me share my screen. 1 second. Yeah, I'm super excited to be here. So, can you see my screen?
00:00:36.492 - 00:00:37.240, Speaker B: All right.
00:00:40.120 - 00:00:41.430, Speaker C: Are you able to?
00:00:43.420 - 00:00:44.392, Speaker A: Looks good.
00:00:44.526 - 00:00:50.490, Speaker C: And then you can hear me as well, right? Cool. Can I go?
00:00:51.500 - 00:00:53.400, Speaker A: Yep, the stage is yours.
00:00:53.560 - 00:01:23.510, Speaker C: Okay, thank you so much. So, hi, everyone. My name is Terrence. I'm from Prismatic Labs. I'm most known for my work on Prism, which Prism is an E Two client. So in this talk, I will go through more on the client implementation side on what our perspective on Renisson, the hackathon, which Protolanda is doing a great job hosting, and the upcoming merge. So let's get started.
00:01:23.510 - 00:02:24.824, Speaker C: So, agenda. So, I will touch base on consensus on execution, most importantly, the separation of concerns here. But I will skim through them rather quickly, since the early presenters already did a really good job on covering them. So if you haven't checked out the previous presentations, I highly recommend you to do so. It's recorded on YouTube. And then for the second half, I will cover more client implementation detail. I will dive deeper into the code changes, some of the design decisions that we made, and my goal is to hope to attract more client implementers into this ecosystem, whether it's for Prismatic Labs, Cinema Prime, Lighthouse, Taco, and the E One stuff, or the E Two stuff.
00:02:24.824 - 00:02:58.564, Speaker C: There's so much to do, and the more people can get, the better. So I read this article by Burnerby a couple of days ago, and there's this quote that caught my eye. It says that Ethereum Today is what E Two developers are calling ETH One, the current proof of work chain that we all know and love. Ethereum Tomorrow. Isn't it one nor e two. It's just ethereum, right? And I highly recommend you to check this piece out. It was a great read.
00:02:58.564 - 00:03:38.290, Speaker C: So, from this I would try to reframe myself from saying E One or E Two. Instead, I would be using the proper terminologies, which is beacon. Chen becomes the consensus layer. It is responsible for the agreement of data, right? And beacon chain will be represented by what today is called the Beacon Node. The Beacon node client. Their prism, their lighthouse, their nimbus, their teku. And I'm sure most of you guys have had experience with them, whether it's for staking, whether it's for playing around.
00:03:38.290 - 00:04:35.036, Speaker C: And then there is this current Ethereum chain, whether it's the execution layer. And then this is responsible for interpretation, it's responsible for transaction of the data. And this is represented by the pre merged proof of work client and that's like get and that's in mind and so much more. So let's go through some graphic representations, right? So now we have essentially two chains. And two chains, they're going on the same time, and both of them have consensus, right? Then you have your current ethereum chain which operate under proof of work consensus. It has its application network, whether gossiping, application block. And then you have your state tree.
00:04:35.036 - 00:05:37.764, Speaker C: It manages receipt, account balances and stuff like that. Then on the other end of the universe, you have your beacon chain, which is operate under proof of stake consensus. And this beacon chain is governed under validators, not the miners. And the chain consists of what we call management data. It's governed by validators, right? You track validated balances, you track the latest slashing. And so that and the network is gossip via consensus objects, how much the validator balances us, what validators are voting, whether the validators are performing poorly or nicely. And then you have your beacon chain state, which tracks the balance, the randomness finality and all the fun stuff, right? So there is a subtle link here that is the deposit contract and because of the bitch and chain itself cannot come to life.
00:05:37.764 - 00:06:51.692, Speaker C: It needs some sort of bootstrap mechanism, right? So it's bootstrap, the stakers on the beach chain is bootstrapped using this deposit contract into the current ethereum chain to deposit 32 E. So there is this very tiny subtle thing here. So here's more graphical representations. And you can kind of think of beacon chain, the consensus layer, as a wrapper on top of the execution layer, which is the so called current ethereum chain. And so under this merge, the execution layer no longer needs proof of work because it can piggyback on the proof of stake consensus that bitch and chain operates in, right? So it no longer needs the agreement on data, it needs the agreement on transaction ordering, right? So its focus becomes execution only. And that's like EVM that's data management, that's transaction ordering, that's the gossiping of the transaction data.
00:06:51.746 - 00:06:51.964, Speaker B: Right?
00:06:52.002 - 00:07:25.930, Speaker C: So noted. You don't need to transaction typical block anymore, you just need to transaction data. And you realize in this model, I also added the data layer as well. And for the sake of time, I won't cover so much into what the data layer transits. But the data layer is more for data hungry applications for robs and stuff like that. And then it's also connected onto the Beijing chain. So the Beijing chain essentially becomes this spine chain that governs through everything.
00:07:25.930 - 00:07:47.296, Speaker C: So now we have two chains, which means that there's essentially two pieces of software. And the hard question is that how do they communicate with each other, how do they interrupt with each other? What's the communication protocol for that?
00:07:47.398 - 00:07:47.952, Speaker B: Right?
00:07:48.086 - 00:08:38.400, Speaker C: So so far we have came up with minimal viable protocols that we can provide to get them to interrupt. There's four essentially JSON RPC calls. There are assemble block, new block, seth, finalized block. I can go over them quickly in detail. So assemble block is when the consensus layer asks the execution layer to help me produce a block. And then where new block is consensus layer asking execution layer to process and verify this block and set head is essentially the transcendence layer telling the execution layer that, hey, this is a new head of the chain because you don't need to run for choice anymore. I can run fortress myself.
00:08:38.400 - 00:09:17.592, Speaker C: And this is a new hat, so just follow it. And finalized block is essentially there's this nice finality property on the proof of stake. Now the execution layer could leverage that property. It helps the execution layer to say, hey, this is finalized block and maybe everything before this finalized block that we no longer care. You can do some nice Pruning and stuff. So, very high level definition, we can go more into detail. And these definitions are also covered in the merge spec and the random spec that proto put out earlier.
00:09:17.592 - 00:09:51.112, Speaker C: So take a look at that as well. So I'm going to take a little turn and start dive more into client implementation details. The client I'm working on is Prison, and it's one of the four implementation. Our client is returning Go. Returning Go. It has this natural interoperability with the current execution client, which I will go more into detail later. And that's our GitHub page.
00:09:51.112 - 00:10:10.632, Speaker C: Give us a follow, give us a star, and yeah, we'll be really happy. So there is some design decisions that we made and I will go through them one by one. So we use protobob and we use gRPC. So you may ask why protobop, why gRPC?
00:10:10.696 - 00:10:11.068, Speaker B: Right?
00:10:11.154 - 00:11:24.148, Speaker C: And we get a lot, right? So we're a huge proponent of protobot gRPC because we love the compactness of the format, the low overhead and then the strong typing, right? Bytes are bytes, integers are integers. And we love the ability to essentially generate schema via spec. And the spec is actually really useful for both servers and client. The spec could be forward and backward compatible changes that cover them really nicely. And it has better communication, primitive, right? You have HTP by default, you get bi directional streaming, you get concurrent request over a single connection and all that, right? And we also can support Rest. We can support GraphQL using its folks as well, right? And then the performance is probably slightly better even for the smaller payload and stuff like. So the argument against protobot gRPC here is that Rest does have the more simplicity, human readable format and stuff like that.
00:11:24.148 - 00:12:13.936, Speaker C: The stainlessness part is very nice and it has more features to support existing infrastructure such as like caches, reverse proxy and all that stuff and stuff like that. But the end of the day is that we can transcode gRPC to JSON very easily with a gateway and that's what we're doing today so we can support both. And this is purely a client implementation detail. Here's some code examples. I will go through some code examples and then I will let you see what it looks like in the protobot gRPC war, right? So this is the beacon block changes under the merge. And Proto showed that earlier as well. This is what's defined it's.
00:12:13.936 - 00:12:17.300, Speaker C: Python it's pretty readable. It's very readable actually.
00:12:17.370 - 00:12:17.844, Speaker B: Right?
00:12:17.962 - 00:13:16.200, Speaker C: You get fields and then you get the ordering and you get how many bytes are in the field. And this is what it looks like for protobuff, right? It is readable as well, right? You get the fields, you get the ordering, you get how many byte size on the SSE for the SSE stuff, right? So this was the execution payload and that's one of the new field. It's essentially what the ETH one block becomes. You translate the E one block into the execution payload and then the payload gets appended inside the big trend block body and stuff. So it's the last field, it's field nine. And here is the bigtrend state changes. So the big chain state changes has this new field.
00:13:16.200 - 00:13:51.030, Speaker C: It's called latest Execution Payload header. And then the header consists of the current data executions block, hash parent, hash Chong base and all that fun stuff. But it's readable in Python and it's also readable under protobox. And here we define the SSD size. So we know like when we marshall Marshall, when you has true root, we can get the consistent data behind it. This is what it looks like.
00:13:52.840 - 00:13:53.590, Speaker B: Right.
00:13:56.920 - 00:14:03.704, Speaker C: Now, the next question we have to answer is that how do consensus and execution nodes start?
00:14:03.822 - 00:14:04.440, Speaker B: Right?
00:14:04.590 - 00:15:22.910, Speaker C: So now there's probably two pieces of software, two pieces of software that we have to consider. So you start your execution node, which is your current ETH one node like Get calidis net and mine and more, right? So you launch a command and the execution node has RPC server. And I'm just using Get as example right now and I use a five, four, five localhost. And then you start your vintage node which essentially has this proof of work node client that listen to F five, four, five, and then you start your validate client. So there's three steps to it and it's not pretty, right? Don't get me wrong, this kind of sucks to have to type this and launch this three times and stuff like that. So a cool hackathon idea that if anyone wants to work on is essentially translate this into a more unique front end stuff. So whether you can launch it with a script or launch it with a UI and stuff like that, and I think the community could really benefit that.
00:15:22.910 - 00:15:30.500, Speaker C: So how do the, how does, how do the interactions look like?
00:15:30.570 - 00:15:31.140, Speaker B: Right?
00:15:31.290 - 00:16:26.656, Speaker C: So you can think of this as two distinct event, right? So there's two things that that so two things that could happen. Either you are processing a block or you are producing a block, right? So to process a block, it's very simple. You get your typical bacon block from the P, two P gossip. And then it goes to the networking service, and then it goes to the stage transition service, right? And at the Stage Transition service, you break that block apart. Then you get this Execution payload which consists of application data. And then you pass that into the if one proof of work client service. And then you pass that to the get your netmi node.
00:16:26.656 - 00:17:19.664, Speaker C: And then they will verify that and then tell you whether, hey, this is valid data, I can perform valid execution on top. Then they will return the status. Then you can safely confirm that, hey, my become blockchain is correct. So that's on the processing side, right? And now there's another event. As a validator, you get to produce block, right? You get to propose blog and that's how you make your most money. So to propose a block, it's the same path, right? Essentially you determine you're the proposer of a slot. Then you ask your Get node be like, hey, I'm producing, can you give me some data to produce? And then get will return execution payload.
00:17:19.664 - 00:18:11.020, Speaker C: And then you package that into the block, you pass it to the validator client. The validator client then signs off on the data and then you broadcast to the entire network. So there's two paths, but it's pretty similar into this process, how we process and how we propose. The only difference is that we add this addition step that we have to check with the Execution client. So I was actually surprised that the lines of changes so the amount of changes were not so much. And it's pretty nice so far. If you look at our branch, it's probably like 500 lines of changes.
00:18:11.020 - 00:18:50.870, Speaker C: But keep in mind, this is not production level code. We're doing a proof of concept here. So this is predominantly so the number of lines of code may increase later, right? But with that said, I do think the heavy hitting is on the ETH One client side. They're doing more subtraction by addition. So respect and props to the E One client for doing a lot of those work as well, right? So prison probably has 25 package as of today. And then the only changes are in those package. So probably 20% of the package are getting changed.
00:18:50.870 - 00:18:56.770, Speaker C: So here's just one of the main addition.
00:18:56.850 - 00:18:57.480, Speaker B: Right?
00:19:02.670 - 00:19:59.654, Speaker C: After you process the block, you check with this application attitude client on the validity of the data. So essentially you take the block, you look inside the body, you get the Execution payload and then you basically pass the translate the payload from our protobot data to JSON data. And then you insert that into the Execution client. And the Execution client will tell you whether the data is valid. And if it's valid, then you can process the block. Very simple, right? And then on the other side, to produce a block, you essentially ask the execution client to give you the latest payload. And then you return the payload back to the beacon block.
00:19:59.654 - 00:20:29.110, Speaker C: And the beacon block gets signed by the validator and gets broadcast over the network. So again, it's very simple. So here's a few screenshots that we did have the interruption running earlier. So I just covered a few screen, I put up a few screenshots for fun. They're the same screenshots that Proto posted earlier. This is just a prison beacon node. Inserting application execution data into the ETH one node.
00:20:29.110 - 00:20:45.350, Speaker C: And this is a view on the if one node. It's producing block and importing new chain segment. And this is the validator client. Validator client are attesting the block and also proposing the block.
00:20:47.470 - 00:20:48.220, Speaker B: Right?
00:20:49.070 - 00:21:36.582, Speaker C: Now I'm going to go through a few more slides of design decisions. So we use Basil and we get this a lot, right? Why Basil? And Prism is a model repo. So instead of using the default Go build system, our team relies on the Basil build system to manage this mono repose structure, right? We love it because of its reproducible build. It has a sandbox environment and the builds are pinned with proper dependencies such as Go versions, right? And this would be guaranteed to have the same outcome despite different developers, users may have different machines.
00:21:36.646 - 00:21:37.770, Speaker B: They did, right?
00:21:37.920 - 00:22:15.510, Speaker C: So some common issues that I've seen that bars introduced because of the environment is different than being shipped to users. There's just messy dependency management when dealing with a monorepo with multiple programming language, right? And bezos is nice. It also has more caching and then it has advanced dependency analysis and stuff. So we have bezo go ethereum repo. It's a basil friendly copy of Go Ethereum. It's nice because we do have Go bindings. We take advantage of using Go.
00:22:15.510 - 00:22:58.520, Speaker C: We create Go bindings on top of JSON RPC, so we no longer need JSON RPC cards for the RPC codes that we need to do and stuff like that. So check out the report if you're interested. It's very cool. And here's just an example of when we call a sample block. If we want to produce execution block and we can just literally dial the Http client, in this case is gas. Netamide, and then we just call assemble block. That's it, plain and simple.
00:22:58.520 - 00:23:27.262, Speaker C: We also use this SSD library, it's called Fest SSD. So a quick rewind, right? What is SSD? So SSD is a serialization format that's used in e two. It essentially conveys two standards. The first standard is encoding and decoding. So how do we decode e two data such as patient block and patient state?
00:23:27.316 - 00:23:27.920, Speaker B: Right.
00:23:30.050 - 00:23:48.370, Speaker C: We essentially want a string of bytes that can be sent over the network or stored in the database. And that string of bytes have to come to consensus with each other. Then the second one is Mercurization. So how do we find the hash of the data? And in this case, we don't say hash, we say the virtual root of the data.
00:23:48.440 - 00:23:49.060, Speaker B: Right?
00:23:49.430 - 00:24:33.140, Speaker C: So we use this FSS library. It's essentially generated code, so it's very fast. And if you're working on this SSC related fields, check out that repo and it is very interesting and there's a lot more work can be done there as well. So, just to recap on the API, we have our own Ethereum API. That's the protobox spec that we implemented. It has more advanced features in terms of just bi directional streaming and it's a little bit more powerful. Then we have the standard E 20 API, which we also comply as well.
00:24:33.140 - 00:25:14.510, Speaker C: And then the default endpoint on the beacon chain on the beacon node is 3500. So if you're playing around with it, you're building cool hackathon project with it. Definitely use those. And the next one is matrix, right? And matrix is super useful in terms of debugging and then monitoring. And then we use Prometheus for scraping matrix. There are default 80 80 on the beacon node, 80 81 on the Bell deck client. And then we use Grafana to basically for visualization.
00:25:14.510 - 00:26:22.982, Speaker C: And here's a cool one that our community member build, right? And then maybe a cool hackathon idea for this is that someone could combine the matrix for BC node and also the execution node, combine them to the same place to visualize performance, to visualize transaction related interesting fields and topics and stuff like that. And just another idea for our Kubexicon project then. Yes, I'm personally very excited to hopefully to have a merge testnet very soon. We're working towards that. I think there's some great value in understanding how the merge will work. Once we have a testnet with the community and then people can interact with DApps and then you can visualize the network performance and stuff like that, I think that will be very useful and very valuable as well and getting towards the end. So to contribute, feel free to go on our discord or check our GitHub open issues.
00:26:22.982 - 00:27:02.020, Speaker C: We have a contribution guide as well, and not just for Prism, prismatic Labs, right? Please do check out other E two clients. They're not here today, but they are not here presenting today, but they also are doing amazing work and they could use help as well. So check out Taku, Lighthouse, Minbots and Lone Star. There's definitely tons of work there and they are also amazing. And that's for me. Thank you for having me. And shout out to Ethbobble for hosting such awesome event.
00:27:04.710 - 00:27:37.680, Speaker A: Great. Thanks for all the insights. We have a couple of questions that I want to relay to you. So first of all, while your talk was going on, there was a lively discussion already going on in the chat with regards to execution, responsibility and basically the overall question was like, why not implement the ETH one execution functionality into the beacon node software directly? The person who was asking this, he already got a reply via the chat, but he wants to hear your opinion on this as well.
00:27:38.050 - 00:27:38.800, Speaker B: Right?
00:27:40.850 - 00:28:33.742, Speaker C: I think there's two sides for this. So I think there's short to midterm and there's long term. I think to mid to short term, having the separation of concerns is nice. Giving the timing constraint and stuff like that. And you also lower the risk, right? You don't want to redevelop EVM, you don't want to redevelop transaction pool, you don't want to redo those, right? It takes years of engineering research expertise of that, right? And then we want to leverage as much as possible. So having a separation of concerns is nice. So now the question is that how can we make the process better, right? Whether it's launching a unified front end to essentially to kind of make people think that we're actually not running multiple process, but we are running multiple process.
00:28:33.742 - 00:28:40.580, Speaker C: So I think that's a really good hackathon each project to think of and I think people should pursue that.
00:28:43.270 - 00:28:51.110, Speaker A: Then another question right here. Will the public testnet also interact with other non prism clients and any time frame for the testnet?
00:28:51.610 - 00:29:37.790, Speaker C: Yeah, so the goal for the testnet is to be multi client ish to start with, so there will be lighthouse. They're doing great work tactical, of course, they're the first one that's doing it and inbuilt as well. So there will definitely be multi client talking with multiple institution clients. Well, right. So, timeline, I think this week we've been making a lot more progress. To be honest, we have probably made so much progress the past week than we have for the whole month. So I think we're close, I think next week and early next week, with the leadership of proto and stuff, we should be able to launch semi DevNet.
00:29:37.790 - 00:29:41.140, Speaker C: And once that goes well, we can go full public.
00:29:43.370 - 00:29:49.320, Speaker A: And then lastly, a rather technical question. Why are you using Bazel instead of pure go?
00:29:49.950 - 00:30:18.880, Speaker C: Oh yeah, I thought I covered that. So, long story short, we use Bezel just because of the reproducible build, because of the mono repo ish setup that we have and also because of the remote and then the local caching and stuff like that. And I get that, right? Bezel, there's a little bit of learning curve and stuff like that, but once you get used to it, it's not too bad. It is super fun.
00:30:21.410 - 00:30:27.780, Speaker A: I think that's it in terms of question, thank you so much for joining and sharing all those insight with us.
