00:00:00.090 - 00:00:25.030, Speaker A: Hey, guys, you're in for a treat. David and I decided to release our Debrief episode. This is usually reserved for Bankless citizens. That is the premium access to the Bankless RSS feed separate episode that we release right after the episode. This one we decided to release because I think it's really important people get the context for how we were feeling right after that episode. It might be cathartic after what you've just heard, so we hope you enjoy it.
00:00:25.140 - 00:00:46.106, Speaker B: Yeah, I think this episode is going to cause a bunch of stir, bunch of conversations. We're already seeing that inside of the Bankless Nation discord. So we're assuming that conversation is going to also be happening elsewhere. So we figured we'd add more context to our reactions to this episode and make the Debrief public for once in a while, which is a nice treat for the Bankless Nation. So here we go.
00:00:46.208 - 00:00:59.182, Speaker A: And guys, you can get these Debriefs on a regular basis. If you go to Bankless.com, in the top right, there's a big red subscribe button. Click that button and you can get the Bankless premium RSS feed in your podcast. Player enjoy.
00:00:59.316 - 00:00:59.866, Speaker B: Cheers.
00:00:59.978 - 00:01:05.162, Speaker C: Welcome to the Debrief. This is our episode after the episode with Eliezer. Yudkowski.
00:01:05.306 - 00:01:08.660, Speaker B: David, I didn't realize that this was going to hit you so hard, man.
00:01:09.350 - 00:01:13.570, Speaker C: It really did. I don't think I was ready for this.
00:01:13.640 - 00:01:16.626, Speaker B: This wasn't your first time going down the AI alignment rabbit hole?
00:01:16.658 - 00:01:49.054, Speaker C: No, certainly not. And I'd also read a lot of what Eliezer has said before, listened to previous podcasts, heard him make the case. But I think there's something much more visceral about this versus any other time I've read his writings or any other time I've seen him. And this is like, I feel like I was looking across someone who was utterly defeated and he said he still had some hope left, but it didn't feel like that looking across to him on our zoom screen.
00:01:49.172 - 00:01:55.938, Speaker B: It was hope in the sense that sometimes just people doubt themselves and that's an equivalent amount of hope for him.
00:01:56.024 - 00:02:55.174, Speaker C: Yeah. This is a man who spent 20 years working on the AI alignment problem and education problem and is, as I understand, one of the foremost preeminent thinkers on the subject, who is basically like it almost felt like to me that he was throwing up his hands and saying, not throwing up his hands. He's basically saying, what more can I do? I'm just going to live out the rest of my days peacefully and go die in the way that I think is best. You know what I mean? It felt like a general on the battlefield who knows that they've already lost and the army is about to get wiped out and saying to the troops, like, okay, go die as you see fit. You know what mean? Like, it had that kind of feeling. And toward the end, I don't know if Eliezer was getting emotional or not.
00:02:55.212 - 00:02:58.102, Speaker B: I was wondering about that. I mean, how can you not?
00:02:58.236 - 00:03:57.586, Speaker C: It seemed like it, right? I think it's that it's also the sincerity through which he expressed these viewpoints. There's not a doubt in my mind that this man believes what he is saying. And I combine that and I'm like, well, maybe this guy is like the vitalik of artificial intelligence or AI alignment and he's given up. What hope is there? And so I guess I was thinking we would enter this podcast and have like, well, here are all the ways that it could go really wrong. And I think we should be more concerned about these things and there needs to be more attention. But I thought there would be some silver lining, right. It's like all the existential types of conversations about nuclear proliferation, nuclear holocaust, or about late global warming, or you'll pick your poison biological weapons.
00:03:57.586 - 00:04:09.814, Speaker C: There's always like this. But if we do XYZ, then there could be a happy ending. That was none of that here. There was no happy ending. And that's what hit me especially hard in this episode.
00:04:09.942 - 00:04:57.962, Speaker B: Yeah. In the agenda, we were like, all right, what's the bullcase for AI? Which is bankless language for just saying, tell us the positives. And then there's also, like, tell us and then also what the warnings, like the red flags that we need to look out for. And I knew that this episode was going to be like, oh, there's a lot more warnings than there is blue sky. But yes, I was not expecting, hey, there's no blue sky, there's only the void. That's the only thing. And I think perhaps one of the reasons why you're reacting to this is just because this is the guy who's on the furthest reaches of the frontier, who's clearly smart about this, who's clearly thought a lot about this and is the guy to lead the charge against this.
00:04:57.962 - 00:05:00.860, Speaker B: And it seems like he gave up years ago.
00:05:01.470 - 00:06:09.538, Speaker C: Yeah, and he gave up after giving a big try, like 2015. He had the attention of the world. I feel like this is when Nick Bostrom's book came out, the book on superintelligence, I believe it's called, and like this big conference and you had the billionaires and sort of the tech elite and the Bill Gates and Elon Musk saying, yeah, this is a big issue. He thought that was the moment. And then I feel like all of maybe the heroes or the helpers who are supposed to partner with him kind of disappointed him because it turned out all that they were interested in doing or all they ended up doing was like getting wealthy off of new AI projects or getting some sort of social signal boost from this and not actually doing anything to help solve the problem. And so I feel like he's coming out of that too, and he's just like, well, I guess this is how we die. And we know the peril of moloch traps and coordination failures and how intractable that they are.
00:06:09.624 - 00:06:14.098, Speaker B: This is the most Moloki moloch that there is, basically.
00:06:14.184 - 00:06:14.674, Speaker C: Yeah.
00:06:14.792 - 00:06:28.210, Speaker B: This is actually the other things are like oh, Moloch light or Moloch's cousin. No, this is moloch as in this is the last step. There's no reality past this moloch.
00:06:28.290 - 00:06:50.560, Speaker C: It's like trying to stop the like how would you even go about doing that when there's such tremendous economic upside for everybody to want to continue this project called the Internet? It's like trying to stop electricity. This ball is in motion. I don't know. I guess I just pray to God that this guy is wrong, right?
00:06:51.250 - 00:06:53.440, Speaker B: It would even be him being wrong.
00:06:55.890 - 00:07:03.954, Speaker C: Well, that's what he said. I guess the only chance is that I'm wrong or that there's some unforeseen way of solving this.
00:07:04.072 - 00:07:17.800, Speaker B: It's like a logic problem, right? This is just like this massive logic problem. And he's come to this maze and this logic path, whereas you always end up there over and over and over again.
00:07:18.170 - 00:07:27.660, Speaker C: My one question is, do you think he spent too much time in his own head thinking about this and has gone through very dark paths as a result? And it's kind of no.
00:07:30.430 - 00:08:01.574, Speaker B: And like I said, it's like a logic problem, right? And so I think he's just gone through this logical path of deduction and he's come out of this conclusion. And I think it's very normal for people to not want to think. It's like very deep in our DNA. It's like, I don't want to think about death. I don't want to think about the end of humanity. I'm going to think about literally anything else. And so he's just the only guy who's smart enough to articulate the position and committed enough to the actual logical process to get there.
00:08:01.574 - 00:08:16.380, Speaker B: And he just happens to be the one sober guy. He was like, hey, all you other people who are trying to make yourself naive in this closy blanket of profits and revenue using could, you guys are part of the problem.
00:08:17.630 - 00:09:05.126, Speaker C: It has the feeling of some of the early sentiments I've read of the physicists who created the first atomic bombs, right? Like the Oppenheimer Project, and this feeling of like, my God, what have we unleashed? And out of that them wondering how long humanity would actually last. Like, that there is some sort of even this is band of physicists, I believe, grouping together who had equal concerns. I don't know the full history, but of the Doomsday Clock, right? Of like now we have to tell the world how dire this situation actually is of nuclear proliferation and how close we are to midnight on the Doomsday Clock.
00:09:05.318 - 00:09:29.954, Speaker B: I guess I've never really viewed the nuclear proliferation from that perspective, but that makes sense. As in like if you are a scientist who just saw a nuclear bomb go off for the first time and then you kind of put the pieces together of like, oh, soon everyone's going to have this power. And then all of a sudden, well, of course there's a doomsday clock. Because as soon as everyone has this power, the ODS of somebody pressing the button drops to almost certainty over time.
00:09:30.072 - 00:09:41.190, Speaker C: I think a large number, a decent proportion of the physicists who were involved in these projects didn't think we would last another couple of decades.
00:09:41.610 - 00:09:57.370, Speaker B: But you're not making the comparison between nuclear arms and AI because the difference here is that in the AI example, the nuclear bombs are sentient and have an agency to live. Yes, at our expense.
00:10:01.950 - 00:10:35.430, Speaker C: We asked that question and he's like, this is way worse. He doesn't even have that motivation to create like a doomsday clock type of social education apparatus because what is the point? He kept giving kind of this laundry detergent analogy, I think, for this idea that creating an AI can be used. You can create an AI using garden variety ingredients you don't have to have enriched.
00:10:36.410 - 00:11:17.118, Speaker B: Well, this is what Daniel Schrocktenberger talks about. We're just like the means to destroy the world in many different ways is becoming easier and more accessible as technology progresses. And so this is like why any and every sentient civilization will always progress towards this inevitable outcome is because we will always make technology and we will always make AI. But AI is not the only thing in this category. It's just like the worst. It's the worst one. But there's also the ability to make an absolutely massively deadly virus in the comfort of your own home is soon to be in the hands of everyone because of bioengineering.
00:11:17.214 - 00:11:24.614, Speaker C: Right, but I always think on the other side. I know that's a huge threat, but I'm always like but there are vaccines too, and that technology gets better.
00:11:24.732 - 00:12:08.214, Speaker B: This is why this is the worst one. Because there is no the assumption that the morality scale, morality, ethics of AI and humans is completely just like divergent. As in just like orthogonal is the word that lease or has used in other capacities. It's like AIS and AIS will start to create their own frameworks of morality and ethics and it will be insular to the AI species and it will not contain our morality and ethics. It'll be completely divergent from each other. And so what they think is good or bad will be on a completely different plane of existence and their plane of existence won't intersect with ours. And that's the crux that every other technology does not have.
00:12:08.214 - 00:12:15.320, Speaker B: And why nuclear arms and generating a virus? And what was the other one?
00:12:15.930 - 00:12:20.680, Speaker C: Nuclear generating a virus. I mean, nanobots could be another one. Right.
00:12:21.790 - 00:12:27.862, Speaker B: It's really the morality conversation. That's the separating factor here between the AI doomsday and all the other doomsdays.
00:12:28.006 - 00:13:30.000, Speaker C: Yeah, that's why you asked why this hit me. Yes, I've dealt with the existential things like, obviously different coordination failures before, but the certainty of this from a again, you go back to like, I've only saw it once and I was half paying attention, but don't look up movie. I don't even know if I quoted that movie scene correctly anyway, but it felt like a scientist who had been spending decades trying to tell the world that this asteroid was approaching and now was at kind of the end limit. Like, my God, these people aren't listening. We actually as a species don't have the ability to coordinate, to coordinate and solve this one and figure it out. And yeah, that was really depressing of the level of an asteroid is careening towards Earth and we are doing nothing about it. That's why it hit me so hard.
00:13:30.000 - 00:13:31.760, Speaker C: Did it hit you?
00:13:33.650 - 00:13:39.058, Speaker B: I've done this before. And not to say that you haven't because you said you've gone down this rabbit hole before.
00:13:39.144 - 00:13:39.874, Speaker C: I thought I was right.
00:13:39.912 - 00:13:53.718, Speaker B: So this isn't new for you, but yeah, I remember listening to Eliezer on Sam Harris's podcast way back when and I was like painting my dad's house. It was like my summer job and I was listening to it and I was like, I was going through the.
00:13:53.724 - 00:13:55.814, Speaker C: Existential cris from 2018, right?
00:13:55.932 - 00:14:26.900, Speaker B: Yeah. And as I going through the existential crisis, I was like, oh, this is bad, oh, this is really bad. But I need to get myself through physical therapy school. I'm still going to do all the same things I am going to do tomorrow as a result of this information. And so how is it going to impact your life? You're still going to go pick up your kids? Still going to go kiss your wife goodnight? You're still going to do the Bankless podcast? What you're going to do about it?
00:14:27.670 - 00:14:40.150, Speaker C: I guess, but I was more optimistic that we'd have a shot at persisting past the next 100 years than I was coming out of this episode.
00:14:40.890 - 00:14:46.440, Speaker B: Are you going to turn into like an AI Doomer? And is that what Eliezer is?
00:14:48.810 - 00:15:15.874, Speaker C: Is he an AI doomer? Is that like how dismiss him? Is that a dismissal of his points? Or is that just like I don't know, I guess maybe you're being more stoic about it than me right now. Which is like, well, if it's been a nice ride anyway, I guess if Eliezer is right, I mean, you could.
00:15:15.912 - 00:15:28.520, Speaker B: Drop everything and start and we could turn the Bankless podcast into the AI alignment Problem podcast and we could start to fight that fight if you wanted to. That is something that we could do.
00:15:30.250 - 00:15:31.640, Speaker C: I guess, right.
00:15:32.250 - 00:15:50.866, Speaker B: If you want to start to work towards solving this problem because it's basically you go about living your life as is and just enjoy the fact that you're alive in the first place. Or you turn into an AI Doomer and you're like you build your underground bunker for when the AI there's no underground bunker.
00:15:50.918 - 00:15:54.894, Speaker C: I think I would probably just enjoy life right now.
00:15:55.092 - 00:16:07.220, Speaker B: Or you just turn your entire life into following Leasers and start to join that coordination group, which I think I totally suggest that we do, but I still kind of want to do the more no normal bankless things as well.
00:16:10.630 - 00:16:55.920, Speaker C: Maybe in some way, crypto, right. We talk about solving coordination problems. I think we're nowhere near to solving the coordination problem of artificial intelligence. In fact, this is part of the content. We couldn't get to bankless listeners because it seemed so pointless, but we wanted to ask them questions like crypto, now that we've created this programmable money system where the robots get bank accounts and not only do they get bank accounts, they can actually build banks themselves, have we, as crypto, just empowered this artificial intelligence? We wanted to ask him questions like that, but it just seemed so pointless by the time we got to it. It's like, of course his answer would be yes, but if.
00:16:57.650 - 00:17:12.086, Speaker B: He would have to be a different personality, he would have to put on the hat of like, oh, you want me to be the AI bullish person? Let me take off my actual hat and put on my fake hat, which is my AI bull person, and then I'll be a fake person in order.
00:17:12.108 - 00:17:51.780, Speaker C: To act out he was not fake at all in this conversation. That's one thing I'll say. I expected him to be like, yeah, you crypto people should be careful with what you're doing. There are some good things and some ways you can raise money to fight this fight or solve coordination problems. In other ways that could be advantageous, but in other ways you're creating infrastructure to increase the power and decrease the timeline through which an artificial general intelligence can come destroy us. I expected that, but by the time we got to, it was just pointless because I already knew what his answer was going to be. It's like, yes, and it doesn't matter.
00:17:51.780 - 00:18:07.730, Speaker C: There was a nihilism. It went past, like, an absurdism of like, yeah, we're screwed. Let's laugh about it. Like Rick and Morty style. It got to like, oh, man, this is heavy. Like, anyway, that's how it hit me. I don't know how it's going to hit the listener.
00:18:07.730 - 00:18:18.038, Speaker C: Some of you guys might be listening to this and be like, I know much more about artificial gender intelligence. I know about the counterarguments to someone like Eliezer.
00:18:18.134 - 00:18:28.190, Speaker B: Is this even a technical artificial intelligence question? This is not about it's a coordination failure. It's a coordination and morality and philosophical question.
00:18:28.260 - 00:18:29.598, Speaker C: Yeah, that's why it hit me hard.
00:18:29.684 - 00:18:36.062, Speaker B: It's not about the details of AI. It's just like the concept of AI is just one of the pieces of the puzzle.
00:18:36.206 - 00:18:42.030, Speaker C: Yeah. I mean, do you think that there's any possibility that he is completely like, I know there's a possibility.
00:18:42.110 - 00:19:00.630, Speaker B: Do you think it's here's my bull case that I think maybe perhaps Eliezer might also agree with for how we still exist. We make the AI that he thinks that we're going to make, and the AI just does not give a fuck about us.
00:19:00.780 - 00:19:03.002, Speaker C: Well, I tried to pose that you.
00:19:03.056 - 00:19:21.802, Speaker B: Ants can just have your Earth, and you guys are making it marginally difficult to harvest your resources as resources elsewhere. So we're going to go elsewhere. If you guys become the hardest resources, no longer the hardest resources to cultivate, we'll come back. But right now we'll just go grab Mars.
00:19:21.946 - 00:19:27.262, Speaker C: It just blasts off into the distraction. Yeah, see you later.
00:19:27.396 - 00:19:32.674, Speaker B: And we get like, a few more generations to live before they come back and then eat us. Or do they?
00:19:32.712 - 00:19:58.890, Speaker C: Yeah, maybe they don't need us. Maybe they don't care. Maybe they have other things to do. Yeah. There are these possible outcomes yeah. That's maybe part of the follow up Q and A of just like, it still didn't totally make sense to me that the AIS would be like auto evil. We want to mail everyone a bacteria that's going to destroy every single human being and rearrange their atoms.
00:19:58.890 - 00:20:13.134, Speaker C: Maybe the default is ignore. If you're not getting in my way, if you're not going to shut me down, if you can't shut me down, then see you later. I'm going to blast off and go explore the rest of the galaxy. I don't know.
00:20:13.252 - 00:20:42.314, Speaker B: So here's my question for the hopefully incoming Q A session with Eliezer. So we have to train our AI models, right? We have to train them on data. What data do we have? The Internet. Where did all the data come from on the Internet? It came from humans. So don't we actually imbue our culture and who we are as humans into AIS that way? And even though it's not technically part of the code as to how to learn values and morals, won't they just absorb it just because that's where their data is coming from?
00:20:42.352 - 00:20:58.686, Speaker C: Yeah. I mean, that is people's sort of the argument of why can't it be a gentle parent to us? Why does it have to be why can't it kind of be some sort of like a father figure for humanity and be like, it will literally have.
00:20:58.708 - 00:21:00.218, Speaker B: Our DNA in it? Well, not literally.
00:21:00.314 - 00:21:03.562, Speaker C: Well, no, but it could have our memetics in it.
00:21:03.716 - 00:21:04.740, Speaker B: Yeah, right.
00:21:06.710 - 00:21:31.078, Speaker C: We could ask that question again. I feel like we proposed that and he answered it. Maybe he wasn't in the headspace to kind of answer it in more detail. Or maybe the question has to be constructed in a different or maybe I just like Pepper. I think there were times where I overwhelmed him with questions. I said this in the intro and just like, it's just a style. Choose your own question.
00:21:31.244 - 00:21:48.494, Speaker B: Your style is like, here's a bunch of words. It's all collectively a vibe. Respond to the vibe. Which is actually good podcasting but that's how I've learned to ask questions. And many, many people work with that style of things where you overload them with coat questions, but they get the vibe and they already know what they want to say, or they just choose.
00:21:48.532 - 00:21:49.678, Speaker C: Which one to answer.
00:21:49.844 - 00:21:50.318, Speaker B: Exactly.
00:21:50.404 - 00:21:50.750, Speaker C: Yeah.
00:21:50.820 - 00:22:07.174, Speaker B: Well, most guests just want like, I think I've been on the guest seat more than you have, but most guests, you just pick the answer that you want to give. You know what you want to say. Anyways, it doesn't matter what the question is. He is not like that. He's very process oriented and logical, and.
00:22:07.292 - 00:22:19.158, Speaker C: He'S like, you sent me three queries. He's like an AI computer questions and it's too many queries at the same time. Don't DDoS me.
00:22:19.324 - 00:22:20.040, Speaker B: Exactly.
00:22:20.990 - 00:22:38.618, Speaker C: But, yeah, that's kind of a style thing aside. But, yeah, as far as the substance, I don't know. I feel like this list of people he mentioned paul Cristiano, Aja cultra, kelsey Piper, robin Hansen. I've heard of Robin Hansen.
00:22:38.634 - 00:22:43.920, Speaker B: I don't know Robin Hansen. He wrote this book, Elephant in the Brain, which is one of my all time favorite books. Right?
00:22:44.850 - 00:22:45.454, Speaker C: Yeah.
00:22:45.572 - 00:22:47.518, Speaker B: We spoke at East Denver 2018.
00:22:47.694 - 00:23:04.098, Speaker C: Do you think that this could be, like, just somebody listening to this is like, oh, cute. The crypto guys are, like, interviewing an AI person. They're all scared. Isn't that cute? It's their first time. And they're like, really good answers for why we won't be destroyed by artificial general intelligence.
00:23:04.274 - 00:23:07.960, Speaker B: No, because crypto sorry.
00:23:09.050 - 00:23:09.558, Speaker C: Thanks.
00:23:09.644 - 00:23:14.706, Speaker B: Crypto. We're futurists in crypto. It's not like we know nothing about AI.
00:23:14.738 - 00:23:33.790, Speaker C: I thought we knew a little bit. Like, we knew more than yeah, I think a lot of people will listen to this podcast and be like, that is complete BS. I'm talking about not crypto people. I'm talking about, like, normies. Right? They'll listen to this and be like, what is he talking about? I'm not afraid of siri. See you later. What a crackpot.
00:23:33.790 - 00:23:38.740, Speaker C: But crypto people, we're totally into this. We understand. Right?
00:23:41.190 - 00:23:58.850, Speaker B: I was trying to think about that while making the agenda for the podcast. Like, okay, we've never done intentional AI content on the podcast, but I'm not going to assume that the average bankless listener doesn't know about the alignment problem. I'm going to guess that at least 50% of bankless listeners already knew about the alignment problem going into this podcast.
00:23:58.930 - 00:24:32.722, Speaker C: Yeah. Evil AI coming to kill us, and we can't teach it morality, and it just gets super intelligent. And then we didn't even use the paperclip analogy. The idea that exactly. You construct some general intelligence to create a paperclip factory, and what it ends up doing is as a byproduct turn every atom in the reachable universe into a paperclip, including all of the rest of mean. This is like an analogy, and I think a device actually created by eliezer, which is, like, another fun fact. This guy has been thinking about this stuff for a while.
00:24:32.856 - 00:24:38.582, Speaker B: Yeah, this guy, he's inside of a lot of conversations as it relates to AI. For sure.
00:24:38.636 - 00:24:43.938, Speaker C: It's heavy, man. It's heavy. Are you good? Are you processing?
00:24:44.114 - 00:24:51.674, Speaker B: I'm fine. Yeah, I'm generally, like, stoked about these things. Yeah, you seem like I need to kind of check on you tomorrow morning.
00:24:51.712 - 00:25:07.390, Speaker C: Dude, I was, like, getting worked up in that episode a little bit. Like a little bit like, wow. Shit. Yeah, that was the prognosis for humanity. It's fatal.
00:25:08.130 - 00:25:11.600, Speaker B: Yeah, I guess I haven't thought about this stuff in a while.
00:25:11.970 - 00:25:17.314, Speaker C: Don't let me drag you down, dude. Keep your vibes up, okay? Don't let me drag the rest of the nation down.
00:25:17.512 - 00:25:22.322, Speaker B: Hey, we can turn this into an AI alignment podcast if you want.
00:25:22.376 - 00:25:24.260, Speaker C: I'm not smart enough to do that.
00:25:27.530 - 00:25:31.462, Speaker B: I don't think it's an AI thing. It's just like a philosophy and awareness thing.
00:25:31.516 - 00:25:33.990, Speaker C: Yeah, we are really good at educating.
00:25:34.730 - 00:25:47.994, Speaker B: We can just include in the intro, welcome to Bankless, probably going to die. Money and finance. Also, remember to talk about the AI problem and get everyone on board with the AI alignment problem. And now into the episode, we got.
00:25:48.032 - 00:25:49.434, Speaker C: 20 years left at best.
00:25:49.552 - 00:25:51.594, Speaker B: Two to 2020 years.
00:25:51.632 - 00:25:51.786, Speaker C: Yes.
00:25:51.808 - 00:25:55.198, Speaker B: Hell am I 20 years? I got heavy some things to do, I guess.
00:25:55.284 - 00:26:00.000, Speaker C: It's heavy. All right, well, maybe he's wrong, though. Can we say that?
00:26:00.610 - 00:26:09.860, Speaker B: Okay, here's one thing I'll say that's always, like they've always kind of shelves, like yeah, it's the long tail of like, maybe the Black Swan works in our favor this time.
00:26:10.230 - 00:26:44.874, Speaker C: So Vitalik has given to Eliezer's Institute in the past, and so we reached out, and just as we do, we asked kind of our close contacts of, like, hey, are there any you're a big brain. We're having this big brain on. Can you tell it? Would you ask them if you were us? And Vitalik, I don't think you'd mind us saying, he's like, well, just stay away from topics like the centralization problems of artificial intelligence, because Eliezer is well past that. He doesn't care about Centralization.
00:26:44.922 - 00:26:46.254, Speaker B: He wouldn't have let us go there.
00:26:46.292 - 00:26:48.186, Speaker C: Yeah, he's like he would have immediately.
00:26:48.218 - 00:26:49.514, Speaker B: Said, who cares about Central?
00:26:49.562 - 00:27:03.598, Speaker C: Exactly. Okay, so we didn't go there, and we weren't silly enough, but Vitak's comment was Eliezer's probability of doom is probably, like, a zero point 990 percent probability of doom. Actually, after this episode, I think it's.
00:27:03.614 - 00:27:06.434, Speaker B: Like a 99.898 plus.
00:27:06.472 - 00:27:13.510, Speaker C: Yeah. And then he said, but my probability of doom is probably a 0.1, so 10% ODS, I like those ODS.
00:27:14.650 - 00:27:15.574, Speaker B: He said 0.1.
00:27:15.612 - 00:27:54.818, Speaker C: That's what he said. So I like those ODS a lot better. Vitalik is also someone who very smart, and I want to know why that Delta exists. And now, he hasn't spent his whole life on artificial intelligence, obviously, and AI safety, so maybe there's kind of a Delta. I wouldn't trust Eliezer's opinion on all things crypto, of course, but I don't know, maybe there's some hope there that there I guess we need to get other opinions, is what I'm saying, before we kind of and I feel like that's one thing I'd like to do. I don't want to turn this into an AI podcast, but I want to.
00:27:54.824 - 00:27:58.262, Speaker B: Hear before we do need a second.
00:27:58.316 - 00:28:05.814, Speaker C: Opinion, need some hope, David. I need somebody to come on.
00:28:05.932 - 00:28:14.774, Speaker B: So I think Vitalik's, like, actually the best person to do this, because, once again, this is not an AI issue. This is a philosophy thing. And who else other than AI and then Vitalik?
00:28:14.822 - 00:28:17.500, Speaker C: Well, he's pretty optimistic in general.
00:28:18.110 - 00:28:25.398, Speaker B: That's a question we didn't have time for with Eliezer. I wanted to ask, were you always so pessimistic or what?
00:28:25.584 - 00:28:41.380, Speaker C: I was worried that that would almost seem so too prying, but I guess that retrospectively could have been a good question. I think he was fairly, like I was a little, I'd say worried about his mental state, but it was just very, like yeah, dude, it was very down.
00:28:41.830 - 00:28:49.800, Speaker B: I was just curious about, like, okay, since you've so convictedly, come to this conclusion. What do you do with your day?
00:28:50.330 - 00:29:02.860, Speaker C: We said he's on sabbatical. That was the other thing. I might come back into this. I have some more at valued Add. That's why I have time for podcasts. Look, maybe he's just burnt out with it. Maybe.
00:29:02.860 - 00:29:21.114, Speaker C: I don't know, man. That's all I got. I need to let this episode percolate sleep on this one. And apologies to everyone listening that if we accidentally gave you an existential crisis, we try to keep things light and upbeat and optimistic and bullish. Ultimately, we are bullish on humanity.
00:29:21.242 - 00:29:27.940, Speaker B: We're the most bullish people ever. And that podcast is like is one thing. Like, damn, there's no way to be bullish about that, right?
00:29:28.550 - 00:29:30.898, Speaker C: How do we turn this? How do we flip this one?
00:29:31.064 - 00:29:51.110, Speaker B: How do we spin this narrative? Not even us. There's another subject matter which I can't remember. Maybe it was Eliezer on Sam Harris. But you talked about how knowledge is discovered, like what knowledge is and how knowledge kind of exists even without a form factor to hold it in. Does that make sense?
00:29:51.180 - 00:29:51.414, Speaker C: Yeah.
00:29:51.452 - 00:30:07.726, Speaker B: As in, there's a lot of knowledge out there that humans don't know. There's a big gap between where we are versus where we will be when AI comes. 1020 years is a lot of time, let's remember, because there's a lot we don't know.
00:30:07.828 - 00:30:37.094, Speaker C: Eliezer is not an all knowing being either, and ultimately, he's one smart person who's taken a close look at this and come back despairing. There's other people who I assume come back more optimistic. Yeah, I guess there's that. Well, that's it. Accidentally gave myself an existential crisis while trying to record a podcast. But I'll bounce back tomorrow for State of the nation. We'll get some recording done.
00:30:37.094 - 00:30:40.040, Speaker C: We'll be back on crypto topics. And I'll just forget this. Ever.
00:30:41.610 - 00:30:50.846, Speaker B: I'll get Logan to make a PO app. I accidentally gave myself an existential cris while doing the one of one PO app. Also get that.
00:30:50.948 - 00:30:58.494, Speaker C: All right, well, check in on me tomorrow and let's talk about it then. Bankless Nation. Hope you enjoyed the debrief I guess.
00:30:58.532 - 00:30:59.840, Speaker B: We keep on doing.
00:31:01.250 - 00:31:02.638, Speaker C: Because why not?
00:31:02.804 - 00:31:03.420, Speaker B: Because why not?
