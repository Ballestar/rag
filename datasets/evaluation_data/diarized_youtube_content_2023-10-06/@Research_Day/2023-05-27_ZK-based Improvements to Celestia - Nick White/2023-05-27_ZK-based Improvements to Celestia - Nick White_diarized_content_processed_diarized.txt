00:00:10.860 - 00:00:47.390, Speaker A: What's up guys? Hi, I flew in from Hawaii this morning. Stoked to be here today. I just want to talk about some ideas we have for improving and augmenting the Celestia protocol using ZK technology. So, quick summary I'll talk a bit about the background of Celestia that's relevant to understand some of these improvements. And then I'll go into each one of them and I'm only highlighting three. There are actually more and there are lots of different ways we want to improve Celestia. So don't think that this is the only things we're thinking about.
00:00:47.390 - 00:01:53.890, Speaker A: So first of all, what is Celestia? The origin of Celestia is this white paper Lazy Ledger, published in 2019 by one of our co founders, Mustafa. And in that he described this goal to build a minimal modular and scalable data availability layer. So what that means is when we say minimal, we want it to be minimal amount of overhead to verify. So you shouldn't need much bandwidth, you shouldn't need much state or storage, and you shouldn't need much execution to verify the chain. And the end goal is really that everyone with a smartphone should be able to verify the celestial blockchain because that's how we reach maximum decentralization modularity. It comes to this idea that actually you can separate execution from consensus and data availability. So Celestia, rather than verifying that transactions in a given block are valid, just verifies that the transactions are available.
00:01:53.890 - 00:02:48.192, Speaker A: And then finally, the piece on scalability is that actually in 2018, mustafa wrote this paper about data availability sampling, which is the first proposal for how you can build a blockchain. That where the block size and the throughput scales with the number of nodes in the network. And so that is what to me, scaling that's the definition of scaling is that you can have bigger and bigger block sizes with more and more participation. Like a standard monolithic chain has a fixed block size. And once you reach that limit, you can't go any further. Some relevant details of how Celestia works for those of you who might not be familiar, data availability sampling requires that you build your block in a very specific way. So you have to take the original transaction data and you divide it into these chunks.
00:02:48.192 - 00:03:51.320, Speaker A: And then you extend these chunks using Reed Solomon erasure coding into this extended block. And then nodes that are sampling choose randomly among that block. And each time they sample successfully, they have a higher confidence that the whole block data is available. And specifically the block data is also mercalized in a special way. So each row and column of this two D square is mercalized into row and column roots and then those are mercalized into this top level data route. So if you're a roll up and you're trying to prove to a client that the data that you published has actually been published, if they have sampled the Celestia block, all you have to show them is that your data was included somewhere within the data route. So that's all the background that you need to know about Celestia and we can move into the more interesting stuff.
00:03:51.320 - 00:04:35.430, Speaker A: What's really exciting is that most of the work is done. So we've built out data availability sampling, we have block reconstruction, we have BFPS bad encoding fraud proofs, which I'll share more about later. We have a namespace Merkel Tree, which enables people to query data according to specific applications. And we've tested this now on the lockspace race with over 1000 light nodes and lots of things. But even though we're shipping, Celestia is not done, this is just V one, this is just the start. And really there's a lot of improvements that we have in the works and ideas that we want to build to make Celestia even better. So that's what this talk is really about.
00:04:35.430 - 00:05:31.684, Speaker A: So quick note, I know very very little about CK. Basically my mental model is what you see on the right, which is like I understand that you can generate the proof with a public input and a witness, and then I know you can verify it with the proof and the public input. That's pretty much my mental model. There's people in this audience who know a lot more than me, and so that's just a disclaimer. So if you came to this talk hoping that you'd learn something interesting about ZK, sorry, there's not talk for you. So the first improvement that I think would be very very interesting is proving the correctness of the encoding of the Celestia block data. So the current way that Celestia works is that a block producer can makes the extended block, they construct the block, but they could do it in a malicious way that actually doesn't follow proper encoding.
00:05:31.684 - 00:06:11.140, Speaker A: And so you couldn't actually reconstruct the block if you wanted to. And so to solve this problem, we have what we call a bad encoding fraud proof. So if a full node in Celestia sees that this is happening, they will generate this fraud proof and circulate it to light nodes and they'll know not to trust that block. The downside of this approach is that even though it's trust minimized, you have the same sort of latency as an optimistic roll up. So you have to wait for this fraud proof window to elapse before you can consider a block final. So a lot of people complain about this, I think it's true. Like this latency is not really desirable.
00:06:11.140 - 00:06:54.160, Speaker A: So an alternative solution, and that you see in protocols like Ethereum with Dank Sharding, also at Vale, is to use KZG commitments to encode and extend the block data. And what's nice about that is that you get correctness out of the box. So there's no fraud proof. Like you get the commitments and you know that they're valid. The downside is that at least in our research and sort of like going deeper into KZG, we realized that it's not really practical at this stage to use because they're very expensive and slow. The KZG commitments are very expensive and slow to compute. So at least for now, it doesn't seem like it's ready for primetime.
00:06:54.160 - 00:08:24.284, Speaker A: And so a potential solution that we're thinking about is actually to stick with read Solomon encoding, but then actually add this layer of ZK proving on top. So essentially what you'll do is you'll extend the block data in the normal way, but in parallel you'll also generate a ZK proof that given this original transaction data, if you do the read Solomon encoding and then the mercurialization you get this specific data route so you can kind of prove the correctness of the code. And so what's cool about this is that you could still kind of like it's kind of like an optimistic roll up with a ZK finality. So you could have produce a block normally and then rather than waiting for the entire fraud proof window, you get this ZK proof that the encoding is correct so you don't have to wait that whole time. So open questions is like, is this actually better than KCGS? Will it be cheaper? Will it actually be faster? I think there's possibility there will be. And also what proving system is best to use? So we've talked a lot with the Risero team, for example, who know a lot about Starks. And it seems like there also could be this cool overlap because Starks rely on a lot of like Reed Solomon math to prove their validity.
00:08:24.284 - 00:09:28.380, Speaker A: So there could be some kind of cool overlap there. A second really cool improvement that we're discussing is building a ZK friendly data route. So as I mentioned earlier, when you want to prove that the data you publish is actually included in Celestial and is available, you need to show people that it's included in the data route. The problem is that a lot of ZK roll ups want to do that in circuit. And the issue is that we build our data route using Shaw 256 and that's a very expensive operation to prove inside of a ZK circuit. And so it's kind of like a really bad, potentially bad user experience for ZK roll ups. A naive solution is you could construct two data routes so you could have the normal shot 256 one, and then you could use like Peterson or Poseidon hashes to build a ZK friendly version of the data route.
00:09:28.380 - 00:10:33.748, Speaker A: But the problem there is that once you have two data routes that are separate, you have to sample over both of them. And so you're kind of like fragmenting the network, either increasing the amount of work that a node needs to do to verify it, or you're kind of fragmenting the security so it doesn't really work, unfortunately. But the ZK inspired solution is that actually you could build, so you use the normal Celestia shaft 256 data route. And then you sample over that one. And then in parallel you build a ZK friendly data route using something like Poseidon or Peterson. But you generate a ZK proof that if you take the same block data and you merklize it, those two routes are basically committing to the same block data, so they're equivalent. So now as a ZK roll up, I can actually prove to you that the data that I'm committing to was included in the ZK friendly route.
00:10:33.748 - 00:11:52.412, Speaker A: And then you just verify that the ZK friendly route is equivalent to the data route that you sampled, like the normal Celestia data route. So this would solve that problem. Some open questions are like which proving system will be most cost effective? As we said, doing shot 256 in a ZK circuit is really expensive. It's like building these data routes will be very expensive, so this will be like a costly thing to do. And also how will this be funded? Like how do you share this cost amongst all the different ZK rollers that might use this service? And I think there are some really cool ideas around proving networks, proving services that could be helpful there. And I want to give a shout out to Mina and Risero also for sharing feedback on this idea. And last but not least, a third idea of how to improve Celestia is a feature that a lot of people I would say this is like the number one or one of the most common complaints I get from people about Celestia's design is that we don't have a way to support trust, minimized bridge, natively in the protocol.
00:11:52.412 - 00:12:59.856, Speaker A: Because Celestia is so minimal, there's no smart contracts, there's basically no execution on chain. That means you can't run a verifying bridge, basically. And so this is kind of a shame because one of the big selling points of rollups is that you can have trust minimized bridges. But if Celestia has to rely on things like Axelr or hyperlane or, I don't know, different kinds of trusted bridges to get the Celestia token up to the roll ups, it just doesn't quite feel right. And so a naive solution shout out to John is to just enshrine a specific roll up into the Celestia protocol and then you could bridge the Tia into that roll up and that roll up can bridge into all the rest of the ecosystem. Essentially. The downsides, there actually aren't that many.
00:12:59.856 - 00:13:50.330, Speaker A: If you design this in the right way, it could be very minimal, potentially, but it could could have some complexity or like undesirable amount of state or execution you need to support. But the really big one is basically that it really compromises credible neutrality in our mind. And that's another big, I would say, value and design goal of Celestia to be credibly neutral. So what that means is we want to make sure that we're not favoring any specific protocol outside we're just a data availability layer and that's it. We don't want to start delving into, oh, we want to do settlement, oh, we want to start launching on roll ups. We want to be just a data availability layer. And so if you enshrine a settlement layer, if you enshrine this bridge, you start to compromise on that.
00:13:50.330 - 00:14:56.750, Speaker A: So a few days ago, Mustafa posted this idea on our forum, which is how can we can solve this problem in a really minimal overhead way using ZK rollups, basically. So the idea is that at the celestial level, we create a new transaction type where the verification key of a ZK program is like the address. So you could send funds to that address. And then if you want to spend funds from that address, you have to provide it a valid proof. And that proof would basically be would show that there would be a ZK roll up associated with this address. And your proof would say, on the ZK roll up, I made a valid withdrawal transaction with this ID, and at this block height is still unspent. And so let me spend this many funds out of this address.
00:14:56.750 - 00:15:53.292, Speaker A: So how this would work in practice is that, for example, you would just send a deposit transaction of Tia to this wallet that is associated with this verification key. And then the roll up would see that and credit you Celestia tokens on the roll up. From there you can bridge wherever you want. And then when you want to go bridge back down to Celestia, you send a withdrawal transaction on the ZK roll up. Maybe it's a burn or whatever it is, and then you generate a proof that you did that and you show that to Celestia and that allows you gives you the right to basically transfer funds from the roll up address back to your address on Celestia. The beauty of this is that it's extremely minimal. It could potentially be stateless, and it's also credibly neutral.
00:15:53.292 - 00:16:45.104, Speaker A: So we're not enshrining anything, we're just adding a new functionality to the Celestia protocol. There are some open questions. So one of the big ones is which proof system should we support? Some people think it should be like groth because it seems like Ethereum is moving in that direction. And in some ways some people made the argument that choosing a proving system will be sort of like not credibly neutral. But the way we think about this is that hopefully using recursion, you could prove like, let's say your ZK roll up or whatever uses a different proving system. You could prove your ZK roll up within whatever proving system Celestia natively supports. So hopefully that's not a problem.
00:16:45.104 - 00:17:28.360, Speaker A: And also at the end of the day, every blockchain, for example, chooses a specific way of signature scheme, for example. And that's kind of inherent to adding this kind of functionality. Another question is, should we include a state commitment corresponding to this verification address because that could make things a lot easier for the implementation, although it does add a slight amount of state to each one of these addresses. And I want to give a shout out to Sovereign Labs and Succinct for feedback on this idea. That's pretty much it. I just have a few other ideas that I want to throw out there for ways that we want to improve Celestia going forward. One is we want to increase censorship resistance.
00:17:28.360 - 00:18:43.404, Speaker A: So ideas like multiplicity from Duality come to mind there because I think the data availability layer should be as censorship resistant as possible if it's going to fulfill its role well. And also adding in protocol MVP support like protocol on building from Skip is also a really interesting idea that we could I think would be a great improvement to Celestia and adding restaking support for Celestia would be really cool. Then we could for example, launch shared sequencer networks that are partially secured by the Celestia token and those are just a handful of things. There's also another ZK idea which is the quantum gravity bridge generating like a ZK version of that. So it's very easy to verify. So anyway, I just want to reiterate that Celestia is never done and I think that these are just a few of the improvements we have. On top of my mind, I didn't mention a lot of things around scalability and networking and the core protocol, but ultimately our vision is so ambitious that I think it's going to be an ongoing effort of research and implementation to be the best, most minimal, modular, scalable DA layer.
00:18:43.404 - 00:19:04.310, Speaker A: And we're always grateful for new ideas. So those of you who are listening thing, if you have other ideas of how we can improve the protocol or you have ideas how we can solve these problems, I would love to hear from you and you always welcome new contributors. So thank you very much. And this is a link to the forum and that's my Twitter. I would love to talk to you guys.
