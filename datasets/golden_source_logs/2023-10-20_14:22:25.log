[
    {
        "event_type": "llm start",
        "model_params": {
            "model": "gpt-4-0613",
            "temperature": 0,
            "max_tokens": null,
            "additional_kwargs": {},
            "max_retries": 10,
            "api_type": "open_ai",
            "api_base": "https://api.openai.com/v1",
            "api_version": "",
            "class_type": "openai",
            "class_name": "openai_llm"
        },
        "user_raw_input": "How do L2 sequencers work?",
        "LLM_input": "How do L2 sequencers work?\n\nTo determine if you should take the action to use the query engine, use its description detailed below. Use the query engine rather than not and do not rely on your prior knowledge.\nThe query engine tool has access to research papers and \nYouTube videos about the following content: Maximal Extractable Value (MEV); loss-versus-rebalancing (LVR); blockchain intents; \nthe Single Unifying Auction for Value Expression (SUAVE); sequencers; transaction ordering, L1s, L2s, transaction ordering, order flow auctions, auction theory, mechanism design.\n\n"
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I need to use a tool to help me answer the question.\nAction: query_engine_tool\nAction Input: {\"input\": \"L2 sequencers\"}",
        "additional_content": [
            {
                "function_call": [
                    {
                        "event_type": "templating start",
                        "instructions": "system: You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\nuser: Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_str": "document_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nThat means you may not have time to coordinate between your different points of presence in order to coordinate on what you're going to do. So there is some limit on your ability to actually use geographic proximity in this distributed system sense simply because if Alice is sitting in one place and multicasts her transaction out across the world to everywhere, then if you see it over here and then try to inform your friend over there of what to do, that Alice's transaction will probably beat you.\n00:17:48.146 - 00:18:37.566, Speaker A: Right? Because you have to pay the triangle penalty for your path. And so there is some resistance to the geographic proximity methods. Essentially someone would need to have a deterministic method of reacting to the transactions they see, which is a little tricky because if you have only local information about the order in which your sequencer saw the information, that's not a guarantee about what the majority will see. So there's some complexity here and arguably it's a bit more difficult to try to get geographically near. So that's basically where we stand. I've given you a little flavor. I think the biggest difference you see in just to summarize between L One ethereum and L2 is in the response time of the sequencer.\n00:18:37.566 - 00:19:06.850, Speaker A: It's in the fact that ordering is done by transactions. Blocks are not built in the normal sense, it's a per transaction ordering process and that has implications as well. So I'd be happy to answer any questions or engage in any discussion or civilized debate. And I think we have about ten minutes for that. I'll repeat the question if not okay.\n00:19:08.100 - 00:19:10.000, Speaker B: Yeah, I'm a huge fan.\n00:19:10.420 - 00:19:11.330, Speaker C: Thank you.\n00:19:12.180 - 00:19:23.540, Speaker B: Just to church while you're talking, is there a possibility, perhaps might it be a good idea might it be a good idea to continue to run your.\n00:19:23.610 - 00:20:01.184, Speaker C: Centralized sequencer in parallel to a distributed version? Thank you. Would it be a good idea to run a centralized sequencer in parallel to a distributed sequencer? Almost like a kind of first of all, to give an alternative to users and secondly, almost like a kind of economic incentive to the distributed actors to behave. You've got this kind of threat that if they start messing with a sequencer, people get to see it and they've got this alternative they can quickly go to of the sort of reputation based centralized. Is that something that's occurred to you? Do you think that might be a good idea?\n00:20:01.382 - 00:20:40.592, Speaker A: So I think there are some difficulties with this. I think it may be the case that when we roll out a distributed sequencer, our centralized sequencer will become one of the parties in that system. That seems pretty likely. You can have different sequencers who publish their opinions about event ordering. Right. There is a core problem, though, that different sequencers. If every user is multicasting their transactions and every sequencer is honestly reporting its order of receiving them, they will report different orders, right.\n00:20:40.592 - 00:21:50.468, Speaker A: Just because of how network latency and delivery works. So there's a sense in which there is only a sequencer who sits in one place is really the only party who knows what truth it should have reported, right. At least sort of in detail. It's difficult in a single instance to figure out who's telling the truth or who's lying. The other thing is that people could of course, if we had a parallel reporting system that got the transaction separately and reported what it saw, a user could manipulate that by multicasting to the sequencer committee now and then waiting a while and sending to us or the other way around. So the fact of differences between arrival time at different places is not necessarily evidence that could be shown to a third party about what went wrong. Now, if you and you wearing a different hat at a different IP address, send transactions to the same sequencer and you in fact can know that A arrived before B, but it puts B before A.\n00:21:50.468 - 00:22:21.680, Speaker A: Or if you detect that your transactions consistently see longer latency than that other person's than your friends, that's some indication that that sequencer may be manipulating order. So in a kind of collective and cumulative way, a dishonest sequencer's dishonesty is likely to become evident, but you can't detect it with an automated mechanism, at least not that I can see. And so you have to rely on social response.\n00:22:24.980 - 00:22:26.128, Speaker B: One thing I was just going to.\n00:22:26.134 - 00:23:07.384, Speaker C: Add there is I just come clean. I'm founding an organization called Zero Mev that attempts to quantify on chain mev in different layers and specifically kind of tempts to differentiate between toxic and neutral. So one way you can do that is you can try and quantify the amount of toxic mev that's happening. So, for example, sandwich attacks a great way of seeing very quickly and very clearly whether users are being exploited on a certain chain. So if you have some reporting of the kind of relatively how many sandwich attacks you're getting on a distributed chain versus the centralized, that could be quite a powerful incentive for that not to happen on the distributed chain.\n00:23:07.432 - 00:24:02.770, Speaker A: I mean, on a centralized chain, of course, the centralized sequencer has the power to do a sandwich attack to people in a distributed sequencer. It would require a suitable supermajority to all coordinate their sandwich attacks right. Which you can't rule out technically, but which the trust requirement would rule out. And if an organization were to report that that was happening, then we and our community would take notice and you could expect the sequencer committee members to be changed. And of course, it's really important in the distributed sequencer model that each sequencer publishes their claimed sequence. And so you have the evidence of if one sequencer seems to be sandwiching a lot, you actually have the evidence that you can point to and say, look, these sequencers are sandwiching and the other ones are not. Something must be happening.\n00:24:07.850 - 00:24:15.530, Speaker D: You've spoken briefly now about how it's possible to detect dishonest sequencing, but what actual incentive is there to be an honest sequencer?\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nIt would require a suitable supermajority to all coordinate their sandwich attacks right. Which you can't rule out technically, but which the trust requirement would rule out. And if an organization were to report that that was happening, then we and our community would take notice and you could expect the sequencer committee members to be changed. And of course, it's really important in the distributed sequencer model that each sequencer publishes their claimed sequence. And so you have the evidence of if one sequencer seems to be sandwiching a lot, you actually have the evidence that you can point to and say, look, these sequencers are sandwiching and the other ones are not. Something must be happening.\n00:24:07.850 - 00:24:15.530, Speaker D: You've spoken briefly now about how it's possible to detect dishonest sequencing, but what actual incentive is there to be an honest sequencer?\n00:24:16.270 - 00:24:19.530, Speaker A: Well, you won't be fired, you won't lose your job as a sequencer.\n00:24:21.150 - 00:24:29.500, Speaker D: That seems like a small incentive there because if I can just spin up another sequencer, I'll just keep creating more and more.\n00:24:30.130 - 00:24:44.740, Speaker A: Right, so the set of sequencers is permissioned and is chosen through a process that involves deliberation by our community. And so you don't automatically get to be a sequencer just because you want to.\n00:24:48.710 - 00:24:49.460, Speaker B: Sure.\n00:24:54.610 - 00:25:39.626, Speaker E: Coming back to the roadmap towards distributed sequencer. If you think of things like cross domain mev and the risk of some of those sequencer colluding with other sequencers from other L two S to extract mev, there's a trade off that becomes quite complicated between a centralized sequencer that is operated by a company that people can trust is honest or operating honestly versus a distributed set of sequencers that might collude in a way that is quite unpredictable. So how do you think about the timeline and the moment when the trade off towards distributed sequencer becomes a positive outcome for the user?\n00:25:39.738 - 00:26:33.374, Speaker A: Sure yeah, let me say a couple of things about that. The first is that if a small minority of the sequencers in this scheme are colluding with outsiders, that has no effect, because the honest supermajority will still provide the distributed first come first serve guarantee, regardless of what a small minority does. The second piece of that is yes. It's really important how these sequencers are chosen. They're likely to be parties who are trusted by our community to be reputable and have reason to be honest. And if they misbehave and parties who would suffer a reputational hit for misbehaving. I think if you think about the process for how you would choose those parties, you would want ones who have a reputation as trustworthy in the community, who have reputation to lose, who have a stake in the success of the community and so on.\n00:26:33.374 - 00:26:43.920, Speaker A: And so I think the considerations that you suggest would play a big role in choosing the sequencers. And should.\n00:26:46.450 - 00:26:46.974, Speaker B: We out?\n00:26:47.012 - 00:26:47.566, Speaker D: That's enough time.\n00:26:47.588 - 00:26:48.480, Speaker A: Thank you, everyone.\n00:26:49.650 - 00:27:05.090, Speaker B: Thank it's.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\n00:00:08.890 - 00:01:09.922, Speaker A: And I want to talk about mev and l two So here's the TLDR from my talk. L2 S work differently from Ethereum in some important respects. And unless you're careful, your assumptions from ethereum may steer you wrong in thinking about mev on L2 S. So what I want to do to sort of flesh this out is I'm going to talk a little bit about how Arbitrum works and the things I talk about will be common across the deployed L2 S, as far as I know. Then I will talk some about how this affects, how these differences affect the mev landscape. And then finally I'll talk some about what we at Arbitrum, how we approach it, what our principles are about this and what we're actually doing. Okay, let me start by talking about how Arbitrum as at this level a typical L2 works.\n00:01:09.922 - 00:01:50.762, Speaker A: And if you've seen one of my other talks this week, you may have seen this diagram before. I'll go through the parts that are relevant for this talk. The key component here is the sequencer in the upper left. So under normal conditions, users send to the sequencer all of their transactions directly by RPC. The sequencer gets those transactions and what it is supposed to do is put those transactions into a sequence that the Sequencer decides, but it is supposed to put them into an Arbitrum first come, first served order. So the sequencer is responsible for putting transactions into sequence order and for nothing else. It's not trusted for any other purpose.\n00:01:50.762 - 00:02:16.982, Speaker A: Anything else it does that it might do wrong is checked for and corrected for by the rest of the system. So that's all it does is sequencing. And I'll talk a little bit about how this works and so on. The sequencer might be a centralized element or it might be a distributed system. More on that later. Okay, the sequencer then does two things with the Sequence thing. One is publishes a sequencer feed very quickly.\n00:02:16.982 - 00:03:07.160, Speaker A: In a second or less, after your transaction has arrived at the sequencer, your transaction will have been put into the sequence and the sequencer will have published it as part of the sequencer feed. That's a feed, a real time feed that anyone can subscribe to. And if the Sequencer is honest, then that will be the order in which transactions occur in the L2 chain. Now over here on the right we have the state transition function. This is basically a function that eats one transaction at a time, processes that transaction, possibly updates the state and then possibly emits an L2 block. This state transition function is fully deterministic. So the outcome at any time will depend only on the Genesis state and on the sequence of transactions that it is processed up to now and on nothing else.\n00:03:07.160 - 00:03:14.262, Speaker A: What that means is that the sequencer's sequence output fully determines the eventual state of the chain.\n00:03:14.406 - 00:03:14.714, Speaker B: Okay?\n00:03:14.752 - 00:04:09.014, Speaker A: So the sequencer publishes its sequencer feed very quickly. This allows users who decide to trust the sequencer on its sequence to get a very low latency response to their transactions. Users love this. This is a huge user experience advantage, and that's the reason why the Sequencer feed exists and indeed, why the sequencer exists at all. But then the other thing the sequencer does is once it has built up a large enough batch of unrecorded transactions, it makes a big batch of transactions, compresses it using a general purpose compression algorithm, and then writes it onto the L One chain. And this is the final and permanent record of what the transaction sequence is. And because of the determinism of the state transition function, it determines completely what the history of the chain will be up through that part of the transaction sequence.\n00:04:09.014 - 00:04:55.610, Speaker A: So from that point on, the wheels inevitably grind to cause those transactions to eventually be settled back to L One. Okay, so there are two interactions that this process has with the L One chain. One to store the compressed batches, and the other to settle the transactions to L One. Now, what's interesting about this is these transactions to L One don't really care about ordering. If the sequencer is honest, then the batch and compress and record transaction is only recording information that's already known. It's simply recording and"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "tool_output": "Context information is below.\n---------------------\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nThat means you may not have time to coordinate between your different points of presence in order to coordinate on what you're going to do. So there is some limit on your ability to actually use geographic proximity in this distributed system sense simply because if Alice is sitting in one place and multicasts her transaction out across the world to everywhere, then if you see it over here and then try to inform your friend over there of what to do, that Alice's transaction will probably beat you.\n00:17:48.146 - 00:18:37.566, Speaker A: Right? Because you have to pay the triangle penalty for your path. And so there is some resistance to the geographic proximity methods. Essentially someone would need to have a deterministic method of reacting to the transactions they see, which is a little tricky because if you have only local information about the order in which your sequencer saw the information, that's not a guarantee about what the majority will see. So there's some complexity here and arguably it's a bit more difficult to try to get geographically near. So that's basically where we stand. I've given you a little flavor. I think the biggest difference you see in just to summarize between L One ethereum and L2 is in the response time of the sequencer.\n00:18:37.566 - 00:19:06.850, Speaker A: It's in the fact that ordering is done by transactions. Blocks are not built in the normal sense, it's a per transaction ordering process and that has implications as well. So I'd be happy to answer any questions or engage in any discussion or civilized debate. And I think we have about ten minutes for that. I'll repeat the question if not okay.\n00:19:08.100 - 00:19:10.000, Speaker B: Yeah, I'm a huge fan.\n00:19:10.420 - 00:19:11.330, Speaker C: Thank you.\n00:19:12.180 - 00:19:23.540, Speaker B: Just to church while you're talking, is there a possibility, perhaps might it be a good idea might it be a good idea to continue to run your.\n00:19:23.610 - 00:20:01.184, Speaker C: Centralized sequencer in parallel to a distributed version? Thank you. Would it be a good idea to run a centralized sequencer in parallel to a distributed sequencer? Almost like a kind of first of all, to give an alternative to users and secondly, almost like a kind of economic incentive to the distributed actors to behave. You've got this kind of threat that if they start messing with a sequencer, people get to see it and they've got this alternative they can quickly go to of the sort of reputation based centralized. Is that something that's occurred to you? Do you think that might be a good idea?\n00:20:01.382 - 00:20:40.592, Speaker A: So I think there are some difficulties with this. I think it may be the case that when we roll out a distributed sequencer, our centralized sequencer will become one of the parties in that system. That seems pretty likely. You can have different sequencers who publish their opinions about event ordering. Right. There is a core problem, though, that different sequencers. If every user is multicasting their transactions and every sequencer is honestly reporting its order of receiving them, they will report different orders, right.\n00:20:40.592 - 00:21:50.468, Speaker A: Just because of how network latency and delivery works. So there's a sense in which there is only a sequencer who sits in one place is really the only party who knows what truth it should have reported, right. At least sort of in detail. It's difficult in a single instance to figure out who's telling the truth or who's lying. The other thing is that people could of course, if we had a parallel reporting system that got the transaction separately and reported what it saw, a user could manipulate that by multicasting to the sequencer committee now and then waiting a while and sending to us or the other way around. So the fact of differences between arrival time at different places is not necessarily evidence that could be shown to a third party about what went wrong. Now, if you and you wearing a different hat at a different IP address, send transactions to the same sequencer and you in fact can know that A arrived before B, but it puts B before A.\n00:21:50.468 - 00:22:21.680, Speaker A: Or if you detect that your transactions consistently see longer latency than that other person's than your friends, that's some indication that that sequencer may be manipulating order. So in a kind of collective and cumulative way, a dishonest sequencer's dishonesty is likely to become evident, but you can't detect it with an automated mechanism, at least not that I can see. And so you have to rely on social response.\n00:22:24.980 - 00:22:26.128, Speaker B: One thing I was just going to.\n00:22:26.134 - 00:23:07.384, Speaker C: Add there is I just come clean. I'm founding an organization called Zero Mev that attempts to quantify on chain mev in different layers and specifically kind of tempts to differentiate between toxic and neutral. So one way you can do that is you can try and quantify the amount of toxic mev that's happening. So, for example, sandwich attacks a great way of seeing very quickly and very clearly whether users are being exploited on a certain chain. So if you have some reporting of the kind of relatively how many sandwich attacks you're getting on a distributed chain versus the centralized, that could be quite a powerful incentive for that not to happen on the distributed chain.\n00:23:07.432 - 00:24:02.770, Speaker A: I mean, on a centralized chain, of course, the centralized sequencer has the power to do a sandwich attack to people in a distributed sequencer. It would require a suitable supermajority to all coordinate their sandwich attacks right. Which you can't rule out technically, but which the trust requirement would rule out. And if an organization were to report that that was happening, then we and our community would take notice and you could expect the sequencer committee members to be changed. And of course, it's really important in the distributed sequencer model that each sequencer publishes their claimed sequence. And so you have the evidence of if one sequencer seems to be sandwiching a lot, you actually have the evidence that you can point to and say, look, these sequencers are sandwiching and the other ones are not. Something must be happening.\n00:24:07.850 - 00:24:15.530, Speaker D: You've spoken briefly now about how it's possible to detect dishonest sequencing, but what actual incentive is there to be an honest sequencer?\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nIt would require a suitable supermajority to all coordinate their sandwich attacks right. Which you can't rule out technically, but which the trust requirement would rule out. And if an organization were to report that that was happening, then we and our community would take notice and you could expect the sequencer committee members to be changed. And of course, it's really important in the distributed sequencer model that each sequencer publishes their claimed sequence. And so you have the evidence of if one sequencer seems to be sandwiching a lot, you actually have the evidence that you can point to and say, look, these sequencers are sandwiching and the other ones are not. Something must be happening.\n00:24:07.850 - 00:24:15.530, Speaker D: You've spoken briefly now about how it's possible to detect dishonest sequencing, but what actual incentive is there to be an honest sequencer?\n00:24:16.270 - 00:24:19.530, Speaker A: Well, you won't be fired, you won't lose your job as a sequencer.\n00:24:21.150 - 00:24:29.500, Speaker D: That seems like a small incentive there because if I can just spin up another sequencer, I'll just keep creating more and more.\n00:24:30.130 - 00:24:44.740, Speaker A: Right, so the set of sequencers is permissioned and is chosen through a process that involves deliberation by our community. And so you don't automatically get to be a sequencer just because you want to.\n00:24:48.710 - 00:24:49.460, Speaker B: Sure.\n00:24:54.610 - 00:25:39.626, Speaker E: Coming back to the roadmap towards distributed sequencer. If you think of things like cross domain mev and the risk of some of those sequencer colluding with other sequencers from other L two S to extract mev, there's a trade off that becomes quite complicated between a centralized sequencer that is operated by a company that people can trust is honest or operating honestly versus a distributed set of sequencers that might collude in a way that is quite unpredictable. So how do you think about the timeline and the moment when the trade off towards distributed sequencer becomes a positive outcome for the user?\n00:25:39.738 - 00:26:33.374, Speaker A: Sure yeah, let me say a couple of things about that. The first is that if a small minority of the sequencers in this scheme are colluding with outsiders, that has no effect, because the honest supermajority will still provide the distributed first come first serve guarantee, regardless of what a small minority does. The second piece of that is yes. It's really important how these sequencers are chosen. They're likely to be parties who are trusted by our community to be reputable and have reason to be honest. And if they misbehave and parties who would suffer a reputational hit for misbehaving. I think if you think about the process for how you would choose those parties, you would want ones who have a reputation as trustworthy in the community, who have reputation to lose, who have a stake in the success of the community and so on.\n00:26:33.374 - 00:26:43.920, Speaker A: And so I think the considerations that you suggest would play a big role in choosing the sequencers. And should.\n00:26:46.450 - 00:26:46.974, Speaker B: We out?\n00:26:47.012 - 00:26:47.566, Speaker D: That's enough time.\n00:26:47.588 - 00:26:48.480, Speaker A: Thank you, everyone.\n00:26:49.650 - 00:27:05.090, Speaker B: Thank it's.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\n00:00:08.890 - 00:01:09.922, Speaker A: And I want to talk about mev and l two So here's the TLDR from my talk. L2 S work differently from Ethereum in some important respects. And unless you're careful, your assumptions from ethereum may steer you wrong in thinking about mev on L2 S. So what I want to do to sort of flesh this out is I'm going to talk a little bit about how Arbitrum works and the things I talk about will be common across the deployed L2 S, as far as I know. Then I will talk some about how this affects, how these differences affect the mev landscape. And then finally I'll talk some about what we at Arbitrum, how we approach it, what our principles are about this and what we're actually doing. Okay, let me start by talking about how Arbitrum as at this level a typical L2 works.\n00:01:09.922 - 00:01:50.762, Speaker A: And if you've seen one of my other talks this week, you may have seen this diagram before. I'll go through the parts that are relevant for this talk. The key component here is the sequencer in the upper left. So under normal conditions, users send to the sequencer all of their transactions directly by RPC. The sequencer gets those transactions and what it is supposed to do is put those transactions into a sequence that the Sequencer decides, but it is supposed to put them into an Arbitrum first come, first served order. So the sequencer is responsible for putting transactions into sequence order and for nothing else. It's not trusted for any other purpose.\n00:01:50.762 - 00:02:16.982, Speaker A: Anything else it does that it might do wrong is checked for and corrected for by the rest of the system. So that's all it does is sequencing. And I'll talk a little bit about how this works and so on. The sequencer might be a centralized element or it might be a distributed system. More on that later. Okay, the sequencer then does two things with the Sequence thing. One is publishes a sequencer feed very quickly.\n00:02:16.982 - 00:03:07.160, Speaker A: In a second or less, after your transaction has arrived at the sequencer, your transaction will have been put into the sequence and the sequencer will have published it as part of the sequencer feed. That's a feed, a real time feed that anyone can subscribe to. And if the Sequencer is honest, then that will be the order in which transactions occur in the L2 chain. Now over here on the right we have the state transition function. This is basically a function that eats one transaction at a time, processes that transaction, possibly updates the state and then possibly emits an L2 block. This state transition function is fully deterministic. So the outcome at any time will depend only on the Genesis state and on the sequence of transactions that it is processed up to now and on nothing else.\n00:03:07.160 - 00:03:14.262, Speaker A: What that means is that the sequencer's sequence output fully determines the eventual state of the chain.\n00:03:14.406 - 00:03:14.714, Speaker B: Okay?\n00:03:14.752 - 00:04:09.014, Speaker A: So the sequencer publishes its sequencer feed very quickly. This allows users who decide to trust the sequencer on its sequence to get a very low latency response to their transactions. Users love this. This is a huge user experience advantage, and that's the reason why the Sequencer feed exists and indeed, why the sequencer exists at all. But then the other thing the sequencer does is once it has built up a large enough batch of unrecorded transactions, it makes a big batch of transactions, compresses it using a general purpose compression algorithm, and then writes it onto the L One chain. And this is the final and permanent record of what the transaction sequence is. And because of the determinism of the state transition function, it determines completely what the history of the chain will be up through that part of the transaction sequence.\n00:04:09.014 - 00:04:55.610, Speaker A: So from that point on, the wheels inevitably grind to cause those transactions to eventually be settled back to L One. Okay, so there are two interactions that this process has with the L One chain. One to store the compressed batches, and the other to settle the transactions to L One. Now, what's interesting about this is these transactions to L One don't really care about ordering. If the sequencer is honest, then the batch and compress and record transaction is only recording information that's already known. It's simply recording and\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Always provide an exhaustive answer to the question, unless told otherwise in the question itself.\nDirectly quote the sources of your knowledge in the same sentence in parentheses. If several files are matched across several years of release dates, favor most recent content. Now answer the question: How do L2 sequencers work?\nAnswer: "
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "L2 sequencers work by receiving transactions directly from users. They are responsible for putting these transactions into a sequence, which they decide, but it should be in a first come, first served order. The sequencer then does two things with the sequence. Firstly, it publishes a sequencer feed very quickly, usually in a second or less after a transaction has arrived. This feed is a real-time feed that anyone can subscribe to. Secondly, once the sequencer has built up a large enough batch of unrecorded transactions, it compresses it using a general-purpose compression algorithm and then writes it onto the L1 chain. This is the final and permanent record of what the transaction sequence is. The sequencer can be a centralized element or a distributed system. The set of sequencers is permissioned and is chosen through a process that involves deliberation by the community."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "that the sequencer's sequence output fully determines the eventual state of the chain.\n00:03:14.406 - 00:03:14.714, Speaker B: Okay?\n00:03:14.752 - 00:04:09.014, Speaker A: So the sequencer publishes its sequencer feed very quickly. This allows users who decide to trust the sequencer on its sequence to get a very low latency response to their transactions. Users love this. This is a huge user experience advantage, and that's the reason why the Sequencer feed exists and indeed, why the sequencer exists at all. But then the other thing the sequencer does is once it has built up a large enough batch of unrecorded transactions, it makes a big batch of transactions, compresses it using a general purpose compression algorithm, and then writes it onto the L One chain. And this is the final and permanent record of what the transaction sequence is. And because of the determinism of the state transition function, it determines completely what the history of the chain will be up through that part of the transaction sequence.\n00:04:09.014 - 00:04:55.610, Speaker A: So from that point on, the wheels inevitably grind to cause those transactions to eventually be settled back to L One. Okay, so there are two interactions that this process has with the L One chain. One to store the compressed batches, and the other to settle the transactions to L One. Now, what's interesting about this is these transactions to L One don't really care about ordering. If the sequencer is honest, then the batch and compress and record transaction is only recording information that's already known. It's simply recording and notarizing information that everyone already knows. And so if it gets delayed a little bit or advanced a little bit, that doesn't affect anything.\n00:04:55.610 - 00:05:23.630, Speaker A: It doesn't affect the history of the L2 chain. It only affects when the L One chain learns of that history. Similarly, over on the right, when you settle the transactions to L One, the result of that is inevitable, fully determined by what already happened. And so again, it doesn't matter that much whether it happens earlier or later. Everyone knows it's going to happen. The only question is when. And so there's not much opportunity or these things are not very sensitive to mev considerations.\n00:05:23.630 - 00:05:39.190, Speaker A: I put an Asterisk on the one on the left, because if the sequencer is lying, which could happen occasionally, it might lie for a little while before it gets fired and replaced by another sequencer. If the sequencer lies, then you might care how quickly it gets caught in the lie.\n00:05:39.770 - 00:05:40.242, Speaker C: Okay?\n00:05:40.316 - 00:05:51.180, Speaker A: But for the most part, there's not a lot of mev sensitivity in these transactions. But of course, what you may be really interested in is what happens regarding mev on layer two.\n00:05:52.110 - 00:05:52.860, Speaker C: Okay.\n00:05:54.990 - 00:06:49.518, Speaker A: Let me stop and talk about who runs the sequencer because I think this is important in thinking about how all of this works. Because the sequencer is really the only party who has opportunity to extract it's, the only party who is definitely sitting on the path that a transaction must go through and potentially has power to extract mev. So currently, the sequencer is centralized.\n\ndocument_type: youtube_video\ntitle: Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=dZfSbtBOGAQ\nrelease_date: 2023-09-04\n\n00:00:00.330 - 00:00:26.066, Speaker A: You. Hey everybody. Good afternoon. Thanks for your time. I'm going to talk about the other side of that distinction that David drew between macro and micro. I have many interesting thoughts, I hope, about macro, but this talk is about micro. And I'm going to talk about what it looks like, what this problem looks like from the side of an L2 or L3s designer.\n00:00:26.066 - 00:01:37.414, Speaker A: And in particular, this is based on both our experience running the Arbitrum sequencer which is the most used so far roll up sequencer and also the research that we've done on this particular topic. So let me start just reviewing and this is pretty close to a diagram that David showed talking about how L2 systems work and just reviewing what is the role of the sequencer because there is some misconception out there about it. So the sequencer is this function up here on the upper left. And its job, and its only job is to receive transactions from users and to publish a canonical sequence of transactions. These are the transactions that have been seen and this is the order in which they've been seen. The execution phase of the protocol then is in charge of figuring out whether those transactions are valid and if so, how they change the state and which blocks they make. So the sequencer you can almost think about a modern state of the art roll up as almost being two chains.\n00:01:37.414 - 00:02:20.360, Speaker A: First, a chain that is built by the sequencer, which is the chain of transactions that have arrived and then second, a logically different chain that consumes those transactions and then updates state and looks like in our case, an Ethereum chain. So that's the role of the sequencer only trusted to vouch for transaction arrivals and some transaction order. All right, so how does l two differ from l one ethereum? There's a bunch of different ways, all of them probably important. First, sequencing is separate from execution and settlement. As I just described, the block time is much faster on L2. On Arbitrum, it's 250 milliseconds 48 x faster than Ethereum. That makes a big difference.\n00:02:20.360 - 00:03:12.860, Speaker A: There's a private mem pool by default, unlike the public mem pool and the kind of peer to peer gossip that goes on of the mem pool. Maybe you have some private order flow on Ethereum but kind of the default is public mem pool. And in L two, typically the default is private mempool. There's a very low cost structure on L2 S and that means costs that might be negligible for an L one might be really appreciably add to the cost on L two. And so we need to optimize cost in ways that L ones don't necessarily have to. But then finally, L2 S have a degree of design flexibility that L One doesn't have simply because L One is a much more mature, much more widely used system. And so it's a lot harder and a lot slower, and rightly so, to change how ethereum works compared to changing how or evolving how an L two works.\n00:03:12.860 - 00:03:34.666, Speaker A: Okay, so we think of this problem not as MEB, but as transaction ordering. And here are our goals for transaction ordering. First, we want low latency. We want to preserve that maybe not 250 millisecond, but pretty close to it block time. We need low cost of operation. We want something that resists front running. We think we owe that to our users.\n00:03:34.666 - 00:04:12.586, Speaker A: Our users ask for it and want it. And that's the sentiment of the Arbitrum Dao. But at the same time, we'd like to be able to capture revenue of mev other than front running. We'd like to be able to ethically monetize that. We want to not build in centralization. And what I mean by that is that we don't want an architecture that is inherently centralized. And by central or centralizing and I mean by that not only sort of a count of how many participants there are, but are we creating an economic structure that tends toward monopoly or oligopoly or concentration of economic power? We want to avoid that to the extent we can.\n00:04:12.586 - 00:04:37.902, Speaker A: And we also want a property that we call independence of your relevant transactions. And I'll give an example later that helps to motivate why this is a good thing. But let me explain briefly now what it is. Here's what we mean. Suppose that Alice wants to backrun some transaction A and Bob wants to backrun some transaction B. And those transactions are unrelated. They're trading in different quantities.\n00:04:37.902 - 00:05:17.694, Speaker A: So Alice's strategy in how she goes about this, what she submits, how she bids, et cetera, should be independent of what Bob does. Ellis's strategy space should be independent. Or in other words, if two opportunities for mev are non interfering, then they should be strategy independent. So the strategy of one is independent of what people in the other one do or even of the existence of the second one. Many mechanisms don't have this property, but it turns out to be valuable, as we'll see later. One of the reasons to understand one of the reasons it's valuable, consider the alternative. Consider a world where everything is entangled.\n00:05:17.694 - 00:05:54.046, Speaker A: Every single opportunity is entangled with every other opportunity. So you can't form a strategy for extracting even simple, a simple mev opportunity without understanding everything that's available and what everyone's else's strategy space is. So then, to complete Pete effectively for anything, you need to analyze everything in the most sophisticated way. And that creates a barrier to entry in the space. Also, in a world where everything is entangled, private information becomes more valuable. If you have some private order flow. Now, you know something about the strategy space that none of your competitors do.\n00:05:54.046 - 00:06:29.510, Speaker A: And because that strategy space is entangled, that's an advantage for you. So that creates an economy of scale. Those who can get more private order flow have a natural advantage. And for both of these reasons, barriers to entry and economies of scale, any market that has those properties is going to tend toward monopoly or oligopoly. And this is what you would see in a world with max entanglement, consistent with what we see, for example, on ethereum, with the increasing concentration of power in block building.\n\ndocument_type: youtube_video\ntitle: MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=kb8nY2-Osjc\nrelease_date: 2023-03-28\n\nThey have everything to do of mev and the lifecycle is pretty similar. Alice gives their transactions to the sequencer. The sequencer will have a list of pending transactions. They'll run some ordering policy. They'll decide the ordering of the transactions. Then they'll post that onto Ethereum, into an inbox and then picked up by the breach.\n00:05:28.230 - 00:06:10.866, Speaker A: Now in this scenario we have this direct communication channel. There's no gasa protocol. Alice can communicate directly with the sequencer. And what we need to be concerned about is when does Alice get a response from the sequencer and what type of response do they get? And that's really going to depend on how the sequencer decides the order of these transactions, then of course what we care about is the sequencer's ordering policy. They can keep the transactions private for as long as they want, decide the ordering and then eventually make that public once they've made their decision. So what ordering policies could the sequencer implement? So we're going to go through three. The first is has extraction first.\n00:06:10.866 - 00:06:43.026, Speaker A: The second is has fee first. And the third is first come, first serve. So let's just dive right in and see what we come up with. Has extraction first and basically when you talk about sequencers in mev and layer two, this is the first ordering policy that everyone talks about. And the reason is that sequencer has ample time to extract value. They could wait a minute or hours before they have to publish that transaction publicly. So a practice both Arbitrum and optimism.\n00:06:43.026 - 00:07:23.482, Speaker A: They typically publish these transactions in about 20 or 30 seconds. But according to the smart contracts, Arbitrum can hold it up to something like 24 hours. And the optimism, I couldn't actually find the value. Someone could probably drop it in the chat, but I'm pretty sure it's like 24 hours they could keep these transactions private for. So if you can hold a transaction pending transactions for 3 hours, let's say, well, you have this big basket of transactions. You have your mev extraction algorithm, you run it on the batch and then you could order your transactions according to the ones that you can extract the most value from. Now in terms of user experience, there is a benefit to this.\n00:07:23.482 - 00:07:53.810, Speaker A: We can have a Robin Vogue style experience. The user could transact for free because later on their transactions used to compute some mev reward. So from the user's perspective it's a free transaction, but they're really paid for it through MPV. Now, the issue with this approach is really the long delay. If we allow the sequencer to extract value for two to 3 hours, well, that sucks for the user. Think of bitcoin. You have to wait 1020 minutes on Bitcoin and everyone gets that it's a terrible user experience.\n00:07:53.810 - 00:08:16.934, Speaker A: So imagine waiting 2 hours just so someone can go extract value from your transaction. That's not a great user experience. Now you could solve this problem. A lot of people think, okay, Sequencers can extract mev. Let's solve this and stop that from happening. So they could implement some fair ordering protocol. Maybe the sequencer can't see the transaction content, maybe you introduce a consensus protocol.\n00:08:16.934 - 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "that the sequencer's sequence output fully determines the eventual state of the chain.\n00:03:14.406 - 00:03:14.714, Speaker B: Okay?\n00:03:14.752 - 00:04:09.014, Speaker A: So the sequencer publishes its sequencer feed very quickly. This allows users who decide to trust the sequencer on its sequence to get a very low latency response to their transactions. Users love this. This is a huge user experience advantage, and that's the reason why the Sequencer feed exists and indeed, why the sequencer exists at all. But then the other thing the sequencer does is once it has built up a large enough batch of unrecorded transactions, it makes a big batch of transactions, compresses it using a general purpose compression algorithm, and then writes it onto the L One chain. And this is the final and permanent record of what the transaction sequence is. And because of the determinism of the state transition function, it determines completely what the history of the chain will be up through that part of the transaction sequence.\n00:04:09.014 - 00:04:55.610, Speaker A: So from that point on, the wheels inevitably grind to cause those transactions to eventually be settled back to L One. Okay, so there are two interactions that this process has with the L One chain. One to store the compressed batches, and the other to settle the transactions to L One. Now, what's interesting about this is these transactions to L One don't really care about ordering. If the sequencer is honest, then the batch and compress and record transaction is only recording information that's already known. It's simply recording and notarizing information that everyone already knows. And so if it gets delayed a little bit or advanced a little bit, that doesn't affect anything.\n00:04:55.610 - 00:05:23.630, Speaker A: It doesn't affect the history of the L2 chain. It only affects when the L One chain learns of that history. Similarly, over on the right, when you settle the transactions to L One, the result of that is inevitable, fully determined by what already happened. And so again, it doesn't matter that much whether it happens earlier or later. Everyone knows it's going to happen. The only question is when. And so there's not much opportunity or these things are not very sensitive to mev considerations.\n00:05:23.630 - 00:05:39.190, Speaker A: I put an Asterisk on the one on the left, because if the sequencer is lying, which could happen occasionally, it might lie for a little while before it gets fired and replaced by another sequencer. If the sequencer lies, then you might care how quickly it gets caught in the lie.\n00:05:39.770 - 00:05:40.242, Speaker C: Okay?\n00:05:40.316 - 00:05:51.180, Speaker A: But for the most part, there's not a lot of mev sensitivity in these transactions. But of course, what you may be really interested in is what happens regarding mev on layer two.\n00:05:52.110 - 00:05:52.860, Speaker C: Okay.\n00:05:54.990 - 00:06:49.518, Speaker A: Let me stop and talk about who runs the sequencer because I think this is important in thinking about how all of this works. Because the sequencer is really the only party who has opportunity to extract it's, the only party who is definitely sitting on the path that a transaction must go through and potentially has power to extract mev. So currently, the sequencer is centralized.\n\ndocument_type: youtube_video\ntitle: Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=dZfSbtBOGAQ\nrelease_date: 2023-09-04\n\n00:00:00.330 - 00:00:26.066, Speaker A: You. Hey everybody. Good afternoon. Thanks for your time. I'm going to talk about the other side of that distinction that David drew between macro and micro. I have many interesting thoughts, I hope, about macro, but this talk is about micro. And I'm going to talk about what it looks like, what this problem looks like from the side of an L2 or L3s designer.\n00:00:26.066 - 00:01:37.414, Speaker A: And in particular, this is based on both our experience running the Arbitrum sequencer which is the most used so far roll up sequencer and also the research that we've done on this particular topic. So let me start just reviewing and this is pretty close to a diagram that David showed talking about how L2 systems work and just reviewing what is the role of the sequencer because there is some misconception out there about it. So the sequencer is this function up here on the upper left. And its job, and its only job is to receive transactions from users and to publish a canonical sequence of transactions. These are the transactions that have been seen and this is the order in which they've been seen. The execution phase of the protocol then is in charge of figuring out whether those transactions are valid and if so, how they change the state and which blocks they make. So the sequencer you can almost think about a modern state of the art roll up as almost being two chains.\n00:01:37.414 - 00:02:20.360, Speaker A: First, a chain that is built by the sequencer, which is the chain of transactions that have arrived and then second, a logically different chain that consumes those transactions and then updates state and looks like in our case, an Ethereum chain. So that's the role of the sequencer only trusted to vouch for transaction arrivals and some transaction order. All right, so how does l two differ from l one ethereum? There's a bunch of different ways, all of them probably important. First, sequencing is separate from execution and settlement. As I just described, the block time is much faster on L2. On Arbitrum, it's 250 milliseconds 48 x faster than Ethereum. That makes a big difference.\n00:02:20.360 - 00:03:12.860, Speaker A: There's a private mem pool by default, unlike the public mem pool and the kind of peer to peer gossip that goes on of the mem pool. Maybe you have some private order flow on Ethereum but kind of the default is public mem pool. And in L two, typically the default is private mempool. There's a very low cost structure on L2 S and that means costs that might be negligible for an L one might be really appreciably add to the cost on L two. And so we need to optimize cost in ways that L ones don't necessarily have to. But then finally, L2 S have a degree of design flexibility that L One doesn't have simply because L One is a much more mature, much more widely used system. And so it's a lot harder and a lot slower, and rightly so, to change how ethereum works compared to changing how or evolving how an L two works.\n00:03:12.860 - 00:03:34.666, Speaker A: Okay, so we think of this problem not as MEB, but as transaction ordering. And here are our goals for transaction ordering. First, we want low latency. We want to preserve that maybe not 250 millisecond, but pretty close to it block time. We need low cost of operation. We want something that resists front running. We think we owe that to our users.\n00:03:34.666 - 00:04:12.586, Speaker A: Our users ask for it and want it. And that's the sentiment of the Arbitrum Dao. But at the same time, we'd like to be able to capture revenue of mev other than front running. We'd like to be able to ethically monetize that. We want to not build in centralization. And what I mean by that is that we don't want an architecture that is inherently centralized. And by central or centralizing and I mean by that not only sort of a count of how many participants there are, but are we creating an economic structure that tends toward monopoly or oligopoly or concentration of economic power? We want to avoid that to the extent we can.\n00:04:12.586 - 00:04:37.902, Speaker A: And we also want a property that we call independence of your relevant transactions. And I'll give an example later that helps to motivate why this is a good thing. But let me explain briefly now what it is. Here's what we mean. Suppose that Alice wants to backrun some transaction A and Bob wants to backrun some transaction B. And those transactions are unrelated. They're trading in different quantities.\n00:04:37.902 - 00:05:17.694, Speaker A: So Alice's strategy in how she goes about this, what she submits, how she bids, et cetera, should be independent of what Bob does. Ellis's strategy space should be independent. Or in other words, if two opportunities for mev are non interfering, then they should be strategy independent. So the strategy of one is independent of what people in the other one do or even of the existence of the second one. Many mechanisms don't have this property, but it turns out to be valuable, as we'll see later. One of the reasons to understand one of the reasons it's valuable, consider the alternative. Consider a world where everything is entangled.\n00:05:17.694 - 00:05:54.046, Speaker A: Every single opportunity is entangled with every other opportunity. So you can't form a strategy for extracting even simple, a simple mev opportunity without understanding everything that's available and what everyone's else's strategy space is. So then, to complete Pete effectively for anything, you need to analyze everything in the most sophisticated way. And that creates a barrier to entry in the space. Also, in a world where everything is entangled, private information becomes more valuable. If you have some private order flow. Now, you know something about the strategy space that none of your competitors do.\n00:05:54.046 - 00:06:29.510, Speaker A: And because that strategy space is entangled, that's an advantage for you. So that creates an economy of scale. Those who can get more private order flow have a natural advantage. And for both of these reasons, barriers to entry and economies of scale, any market that has those properties is going to tend toward monopoly or oligopoly. And this is what you would see in a world with max entanglement, consistent with what we see, for example, on ethereum, with the increasing concentration of power in block building.\n\ndocument_type: youtube_video\ntitle: MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=kb8nY2-Osjc\nrelease_date: 2023-03-28\n\nThey have everything to do of mev and the lifecycle is pretty similar. Alice gives their transactions to the sequencer. The sequencer will have a list of pending transactions. They'll run some ordering policy. They'll decide the ordering of the transactions. Then they'll post that onto Ethereum, into an inbox and then picked up by the breach.\n00:05:28.230 - 00:06:10.866, Speaker A: Now in this scenario we have this direct communication channel. There's no gasa protocol. Alice can communicate directly with the sequencer. And what we need to be concerned about is when does Alice get a response from the sequencer and what type of response do they get? And that's really going to depend on how the sequencer decides the order of these transactions, then of course what we care about is the sequencer's ordering policy. They can keep the transactions private for as long as they want, decide the ordering and then eventually make that public once they've made their decision. So what ordering policies could the sequencer implement? So we're going to go through three. The first is has extraction first.\n00:06:10.866 - 00:06:43.026, Speaker A: The second is has fee first. And the third is first come, first serve. So let's just dive right in and see what we come up with. Has extraction first and basically when you talk about sequencers in mev and layer two, this is the first ordering policy that everyone talks about. And the reason is that sequencer has ample time to extract value. They could wait a minute or hours before they have to publish that transaction publicly. So a practice both Arbitrum and optimism.\n00:06:43.026 - 00:07:23.482, Speaker A: They typically publish these transactions in about 20 or 30 seconds. But according to the smart contracts, Arbitrum can hold it up to something like 24 hours. And the optimism, I couldn't actually find the value. Someone could probably drop it in the chat, but I'm pretty sure it's like 24 hours they could keep these transactions private for. So if you can hold a transaction pending transactions for 3 hours, let's say, well, you have this big basket of transactions. You have your mev extraction algorithm, you run it on the batch and then you could order your transactions according to the ones that you can extract the most value from. Now in terms of user experience, there is a benefit to this.\n00:07:23.482 - 00:07:53.810, Speaker A: We can have a Robin Vogue style experience. The user could transact for free because later on their transactions used to compute some mev reward. So from the user's perspective it's a free transaction, but they're really paid for it through MPV. Now, the issue with this approach is really the long delay. If we allow the sequencer to extract value for two to 3 hours, well, that sucks for the user. Think of bitcoin. You have to wait 1020 minutes on Bitcoin and everyone gets that it's a terrible user experience.\n00:07:53.810 - 00:08:16.934, Speaker A: So imagine waiting 2 hours just so someone can go extract value from your transaction. That's not a great user experience. Now you could solve this problem. A lot of people think, okay, Sequencers can extract mev. Let's solve this and stop that from happening. So they could implement some fair ordering protocol. Maybe the sequencer can't see the transaction content, maybe you introduce a consensus protocol.\n00:08:16.934 - 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance",
                        "previous_answer": "L2 sequencers work by receiving transactions directly from users. They are responsible for putting these transactions into a sequence, which they decide, but it should be in a first come, first served order. The sequencer then does two things with the sequence. Firstly, it publishes a sequencer feed very quickly, usually in a second or less after a transaction has arrived. This feed is a real-time feed that anyone can subscribe to. Secondly, once the sequencer has built up a large enough batch of unrecorded transactions, it compresses it using a general-purpose compression algorithm and then writes it onto the L1 chain. This is the final and permanent record of what the transaction sequence is. The sequencer can be a centralized element or a distributed system. The set of sequencers is permissioned and is chosen through a process that involves deliberation by the community."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "L2 sequencers receive transactions from users and publish a canonical sequence of transactions. They are responsible for determining the order of these transactions. Once a large enough batch of unrecorded transactions is accumulated, the sequencer compresses it using a general-purpose compression algorithm and writes it onto the L1 chain, creating a final and permanent record of the transaction sequence. The sequencer's sequence output fully determines the eventual state of the chain. The sequencer can hold transactions privately for a certain period, during which it can decide the order and then make it public once the decision is made. Currently, the sequencer is centralized, but it's important to note that the architecture doesn't inherently require centralization."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "we allow the sequencer to extract value for two to 3 hours, well, that sucks for the user. Think of bitcoin. You have to wait 1020 minutes on Bitcoin and everyone gets that it's a terrible user experience.\n00:07:53.810 - 00:08:16.934, Speaker A: So imagine waiting 2 hours just so someone can go extract value from your transaction. That's not a great user experience. Now you could solve this problem. A lot of people think, okay, Sequencers can extract mev. Let's solve this and stop that from happening. So they could implement some fair ordering protocol. Maybe the sequencer can't see the transaction content, maybe you introduce a consensus protocol.\n00:08:16.934 - 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance that a sequencer can make more money by having an open market of searchers do the hard work as opposed to trying to extract the mev themselves. And if that's the case then their financial incentive is not to extract the value but allow someone else to do it. So we don't have to worry about the fair ordering problem for now. And if asked the Kias well that means the sequencer's ordering policy is the order transactions by transaction fee. Whoever pays the has fee will get ordered first in the list. And so this is basically payment for order flow.\n00:09:23.050 - 00:10:09.194, Speaker A: Maybe some mev people hate me for saying that but it's basically that anyway, the sequencer will get the list of pending transactions and give it to the searchers. The searchers can"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "we allow the sequencer to extract value for two to 3 hours, well, that sucks for the user. Think of bitcoin. You have to wait 1020 minutes on Bitcoin and everyone gets that it's a terrible user experience.\n00:07:53.810 - 00:08:16.934, Speaker A: So imagine waiting 2 hours just so someone can go extract value from your transaction. That's not a great user experience. Now you could solve this problem. A lot of people think, okay, Sequencers can extract mev. Let's solve this and stop that from happening. So they could implement some fair ordering protocol. Maybe the sequencer can't see the transaction content, maybe you introduce a consensus protocol.\n00:08:16.934 - 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance that a sequencer can make more money by having an open market of searchers do the hard work as opposed to trying to extract the mev themselves. And if that's the case then their financial incentive is not to extract the value but allow someone else to do it. So we don't have to worry about the fair ordering problem for now. And if asked the Kias well that means the sequencer's ordering policy is the order transactions by transaction fee. Whoever pays the has fee will get ordered first in the list. And so this is basically payment for order flow.\n00:09:23.050 - 00:10:09.194, Speaker A: Maybe some mev people hate me for saying that but it's basically that anyway, the sequencer will get the list of pending transactions and give it to the searchers. The searchers can",
                        "previous_answer": "L2 sequencers work by receiving transactions directly from users. They are responsible for putting these transactions into a sequence, which they decide, but it should be in a first come, first served order. The sequencer then does two things with the sequence. Firstly, it publishes a sequencer feed very quickly, usually in a second or less after a transaction has arrived. This feed is a real-time feed that anyone can subscribe to. Secondly, once the sequencer has built up a large enough batch of unrecorded transactions, it compresses it using a general-purpose compression algorithm and then writes it onto the L1 chain. This is the final and permanent record of what the transaction sequence is. The sequencer can be a centralized element or a distributed system. The set of sequencers is permissioned and is chosen through a process that involves deliberation by the community."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers operate by receiving transactions directly from users and arranging them into a sequence. The order is determined by the sequencer, ideally following a first-come, first-served basis. The sequencer then quickly publishes a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a substantial batch of unrecorded transactions, the sequencer compresses it using a general-purpose compression algorithm and records it onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be either a centralized entity or a distributed system, and the selection of sequencers is a permissioned process involving community deliberation. In some cases, sequencers may allow an open market of searchers to extract maximum extractable value (MEV) rather than doing it themselves, as this could potentially lead to higher profits."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "- 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance that a sequencer can make more money by having an open market of searchers do the hard work as opposed to trying to extract the mev themselves. And if that's the case then their financial incentive is not to extract the value but allow someone else to do it. So we don't have to worry about the fair ordering problem for now. And if asked the Kias well that means the sequencer's ordering policy is the order transactions by transaction fee. Whoever pays the has fee will get ordered first in the list. And so this is basically payment for order flow.\n00:09:23.050 - 00:10:09.194, Speaker A: Maybe some mev people hate me for saying that but it's basically that anyway, the sequencer will get the list of pending transactions and give it to the searchers. The searchers can crunch and crunch and crunch, extract as much mev as they want and then eventually send a bundle to the sequencer with a payment. And they'll say sequencer, if you clear my bundle you'll make this amount of money then the sequencer takes that bundle, they take the payment and of course they order it according to the payments they receive. Very much similar to what happens on Ethereum today. You're sharing the mev between the sequencer and the searchers. And again user experience stuff very different to the previous case. Users could still have free transactions because the transaction fee is actually the mev that's extracted.\n00:10:09.194 - 00:10:41.690, Speaker A: But again, this could have a long delay. Users don't want to at least I don't want to build a layer two where users are waiting for two to 3 hours for their transaction to be confirmed. So this is why most rule apps implement first come, first serve because they want to prioritize the user experience. What do I mean by this? Well, the user will send their transactions to the sequencer. The sequencer will timestamp this and then simply order the transactions according to the timestamp. And this has a wonderful user experience. It's a bit like transacting on Coinbase.\n00:10:41.690 - 00:11:27.490, Speaker A: You send your transaction to the service provider and they return back a response to say it's confirmed. This is how it's executed under a second. Typically when people talk about first come, first serve they say oh well there's no mev here hiding under the covers.\n\ndocument_type: youtube_video\ntitle: Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=dZfSbtBOGAQ\nrelease_date: 2023-09-04\n\n00:22:51.358 - 00:22:51.506, Speaker A: Right.\n00:22:51.528 - 00:22:56.354, Speaker B: It doesn't require a private mempool for the sequencer. You can still kind of gossip transactions.\n00:22:56.402 - 00:23:12.774, Speaker A: Around or no, let's see. So with the centralized sequencer, there's not really a need for gossip. Right. The users who want their transactions in fast will send them directly to the sequencer. With the decentralized sequencer. Yes. You can have gossip.\n00:23:12.774 - 00:24:11.630, Speaker A: It makes sense to have gossip as part of this. Again, someone who wants their transaction in fast will send it directly to a quorum of the committee. But because of a concern about front running with the committee, if the transaction becomes known early on to some dishonest member, then you might worry that they'll inject a front running transaction. And there's things you can do about that within the consensus protocol, because other members of the committee will see that front running transaction arriving relatively late or else being known only to a small number of possibly colluding parties. So it's an interesting problem, what you can do to detect or prevent that. One thing that we know works is threshold encryption of the transactions where there's consensus among the committee on the order which they commit to publicly before they decrypt. Right.\n00:24:11.630 - 00:24:19.630, Speaker A: And that way they at least don't know what is the content of the encrypted transactions until it's too late to front run them.\n00:24:19.780 - 00:24:20.382, Speaker B: Thank you.\n00:24:20.436 - 00:24:21.040, Speaker A: Thanks.\n00:24:21.810 - 00:24:38.842, Speaker C: Hey, Ed, thank you for the talk. Maybe I'm not understanding something correctly, but okay. I think there's a few moving parts here. So are you saying that first the sequencer commits to some order and someone decrypts it and then the debug gets committed?\n00:24:38.926 - 00:25:21.662, Speaker A: Is that sort of the that's basically the idea, yes. In a world of threshold encryption, you want the sequencer or the committee to commit to a set of transactions that have arrived within the time window, and some of those might still be encrypted. Right. Then you decrypt having committed to that, you're now committed and everything will follow deterministically from there, right? First the decryption, the threshold decryption by the committee, then the sorting and then the filtering of unfunded transactions. As long as the committee is threshold honest, those things are all deterministic and can't be changed once they've committed to this set of arrived transactions.\n00:25:21.726 - 00:25:22.642, Speaker C: Got it. Thank you.\n00:25:22.696 - 00:25:23.620, Speaker A: Yeah, thanks.\n00:25:25.370 - 00:25:25.846, Speaker C: Go ahead.\n00:25:25.868 - 00:25:51.870, Speaker D: Sorry, I think I must be misunderstanding something. Because in order to verify that what the committee committed to is correct, that they committed and it's encrypted, then it seems like you're reintroducing the problem of a free data storage layer for the Internet because you have to have that set of transactions available to verify the commitment, and you haven't verified that those are funded.\n00:25:53.890 - 00:26:00.238, Speaker A: Right. You can commit to the set without.\n00:26:00.324 - 00:26:06.498, Speaker D: Publishing them, but some of them might be encrypted. So have you also verified that they're funded on an encrypted state or?\n00:26:06.504 - 00:26:07.154, Speaker A: No, not yet.\n00:26:07.192 - 00:26:08.162, Speaker D: No, exactly.\n00:26:08.296 - 00:26:52.458, Speaker A: Right. So the committee members know the transactions, they know the set of transactions, they reach consensus among themselves on the set of transactions, including the encrypted ones. They commit cryptographically to that without publishing any plain text. Right. Then they collectively just among themselves decrypt everything and then so they decrypt everything and then they sort and filter and then they jointly sign the result of that. Right. And so, as I've described the scheme here, no one outside the committee is going to see what the contents of those transactions are.\n00:26:52.458 - 00:27:42.606, Speaker A: You are trusting the committee you're threshold trusting the committee to do the filtering correctly on transactions that were encrypted. If you make them publish the decryptions permanently, you obviously have the problem you described. Now, there are other schemes where they can prove that something about the for example, they can prove what some commitment to the transaction was so that a person can make a claim, hey, I submitted this transaction, they say they discarded it. Here's the pre image of that commitment that they say they discarded and you can see it's valid. So there's schemes like that you can use. Yes, Dan, I think yeah.\n00:27:42.788 - 00:27:51.330, Speaker C: So did I understand correctly that you're building both a centralized and a decentralized sequencer and then users can choose which one they submit transactions to.\n00:27:51.480 - 00:28:00.406, Speaker A: As a practical matter, building a central, enhancing the centralized sequencer to do this can happen much more quickly than building the decentralized one.\n00:28:00.428 - 00:28:01.638, Speaker C: But eventually there'll be two.\n00:28:01.724 - 00:28:06.774, Speaker A: Eventually, yeah, eventually we expect there'll be centralized and decentralized users can choose which.\n00:28:06.812 - 00:28:08.310, Speaker C: One to submit to.\n00:28:08.460 - 00:28:24.990, Speaker A: No. So the chain as a whole needs to choose. The chain needs a sequencer so each chain can decide for themselves the sort of flagship Arbitrum chain would for that chain, it would be decided by governance, but for an app chain, it's up to whoever governs that app chain. Okay, yeah, perfect.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nIf the sequencer lies, then you might care how quickly it gets caught in the lie.\n00:05:39.770 - 00:05:40.242, Speaker C: Okay?\n00:05:40.316 - 00:05:51.180, Speaker A: But for the most part, there's not a lot of mev sensitivity in these transactions. But of course, what you may be really interested in is what happens regarding mev on layer two.\n00:05:52.110 - 00:05:52.860, Speaker C: Okay.\n00:05:54.990 - 00:06:49.518, Speaker A: Let me stop and talk about who runs the sequencer because I think this is important in thinking about how all of this works. Because the sequencer is really the only party who has opportunity to extract it's, the only party who is definitely sitting on the path that a transaction must go through and potentially has power to extract mev. So currently, the sequencer is centralized. It's run by us, the Arbitrum team, and so if you trust us to run the sequencer honestly, and if you believe we're running the code that we published, then you're good to go. Over time though, we will move to a distributed sequencer. The sequencer will become a distributed system. Now importantly, we are not following the paradigm that some decentralized sequencing systems use, where you have a rotating sequencer, which is basically a rotating centralization system, where each step or round or block is centralized and you rotate who gets to do those things instead.\n00:06:49.518 - 00:07:35.150, Speaker A: This is a fully distributed system. I'll talk in a minute about how it works. But the guarantee we give is that as long as K out of N of those participants are honest for suitable values, think more than two thirds honest, then the result will be honest. It will meet the specification. I'll talk in a minute about what that is. It's also worth noting that in this scheme, dishonest behavior by individual sequencer nodes will usually be detectable, not provable to a third party, but because it's detectable, that means if a sequencer consistently cheats, then that will become common knowledge that that's a cheating sequencer. And so a social mechanism or a governance mechanism can kick in to replace that lying sequencer.\n00:07:35.890 - 00:07:36.542, Speaker C: Okay?\n00:07:36.676 - 00:08:36.690, Speaker A: So in a distributed sequencer world, you provide a distributed first come, first served type of policy. And here's what that means. In particular, the story of a transaction basically is that the user multicast their transaction to all of the sequencer instances. Think 15 to 20 of them user multicast their transaction to all of them, or almost all of them. Each sequencer then publishes its own ordering of the order in which it received the transactions, or so it claims. And then there's a fair sequence merging algorithm that takes those individual claimed sequences from all of the sequencers and merges them to produce a consensus sequence. And the guarantee is something that you might informally call majority first come, first served, which is roughly that if a supermajority of the sequencers are honest, then if your transaction arrived before mine at a supermajority of the sequencers, yours will be before mine in the order.\n00:08:36.690 - 00:09:10.570, Speaker A: There's some technical nuances here and a big shout out to the research team at Cornell Tech, mahin Mckalkar and Ari Jewels and other collaborators who produced the work that makes this possible. Basically, they showed exactly what properties could and could not be achieved and good algorithms for achieving them. But in any case, this is the guarantee that if a supermajority are honest, that the result will be first come, first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "- 00:08:41.346, Speaker A: What does fair mean in this context? I don't really care. It's not important for this talk. I think Tarun's going to talk about a linear it's going to be pretty spicy. But for now I'm going to argue differently. I'm going to argue we don't need to solve the problem. Because if you look at proof of stake ethereum today, you have this open market of searchers PBS and you have the stakers who are basically competing in a lottery. The searchers do all the hard work.\n00:08:41.346 - 00:09:23.050, Speaker A: They extract the mev, they pay a bribe to the proposer, the proposer gets a transaction fee, then they include the bundle. And so there's a very good chance that a sequencer can make more money by having an open market of searchers do the hard work as opposed to trying to extract the mev themselves. And if that's the case then their financial incentive is not to extract the value but allow someone else to do it. So we don't have to worry about the fair ordering problem for now. And if asked the Kias well that means the sequencer's ordering policy is the order transactions by transaction fee. Whoever pays the has fee will get ordered first in the list. And so this is basically payment for order flow.\n00:09:23.050 - 00:10:09.194, Speaker A: Maybe some mev people hate me for saying that but it's basically that anyway, the sequencer will get the list of pending transactions and give it to the searchers. The searchers can crunch and crunch and crunch, extract as much mev as they want and then eventually send a bundle to the sequencer with a payment. And they'll say sequencer, if you clear my bundle you'll make this amount of money then the sequencer takes that bundle, they take the payment and of course they order it according to the payments they receive. Very much similar to what happens on Ethereum today. You're sharing the mev between the sequencer and the searchers. And again user experience stuff very different to the previous case. Users could still have free transactions because the transaction fee is actually the mev that's extracted.\n00:10:09.194 - 00:10:41.690, Speaker A: But again, this could have a long delay. Users don't want to at least I don't want to build a layer two where users are waiting for two to 3 hours for their transaction to be confirmed. So this is why most rule apps implement first come, first serve because they want to prioritize the user experience. What do I mean by this? Well, the user will send their transactions to the sequencer. The sequencer will timestamp this and then simply order the transactions according to the timestamp. And this has a wonderful user experience. It's a bit like transacting on Coinbase.\n00:10:41.690 - 00:11:27.490, Speaker A: You send your transaction to the service provider and they return back a response to say it's confirmed. This is how it's executed under a second. Typically when people talk about first come, first serve they say oh well there's no mev here hiding under the covers.\n\ndocument_type: youtube_video\ntitle: Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=dZfSbtBOGAQ\nrelease_date: 2023-09-04\n\n00:22:51.358 - 00:22:51.506, Speaker A: Right.\n00:22:51.528 - 00:22:56.354, Speaker B: It doesn't require a private mempool for the sequencer. You can still kind of gossip transactions.\n00:22:56.402 - 00:23:12.774, Speaker A: Around or no, let's see. So with the centralized sequencer, there's not really a need for gossip. Right. The users who want their transactions in fast will send them directly to the sequencer. With the decentralized sequencer. Yes. You can have gossip.\n00:23:12.774 - 00:24:11.630, Speaker A: It makes sense to have gossip as part of this. Again, someone who wants their transaction in fast will send it directly to a quorum of the committee. But because of a concern about front running with the committee, if the transaction becomes known early on to some dishonest member, then you might worry that they'll inject a front running transaction. And there's things you can do about that within the consensus protocol, because other members of the committee will see that front running transaction arriving relatively late or else being known only to a small number of possibly colluding parties. So it's an interesting problem, what you can do to detect or prevent that. One thing that we know works is threshold encryption of the transactions where there's consensus among the committee on the order which they commit to publicly before they decrypt. Right.\n00:24:11.630 - 00:24:19.630, Speaker A: And that way they at least don't know what is the content of the encrypted transactions until it's too late to front run them.\n00:24:19.780 - 00:24:20.382, Speaker B: Thank you.\n00:24:20.436 - 00:24:21.040, Speaker A: Thanks.\n00:24:21.810 - 00:24:38.842, Speaker C: Hey, Ed, thank you for the talk. Maybe I'm not understanding something correctly, but okay. I think there's a few moving parts here. So are you saying that first the sequencer commits to some order and someone decrypts it and then the debug gets committed?\n00:24:38.926 - 00:25:21.662, Speaker A: Is that sort of the that's basically the idea, yes. In a world of threshold encryption, you want the sequencer or the committee to commit to a set of transactions that have arrived within the time window, and some of those might still be encrypted. Right. Then you decrypt having committed to that, you're now committed and everything will follow deterministically from there, right? First the decryption, the threshold decryption by the committee, then the sorting and then the filtering of unfunded transactions. As long as the committee is threshold honest, those things are all deterministic and can't be changed once they've committed to this set of arrived transactions.\n00:25:21.726 - 00:25:22.642, Speaker C: Got it. Thank you.\n00:25:22.696 - 00:25:23.620, Speaker A: Yeah, thanks.\n00:25:25.370 - 00:25:25.846, Speaker C: Go ahead.\n00:25:25.868 - 00:25:51.870, Speaker D: Sorry, I think I must be misunderstanding something. Because in order to verify that what the committee committed to is correct, that they committed and it's encrypted, then it seems like you're reintroducing the problem of a free data storage layer for the Internet because you have to have that set of transactions available to verify the commitment, and you haven't verified that those are funded.\n00:25:53.890 - 00:26:00.238, Speaker A: Right. You can commit to the set without.\n00:26:00.324 - 00:26:06.498, Speaker D: Publishing them, but some of them might be encrypted. So have you also verified that they're funded on an encrypted state or?\n00:26:06.504 - 00:26:07.154, Speaker A: No, not yet.\n00:26:07.192 - 00:26:08.162, Speaker D: No, exactly.\n00:26:08.296 - 00:26:52.458, Speaker A: Right. So the committee members know the transactions, they know the set of transactions, they reach consensus among themselves on the set of transactions, including the encrypted ones. They commit cryptographically to that without publishing any plain text. Right. Then they collectively just among themselves decrypt everything and then so they decrypt everything and then they sort and filter and then they jointly sign the result of that. Right. And so, as I've described the scheme here, no one outside the committee is going to see what the contents of those transactions are.\n00:26:52.458 - 00:27:42.606, Speaker A: You are trusting the committee you're threshold trusting the committee to do the filtering correctly on transactions that were encrypted. If you make them publish the decryptions permanently, you obviously have the problem you described. Now, there are other schemes where they can prove that something about the for example, they can prove what some commitment to the transaction was so that a person can make a claim, hey, I submitted this transaction, they say they discarded it. Here's the pre image of that commitment that they say they discarded and you can see it's valid. So there's schemes like that you can use. Yes, Dan, I think yeah.\n00:27:42.788 - 00:27:51.330, Speaker C: So did I understand correctly that you're building both a centralized and a decentralized sequencer and then users can choose which one they submit transactions to.\n00:27:51.480 - 00:28:00.406, Speaker A: As a practical matter, building a central, enhancing the centralized sequencer to do this can happen much more quickly than building the decentralized one.\n00:28:00.428 - 00:28:01.638, Speaker C: But eventually there'll be two.\n00:28:01.724 - 00:28:06.774, Speaker A: Eventually, yeah, eventually we expect there'll be centralized and decentralized users can choose which.\n00:28:06.812 - 00:28:08.310, Speaker C: One to submit to.\n00:28:08.460 - 00:28:24.990, Speaker A: No. So the chain as a whole needs to choose. The chain needs a sequencer so each chain can decide for themselves the sort of flagship Arbitrum chain would for that chain, it would be decided by governance, but for an app chain, it's up to whoever governs that app chain. Okay, yeah, perfect.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nIf the sequencer lies, then you might care how quickly it gets caught in the lie.\n00:05:39.770 - 00:05:40.242, Speaker C: Okay?\n00:05:40.316 - 00:05:51.180, Speaker A: But for the most part, there's not a lot of mev sensitivity in these transactions. But of course, what you may be really interested in is what happens regarding mev on layer two.\n00:05:52.110 - 00:05:52.860, Speaker C: Okay.\n00:05:54.990 - 00:06:49.518, Speaker A: Let me stop and talk about who runs the sequencer because I think this is important in thinking about how all of this works. Because the sequencer is really the only party who has opportunity to extract it's, the only party who is definitely sitting on the path that a transaction must go through and potentially has power to extract mev. So currently, the sequencer is centralized. It's run by us, the Arbitrum team, and so if you trust us to run the sequencer honestly, and if you believe we're running the code that we published, then you're good to go. Over time though, we will move to a distributed sequencer. The sequencer will become a distributed system. Now importantly, we are not following the paradigm that some decentralized sequencing systems use, where you have a rotating sequencer, which is basically a rotating centralization system, where each step or round or block is centralized and you rotate who gets to do those things instead.\n00:06:49.518 - 00:07:35.150, Speaker A: This is a fully distributed system. I'll talk in a minute about how it works. But the guarantee we give is that as long as K out of N of those participants are honest for suitable values, think more than two thirds honest, then the result will be honest. It will meet the specification. I'll talk in a minute about what that is. It's also worth noting that in this scheme, dishonest behavior by individual sequencer nodes will usually be detectable, not provable to a third party, but because it's detectable, that means if a sequencer consistently cheats, then that will become common knowledge that that's a cheating sequencer. And so a social mechanism or a governance mechanism can kick in to replace that lying sequencer.\n00:07:35.890 - 00:07:36.542, Speaker C: Okay?\n00:07:36.676 - 00:08:36.690, Speaker A: So in a distributed sequencer world, you provide a distributed first come, first served type of policy. And here's what that means. In particular, the story of a transaction basically is that the user multicast their transaction to all of the sequencer instances. Think 15 to 20 of them user multicast their transaction to all of them, or almost all of them. Each sequencer then publishes its own ordering of the order in which it received the transactions, or so it claims. And then there's a fair sequence merging algorithm that takes those individual claimed sequences from all of the sequencers and merges them to produce a consensus sequence. And the guarantee is something that you might informally call majority first come, first served, which is roughly that if a supermajority of the sequencers are honest, then if your transaction arrived before mine at a supermajority of the sequencers, yours will be before mine in the order.\n00:08:36.690 - 00:09:10.570, Speaker A: There's some technical nuances here and a big shout out to the research team at Cornell Tech, mahin Mckalkar and Ari Jewels and other collaborators who produced the work that makes this possible. Basically, they showed exactly what properties could and could not be achieved and good algorithms for achieving them. But in any case, this is the guarantee that if a supermajority are honest, that the result will be first come, first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle",
                        "previous_answer": "Layer 2 sequencers operate by receiving transactions directly from users and arranging them into a sequence. The order is determined by the sequencer, ideally following a first-come, first-served basis. The sequencer then quickly publishes a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a substantial batch of unrecorded transactions, the sequencer compresses it using a general-purpose compression algorithm and records it onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be either a centralized entity or a distributed system, and the selection of sequencers is a permissioned process involving community deliberation. In some cases, sequencers may allow an open market of searchers to extract maximum extractable value (MEV) rather than doing it themselves, as this could potentially lead to higher profits."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers work by receiving transactions from users and organizing them into a sequence. The order is typically determined by the sequencer, often following a first-come, first-served policy. The sequencer then publishes a real-time feed, which anyone can subscribe to. After accumulating a significant batch of unrecorded transactions, the sequencer compresses it using a general-purpose compression algorithm and records it onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be either a centralized entity or a distributed system. In a distributed sequencer system, the user multicasts their transaction to all of the sequencer instances. Each sequencer then publishes its own ordering of the transactions it received. A fair sequence merging algorithm then merges these individual sequences to produce a consensus sequence. In some cases, sequencers may allow an open market of searchers to extract maximum extractable value (MEV) rather than doing it themselves, as this could potentially lead to higher profits."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "majority first come, first served, which is roughly that if a supermajority of the sequencers are honest, then if your transaction arrived before mine at a supermajority of the sequencers, yours will be before mine in the order.\n00:08:36.690 - 00:09:10.570, Speaker A: There's some technical nuances here and a big shout out to the research team at Cornell Tech, mahin Mckalkar and Ari Jewels and other collaborators who produced the work that makes this possible. Basically, they showed exactly what properties could and could not be achieved and good algorithms for achieving them. But in any case, this is the guarantee that if a supermajority are honest, that the result will be first come, first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle number one is to reduce users costs. Reduce users total costs, right? That's one of the big reasons people come to an L. Two, they want lower cost and we want to reduce total cost.\n00:10:00.410 - 00:10:37.538, Speaker A: So that includes the cost of any slippage or other loss that a user suffers because someone reordered on them. So to the extent that someone is a victim of mev extraction, then that implies that we should reduce mev extraction where we can there can be some pro social mev extraction. We're fine if users can opt into it. I'll talk about that later. But we don't want to force people into a world where someone can extract mev from them because that is a cost. And our goal is to"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "majority first come, first served, which is roughly that if a supermajority of the sequencers are honest, then if your transaction arrived before mine at a supermajority of the sequencers, yours will be before mine in the order.\n00:08:36.690 - 00:09:10.570, Speaker A: There's some technical nuances here and a big shout out to the research team at Cornell Tech, mahin Mckalkar and Ari Jewels and other collaborators who produced the work that makes this possible. Basically, they showed exactly what properties could and could not be achieved and good algorithms for achieving them. But in any case, this is the guarantee that if a supermajority are honest, that the result will be first come, first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle number one is to reduce users costs. Reduce users total costs, right? That's one of the big reasons people come to an L. Two, they want lower cost and we want to reduce total cost.\n00:10:00.410 - 00:10:37.538, Speaker A: So that includes the cost of any slippage or other loss that a user suffers because someone reordered on them. So to the extent that someone is a victim of mev extraction, then that implies that we should reduce mev extraction where we can there can be some pro social mev extraction. We're fine if users can opt into it. I'll talk about that later. But we don't want to force people into a world where someone can extract mev from them because that is a cost. And our goal is to",
                        "previous_answer": "Layer 2 sequencers operate by receiving transactions directly from users and arranging them into a sequence. The order is determined by the sequencer, ideally following a first-come, first-served basis. The sequencer then quickly publishes a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a substantial batch of unrecorded transactions, the sequencer compresses it using a general-purpose compression algorithm and records it onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be either a centralized entity or a distributed system, and the selection of sequencers is a permissioned process involving community deliberation. In some cases, sequencers may allow an open market of searchers to extract maximum extractable value (MEV) rather than doing it themselves, as this could potentially lead to higher profits."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers function by receiving transactions directly from users and organizing them into a specific order. This order is ideally determined on a first-come, first-served basis, which is ensured if a supermajority of the sequencers are honest. The sequencer then promptly releases a real-time feed, known as a sequencer feed, to which anyone can subscribe. After gathering a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and logs them onto the Layer 1 chain, thereby creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may permit an open market of searchers to extract maximum extractable value (MEV) instead of doing it themselves, as this could potentially result in higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to modify the order."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle number one is to reduce users costs. Reduce users total costs, right? That's one of the big reasons people come to an L. Two, they want lower cost and we want to reduce total cost.\n00:10:00.410 - 00:10:37.538, Speaker A: So that includes the cost of any slippage or other loss that a user suffers because someone reordered on them. So to the extent that someone is a victim of mev extraction, then that implies that we should reduce mev extraction where we can there can be some pro social mev extraction. We're fine if users can opt into it. I'll talk about that later. But we don't want to force people into a world where someone can extract mev from them because that is a cost. And our goal is to minimize cost. But also the second principle regarding cost and fees is we try to make users cost incentive compatible.\n00:10:37.538 - 00:11:58.590, Speaker A: And in particular, that means we want to incent behavior on users behalf, which reduces the use of scarce resources in the system and which preferentially reduces the use of the scarcest resources. So to the extent that a user's costs are perfectly aligned with their usage of scarce resources, then users have the incentives we want them to have, and our chain and our users will use those resources efficiently. To the extent that there's some other element of cost which doesn't correlate with that use of scarce resources and users are optimizing some combination of those things, say some combination of resource usage and mev resistance, then you're going to get a worse result for resource use and our chain will be less efficient. So that user may be saving their cost by making themselves mev resistant but using more resource. But by doing so, they're transferring cost onto other users because those resources become marginally more expensive. All right, so this is why we would prefer to keep mev considerations mostly out of the cost equation of our users unless the users have some reason that they think mev will benefit them. All right, observation number three users really love the 1 second transaction latency.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nSo to the extent that a user's costs are perfectly aligned with their usage of scarce resources, then users have the incentives we want them to have, and our chain and our users will use those resources efficiently. To the extent that there's some other element of cost which doesn't correlate with that use of scarce resources and users are optimizing some combination of those things, say some combination of resource usage and mev resistance, then you're going to get a worse result for resource use and our chain will be less efficient. So that user may be saving their cost by making themselves mev resistant but using more resource. But by doing so, they're transferring cost onto other users because those resources become marginally more expensive. All right, so this is why we would prefer to keep mev considerations mostly out of the cost equation of our users unless the users have some reason that they think mev will benefit them. All right, observation number three users really love the 1 second transaction latency.\n00:11:58.590 - 00:13:04.098, Speaker A: You can't really appreciate this until you ship a system that does it and you hear from your users and they love it for two reasons. One is its basic doctrine in the user experience world that there's kind of a boundary around 1 second that things that take more than 1 second feel like they take a long time and you're tempted to go get a cup of coffee. Things that take 1 second, you notice the lag, but that's pretty good. People want to have not only a low mean latency but also a low variance in the latency. And that is important and it's something we want to preserve. Notice the consequences of this for mev extraction, which is that if we were to build up a big mem pool and let an mev extractor choose from that big mem pool, that would necessarily increase the mean and variance of the latency that users experience and that would be bad for users. Okay, so what is our policy? Our policy is the sequencer provides a first come, first served ordering of transactions that's best from the standpoint of latency mean and variance.\n00:13:04.098 - 00:13:56.918, Speaker A: It's simple to implement, it's simple to explain to users. Okay, let me talk about the implications of these things for mev more generally across L two S. The first one is that the mean invariance of latency on these systems is much lower than on ethereum and that the mean is important because it's sort of a measure of how much of a mem pool you can build up and reorder before you're starting to increase the time. Very much. And the variance is also really important that Ethereum already has a high variance in the latency to get your transaction recorded. And so if you add some more variance because of mev extraction, users don't notice it that much. But on a system with both very low mean and very low variance, users notice a lot and it degrades the user experience.\n00:13:56.918 - 00:15:03.680, Speaker A: That means that there's only probably a fraction of a second of mempool that you can build up and reorder in an mev extractor before you start to degrade the overall user experience of the system. Right? So here an mev extracting sequencer would either significantly increase latency for many transactions or be able to only manipulate sort of the timing of individual transactions by a fraction of a second, so much less opportunity for mev extraction without paying a price in other areas that are really important to us. Okay, some other topics to discuss. One of the things that has been proposed is the idea of putting an mev extractor in front of the sequencer on a kind of opt in basis. Anybody could do this. Of course, our sequencer or distributed sequencer is a set of nodes that are trusted or collectively k of n trusted by our community. And of course our community isn't going to be involved in the deliberations about how to select them.\n00:15:03.680 - 00:16:25.640, Speaker A: But anyone who wants to, whether users or we, like it or not, can put an external mev extractor in front of the sequencer and users can choose to use that as a proxy for submitting their transactions. So that is the thing that can happen if you're a user, for example, who's doing a DeFi transaction and you recognize that you're creating an arbitrage opportunity behind your transaction and you'd like to be able to sell that opportunity. You could choose to route through an intermediary and of course there can be multiple of those and they will compete with each other on user value. That's a thing that A, we can't stop and b is probably a good thing as long as it's not mandatory for users, which it won't be. Users can go directly to our sequencer or sequencers or they can go through this intermediary. Go through an intermediary, you'll get slightly slower response because of the intermediary but maybe you can extract some value. The other thing, of course, the other piece here is because our sequencer is currently sitting geographically somewhere and because the members of the distributed sequencer set will be sitting geographically in a bunch of places, probably around the world, people may compete to get geographically close or network latency close to those sequencers in order to get faster response time.\n00:16:25.640 - 00:17:48.146, Speaker A: And that will probably exist. We don't have hard evidence but we strongly suspect people are already doing this. That could exist for private use or it could be sold as a service and in general anything that has value for private use probably could be sold as a service, as a business. We expect that as things mature and develop that that might well happen. The consequences of the geographically distributed sequencer committee here are interesting because if you have a point of presence if there are 20 sequencers scattered around the world and you have a point of presence at each one and what you need is to get your transaction to a supermajority of them before somebody else. That means you may not have time to coordinate between your different points of presence in order to coordinate on what you're going to do. So there is some limit on your ability to actually use geographic proximity in this distributed system sense simply because if Alice is sitting in one place and multicasts her transaction out across the world to everywhere, then if you see it over here and then try to inform your friend over there of what to do, that Alice's transaction will probably beat you.\n00:17:48.146 - 00:18:37.566, Speaker A: Right? Because you have to pay the triangle penalty for your path. And so there is some resistance to the geographic proximity methods. Essentially someone would need to have a deterministic method of reacting to the transactions they see, which is a little tricky because if you have only local information about the order in which your sequencer saw the information, that's not a guarantee about what the majority will see.\n\ntotal_pages: 34\nsource: 13\ndocument_type: research_paper\ntitle: Credible Decentralized Exchange Design via Verifiable Sequencing Rules\nauthors: Matheus V. X. Ferreira, David C. Parkes\npdf_link: http://arxiv.org/pdf/2209.15569v2\nrelease_date: 2022-09-30\n\n3\nCommunication Model\nThe risk of market manipulation in liquidity pool exchanges arrives from how users communicate\ntheir transactions with the exchange.\nSuppose users 1, 2, . . . , |A| want to execute transactions\nA1, A2, . . . , A|A| at state X0. Note a single entity could control multiple users, but that is not\nrelevant for our analysis. Each user privately sends their transaction to the miner. The miner\naggregates observed transactions into a block B, which we model as a set of potentially unbounded\nsize.\nThe order of transactions in the block de\ufb01nes the execution ordering\u2014the order by which\ntransactions execute in the decentralized exchange. In our model, miners pick the block (i.e., the\ntransactions to include), but use a sequencing rule S to determine the execution ordering.\nDe\ufb01nition 3.1 (Sequencing Rule). A sequencing rule S is a function from a state X (of the\nliquidity reserves before any transaction executes) and a set of transactions B to a non-empty set\nsystem S(X, B) containing permutations of B.\nFirst, we would like a sequencing rule to be e\ufb03ciently computable in order to minimize the\ncomputational burden on miners.\nDe\ufb01nition 3.2 (E\ufb03cient Sequencer). A sequencing rule S is (computationally) e\ufb03cient, if for all\ninitial state X = (X1, X2) and block B, there is an algorithm that takes (X0, B) and outputs some\nT \u2208 S(X0, B) in time O(log(X1 + X2)|B|).\nAny block sequencing algorithm requires at least log(X1 + X2)|B| computation to read the\ncontent of B.\nThus our de\ufb01nition requires that a sequencing rule imposes at most a constant\nmultiplicative computational overhead when compared with the status quo, i.e., where the miner\ncomputes their favorite ordering of B.\nWe are ready to de\ufb01ne the trading game (X0, {Ai}, S) between users and a miner. The game\ntakes as input a transaction, Ai, from each user i, the initial state X0, and a sequencing rule S.\nThe outcome of the game is an execution ordering on a set of transactions and associated sequence\nof states, where the transactions that are ordered can include a subset of user transactions and\nadditional transactions that may be introduced by the miner. In the case of an honest miner, the\ngame proceeds as in Algorithm 2:\n13\n\ndocument_type: youtube_video\ntitle: MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=kb8nY2-Osjc\nrelease_date: 2023-03-28\n\n00:00:06.650 - 00:00:34.854, Speaker A: And we will now transition on to the next talk. We're going to have Pat McCorry coming on and he's going to be talking about a closer look at the sequencer's role in MEB trying to unmute myself. I was enjoying that by the way. I was happy to be delayed. That was great, Kyle. Let's get started then. So today what I'm going to talk about is layers of layers of layers, hopefully taking inspiration from layer one and a plan of the layer two.\n00:00:34.854 - 00:01:15.854, Speaker A: And what we're mostly going to look at is the sequencer and how we can think about mev around the sequencer. So as we know, just a bit of background. In mev land there's typically three agents that we care about. One is the honest user. All the honest user wants to do is buy and sell their mooncap. We have a proposer and the proposer's job is to take a list of pending transactions and decide the final ordering of those transactions. And third, we have the searcher, those mev bots, those people living in their bedrooms where they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "first served in sort of the natural sense that you can define that in a distributed system with asynchronous clocks.\n00:09:11.230 - 00:09:11.738, Speaker C: Okay?\n00:09:11.824 - 00:10:00.410, Speaker A: Now, you can also combine with this the use of public key encryption of the transaction and threshold decryption by the sequencer committee members that sort of knits together in a really cool way so that people in the middle, including the sequencers themselves, don't know the contents of the transactions until it's too late for them to modify the order. Okay, so that's sort of the technical background. Let me now turn to how we on the Arbitrum team think about mev. I'm going to give you two principles an observation and then tell you what our policy is. Principle number one is to reduce users costs. Reduce users total costs, right? That's one of the big reasons people come to an L. Two, they want lower cost and we want to reduce total cost.\n00:10:00.410 - 00:10:37.538, Speaker A: So that includes the cost of any slippage or other loss that a user suffers because someone reordered on them. So to the extent that someone is a victim of mev extraction, then that implies that we should reduce mev extraction where we can there can be some pro social mev extraction. We're fine if users can opt into it. I'll talk about that later. But we don't want to force people into a world where someone can extract mev from them because that is a cost. And our goal is to minimize cost. But also the second principle regarding cost and fees is we try to make users cost incentive compatible.\n00:10:37.538 - 00:11:58.590, Speaker A: And in particular, that means we want to incent behavior on users behalf, which reduces the use of scarce resources in the system and which preferentially reduces the use of the scarcest resources. So to the extent that a user's costs are perfectly aligned with their usage of scarce resources, then users have the incentives we want them to have, and our chain and our users will use those resources efficiently. To the extent that there's some other element of cost which doesn't correlate with that use of scarce resources and users are optimizing some combination of those things, say some combination of resource usage and mev resistance, then you're going to get a worse result for resource use and our chain will be less efficient. So that user may be saving their cost by making themselves mev resistant but using more resource. But by doing so, they're transferring cost onto other users because those resources become marginally more expensive. All right, so this is why we would prefer to keep mev considerations mostly out of the cost equation of our users unless the users have some reason that they think mev will benefit them. All right, observation number three users really love the 1 second transaction latency.\n\ndocument_type: youtube_video\ntitle: L2 sequencing and MEV - Ed Felten (Arbitrum)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=qxml80TparY\nrelease_date: 2022-05-22\n\nSo to the extent that a user's costs are perfectly aligned with their usage of scarce resources, then users have the incentives we want them to have, and our chain and our users will use those resources efficiently. To the extent that there's some other element of cost which doesn't correlate with that use of scarce resources and users are optimizing some combination of those things, say some combination of resource usage and mev resistance, then you're going to get a worse result for resource use and our chain will be less efficient. So that user may be saving their cost by making themselves mev resistant but using more resource. But by doing so, they're transferring cost onto other users because those resources become marginally more expensive. All right, so this is why we would prefer to keep mev considerations mostly out of the cost equation of our users unless the users have some reason that they think mev will benefit them. All right, observation number three users really love the 1 second transaction latency.\n00:11:58.590 - 00:13:04.098, Speaker A: You can't really appreciate this until you ship a system that does it and you hear from your users and they love it for two reasons. One is its basic doctrine in the user experience world that there's kind of a boundary around 1 second that things that take more than 1 second feel like they take a long time and you're tempted to go get a cup of coffee. Things that take 1 second, you notice the lag, but that's pretty good. People want to have not only a low mean latency but also a low variance in the latency. And that is important and it's something we want to preserve. Notice the consequences of this for mev extraction, which is that if we were to build up a big mem pool and let an mev extractor choose from that big mem pool, that would necessarily increase the mean and variance of the latency that users experience and that would be bad for users. Okay, so what is our policy? Our policy is the sequencer provides a first come, first served ordering of transactions that's best from the standpoint of latency mean and variance.\n00:13:04.098 - 00:13:56.918, Speaker A: It's simple to implement, it's simple to explain to users. Okay, let me talk about the implications of these things for mev more generally across L two S. The first one is that the mean invariance of latency on these systems is much lower than on ethereum and that the mean is important because it's sort of a measure of how much of a mem pool you can build up and reorder before you're starting to increase the time. Very much. And the variance is also really important that Ethereum already has a high variance in the latency to get your transaction recorded. And so if you add some more variance because of mev extraction, users don't notice it that much. But on a system with both very low mean and very low variance, users notice a lot and it degrades the user experience.\n00:13:56.918 - 00:15:03.680, Speaker A: That means that there's only probably a fraction of a second of mempool that you can build up and reorder in an mev extractor before you start to degrade the overall user experience of the system. Right? So here an mev extracting sequencer would either significantly increase latency for many transactions or be able to only manipulate sort of the timing of individual transactions by a fraction of a second, so much less opportunity for mev extraction without paying a price in other areas that are really important to us. Okay, some other topics to discuss. One of the things that has been proposed is the idea of putting an mev extractor in front of the sequencer on a kind of opt in basis. Anybody could do this. Of course, our sequencer or distributed sequencer is a set of nodes that are trusted or collectively k of n trusted by our community. And of course our community isn't going to be involved in the deliberations about how to select them.\n00:15:03.680 - 00:16:25.640, Speaker A: But anyone who wants to, whether users or we, like it or not, can put an external mev extractor in front of the sequencer and users can choose to use that as a proxy for submitting their transactions. So that is the thing that can happen if you're a user, for example, who's doing a DeFi transaction and you recognize that you're creating an arbitrage opportunity behind your transaction and you'd like to be able to sell that opportunity. You could choose to route through an intermediary and of course there can be multiple of those and they will compete with each other on user value. That's a thing that A, we can't stop and b is probably a good thing as long as it's not mandatory for users, which it won't be. Users can go directly to our sequencer or sequencers or they can go through this intermediary. Go through an intermediary, you'll get slightly slower response because of the intermediary but maybe you can extract some value. The other thing, of course, the other piece here is because our sequencer is currently sitting geographically somewhere and because the members of the distributed sequencer set will be sitting geographically in a bunch of places, probably around the world, people may compete to get geographically close or network latency close to those sequencers in order to get faster response time.\n00:16:25.640 - 00:17:48.146, Speaker A: And that will probably exist. We don't have hard evidence but we strongly suspect people are already doing this. That could exist for private use or it could be sold as a service and in general anything that has value for private use probably could be sold as a service, as a business. We expect that as things mature and develop that that might well happen. The consequences of the geographically distributed sequencer committee here are interesting because if you have a point of presence if there are 20 sequencers scattered around the world and you have a point of presence at each one and what you need is to get your transaction to a supermajority of them before somebody else. That means you may not have time to coordinate between your different points of presence in order to coordinate on what you're going to do. So there is some limit on your ability to actually use geographic proximity in this distributed system sense simply because if Alice is sitting in one place and multicasts her transaction out across the world to everywhere, then if you see it over here and then try to inform your friend over there of what to do, that Alice's transaction will probably beat you.\n00:17:48.146 - 00:18:37.566, Speaker A: Right? Because you have to pay the triangle penalty for your path. And so there is some resistance to the geographic proximity methods. Essentially someone would need to have a deterministic method of reacting to the transactions they see, which is a little tricky because if you have only local information about the order in which your sequencer saw the information, that's not a guarantee about what the majority will see.\n\ntotal_pages: 34\nsource: 13\ndocument_type: research_paper\ntitle: Credible Decentralized Exchange Design via Verifiable Sequencing Rules\nauthors: Matheus V. X. Ferreira, David C. Parkes\npdf_link: http://arxiv.org/pdf/2209.15569v2\nrelease_date: 2022-09-30\n\n3\nCommunication Model\nThe risk of market manipulation in liquidity pool exchanges arrives from how users communicate\ntheir transactions with the exchange.\nSuppose users 1, 2, . . . , |A| want to execute transactions\nA1, A2, . . . , A|A| at state X0. Note a single entity could control multiple users, but that is not\nrelevant for our analysis. Each user privately sends their transaction to the miner. The miner\naggregates observed transactions into a block B, which we model as a set of potentially unbounded\nsize.\nThe order of transactions in the block de\ufb01nes the execution ordering\u2014the order by which\ntransactions execute in the decentralized exchange. In our model, miners pick the block (i.e., the\ntransactions to include), but use a sequencing rule S to determine the execution ordering.\nDe\ufb01nition 3.1 (Sequencing Rule). A sequencing rule S is a function from a state X (of the\nliquidity reserves before any transaction executes) and a set of transactions B to a non-empty set\nsystem S(X, B) containing permutations of B.\nFirst, we would like a sequencing rule to be e\ufb03ciently computable in order to minimize the\ncomputational burden on miners.\nDe\ufb01nition 3.2 (E\ufb03cient Sequencer). A sequencing rule S is (computationally) e\ufb03cient, if for all\ninitial state X = (X1, X2) and block B, there is an algorithm that takes (X0, B) and outputs some\nT \u2208 S(X0, B) in time O(log(X1 + X2)|B|).\nAny block sequencing algorithm requires at least log(X1 + X2)|B| computation to read the\ncontent of B.\nThus our de\ufb01nition requires that a sequencing rule imposes at most a constant\nmultiplicative computational overhead when compared with the status quo, i.e., where the miner\ncomputes their favorite ordering of B.\nWe are ready to de\ufb01ne the trading game (X0, {Ai}, S) between users and a miner. The game\ntakes as input a transaction, Ai, from each user i, the initial state X0, and a sequencing rule S.\nThe outcome of the game is an execution ordering on a set of transactions and associated sequence\nof states, where the transactions that are ordered can include a subset of user transactions and\nadditional transactions that may be introduced by the miner. In the case of an honest miner, the\ngame proceeds as in Algorithm 2:\n13\n\ndocument_type: youtube_video\ntitle: MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=kb8nY2-Osjc\nrelease_date: 2023-03-28\n\n00:00:06.650 - 00:00:34.854, Speaker A: And we will now transition on to the next talk. We're going to have Pat McCorry coming on and he's going to be talking about a closer look at the sequencer's role in MEB trying to unmute myself. I was enjoying that by the way. I was happy to be delayed. That was great, Kyle. Let's get started then. So today what I'm going to talk about is layers of layers of layers, hopefully taking inspiration from layer one and a plan of the layer two.\n00:00:34.854 - 00:01:15.854, Speaker A: And what we're mostly going to look at is the sequencer and how we can think about mev around the sequencer. So as we know, just a bit of background. In mev land there's typically three agents that we care about. One is the honest user. All the honest user wants to do is buy and sell their mooncap. We have a proposer and the proposer's job is to take a list of pending transactions and decide the final ordering of those transactions. And third, we have the searcher, those mev bots, those people living in their bedrooms where they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get",
                        "previous_answer": "Layer 2 sequencers function by receiving transactions directly from users and organizing them into a specific order. This order is ideally determined on a first-come, first-served basis, which is ensured if a supermajority of the sequencers are honest. The sequencer then promptly releases a real-time feed, known as a sequencer feed, to which anyone can subscribe. After gathering a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and logs them onto the Layer 1 chain, thereby creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may permit an open market of searchers to extract maximum extractable value (MEV) instead of doing it themselves, as this could potentially result in higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to modify the order."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers receive transactions from users and arrange them in a specific sequence. This sequence is typically determined on a first-come, first-served basis, which is maintained if a supermajority of the sequencers are honest. The sequencer then releases a real-time feed, known as a sequencer feed, that anyone can subscribe to. After collecting a substantial number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and logs them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a single entity or a distributed system, and the selection of sequencers is a process that involves community input. In some cases, sequencers may allow an open market of searchers to extract maximum extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Furthermore, the use of public key encryption for the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "was enjoying that by the way. I was happy to be delayed. That was great, Kyle. Let's get started then. So today what I'm going to talk about is layers of layers of layers, hopefully taking inspiration from layer one and a plan of the layer two.\n00:00:34.854 - 00:01:15.854, Speaker A: And what we're mostly going to look at is the sequencer and how we can think about mev around the sequencer. So as we know, just a bit of background. In mev land there's typically three agents that we care about. One is the honest user. All the honest user wants to do is buy and sell their mooncap. We have a proposer and the proposer's job is to take a list of pending transactions and decide the final ordering of those transactions. And third, we have the searcher, those mev bots, those people living in their bedrooms where they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get their transaction and communicate it to the proposer? So we have to consider the communication channel by default. We will use a gossip protocol or the peer to peer network. Alice will send a transaction to a peer. They'll take the transaction and pass it on to their peers and eventually, within one to 2 seconds, every peer in the network will get a copy of this transaction, including their proposer.\n00:01:59.566 - 00:02:51.006, Speaker A: They'll take this transaction and hopefully include it in their block. Now, the issue is that it's a peer to peer network, so anyone could be on it, including a searcher, so they could listen out for the user's transaction, inspect it, find an mev opportunity and then front run the user and steal the profit. So it's a bit like a dark forest in a way. And there could be not just one searcher, but many searchers that are all competing for exactly the same mev opportunity, the same profit. And this leads to something called"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "was enjoying that by the way. I was happy to be delayed. That was great, Kyle. Let's get started then. So today what I'm going to talk about is layers of layers of layers, hopefully taking inspiration from layer one and a plan of the layer two.\n00:00:34.854 - 00:01:15.854, Speaker A: And what we're mostly going to look at is the sequencer and how we can think about mev around the sequencer. So as we know, just a bit of background. In mev land there's typically three agents that we care about. One is the honest user. All the honest user wants to do is buy and sell their mooncap. We have a proposer and the proposer's job is to take a list of pending transactions and decide the final ordering of those transactions. And third, we have the searcher, those mev bots, those people living in their bedrooms where they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get their transaction and communicate it to the proposer? So we have to consider the communication channel by default. We will use a gossip protocol or the peer to peer network. Alice will send a transaction to a peer. They'll take the transaction and pass it on to their peers and eventually, within one to 2 seconds, every peer in the network will get a copy of this transaction, including their proposer.\n00:01:59.566 - 00:02:51.006, Speaker A: They'll take this transaction and hopefully include it in their block. Now, the issue is that it's a peer to peer network, so anyone could be on it, including a searcher, so they could listen out for the user's transaction, inspect it, find an mev opportunity and then front run the user and steal the profit. So it's a bit like a dark forest in a way. And there could be not just one searcher, but many searchers that are all competing for exactly the same mev opportunity, the same profit. And this leads to something called",
                        "previous_answer": "Layer 2 sequencers function by receiving transactions directly from users and organizing them into a specific order. This order is ideally determined on a first-come, first-served basis, which is ensured if a supermajority of the sequencers are honest. The sequencer then promptly releases a real-time feed, known as a sequencer feed, to which anyone can subscribe. After gathering a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and logs them onto the Layer 1 chain, thereby creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may permit an open market of searchers to extract maximum extractable value (MEV) instead of doing it themselves, as this could potentially result in higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to modify the order."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers operate by receiving transactions directly from users and arranging them in a specific sequence. This sequence is ideally established on a first-come, first-served basis, which is guaranteed if the majority of the sequencers are honest. The sequencer then provides a real-time feed, known as a sequencer feed, to which anyone can subscribe. After collecting a substantial number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and records them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some cases, sequencers may allow an open market of searchers to maximize extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Furthermore, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get their transaction and communicate it to the proposer? So we have to consider the communication channel by default. We will use a gossip protocol or the peer to peer network. Alice will send a transaction to a peer. They'll take the transaction and pass it on to their peers and eventually, within one to 2 seconds, every peer in the network will get a copy of this transaction, including their proposer.\n00:01:59.566 - 00:02:51.006, Speaker A: They'll take this transaction and hopefully include it in their block. Now, the issue is that it's a peer to peer network, so anyone could be on it, including a searcher, so they could listen out for the user's transaction, inspect it, find an mev opportunity and then front run the user and steal the profit. So it's a bit like a dark forest in a way. And there could be not just one searcher, but many searchers that are all competing for exactly the same mev opportunity, the same profit. And this leads to something called priority gas auctions, which I'm sure people have spoke about already, where the searchers will bid each other up. Transaction fee 123456. And it's up to the proposer to pick the transaction with the largest fee that pays them the most money and decide who wins the auction.\n00:02:51.006 - 00:03:16.214, Speaker A: Now, I stole this from a blog post by Tolklasoda E. I probably didn't butcher that name a little bit. But you can see here in the graph within a ten second period there's basically like hundreds of transactions being sent. Then eventually Mr. Blue wins and he wins the option and gets the movie opportunity. Now there's two problems of this approach. One is wasteful of gas.\n00:03:16.214 - 00:03:49.150, Speaker A: You have one transaction that's successful followed by a list of failed transactions and that's a waste of block space. And two, it's unrestricted mev. You're taking the user's transaction, throwing it to the wolves and just hoping it gets to the other side. Okay? Because it's completely unrestricted what these bots can do. So a few years ago, a wonderful company called Flashbot, I wonder who they are. They came along and tried to solve the problem for Ethereum. We give the transaction directly to the Flashbots and they'll give it directly on to the block proposer.\n00:03:49.150 - 00:04:29.300, Speaker A: So then the MVV bots or the searchers, they can't find the opportunity and they can't extract value from the user's transaction. So stepping back a bit, what can we extract from this scenario? Well, we have to consider the proposer's ordering policy. In this case, they're picking the transactions based on the fee. Highest fee first, lowest fee at the bottom and then around 12 seconds to do this. As we saw with the priority gas option. What we need to consider as well is the communication channel. How does the block proposer learn about the transaction and how do the searchers find it as well? And finally, sometimes we forget about the poor little user, but we have to think about the user experience.\n00:04:29.300 - 00:05:00.778, Speaker A: How long does it take for a user to be informed that their transactions confirmed and how it was executed? Because at the end of the day they want to execute and participate on this network. So that's Ethereum in that in a nutshell. We took some cool ideas from that. So now let's move on to layer two and roll up land. And in roll up land there's typically three actors the honest user, the sequencer and the executor. Now for once we're not going to talk about the executor. They have very little to do of mev.\n00:05:00.778 - 00:05:28.230, Speaker A: We care about the sequencer, who's basically the proposer and gets to decide those lists of transactions and their ordering. They have everything to do of mev and the lifecycle is pretty similar. Alice gives their transactions to the sequencer. The sequencer will have a list of pending transactions. They'll run some ordering policy. They'll decide the ordering of the transactions. Then they'll post that onto Ethereum, into an inbox and then picked up by the breach.\n00:05:28.230 - 00:06:10.866, Speaker A: Now in this scenario we have this direct communication channel. There's no gasa protocol. Alice can communicate directly with the sequencer. And what we need to be concerned about is when does Alice get a response from the sequencer and what type of response do they get? And that's really going to depend on how the sequencer decides the order of these transactions, then of course what we care about is the sequencer's ordering policy."
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "they're looking for these mev opportunities, they bundle it up and they pay the proposer to include this bundle in the list of transactions in the ordered list of transactions.\n00:01:15.854 - 00:01:59.566, Speaker A: So that's mev land in a nutshell. So let's begin with Ethereum Mainnet and let's look at the lifecycle of a transaction and let's see what we can extract from this scenario. So we have a user, Alice, again, who wants to buy and sell Mooncats and we have the proposers. So the question is, how does the user get their transaction and communicate it to the proposer? So we have to consider the communication channel by default. We will use a gossip protocol or the peer to peer network. Alice will send a transaction to a peer. They'll take the transaction and pass it on to their peers and eventually, within one to 2 seconds, every peer in the network will get a copy of this transaction, including their proposer.\n00:01:59.566 - 00:02:51.006, Speaker A: They'll take this transaction and hopefully include it in their block. Now, the issue is that it's a peer to peer network, so anyone could be on it, including a searcher, so they could listen out for the user's transaction, inspect it, find an mev opportunity and then front run the user and steal the profit. So it's a bit like a dark forest in a way. And there could be not just one searcher, but many searchers that are all competing for exactly the same mev opportunity, the same profit. And this leads to something called priority gas auctions, which I'm sure people have spoke about already, where the searchers will bid each other up. Transaction fee 123456. And it's up to the proposer to pick the transaction with the largest fee that pays them the most money and decide who wins the auction.\n00:02:51.006 - 00:03:16.214, Speaker A: Now, I stole this from a blog post by Tolklasoda E. I probably didn't butcher that name a little bit. But you can see here in the graph within a ten second period there's basically like hundreds of transactions being sent. Then eventually Mr. Blue wins and he wins the option and gets the movie opportunity. Now there's two problems of this approach. One is wasteful of gas.\n00:03:16.214 - 00:03:49.150, Speaker A: You have one transaction that's successful followed by a list of failed transactions and that's a waste of block space. And two, it's unrestricted mev. You're taking the user's transaction, throwing it to the wolves and just hoping it gets to the other side. Okay? Because it's completely unrestricted what these bots can do. So a few years ago, a wonderful company called Flashbot, I wonder who they are. They came along and tried to solve the problem for Ethereum. We give the transaction directly to the Flashbots and they'll give it directly on to the block proposer.\n00:03:49.150 - 00:04:29.300, Speaker A: So then the MVV bots or the searchers, they can't find the opportunity and they can't extract value from the user's transaction. So stepping back a bit, what can we extract from this scenario? Well, we have to consider the proposer's ordering policy. In this case, they're picking the transactions based on the fee. Highest fee first, lowest fee at the bottom and then around 12 seconds to do this. As we saw with the priority gas option. What we need to consider as well is the communication channel. How does the block proposer learn about the transaction and how do the searchers find it as well? And finally, sometimes we forget about the poor little user, but we have to think about the user experience.\n00:04:29.300 - 00:05:00.778, Speaker A: How long does it take for a user to be informed that their transactions confirmed and how it was executed? Because at the end of the day they want to execute and participate on this network. So that's Ethereum in that in a nutshell. We took some cool ideas from that. So now let's move on to layer two and roll up land. And in roll up land there's typically three actors the honest user, the sequencer and the executor. Now for once we're not going to talk about the executor. They have very little to do of mev.\n00:05:00.778 - 00:05:28.230, Speaker A: We care about the sequencer, who's basically the proposer and gets to decide those lists of transactions and their ordering. They have everything to do of mev and the lifecycle is pretty similar. Alice gives their transactions to the sequencer. The sequencer will have a list of pending transactions. They'll run some ordering policy. They'll decide the ordering of the transactions. Then they'll post that onto Ethereum, into an inbox and then picked up by the breach.\n00:05:28.230 - 00:06:10.866, Speaker A: Now in this scenario we have this direct communication channel. There's no gasa protocol. Alice can communicate directly with the sequencer. And what we need to be concerned about is when does Alice get a response from the sequencer and what type of response do they get? And that's really going to depend on how the sequencer decides the order of these transactions, then of course what we care about is the sequencer's ordering policy.",
                        "previous_answer": "Layer 2 sequencers operate by receiving transactions directly from users and arranging them in a specific sequence. This sequence is ideally established on a first-come, first-served basis, which is guaranteed if the majority of the sequencers are honest. The sequencer then provides a real-time feed, known as a sequencer feed, to which anyone can subscribe. After collecting a substantial number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and records them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some cases, sequencers may allow an open market of searchers to maximize extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Furthermore, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Layer 2 sequencers function by directly receiving transactions from users. They then arrange these transactions in a specific order, which is typically determined by a first-come, first-served policy, assuming the majority of sequencers are honest. The sequencer then provides a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and records them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may allow an open market of searchers to maximize extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order."
                    }
                ]
            }
        ]
    },
    {
        "event_type": "function_call end",
        "tool_output": "\nLayer 2 sequencers function by directly receiving transactions from users. They then arrange these transactions in a specific order, which is typically determined by a first-come, first-served policy, assuming the majority of sequencers are honest. The sequencer then provides a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and records them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may allow an open market of searchers to maximize extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order.\n\nFetched based on the following sources: \n[Title]: Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923, [Channel name]: Flashbots, [Video Link]: https://www.youtube.com/watch?v=dZfSbtBOGAQ, [Published date]: 2023-09-04\n[Title]: MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry, [Channel name]: ETHGlobal, [Video Link]: https://www.youtube.com/watch?v=kb8nY2-Osjc, [Published date]: 2023-03-28\n[Title]: Credible Decentralized Exchange Design via Verifiable Sequencing Rules, [Authors]: Matheus V. X. Ferreira, David C. Parkes, [Link]: http://arxiv.org/pdf/2209.15569v2, [Release date]: 2022-09-30\n[Title]: L2 sequencing and MEV - Ed Felten (Arbitrum), [Channel name]: Flashbots, [Video Link]: https://www.youtube.com/watch?v=qxml80TparY, [Published date]: 2022-05-22\n"
    },
    {
        "event_type": "llm start",
        "retrieved_context": null,
        "previous_answer": null
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I can answer without using any more tools.\nAnswer: Layer 2 sequencers work by directly receiving transactions from users and arranging these transactions in a specific order, typically based on a first-come, first-served policy. This ordering is provided as a real-time feed, known as a sequencer feed, which anyone can subscribe to. After accumulating a significant number of unrecorded transactions, the sequencer compresses them using a general-purpose compression algorithm and records them onto the Layer 1 chain, creating a permanent transaction record. The sequencer can be a centralized entity or a distributed system, and the selection of sequencers is a permissioned process that involves community discussion. In some instances, sequencers may allow an open market of searchers to maximize extractable value (MEV) instead of doing it themselves, as this could potentially lead to higher profits. Additionally, the use of public key encryption of the transaction and threshold decryption by the sequencer committee members can ensure that the contents of the transactions remain unknown until it's too late to alter the order (\"Transaction Ordering: The View from L2 (and L3) - Ed Felten | MEV-SBC \u201923\", \"MEVeconomics in L2 - A Closer Look at the Sequencer's Role and MEV - Patrick McCorry\", \"Credible Decentralized Exchange Design via Verifiable Sequencing Rules\", \"L2 sequencing and MEV - Ed Felten (Arbitrum)\").",
        "subjective grade from 1 to 10": ""
    },
    {
        "event_type": "llm start",
        "model_params": {
            "model": "gpt-4-0613",
            "temperature": 0,
            "max_tokens": null,
            "additional_kwargs": {},
            "max_retries": 10,
            "api_type": "open_ai",
            "api_base": "https://api.openai.com/v1",
            "api_version": "",
            "class_type": "openai",
            "class_name": "openai_llm"
        },
        "user_raw_input": "Do an exhaustive breakdown of the MEV supply chain",
        "LLM_input": "Do an exhaustive breakdown of the MEV supply chain\n\nTo determine if you should take the action to use the query engine, use its description detailed below. Use the query engine rather than not and do not rely on your prior knowledge.\nThe query engine tool has access to research papers and \nYouTube videos about the following content: Maximal Extractable Value (MEV); loss-versus-rebalancing (LVR); blockchain intents; \nthe Single Unifying Auction for Value Expression (SUAVE); sequencers; transaction ordering, L1s, L2s, transaction ordering, order flow auctions, auction theory, mechanism design.\n\n"
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I need to use a tool to help me answer the question.\nAction: query_engine_tool\nAction Input: {\"input\": \"MEV supply chain\"}",
        "additional_content": [
            {
                "function_call": [
                    {
                        "event_type": "templating start",
                        "instructions": "system: You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\nuser: Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_str": "total_pages: 11\nsource: 5\ndocument_type: research_paper\ntitle: Multi-block MEV\nauthors: Johannes Rude Jensen, Victor von Wachter, Omri Ross\npdf_link: http://arxiv.org/pdf/2303.04430v2\nrelease_date: 2023-03-08\n\nAbbreviated paper title\n5\nContinuous strategies are likely the favorable option, as they may fail without\nsignificant cost to the builder.\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThat's what's known as defense in depth and like layered defense to make attacks more expensive. If the public can tell, on the other hand, if it produces objective evidence, or if it produces evidence at all, you can build protocols that are either subjectively or objectively secure without relying on intel and without relying on SGX or trust, because you have this kind of additional falsifiability when it fails. So anyway, that's all I wanted to say at Flashbots, we're building both protocols.\n00:34:09.800 - 00:34:48.200, Speaker A: I would love to talk to you guys about where SGX fits in this ecosystem and where it doesn't, and let's chat more about it. Sorry for the technical issues earlier. I'm going to go dig into my Linux configs and make sure doesn't happen during the rest of the day. Thanks everyone for listening. And probably no time for questions, but feel free to ask them on Twitter, I think. Great, thank you. So there seems to be a lively discussion on the Eastglobal TV live chats, so if any of the speakers here have time, can also hop onto Eastglobal TV to interact with our global audience.\n00:34:48.200 - 00:35:31.476, Speaker A: Next up we have Alejo. Alejo, would you mind directly sharing screen? Perfect. Yes. Can you see my presentation now? Yes. Awesome, thank you. All right, so I'm going to talk about near future, which is 1559 upcoming. This is joint work with Christophe from Nethermind.\n00:35:31.476 - 00:36:27.150, Speaker A: And let me also say that this is very much a work in progress. Hopefully we'll have a write up of this soon. So perhaps some of you already thought out the question of MEB in 1559 and the first conclusion one might arrive to, there's nothing new under the sun here, we're going to burn the base fee, but there's no new mev extraction opportunity that's raining on us. So it's nothing really strange, but still we think there's some. So this is perhaps correct at order zero, but we still think there's some interesting things to be said about one and mev. So hopefully this serves to spark some further thought on this. So I only have 15 minutes and I already spend my talk index time, so it's going to be three surprise topics coming.\n00:36:27.150 - 00:37:16.984, Speaker A: First one is on minor economics, so naturally there's the question of okay, now that. They will have less revenue after 1559? Will they be incentivized to extract more mev in particular in nefarious way like Charlie pointed out before, is there going to be more reorgs and time bandits and so so, okay, this is an interesting question. First we realize we need some better understanding of minor economics actually. So the first question we ask ourselves is will miners switch lanes? Meaning will they be incentivized to switch to other blockchains after the reward cut? So we built a super simple model. I'm going to run through it to try to assess this question. This is an equation for profit. So the profit is just reward minus cost.\n00:37:16.984 - 00:38:16.872, Speaker A: There are some equations for hash rate. There's a total hash rate that's distributed between ethereum and x other GPU based blockchains after one five nine. This looks again distributed between post one five nine ethereum and some other blockchains. Assuming the total hash rate stays the same and there's a revenue drop due to the burning of fees in 1559 parameterized by some parameter gamma, then we state some equilibrium here. We say like the profit per hash rate is the same in ethereum and in other blockchains. Otherwise it would be a non equilibrium situation where miners would be switching and another equilibrium holds after the London fork. So we plug in some algebra here, we do some rearranging and we can find an expression for the actual fraction of hash rate that stays in ethereum after the London fork.\n00:38:16.872 - 00:39:07.576, Speaker A: Let me say that this is of course a very naive model. It doesn't consider in particular the cost of switching or the fact that the revenues are denominated in different currencies and miners might have more or less trust in the different currencies. So again, this is a very coarse first approach to this question. But we can do some nice colored graphs here. You see the fraction of miners staying in ethereum after the forecast function of the revenue drop after 1559. And we have another parameter which is again the size of these other blockchains in terms of revenue for miners in proportion to ethereum. So we can plug in some numbers here with actual data from the different revenues, the sizes of other blockchains.\n00:39:07.576 - 00:39:30.064, Speaker A: And we actually find that we're in the red cross there in the graph. It looks like most miners will stay in ethereum. According to this prediction it's zero point 98. So 98% of miners would stay. The equilibrium would be at 98% of miners staying. Of course these numbers are super tricky. They change by orders of magnitude.\n00:39:30.064 - 00:40:17.220, Speaker A: So you're all aware of how gas prices change. So take this with a grain of salt. So okay, this is for again the minor economics and switching to other chains. How does mev play a part here? So first question that comes to mind is how much extra mev miners will have to extract to match profits before 1559. And this is a whooping 150% more. So they would have to go through a lot of effort to try to increase their extraction widely to actually compensate for the drop in minor revenue. But of course this might be like prohibitive or it wouldn't be easy, of course, to try to come up with strategies to do that.\n00:40:17.220 - 00:41:15.256, Speaker A: So the next question is try to integrate the two things and compare the extraction with the switching so we can ask how much extra mev would need to be extracted to match their revenue hike from switching chains. Okay, so to achieve this equilibrium, in achieving this equilibrium, they would increase their profits a little bit. What would that be compared to if instead of switching, they would extract more mev, and this is a more reasonable number of 22%. Of course, these numbers don't tell us what miners will actually do. It's not enough to think about these numbers in terms and try to come to conclusions, but it gives us a framework to compare the two different things that are switching to other chains and extracting more mev.\n\ndocument_type: youtube_video\ntitle: The threat of MEV centralization: an anatomy of the transaction supply chain - Hasu (Flashbots)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=GmBqoBr6yl4\nrelease_date: 2022-05-22\n\n00:00:08.570 - 00:01:24.370, Speaker A: So welcome to my second talk, the threat of mev centralization an anatomy of the transaction supply chain. This is already my second talk of the day. If you didn't catch the first one, my name is Hasu, strategy lead at flashboards, also teach advisor at Lido and I've been thinking and writing about blockspace, market design and MUV for a couple years already. So back in the day when designing your block space market was easy and mev didn't exist and the only threats we had to worry about were things like selfish mining attacks and so on. In this talk I want to build on a new mental model that Stefan of Flashpoint introduced in his first talk of the day and we call it the mev or transaction supply chain. This supply chain includes all of the major parties that are involved in starting from the user getting a transaction included on chain. So it starts with the user.\n00:01:24.370 - 00:01:51.920, Speaker A: The user has an intent to transact and they express this intent to the wallet. The wallet turns it into a transaction and then some transactions get picked up or manipulated by searchers and turned into bundles, some don't. And then a builder will select the transactions to be included in the next block, send this block to a minor validator who will attest to.\n00:01:54.480 - 00:01:55.084, Speaker B: It.\n00:01:55.202 - 00:03:09.200, Speaker A: So we often talk about the guarantees that we want to get from public blockchains and one major one is censorship resistance and the other one you could say is market power for users. And where do these guarantees really come from? Almost all of them come from the competition between miners and validators the competition that they have among each other. Right, so there are some cool cryptographic properties of blockchains, but a lot of the guarantees actually come from how the economics of the system work. That there is not one party in control who selects what transactions go into a block, but there are several parties who all take turns making blocks, and they are in competition with each other. So that's how you end up with users to get their transactions mined at the cheapest possible price, which is the cost of production of block space. So what is the marginal cost of including another transaction? As a miner, it is only the increased uncle rate and now that we have ERP one peppermint nine, it's also slightly higher. It's also the cost of increasing the base fee in the next block.\n00:03:09.200 - 00:04:49.010, Speaker A: So that is a major point and censorship resistance. Whenever you have multiple parties competing to to include a user's transaction there is really not much point in censoring because why like if you're the first miner in turn can you really say I'm not going to include this user at a price of ten way or because I don't like that they user of sushi swap instead of uniswap? No, you cannot do that because then the next miner will just include you and the miner who didn't will slowly go out of business. So what is the scenario that we are concerned about? This is also what Steph talked about in this talk very briefly. It is what we call mev Dystopia, and that is this entire supply chain that you saw collapsing into a single vertically integrated party. And when you have this outcome, then what will happen is that all blocks on public blockchains will be made by this one party and that destroys all of the guarantees that we wanted and that we established in the previous slide. So no competition, strong vertical integration means no competition between suppliers in the different stages of the supply chain and it means worse guarantees for users. It means a loss of censorship, resistance and a monopoly that can charge arbitrarily higher fees from users because they do not have to be worried about another block builder coming in and including the user at a cheaper price.\n00:04:49.010 - 00:05:46.256, Speaker A: So what we are concerned about today, what are the paths from here, which is today we say we have a reasonably flat supply chain to this Dystopia that we are trying to avoid at oil cost. That's the topic of today's talk. We will cover both scenarios where it has been already avoided and where we are looking right now and we are concerned about increasing vertical integration in the supply chain. Before we go into the risks, I want to dive into one particular part of this to make it very clear. The builder and the Validator miner are two separate parties and they have been two separate parties for a long time. So the validators and miners have for years not been the party who selects what transactions go into a block. So the role of the builder has for many years now been played by the mining pool.\n00:05:46.256 - 00:07:10.220, Speaker A: The mining pool selects the transactions and puts them into a block and then they forward only the hash of the block header to the miner Validators, who then attest to it with a proof of work or proof of stake, giving it economic weights in the pork chase rule. And that is what allows all of the nodes in the network to come to consensus, right? It's not enough to just have a block that has transactions. All of the other nodes need to be able to see one block that has a lot of economic weight, so they know exactly which of many infinite forks they need to follow and stay in sync with the network. So this separation between the builder and the builder and especially the focus on the builder, that will be a topic of today's talk. So we already touched on this in my previous talk, but building used to be very easy going back to this slide. So back in the day when all a mining pool had to do was have a decent view of the mempool, they had to see all of the transactions with a good latency that the users propagate in the network. So they always have all of the inputs in order to just run a very simple algorithm which is sort these transactions by their gas price or by the price per gas and then select the highest paying ones that they can.\n00:07:10.220 - 00:08:23.684, Speaker A: Right? So if there's like 10 million gas available, then they select the highest 10 million gas of transactions. But mev makes building hard. Why? Because extracting mev is not just a matter of running a simple algorithm like the miner is used to, it is a matter of identifying what the mev transactions are then making these mev transactions, possibly maintaining inventory of coins and managing risk and so on. This is all skills that has nothing to do with what a builder or a validator or a miner used to be good at in the past. This is all skills from the domain of trading.\n\ndocument_type: youtube_video\ntitle: It\u2019s My MEV and I Want It Now - Tom Schmidt\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=mHsU2hSNNcM\nrelease_date: 2023-03-28\n\nSo ultimately, I think when we sort of look ahead, the big takeaway here is the people further up the supply chain have more power and so naturally everybody is trying to move further up. But it's tricky given some interests are sort of already entrenched and so everybody's acting really rationally. Nobody is trying to be malicious here and so we can't just really finger wag and say, hey, you should not be privatizing order flow, you should not be self extracting. Instead, we should make it economically reasonable and rational for them to give up their order flow and participate in an open auction.\n00:17:01.070 - 00:17:38.026, Speaker A: And I think that's where I'd like to see more sort of discussion happening versus just sort of assuming that it will happen. I think things like Mev share are a great step and obviously making more accessible for people to participate in an open auction. But I think ultimately the question we need to make sure we're answering is whether or not it makes sense for these different actors to open up order flow to an open auction versus, again, sort of keeping it private. So that was it. Kind of quick. Happy to answer any questions if people have them. Actually, I declare the tabs.\n00:17:38.026 -"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "tool_output": "Context information is below.\n---------------------\ntotal_pages: 11\nsource: 5\ndocument_type: research_paper\ntitle: Multi-block MEV\nauthors: Johannes Rude Jensen, Victor von Wachter, Omri Ross\npdf_link: http://arxiv.org/pdf/2303.04430v2\nrelease_date: 2023-03-08\n\nAbbreviated paper title\n5\nContinuous strategies are likely the favorable option, as they may fail without\nsignificant cost to the builder.\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThat's what's known as defense in depth and like layered defense to make attacks more expensive. If the public can tell, on the other hand, if it produces objective evidence, or if it produces evidence at all, you can build protocols that are either subjectively or objectively secure without relying on intel and without relying on SGX or trust, because you have this kind of additional falsifiability when it fails. So anyway, that's all I wanted to say at Flashbots, we're building both protocols.\n00:34:09.800 - 00:34:48.200, Speaker A: I would love to talk to you guys about where SGX fits in this ecosystem and where it doesn't, and let's chat more about it. Sorry for the technical issues earlier. I'm going to go dig into my Linux configs and make sure doesn't happen during the rest of the day. Thanks everyone for listening. And probably no time for questions, but feel free to ask them on Twitter, I think. Great, thank you. So there seems to be a lively discussion on the Eastglobal TV live chats, so if any of the speakers here have time, can also hop onto Eastglobal TV to interact with our global audience.\n00:34:48.200 - 00:35:31.476, Speaker A: Next up we have Alejo. Alejo, would you mind directly sharing screen? Perfect. Yes. Can you see my presentation now? Yes. Awesome, thank you. All right, so I'm going to talk about near future, which is 1559 upcoming. This is joint work with Christophe from Nethermind.\n00:35:31.476 - 00:36:27.150, Speaker A: And let me also say that this is very much a work in progress. Hopefully we'll have a write up of this soon. So perhaps some of you already thought out the question of MEB in 1559 and the first conclusion one might arrive to, there's nothing new under the sun here, we're going to burn the base fee, but there's no new mev extraction opportunity that's raining on us. So it's nothing really strange, but still we think there's some. So this is perhaps correct at order zero, but we still think there's some interesting things to be said about one and mev. So hopefully this serves to spark some further thought on this. So I only have 15 minutes and I already spend my talk index time, so it's going to be three surprise topics coming.\n00:36:27.150 - 00:37:16.984, Speaker A: First one is on minor economics, so naturally there's the question of okay, now that. They will have less revenue after 1559? Will they be incentivized to extract more mev in particular in nefarious way like Charlie pointed out before, is there going to be more reorgs and time bandits and so so, okay, this is an interesting question. First we realize we need some better understanding of minor economics actually. So the first question we ask ourselves is will miners switch lanes? Meaning will they be incentivized to switch to other blockchains after the reward cut? So we built a super simple model. I'm going to run through it to try to assess this question. This is an equation for profit. So the profit is just reward minus cost.\n00:37:16.984 - 00:38:16.872, Speaker A: There are some equations for hash rate. There's a total hash rate that's distributed between ethereum and x other GPU based blockchains after one five nine. This looks again distributed between post one five nine ethereum and some other blockchains. Assuming the total hash rate stays the same and there's a revenue drop due to the burning of fees in 1559 parameterized by some parameter gamma, then we state some equilibrium here. We say like the profit per hash rate is the same in ethereum and in other blockchains. Otherwise it would be a non equilibrium situation where miners would be switching and another equilibrium holds after the London fork. So we plug in some algebra here, we do some rearranging and we can find an expression for the actual fraction of hash rate that stays in ethereum after the London fork.\n00:38:16.872 - 00:39:07.576, Speaker A: Let me say that this is of course a very naive model. It doesn't consider in particular the cost of switching or the fact that the revenues are denominated in different currencies and miners might have more or less trust in the different currencies. So again, this is a very coarse first approach to this question. But we can do some nice colored graphs here. You see the fraction of miners staying in ethereum after the forecast function of the revenue drop after 1559. And we have another parameter which is again the size of these other blockchains in terms of revenue for miners in proportion to ethereum. So we can plug in some numbers here with actual data from the different revenues, the sizes of other blockchains.\n00:39:07.576 - 00:39:30.064, Speaker A: And we actually find that we're in the red cross there in the graph. It looks like most miners will stay in ethereum. According to this prediction it's zero point 98. So 98% of miners would stay. The equilibrium would be at 98% of miners staying. Of course these numbers are super tricky. They change by orders of magnitude.\n00:39:30.064 - 00:40:17.220, Speaker A: So you're all aware of how gas prices change. So take this with a grain of salt. So okay, this is for again the minor economics and switching to other chains. How does mev play a part here? So first question that comes to mind is how much extra mev miners will have to extract to match profits before 1559. And this is a whooping 150% more. So they would have to go through a lot of effort to try to increase their extraction widely to actually compensate for the drop in minor revenue. But of course this might be like prohibitive or it wouldn't be easy, of course, to try to come up with strategies to do that.\n00:40:17.220 - 00:41:15.256, Speaker A: So the next question is try to integrate the two things and compare the extraction with the switching so we can ask how much extra mev would need to be extracted to match their revenue hike from switching chains. Okay, so to achieve this equilibrium, in achieving this equilibrium, they would increase their profits a little bit. What would that be compared to if instead of switching, they would extract more mev, and this is a more reasonable number of 22%. Of course, these numbers don't tell us what miners will actually do. It's not enough to think about these numbers in terms and try to come to conclusions, but it gives us a framework to compare the two different things that are switching to other chains and extracting more mev.\n\ndocument_type: youtube_video\ntitle: The threat of MEV centralization: an anatomy of the transaction supply chain - Hasu (Flashbots)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=GmBqoBr6yl4\nrelease_date: 2022-05-22\n\n00:00:08.570 - 00:01:24.370, Speaker A: So welcome to my second talk, the threat of mev centralization an anatomy of the transaction supply chain. This is already my second talk of the day. If you didn't catch the first one, my name is Hasu, strategy lead at flashboards, also teach advisor at Lido and I've been thinking and writing about blockspace, market design and MUV for a couple years already. So back in the day when designing your block space market was easy and mev didn't exist and the only threats we had to worry about were things like selfish mining attacks and so on. In this talk I want to build on a new mental model that Stefan of Flashpoint introduced in his first talk of the day and we call it the mev or transaction supply chain. This supply chain includes all of the major parties that are involved in starting from the user getting a transaction included on chain. So it starts with the user.\n00:01:24.370 - 00:01:51.920, Speaker A: The user has an intent to transact and they express this intent to the wallet. The wallet turns it into a transaction and then some transactions get picked up or manipulated by searchers and turned into bundles, some don't. And then a builder will select the transactions to be included in the next block, send this block to a minor validator who will attest to.\n00:01:54.480 - 00:01:55.084, Speaker B: It.\n00:01:55.202 - 00:03:09.200, Speaker A: So we often talk about the guarantees that we want to get from public blockchains and one major one is censorship resistance and the other one you could say is market power for users. And where do these guarantees really come from? Almost all of them come from the competition between miners and validators the competition that they have among each other. Right, so there are some cool cryptographic properties of blockchains, but a lot of the guarantees actually come from how the economics of the system work. That there is not one party in control who selects what transactions go into a block, but there are several parties who all take turns making blocks, and they are in competition with each other. So that's how you end up with users to get their transactions mined at the cheapest possible price, which is the cost of production of block space. So what is the marginal cost of including another transaction? As a miner, it is only the increased uncle rate and now that we have ERP one peppermint nine, it's also slightly higher. It's also the cost of increasing the base fee in the next block.\n00:03:09.200 - 00:04:49.010, Speaker A: So that is a major point and censorship resistance. Whenever you have multiple parties competing to to include a user's transaction there is really not much point in censoring because why like if you're the first miner in turn can you really say I'm not going to include this user at a price of ten way or because I don't like that they user of sushi swap instead of uniswap? No, you cannot do that because then the next miner will just include you and the miner who didn't will slowly go out of business. So what is the scenario that we are concerned about? This is also what Steph talked about in this talk very briefly. It is what we call mev Dystopia, and that is this entire supply chain that you saw collapsing into a single vertically integrated party. And when you have this outcome, then what will happen is that all blocks on public blockchains will be made by this one party and that destroys all of the guarantees that we wanted and that we established in the previous slide. So no competition, strong vertical integration means no competition between suppliers in the different stages of the supply chain and it means worse guarantees for users. It means a loss of censorship, resistance and a monopoly that can charge arbitrarily higher fees from users because they do not have to be worried about another block builder coming in and including the user at a cheaper price.\n00:04:49.010 - 00:05:46.256, Speaker A: So what we are concerned about today, what are the paths from here, which is today we say we have a reasonably flat supply chain to this Dystopia that we are trying to avoid at oil cost. That's the topic of today's talk. We will cover both scenarios where it has been already avoided and where we are looking right now and we are concerned about increasing vertical integration in the supply chain. Before we go into the risks, I want to dive into one particular part of this to make it very clear. The builder and the Validator miner are two separate parties and they have been two separate parties for a long time. So the validators and miners have for years not been the party who selects what transactions go into a block. So the role of the builder has for many years now been played by the mining pool.\n00:05:46.256 - 00:07:10.220, Speaker A: The mining pool selects the transactions and puts them into a block and then they forward only the hash of the block header to the miner Validators, who then attest to it with a proof of work or proof of stake, giving it economic weights in the pork chase rule. And that is what allows all of the nodes in the network to come to consensus, right? It's not enough to just have a block that has transactions. All of the other nodes need to be able to see one block that has a lot of economic weight, so they know exactly which of many infinite forks they need to follow and stay in sync with the network. So this separation between the builder and the builder and especially the focus on the builder, that will be a topic of today's talk. So we already touched on this in my previous talk, but building used to be very easy going back to this slide. So back in the day when all a mining pool had to do was have a decent view of the mempool, they had to see all of the transactions with a good latency that the users propagate in the network. So they always have all of the inputs in order to just run a very simple algorithm which is sort these transactions by their gas price or by the price per gas and then select the highest paying ones that they can.\n00:07:10.220 - 00:08:23.684, Speaker A: Right? So if there's like 10 million gas available, then they select the highest 10 million gas of transactions. But mev makes building hard. Why? Because extracting mev is not just a matter of running a simple algorithm like the miner is used to, it is a matter of identifying what the mev transactions are then making these mev transactions, possibly maintaining inventory of coins and managing risk and so on. This is all skills that has nothing to do with what a builder or a validator or a miner used to be good at in the past. This is all skills from the domain of trading.\n\ndocument_type: youtube_video\ntitle: It\u2019s My MEV and I Want It Now - Tom Schmidt\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=mHsU2hSNNcM\nrelease_date: 2023-03-28\n\nSo ultimately, I think when we sort of look ahead, the big takeaway here is the people further up the supply chain have more power and so naturally everybody is trying to move further up. But it's tricky given some interests are sort of already entrenched and so everybody's acting really rationally. Nobody is trying to be malicious here and so we can't just really finger wag and say, hey, you should not be privatizing order flow, you should not be self extracting. Instead, we should make it economically reasonable and rational for them to give up their order flow and participate in an open auction.\n00:17:01.070 - 00:17:38.026, Speaker A: And I think that's where I'd like to see more sort of discussion happening versus just sort of assuming that it will happen. I think things like Mev share are a great step and obviously making more accessible for people to participate in an open auction. But I think ultimately the question we need to make sure we're answering is whether or not it makes sense for these different actors to open up order flow to an open auction versus, again, sort of keeping it private. So that was it. Kind of quick. Happy to answer any questions if people have them. Actually, I declare the tabs.\n00:17:38.026 -\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Always provide an exhaustive answer to the question, unless told otherwise in the question itself.\nDirectly quote the sources of your knowledge in the same sentence in parentheses. If several files are matched across several years of release dates, favor most recent content. Now answer the question: Do an exhaustive breakdown of the MEV supply chain\nAnswer: "
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The MEV or transaction supply chain is a concept that involves all the major parties that contribute to getting a user's transaction included on-chain. It begins with the user who has an intent to transact. This intent is then expressed to the wallet, which turns it into a transaction. Some transactions get picked up or manipulated by searchers and turned into bundles, while others don't. \n\nThe next step involves a builder who selects the transactions to be included in the next block. This block is then sent to a miner or validator who attests to it. This supply chain is crucial in providing guarantees such as censorship resistance and market power for users. These guarantees largely come from the competition between miners and validators. \n\nHowever, there are concerns about the MEV supply chain collapsing into a single vertically integrated party, which is referred to as MEV Dystopia. This scenario would mean that all blocks on public blockchains would be made by this one party, leading to a loss of censorship resistance and a monopoly that can charge arbitrarily higher fees from users. \n\nThe builder and the validator or miner are two separate parties in this supply chain. For many years, the role of the builder has been played by the mining pool, which selects the transactions and puts them into a block. The miner or validators then attest to it with a proof of work or proof of stake, giving it economic weight in the fork choice rule. \n\nThe process of building used to be simple when all a mining pool had to do was have a decent view of the mempool and select the highest paying transactions. However, the introduction of MEV has made building more complex as it involves identifying MEV transactions, making these transactions, possibly maintaining inventory of coins, and managing risk. \n\nIn the future, it is expected that everyone will try to move further up the supply chain as those further up have more power. However, this is tricky given some interests are already entrenched. The goal is to make it economically reasonable and rational for actors to give up their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "all skills from the domain of trading.\n\ndocument_type: youtube_video\ntitle: It\u2019s My MEV and I Want It Now - Tom Schmidt\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=mHsU2hSNNcM\nrelease_date: 2023-03-28\n\nSo ultimately, I think when we sort of look ahead, the big takeaway here is the people further up the supply chain have more power and so naturally everybody is trying to move further up. But it's tricky given some interests are sort of already entrenched and so everybody's acting really rationally. Nobody is trying to be malicious here and so we can't just really finger wag and say, hey, you should not be privatizing order flow, you should not be self extracting. Instead, we should make it economically reasonable and rational for them to give up their order flow and participate in an open auction.\n00:17:01.070 - 00:17:38.026, Speaker A: And I think that's where I'd like to see more sort of discussion happening versus just sort of assuming that it will happen. I think things like Mev share are a great step and obviously making more accessible for people to participate in an open auction. But I think ultimately the question we need to make sure we're answering is whether or not it makes sense for these different actors to open up order flow to an open auction versus, again, sort of keeping it private. So that was it. Kind of quick. Happy to answer any questions if people have them. Actually, I declare the tabs.\n00:17:38.026 - 00:17:57.986, Speaker A: I don't get echo, but happy to answer questions. We are running a bit on the time. That's right. I wish we're still behind on time. So we are just going to move over to Quintess. But thank you. The viewers who are confused by Truin's equations appreciated the break on this one and we will turn it over to Quintess.\n00:17:57.986 - 00:18:00.020, Speaker A: Now, who's going to talk about cat, I think.\n\ndocument_type: youtube_video\ntitle: Flashbots Research Workshop #7: Cross-domain MEV AMA\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=uR0Z0nWXeFU\nrelease_date: 2021-12-23\n\nAnd you have that in traditional markets as well, right, where market makers have a global fair price of a particular asset and that they trade against on many venues. But establishing that global fair price is their secret sauce and they have potentially different ways of arriving that global fair price. And that global fair price can slightly differ from others global fair price. And that's pretty interesting. So it's something that, again, recreates a lot of the dynamics that we see in traditional markets. That's also a big takeaway from this paper, I would say, is that this world looks maybe more like traditional markets and traditional markets, market making and cross chain arbitrage or like structural ARP that would take place in such markets.\n00:31:58.110 - 00:32:19.510, Speaker A: I know there's questions. I'm going to address them. Finally, we end with open questions. And so these are I kind of mentioned a bunch of open questions already. I would say to some extent all of these negative externalities are open questions. So we look into how to best define the action space. This was added by Phil.\n00:32:19.510 - 00:33:27.622, Speaker A: There's a lot of modeling that needs to be done. There's a lot of complexity that we swept under the rug to some extent and there's definitely more to be done there. I would say this joins a little article on the formalization of mev and the limitations to the definition that we introduced above. So there's a lot of work that needs to be done. There maybe other forms of mev than cross domain arbitrage. People are talking about cross domain governance, voting, cross domain lending, and there might be other use cases there. So maybe thinking about those more and how ordering, having ordering power on many domains at once, when these interactions take place happen, then what does a protocol for sequencer Kazoon look like and what are its desirable properties? This is kind of taking the flashbacks approach of if sequencers colluding with one another is inevitable, at least if you follow the financial incentive, then what would a protocol for this to happen look like? And then maybe from there we can also deduce some things about how the market will evolve and then we can mitigate some of the negative externalities that would arise.\n00:33:27.622 - 00:34:15.260, Speaker A: It's to some extent the approach we've taken at Foshbots for many things that. We do. And so we think about that a lot. How can we identify and quantify cross genome EV extraction is another important one. So before on Ethereum, there's to some extent full transparency, there's some opacity, but you can measure and quantify mev extraction. And people at Flashbots working on mevinspec, Pi and the Explore dashboard, et cetera, do a great job of doing that here. If you think back to the thing that I mentioned before of a trader seizing an opportunity by submitting two transactions on each domain, it's very hard to link those transactions back to a single strategy that one has.\n00:34:15.260 - 00:35:20.070, Speaker A: And so you need to do a lot more guessing work and to some extent use heuristics more and also maybe do some D anonymization, right? Or at least some clustering of activity such that you can identify a player that exists on multiple domains at once. This is much more complicated. And what worries us there is one of our goal is to illuminate the dark forest or bring more transparency and clarity to what's happening in these markets. So this introduces more opacity. So it is something that is on our mind and we're thinking a lot about. And finally, we want to look into distributed and parallel programming literature. So there is a appendix, who's a co author of this paper as well, who draws a parallel between this notion of cross domain mev with not only do we potentially want to look into more traditional can you still hear me?\n00:35:20.520 - 00:35:22.070, Speaker B: Yeah. Now you're back.\n00:35:24.140 - 00:36:09.988, Speaker A: We want to look at existing theory, basically, and existing academic literature, not only in financial theory but also in computer science that have looked at these problems that to some extent it is like multi threaded computing a little bit. There are differences there. Part of the differences are surfaced by Tarun. But it's very interesting to make this parallel. And we're hoping that maybe someone who has a lot of expertise there might and understands DeFi a little bit might be interested in picking that up and doing more there. All right, and five questions, by the way, Phil and Alejo, if you have anything to add. I kind of talked for a while right now.\n00:36:09.988 - 00:36:24.250, Speaker A: I hope that was helpful to people. Okay, just a curly fly. I'm quite okay with builders being very centralized, as long as validators are not. But even PBS on each chain doesn't guarantee that if cross chain mev is much higher.\n00:36:27.360 - 00:36:30.940, Speaker B: That was a clarification on the previous discussion.\n00:36:32.960 - 00:37:25.804, Speaker A: More practical question about definitions again from is this still Francesco or is this for someone else? Okay. My intuition of what we care about tends towards highest bid in a full block auction rather than highest amount that can be extracted. Example of the difference is Justin Sun potentially being liquidated for one block and being willing to buy that block for up to the amount he would lose, though there's no participant who can extract that much value from the block. Justin sun may bid that much, but because he is extracting that much from the block difference for a single player, seems to me something like balance change review best block versus best delta between Balance Change Review best blog versus Balance change with a block made by others. I don't know how much this is relevant in practice. I feel like it might be irrelevant most of the time, definitions agree, but capture some important tail events. Okay, I didn't actually ask question.\n00:37:25.804 - 00:37:49.556, Speaker A: I guess it's just what you think about it. Yeah, it's hard for me to reason about your question. The first thing that comes up to mind is, again, Alejo's post on the formalization of mev and how you consider a player in that definition and potentially not considering a player at all. Phil or Alejo, if you've read the question, do you have I'm going to read it again, but if you have.\n00:37:49.578 - 00:38:43.584, Speaker B: Any I can answer.\n\ndocument_type: youtube_video\ntitle: Why your blockchain needs an MEV solution - Hasu (Flashbots)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=sYFuFLe9kp0\nrelease_date: 2022-05-22\n\n00:00:11.370 - 00:00:54.750, Speaker A: Quick word about me. I'm hatu I strategy at flashpot. I'm also a strategic advisor to Lido. I've been thinking and writing about blockspace market and what me be for over three years now. This time I've seen a lot of things and in this talk I want to share my insight why you can't get around if you're developing a blockchain. Whether it's layer one solution side chain or it's layer two, you want to build a block based market that allows for efficient NVX section. So to see why, let's look at what a blockchain actually is from first principle.\n00:00:54.750 - 00:02:35.294, Speaker A: Every blockchain has three components first, a virtual machine that is replicated on thousands if not millions of computers at the same time a consensus mechanism that keeps them all synchronized with each other and the consensus actually breaks down into two parts. First is sequencing, just selecting the next selection to go into the block. In other words, propose a new fork of the chain that has one we block at the tip and then at the station that is attaching to the fork containing that we block at the tip giving it economic weight in the fork just rule and that weight in present day Ethereum is a proof of work. After the merge it will be proof of stake but in layer two context, for example, it can also be the absence of a fraud proof together with data availability or the presence of a zero knowledge proof or validity proof. And the last component that we need is a block based market and that is the topic of today's talk. So why do you need a block space market? The question that almost everyone asks in the beginning the idea about crypto is why can't we have infinite block space? Why can't we just make blocks as large as we want and that will create a large amount of supply for users. So that's why I gave the definition of what a blockchain is.\n00:02:35.294 - 00:03:22.090, Speaker A: So every transaction is due, must be replayed and stored by oil node forever. I put an asterisk here because it's not that strictly true. But in either case, oil nodes have to store what the current state is, and that can also get very large, very quick. To maximize the number of participants in a decentralized system. We want to minimize the amount of resources that each of them has to consume in order to stay in sync with the network and also catch up with from scratch or from a last checkpoint. And that means you have to limit resource consumption to a point where people with consumer based hardware assets can also participate. So that's the reason why ethereum for example, has a gas limit among other reasons.\n00:03:22.090 - 00:04:30.562, Speaker A: So if you are starting from the assumption you need finite box space, then the question is how do you allocate this box space efficiently? And different blockchains have tried different market mechanisms for that first Converse Soap or various forms of batch auctions, as we will see. So to look back around to the first slide, a transaction starts in the block space market, then it's picked up by a sequencer, then it's tested to a block and eventually makes it into the next update of the VM space. Designing block space markets used to be really easy. So three years ago there were very few people thinking about block space market design. Why? Because it was not an interesting problem. The problem that people were thinking about at the time was gas price estimation. So that was the only problem that people were dealing in real time so much to get your transaction included in the time that you want.\n00:04:30.562 - 00:05:22.530, Speaker A: And because gas price estimation was really poor, it led to a lot of underlying overbidding. Transactions stuck or people were overpaying or prices were swinging really hard. And so some time ago we implemented ESC one five nine turning the market from fixed supply. So there used to be like a fixed limit on the amount of gas that can be in a block and then the price would float demand and ERT one five nine replaced that with a floating supply and the market calling you a fixed price. And this leads to the same amount of S being consumed, but it has way better us. And this update has been so good that it has since been fixed by Sapphire virtually every smart contract blockchain that exists. So this has been a large success.\n00:05:22.530 - 00:06:29.050, Speaker A: You can consider the original problem of gas estimation. Soil mev enters the room and as you can see, the model of Mev that is being extracted from public"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "all skills from the domain of trading.\n\ndocument_type: youtube_video\ntitle: It\u2019s My MEV and I Want It Now - Tom Schmidt\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=mHsU2hSNNcM\nrelease_date: 2023-03-28\n\nSo ultimately, I think when we sort of look ahead, the big takeaway here is the people further up the supply chain have more power and so naturally everybody is trying to move further up. But it's tricky given some interests are sort of already entrenched and so everybody's acting really rationally. Nobody is trying to be malicious here and so we can't just really finger wag and say, hey, you should not be privatizing order flow, you should not be self extracting. Instead, we should make it economically reasonable and rational for them to give up their order flow and participate in an open auction.\n00:17:01.070 - 00:17:38.026, Speaker A: And I think that's where I'd like to see more sort of discussion happening versus just sort of assuming that it will happen. I think things like Mev share are a great step and obviously making more accessible for people to participate in an open auction. But I think ultimately the question we need to make sure we're answering is whether or not it makes sense for these different actors to open up order flow to an open auction versus, again, sort of keeping it private. So that was it. Kind of quick. Happy to answer any questions if people have them. Actually, I declare the tabs.\n00:17:38.026 - 00:17:57.986, Speaker A: I don't get echo, but happy to answer questions. We are running a bit on the time. That's right. I wish we're still behind on time. So we are just going to move over to Quintess. But thank you. The viewers who are confused by Truin's equations appreciated the break on this one and we will turn it over to Quintess.\n00:17:57.986 - 00:18:00.020, Speaker A: Now, who's going to talk about cat, I think.\n\ndocument_type: youtube_video\ntitle: Flashbots Research Workshop #7: Cross-domain MEV AMA\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=uR0Z0nWXeFU\nrelease_date: 2021-12-23\n\nAnd you have that in traditional markets as well, right, where market makers have a global fair price of a particular asset and that they trade against on many venues. But establishing that global fair price is their secret sauce and they have potentially different ways of arriving that global fair price. And that global fair price can slightly differ from others global fair price. And that's pretty interesting. So it's something that, again, recreates a lot of the dynamics that we see in traditional markets. That's also a big takeaway from this paper, I would say, is that this world looks maybe more like traditional markets and traditional markets, market making and cross chain arbitrage or like structural ARP that would take place in such markets.\n00:31:58.110 - 00:32:19.510, Speaker A: I know there's questions. I'm going to address them. Finally, we end with open questions. And so these are I kind of mentioned a bunch of open questions already. I would say to some extent all of these negative externalities are open questions. So we look into how to best define the action space. This was added by Phil.\n00:32:19.510 - 00:33:27.622, Speaker A: There's a lot of modeling that needs to be done. There's a lot of complexity that we swept under the rug to some extent and there's definitely more to be done there. I would say this joins a little article on the formalization of mev and the limitations to the definition that we introduced above. So there's a lot of work that needs to be done. There maybe other forms of mev than cross domain arbitrage. People are talking about cross domain governance, voting, cross domain lending, and there might be other use cases there. So maybe thinking about those more and how ordering, having ordering power on many domains at once, when these interactions take place happen, then what does a protocol for sequencer Kazoon look like and what are its desirable properties? This is kind of taking the flashbacks approach of if sequencers colluding with one another is inevitable, at least if you follow the financial incentive, then what would a protocol for this to happen look like? And then maybe from there we can also deduce some things about how the market will evolve and then we can mitigate some of the negative externalities that would arise.\n00:33:27.622 - 00:34:15.260, Speaker A: It's to some extent the approach we've taken at Foshbots for many things that. We do. And so we think about that a lot. How can we identify and quantify cross genome EV extraction is another important one. So before on Ethereum, there's to some extent full transparency, there's some opacity, but you can measure and quantify mev extraction. And people at Flashbots working on mevinspec, Pi and the Explore dashboard, et cetera, do a great job of doing that here. If you think back to the thing that I mentioned before of a trader seizing an opportunity by submitting two transactions on each domain, it's very hard to link those transactions back to a single strategy that one has.\n00:34:15.260 - 00:35:20.070, Speaker A: And so you need to do a lot more guessing work and to some extent use heuristics more and also maybe do some D anonymization, right? Or at least some clustering of activity such that you can identify a player that exists on multiple domains at once. This is much more complicated. And what worries us there is one of our goal is to illuminate the dark forest or bring more transparency and clarity to what's happening in these markets. So this introduces more opacity. So it is something that is on our mind and we're thinking a lot about. And finally, we want to look into distributed and parallel programming literature. So there is a appendix, who's a co author of this paper as well, who draws a parallel between this notion of cross domain mev with not only do we potentially want to look into more traditional can you still hear me?\n00:35:20.520 - 00:35:22.070, Speaker B: Yeah. Now you're back.\n00:35:24.140 - 00:36:09.988, Speaker A: We want to look at existing theory, basically, and existing academic literature, not only in financial theory but also in computer science that have looked at these problems that to some extent it is like multi threaded computing a little bit. There are differences there. Part of the differences are surfaced by Tarun. But it's very interesting to make this parallel. And we're hoping that maybe someone who has a lot of expertise there might and understands DeFi a little bit might be interested in picking that up and doing more there. All right, and five questions, by the way, Phil and Alejo, if you have anything to add. I kind of talked for a while right now.\n00:36:09.988 - 00:36:24.250, Speaker A: I hope that was helpful to people. Okay, just a curly fly. I'm quite okay with builders being very centralized, as long as validators are not. But even PBS on each chain doesn't guarantee that if cross chain mev is much higher.\n00:36:27.360 - 00:36:30.940, Speaker B: That was a clarification on the previous discussion.\n00:36:32.960 - 00:37:25.804, Speaker A: More practical question about definitions again from is this still Francesco or is this for someone else? Okay. My intuition of what we care about tends towards highest bid in a full block auction rather than highest amount that can be extracted. Example of the difference is Justin Sun potentially being liquidated for one block and being willing to buy that block for up to the amount he would lose, though there's no participant who can extract that much value from the block. Justin sun may bid that much, but because he is extracting that much from the block difference for a single player, seems to me something like balance change review best block versus best delta between Balance Change Review best blog versus Balance change with a block made by others. I don't know how much this is relevant in practice. I feel like it might be irrelevant most of the time, definitions agree, but capture some important tail events. Okay, I didn't actually ask question.\n00:37:25.804 - 00:37:49.556, Speaker A: I guess it's just what you think about it. Yeah, it's hard for me to reason about your question. The first thing that comes up to mind is, again, Alejo's post on the formalization of mev and how you consider a player in that definition and potentially not considering a player at all. Phil or Alejo, if you've read the question, do you have I'm going to read it again, but if you have.\n00:37:49.578 - 00:38:43.584, Speaker B: Any I can answer.\n\ndocument_type: youtube_video\ntitle: Why your blockchain needs an MEV solution - Hasu (Flashbots)\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=sYFuFLe9kp0\nrelease_date: 2022-05-22\n\n00:00:11.370 - 00:00:54.750, Speaker A: Quick word about me. I'm hatu I strategy at flashpot. I'm also a strategic advisor to Lido. I've been thinking and writing about blockspace market and what me be for over three years now. This time I've seen a lot of things and in this talk I want to share my insight why you can't get around if you're developing a blockchain. Whether it's layer one solution side chain or it's layer two, you want to build a block based market that allows for efficient NVX section. So to see why, let's look at what a blockchain actually is from first principle.\n00:00:54.750 - 00:02:35.294, Speaker A: Every blockchain has three components first, a virtual machine that is replicated on thousands if not millions of computers at the same time a consensus mechanism that keeps them all synchronized with each other and the consensus actually breaks down into two parts. First is sequencing, just selecting the next selection to go into the block. In other words, propose a new fork of the chain that has one we block at the tip and then at the station that is attaching to the fork containing that we block at the tip giving it economic weight in the fork just rule and that weight in present day Ethereum is a proof of work. After the merge it will be proof of stake but in layer two context, for example, it can also be the absence of a fraud proof together with data availability or the presence of a zero knowledge proof or validity proof. And the last component that we need is a block based market and that is the topic of today's talk. So why do you need a block space market? The question that almost everyone asks in the beginning the idea about crypto is why can't we have infinite block space? Why can't we just make blocks as large as we want and that will create a large amount of supply for users. So that's why I gave the definition of what a blockchain is.\n00:02:35.294 - 00:03:22.090, Speaker A: So every transaction is due, must be replayed and stored by oil node forever. I put an asterisk here because it's not that strictly true. But in either case, oil nodes have to store what the current state is, and that can also get very large, very quick. To maximize the number of participants in a decentralized system. We want to minimize the amount of resources that each of them has to consume in order to stay in sync with the network and also catch up with from scratch or from a last checkpoint. And that means you have to limit resource consumption to a point where people with consumer based hardware assets can also participate. So that's the reason why ethereum for example, has a gas limit among other reasons.\n00:03:22.090 - 00:04:30.562, Speaker A: So if you are starting from the assumption you need finite box space, then the question is how do you allocate this box space efficiently? And different blockchains have tried different market mechanisms for that first Converse Soap or various forms of batch auctions, as we will see. So to look back around to the first slide, a transaction starts in the block space market, then it's picked up by a sequencer, then it's tested to a block and eventually makes it into the next update of the VM space. Designing block space markets used to be really easy. So three years ago there were very few people thinking about block space market design. Why? Because it was not an interesting problem. The problem that people were thinking about at the time was gas price estimation. So that was the only problem that people were dealing in real time so much to get your transaction included in the time that you want.\n00:04:30.562 - 00:05:22.530, Speaker A: And because gas price estimation was really poor, it led to a lot of underlying overbidding. Transactions stuck or people were overpaying or prices were swinging really hard. And so some time ago we implemented ESC one five nine turning the market from fixed supply. So there used to be like a fixed limit on the amount of gas that can be in a block and then the price would float demand and ERT one five nine replaced that with a floating supply and the market calling you a fixed price. And this leads to the same amount of S being consumed, but it has way better us. And this update has been so good that it has since been fixed by Sapphire virtually every smart contract blockchain that exists. So this has been a large success.\n00:05:22.530 - 00:06:29.050, Speaker A: You can consider the original problem of gas estimation. Soil mev enters the room and as you can see, the model of Mev that is being extracted from public",
                        "previous_answer": "The MEV or transaction supply chain is a concept that involves all the major parties that contribute to getting a user's transaction included on-chain. It begins with the user who has an intent to transact. This intent is then expressed to the wallet, which turns it into a transaction. Some transactions get picked up or manipulated by searchers and turned into bundles, while others don't. \n\nThe next step involves a builder who selects the transactions to be included in the next block. This block is then sent to a miner or validator who attests to it. This supply chain is crucial in providing guarantees such as censorship resistance and market power for users. These guarantees largely come from the competition between miners and validators. \n\nHowever, there are concerns about the MEV supply chain collapsing into a single vertically integrated party, which is referred to as MEV Dystopia. This scenario would mean that all blocks on public blockchains would be made by this one party, leading to a loss of censorship resistance and a monopoly that can charge arbitrarily higher fees from users. \n\nThe builder and the validator or miner are two separate parties in this supply chain. For many years, the role of the builder has been played by the mining pool, which selects the transactions and puts them into a block. The miner or validators then attest to it with a proof of work or proof of stake, giving it economic weight in the fork choice rule. \n\nThe process of building used to be simple when all a mining pool had to do was have a decent view of the mempool and select the highest paying transactions. However, the introduction of MEV has made building more complex as it involves identifying MEV transactions, making these transactions, possibly maintaining inventory of coins, and managing risk. \n\nIn the future, it is expected that everyone will try to move further up the supply chain as those further up have more power. However, this is tricky given some interests are already entrenched. The goal is to make it economically reasonable and rational for actors to give up their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The MEV or transaction supply chain is a complex process that involves several key players who contribute to the inclusion of a user's transaction on-chain. It starts with the user who initiates a transaction. This transaction is then processed by the wallet, which converts the user's intent into a transaction. Some transactions are manipulated or picked up by searchers and transformed into bundles, while others are not.\n\nThe next stage involves a builder, who selects the transactions to be included in the next block. This block is then forwarded to a miner or validator who attests to it. This supply chain is vital in providing guarantees such as censorship resistance and market power for users. These guarantees are largely derived from the competition between miners and validators.\n\nHowever, there are concerns about the MEV supply chain collapsing into a single vertically integrated entity, a scenario referred to as MEV Dystopia. In this situation, one party would create all blocks on public blockchains, leading to a loss of censorship resistance and a monopoly that could impose arbitrarily high fees on users.\n\nThe builder and the validator or miner are two distinct entities in this supply chain. For many years, mining pools have performed the role of the builder, selecting transactions and incorporating them into a block. The miner or validators then attest to it with a proof of work or proof of stake, giving it economic weight in the fork choice rule.\n\nThe building process used to be straightforward when all a mining pool needed was a decent view of the mempool and the ability to select the highest paying transactions. However, the introduction of MEV has complicated the building process as it involves identifying MEV transactions, creating these transactions, possibly maintaining an inventory of coins, and managing risk.\n\nIn the future, it is anticipated that everyone will attempt to move further up the supply chain, as those higher up have more power. However, this is challenging given that some interests are already entrenched. The objective is to make it economically reasonable and rational for actors to relinquish their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "the next update of the VM space. Designing block space markets used to be really easy. So three years ago there were very few people thinking about block space market design. Why? Because it was not an interesting problem. The problem that people were thinking about at the time was gas price estimation. So that was the only problem that people were dealing in real time so much to get your transaction included in the time that you want.\n00:04:30.562 - 00:05:22.530, Speaker A: And because gas price estimation was really poor, it led to a lot of underlying overbidding. Transactions stuck or people were overpaying or prices were swinging really hard. And so some time ago we implemented ESC one five nine turning the market from fixed supply. So there used to be like a fixed limit on the amount of gas that can be in a block and then the price would float demand and ERT one five nine replaced that with a floating supply and the market calling you a fixed price. And this leads to the same amount of S being consumed, but it has way better us. And this update has been so good that it has since been fixed by Sapphire virtually every smart contract blockchain that exists. So this has been a large success.\n00:05:22.530 - 00:06:29.050, Speaker A: You can consider the original problem of gas estimation. Soil mev enters the room and as you can see, the model of Mev that is being extracted from public blockchains is going up. In sum, and what this means, basically, mev has made block based market design hard again. Why is that? Mev transactions have complex preferences for placement. So an Mev transaction wants to be positioned at a very specific point in a block, also in relation to other target transactions. For example, if the mev transaction is doing inter block arbitrage, so arbitrage between two blocks are being mined, then it wants to be first in the block or very early. If the arbitrage is done within a block, then it wants to be positioned right after a target transaction.\n00:06:29.050 - 00:07:18.694, Speaker A: For example, like a trade that unbalances a decks Dex pool, for example. If it's a liquidation, then this is a similar format. So you want to be positioned right after the Oracle update, so you can be the first to public charge away the opportunity. And with the sandwich it gets even more complicated because you want to be right before and after a target transaction. And it doesn't even have to be just one. A sandwich can have in the middle an arbitrarily large number of trades as long as they're all going in the same direction. So you might even want to say, I want to put three trades here in the order and then put mine in front of the back.\n00:07:18.694 - 00:08:21.450, Speaker A: So these are typical challenges that we're seeing. So what this means is if you have an unexpressive market I. E. Market where that doesn't allow the bidders to express their complex preferences and you have bidders with complex preferences and these bidders have a large incentive to get their transaction exactly the point where they want, then the result is a lot of negative externalities for your blockchain. So if your market mechanism at least include all of the ones that we will go through in a second informs what mev extraction strategy, search house trading terms, et"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "the next update of the VM space. Designing block space markets used to be really easy. So three years ago there were very few people thinking about block space market design. Why? Because it was not an interesting problem. The problem that people were thinking about at the time was gas price estimation. So that was the only problem that people were dealing in real time so much to get your transaction included in the time that you want.\n00:04:30.562 - 00:05:22.530, Speaker A: And because gas price estimation was really poor, it led to a lot of underlying overbidding. Transactions stuck or people were overpaying or prices were swinging really hard. And so some time ago we implemented ESC one five nine turning the market from fixed supply. So there used to be like a fixed limit on the amount of gas that can be in a block and then the price would float demand and ERT one five nine replaced that with a floating supply and the market calling you a fixed price. And this leads to the same amount of S being consumed, but it has way better us. And this update has been so good that it has since been fixed by Sapphire virtually every smart contract blockchain that exists. So this has been a large success.\n00:05:22.530 - 00:06:29.050, Speaker A: You can consider the original problem of gas estimation. Soil mev enters the room and as you can see, the model of Mev that is being extracted from public blockchains is going up. In sum, and what this means, basically, mev has made block based market design hard again. Why is that? Mev transactions have complex preferences for placement. So an Mev transaction wants to be positioned at a very specific point in a block, also in relation to other target transactions. For example, if the mev transaction is doing inter block arbitrage, so arbitrage between two blocks are being mined, then it wants to be first in the block or very early. If the arbitrage is done within a block, then it wants to be positioned right after a target transaction.\n00:06:29.050 - 00:07:18.694, Speaker A: For example, like a trade that unbalances a decks Dex pool, for example. If it's a liquidation, then this is a similar format. So you want to be positioned right after the Oracle update, so you can be the first to public charge away the opportunity. And with the sandwich it gets even more complicated because you want to be right before and after a target transaction. And it doesn't even have to be just one. A sandwich can have in the middle an arbitrarily large number of trades as long as they're all going in the same direction. So you might even want to say, I want to put three trades here in the order and then put mine in front of the back.\n00:07:18.694 - 00:08:21.450, Speaker A: So these are typical challenges that we're seeing. So what this means is if you have an unexpressive market I. E. Market where that doesn't allow the bidders to express their complex preferences and you have bidders with complex preferences and these bidders have a large incentive to get their transaction exactly the point where they want, then the result is a lot of negative externalities for your blockchain. So if your market mechanism at least include all of the ones that we will go through in a second informs what mev extraction strategy, search house trading terms, et",
                        "previous_answer": "The MEV or transaction supply chain is a concept that involves all the major parties that contribute to getting a user's transaction included on-chain. It begins with the user who has an intent to transact. This intent is then expressed to the wallet, which turns it into a transaction. Some transactions get picked up or manipulated by searchers and turned into bundles, while others don't. \n\nThe next step involves a builder who selects the transactions to be included in the next block. This block is then sent to a miner or validator who attests to it. This supply chain is crucial in providing guarantees such as censorship resistance and market power for users. These guarantees largely come from the competition between miners and validators. \n\nHowever, there are concerns about the MEV supply chain collapsing into a single vertically integrated party, which is referred to as MEV Dystopia. This scenario would mean that all blocks on public blockchains would be made by this one party, leading to a loss of censorship resistance and a monopoly that can charge arbitrarily higher fees from users. \n\nThe builder and the validator or miner are two separate parties in this supply chain. For many years, the role of the builder has been played by the mining pool, which selects the transactions and puts them into a block. The miner or validators then attest to it with a proof of work or proof of stake, giving it economic weight in the fork choice rule. \n\nThe process of building used to be simple when all a mining pool had to do was have a decent view of the mempool and select the highest paying transactions. However, the introduction of MEV has made building more complex as it involves identifying MEV transactions, making these transactions, possibly maintaining inventory of coins, and managing risk. \n\nIn the future, it is expected that everyone will try to move further up the supply chain as those further up have more power. However, this is tricky given some interests are already entrenched. The goal is to make it economically reasonable and rational for actors to give up their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The Miner Extractable Value (MEV) supply chain is a complex process involving several key players, each contributing to the successful inclusion of a user's transaction on the blockchain. The process begins with the user, who expresses an intent to transact. This intent is then converted into a transaction by the user's wallet. Some of these transactions are manipulated or picked up by entities known as searchers, who bundle them, while others are not.\n\nThe next stage involves a builder, who is responsible for selecting the transactions that will be included in the upcoming block. This block is then forwarded to a miner or validator, who provides their attestation. The competition between miners and validators is crucial in this supply chain, as it provides important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to collapse into a single, vertically integrated entity, a scenario referred to as MEV Dystopia. In this situation, one party would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the creation of a monopoly capable of charging users arbitrarily high fees.\n\nHistorically, the roles of the builder and the miner or validator have been distinct. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, providing them with economic weight in the fork choice rule.\n\nThe introduction of MEV has complicated the building process. Builders must now identify MEV transactions, create these transactions, potentially maintain an inventory of coins, and manage associated risks. As we move forward, it is anticipated that all parties will strive to move further up the supply chain, as those higher up wield more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to create a system where it is economically rational for actors to relinquish their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "transaction is doing inter block arbitrage, so arbitrage between two blocks are being mined, then it wants to be first in the block or very early. If the arbitrage is done within a block, then it wants to be positioned right after a target transaction.\n00:06:29.050 - 00:07:18.694, Speaker A: For example, like a trade that unbalances a decks Dex pool, for example. If it's a liquidation, then this is a similar format. So you want to be positioned right after the Oracle update, so you can be the first to public charge away the opportunity. And with the sandwich it gets even more complicated because you want to be right before and after a target transaction. And it doesn't even have to be just one. A sandwich can have in the middle an arbitrarily large number of trades as long as they're all going in the same direction. So you might even want to say, I want to put three trades here in the order and then put mine in front of the back.\n00:07:18.694 - 00:08:21.450, Speaker A: So these are typical challenges that we're seeing. So what this means is if you have an unexpressive market I. E. Market where that doesn't allow the bidders to express their complex preferences and you have bidders with complex preferences and these bidders have a large incentive to get their transaction exactly the point where they want, then the result is a lot of negative externalities for your blockchain. So if your market mechanism at least include all of the ones that we will go through in a second informs what mev extraction strategy, search house trading terms, et cetera will use. And depending how expressive your market is and if it's optimized for media extraction, we will see various forms of negative extraction.\n\ndocument_type: youtube_video\ntitle: State of the MEV Union - Jon Charbonneau\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=-STbGjZc_OU\nrelease_date: 2023-03-28\n\n00:00:07.130 - 00:00:33.640, Speaker A: All right, everyone. Welcome to Math Economic. We've got a ton of great research topics set up here at the entry section of Mev Ethereum All Things Economics. First, shout out to ETH Global and Flaspbots for putting all this together, especially Tina and Sarah. And thank you again, Tina, for informing me that I'd volunteer to be roastmaster for this. Just a quick scheduling note. All the presentations here will have some Q A at the end, so please submit any questions you've got.\n00:00:33.640 - 00:00:58.640, Speaker A: If you're on the live stream, we will be checking them. I'll prioritize the spicy ones, so please, no softball questions. With that, I will get into the first presentation. There we go. So this will just be a very broad overview to kind of set the stage for all of the other talks today. This is basically the state of affairs for everything. MUV, right.\n00:00:58.640 - 00:01:52.210, Speaker A: So basically, as everyone kind of knows, merge happened at the end of last year, completely changed the block production supply chain. This is broadly what it looks like right now. This picture comes from Barnaby, who's obviously working on a lot of the stuff over at the Theorem Foundation. So broad transaction supply chain users submit transactions. Searchers will take a lot of those transactions, make specific bundles, they send them to builders, those go through relays and eventually Validators will take those blocks that builders make for them and propose those to the network. This kind of separates that specialized task of actually building blocks so the Validators can have permissionless, access to mev and not have decentralizing pressures of needing to be sophisticated actors who need to do that more complicated task. So this is what the builder market has looked like since the Merge.\n00:01:52.210 - 00:02:47.554, Speaker A: As you can see, it has diversified a lot since the start of it, right around the time of the merge. It was rather centralized in the beginning where there just weren't a whole lot of parties there yet. Since then it has broken up very nicely, such that you have a lot of different participants who are all taking ten to 15 20% market share and then a long tail of much smaller ones as well. So, as you can see kind of in this graph below at the bottom, the table below at the bottom here, some of these builders are able to take a nice margin on that. For some of the bigger ones, these are the profits that some of them have picked up over the last week. For some of the big builders, some of them do run at a profit like as you can see with some of the bigger ones, like Beaver here, Rsing build through zero 69, and others just run at Flat, where they pass everything through, which is the Illuminate Democratized tribute. That one splash bots.\n00:02:47.554 - 00:03:46.210, Speaker A: That's what they do. One of the other exciting things with builders as well is we'll start to see going forward, hopefully is their ability in their kind of role here to add a lot of additional features going forward, which I would recommend you just see Alex Stokes'talks on those topics because that is a very long topic in and of itself. So the big question here is that everything looks pretty good from the start of the merge. I think most people would agree that we're pretty happy with the way that the builder market is shaped out, that there's a good amount of decentralization now in that there's a lot of different participants in there who are acting pretty competitively. So I would say it's a good start. But there's obviously concern that there are very natural, very strong centralizing tendencies in this over time. So that's where it comes in, trying to build an actually fully decentralized block building network that kind of plugs in here and is able to have many, many participants who actually contribute to even building a single block.\n00:03:46.210 - 00:04:24.142, Speaker A: And that is exactly what Suave is, as I'm sure everyone here has heard by now. Here's just a very quick overview of it. So moving on down to the next step of the supply chain relays is basically kind of just the pipes in the middle between the builders and the validators who were sending those blocks along. Again, you see the very similar trend where at the beginning it was very centralized. It was basically everyone was using Flashbots for the most part. And since those first couple of months, we have seen a lot of diversification on there, which has been really nice. And the biggest one now is actually not Flashbots.\n00:04:24.142 - 00:05:27.794, Speaker A: And it is, I believe, last I checked, it is the ultrasound relay, especially after they launched very recently, I believe a week ago. They're optimistic relays. So that is kind of a new concept where you start to realize that there's a meaningful latency advantage. If you cut out that kind of step of not requiring the relays to actually validate the block that they are sending along to the proposers, that cuts off 100, 200 milliseconds of latency and that does add a meaningful benefit. So it does come along with some risks because now they are no longer checking the blocks that builders are sending them to the full extent, but they're also collateralized by these builders. So builders who want to be able to get that extra latency edge and give their blocks a little higher chance of getting accepted, what they'll effectively do is they will just put up some form of collateral and hey. If my payment that I said that I was going to give the proposer doesn't go through for some reason, if something was screwed up, then you could take my collateral and basically give it to them and I get kicked off of that relay.\n00:05:27.794 - 00:06:23.020, Speaker A: So there's some element of trust in there. But there is also this fallback option to make sure that proposers don't just completely get rugged on it and recommend checking out the meth boost community calls. There's been a couple of them now where a lot of this stuff gets discussed. So the general trend on this, I'm sure everyone has seen this chart by now, and this goes kind of hand in hand with a lot of that diversification that we saw throughout the relay and builder market is early on. The large majority of relay and builder adoption that we saw was primarily coming from ones who filter out OFAC transactions, which obviously got a lot of people worried in the earlier parts of the year, particularly if that started to continue to trend up. You'd potentially have to wait a rather long time if you were one of those sanctioned listed addresses.\n\ntotal_pages: 25\nsource: 7\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\ntime until expiration, as in Figure 1. As the time with which extractors have to\nact on a protocol or transaction approaches 0, so too does the time-extractable\nvalue, collapsing to the intrinsic-extractable value.\n3\nCross-Domain Protocols and MEV\nThis section introduces a list of protocols at the core of cross-domain conver-\nsation. All of these protocols play some role in cross-domain MEV, with some\nprotocols explicitly intended to provide some sort of MEV mitigation. In each\nof the protocols we discuss both the type of value extraction taking place (Sec-\ntion 2.1), and where the extractable value typically originates (Section 2.2).\nWith each protocol we describe the ideal protocol functionality, current im-\nplementations, and open questions that exist towards implementing the ideal\nfunctionality specified. We also describe the state of MEV in each, including,\nbut not limited to, cross-domain MEV.\nIn the analysis that follows, we consider the centralization of power at any\npoint in the transaction/block-building/MEV supply-chain, whether relaying,\nbuilding, running an auction, block proposing, as negative for protocols.\nIf\nprotocols were to centralize power to a truly trusted player/set of players, this\nwould fine. However, the existence of such players ad infinitum is unrealistic,\nand cannot be guaranteed in practice without strict rules and incentives. These\nrules and incentives do not exist as a given, and require careful construction.\nIn-keeping with the core decentralization values of blockchains, we highlight\ncentralizing effects that protocols have. Furthermore, in each such protocol, we\nleave the \u201cCan the protocol protect against such effects?\u201d as an important open\nquestion.\n3.1\nFramework for Analysis\nWe consider blockchains as state machines5. Blockchains can be represented\nas a directed acyclic graph, with each block containing state machine updates.\nThe updates in a block B act on the state achieved by applying all blocks\nin the directed sub-graph reachable from B. Where multiple competing sub-\ngraphs exist, blockchain protocols have tie-breaking rules for deciding which sub-\ngraph to choose (most blocks, greatest height, etc.). Unless otherwise stated,\nblocks contain a sequence of transactions (state machine updates) which must\nbe applied in order6, with this sequence chosen by a single elected block proposer\nfor each block7. Blocks are added to the blockchain at discrete time intervals.\nWhere appropriate, our analysis focuses on the MEV occurring in a an AMM,\nwhere order submission is controlled by the respective protocols. We isolate\n5A state machine consists of set of variables, and sequence of commands/updates on those\nvariables, producing some output.\n6Transactions can also be batch executed as in Section 3.8.\n7Protocols can force proposers to out-source sequencing to a dedicated sequencer (see\nSections 3.2 and 3.3), or incentivize proposers to auction the right to sequence blocks (see\nSection 3.7 and Appendix A).\n7\n\ntotal_pages: 25\nsource: 15\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\n3.6.3\nOpen Questions\nBoth of these solutions effectively require the builder/searchers to arbitrage the\npool against this true price, and then provide liquidity to the user orders. In\nthese solutions, the pool reserves play a limited role beyond providing users\nwith an estimate of how their orders will be executed. On the other hand, the\nend game in PBS also appears to be for builders to implicitly provide liquidity\nto user orders, taking on inventory risk in order to extract all available value\nfrom a block. Importantly, McAMMs and Diamond return the value to where\nit is being extracted, as opposed to the block proposers in PBS, so has clear\nadvantages over existing solutions in this regard.\nPriority auctions likely incentivize private order-flow. By ensuring a signifi-\ncant share of order-flow, an extractor can better price priority auctions, allowing\nsuch an extractor to minimally increment the second best auction bid, guaran-\nteeing priority. This creates a cycle, attracting more private flow. This in turn\nallows the extractor to provide private order-flow extracting arbitrarily high\nrents as the only player in town with access to the protocols offering priority.\nSuch negative centralizing externalities need to be properly addressed.\nAdditionally, in the case where auctions must be carried out by a trusted\nauctioneer, such as in McAMMs, it is not clear if the incentives of the auctioneer\nare aligned with that of protocol. A clear specification and analysis of the role\nof the auctioneer is required.\n3.7\nSlot Auctions\n3.7.1\nIdeal Functionality\nBlock proposers auction off the right to propose a block ahead of time. For a\nplayer winning multiple slot auctions across several chains, this"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "transaction is doing inter block arbitrage, so arbitrage between two blocks are being mined, then it wants to be first in the block or very early. If the arbitrage is done within a block, then it wants to be positioned right after a target transaction.\n00:06:29.050 - 00:07:18.694, Speaker A: For example, like a trade that unbalances a decks Dex pool, for example. If it's a liquidation, then this is a similar format. So you want to be positioned right after the Oracle update, so you can be the first to public charge away the opportunity. And with the sandwich it gets even more complicated because you want to be right before and after a target transaction. And it doesn't even have to be just one. A sandwich can have in the middle an arbitrarily large number of trades as long as they're all going in the same direction. So you might even want to say, I want to put three trades here in the order and then put mine in front of the back.\n00:07:18.694 - 00:08:21.450, Speaker A: So these are typical challenges that we're seeing. So what this means is if you have an unexpressive market I. E. Market where that doesn't allow the bidders to express their complex preferences and you have bidders with complex preferences and these bidders have a large incentive to get their transaction exactly the point where they want, then the result is a lot of negative externalities for your blockchain. So if your market mechanism at least include all of the ones that we will go through in a second informs what mev extraction strategy, search house trading terms, et cetera will use. And depending how expressive your market is and if it's optimized for media extraction, we will see various forms of negative extraction.\n\ndocument_type: youtube_video\ntitle: State of the MEV Union - Jon Charbonneau\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=-STbGjZc_OU\nrelease_date: 2023-03-28\n\n00:00:07.130 - 00:00:33.640, Speaker A: All right, everyone. Welcome to Math Economic. We've got a ton of great research topics set up here at the entry section of Mev Ethereum All Things Economics. First, shout out to ETH Global and Flaspbots for putting all this together, especially Tina and Sarah. And thank you again, Tina, for informing me that I'd volunteer to be roastmaster for this. Just a quick scheduling note. All the presentations here will have some Q A at the end, so please submit any questions you've got.\n00:00:33.640 - 00:00:58.640, Speaker A: If you're on the live stream, we will be checking them. I'll prioritize the spicy ones, so please, no softball questions. With that, I will get into the first presentation. There we go. So this will just be a very broad overview to kind of set the stage for all of the other talks today. This is basically the state of affairs for everything. MUV, right.\n00:00:58.640 - 00:01:52.210, Speaker A: So basically, as everyone kind of knows, merge happened at the end of last year, completely changed the block production supply chain. This is broadly what it looks like right now. This picture comes from Barnaby, who's obviously working on a lot of the stuff over at the Theorem Foundation. So broad transaction supply chain users submit transactions. Searchers will take a lot of those transactions, make specific bundles, they send them to builders, those go through relays and eventually Validators will take those blocks that builders make for them and propose those to the network. This kind of separates that specialized task of actually building blocks so the Validators can have permissionless, access to mev and not have decentralizing pressures of needing to be sophisticated actors who need to do that more complicated task. So this is what the builder market has looked like since the Merge.\n00:01:52.210 - 00:02:47.554, Speaker A: As you can see, it has diversified a lot since the start of it, right around the time of the merge. It was rather centralized in the beginning where there just weren't a whole lot of parties there yet. Since then it has broken up very nicely, such that you have a lot of different participants who are all taking ten to 15 20% market share and then a long tail of much smaller ones as well. So, as you can see kind of in this graph below at the bottom, the table below at the bottom here, some of these builders are able to take a nice margin on that. For some of the bigger ones, these are the profits that some of them have picked up over the last week. For some of the big builders, some of them do run at a profit like as you can see with some of the bigger ones, like Beaver here, Rsing build through zero 69, and others just run at Flat, where they pass everything through, which is the Illuminate Democratized tribute. That one splash bots.\n00:02:47.554 - 00:03:46.210, Speaker A: That's what they do. One of the other exciting things with builders as well is we'll start to see going forward, hopefully is their ability in their kind of role here to add a lot of additional features going forward, which I would recommend you just see Alex Stokes'talks on those topics because that is a very long topic in and of itself. So the big question here is that everything looks pretty good from the start of the merge. I think most people would agree that we're pretty happy with the way that the builder market is shaped out, that there's a good amount of decentralization now in that there's a lot of different participants in there who are acting pretty competitively. So I would say it's a good start. But there's obviously concern that there are very natural, very strong centralizing tendencies in this over time. So that's where it comes in, trying to build an actually fully decentralized block building network that kind of plugs in here and is able to have many, many participants who actually contribute to even building a single block.\n00:03:46.210 - 00:04:24.142, Speaker A: And that is exactly what Suave is, as I'm sure everyone here has heard by now. Here's just a very quick overview of it. So moving on down to the next step of the supply chain relays is basically kind of just the pipes in the middle between the builders and the validators who were sending those blocks along. Again, you see the very similar trend where at the beginning it was very centralized. It was basically everyone was using Flashbots for the most part. And since those first couple of months, we have seen a lot of diversification on there, which has been really nice. And the biggest one now is actually not Flashbots.\n00:04:24.142 - 00:05:27.794, Speaker A: And it is, I believe, last I checked, it is the ultrasound relay, especially after they launched very recently, I believe a week ago. They're optimistic relays. So that is kind of a new concept where you start to realize that there's a meaningful latency advantage. If you cut out that kind of step of not requiring the relays to actually validate the block that they are sending along to the proposers, that cuts off 100, 200 milliseconds of latency and that does add a meaningful benefit. So it does come along with some risks because now they are no longer checking the blocks that builders are sending them to the full extent, but they're also collateralized by these builders. So builders who want to be able to get that extra latency edge and give their blocks a little higher chance of getting accepted, what they'll effectively do is they will just put up some form of collateral and hey. If my payment that I said that I was going to give the proposer doesn't go through for some reason, if something was screwed up, then you could take my collateral and basically give it to them and I get kicked off of that relay.\n00:05:27.794 - 00:06:23.020, Speaker A: So there's some element of trust in there. But there is also this fallback option to make sure that proposers don't just completely get rugged on it and recommend checking out the meth boost community calls. There's been a couple of them now where a lot of this stuff gets discussed. So the general trend on this, I'm sure everyone has seen this chart by now, and this goes kind of hand in hand with a lot of that diversification that we saw throughout the relay and builder market is early on. The large majority of relay and builder adoption that we saw was primarily coming from ones who filter out OFAC transactions, which obviously got a lot of people worried in the earlier parts of the year, particularly if that started to continue to trend up. You'd potentially have to wait a rather long time if you were one of those sanctioned listed addresses.\n\ntotal_pages: 25\nsource: 7\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\ntime until expiration, as in Figure 1. As the time with which extractors have to\nact on a protocol or transaction approaches 0, so too does the time-extractable\nvalue, collapsing to the intrinsic-extractable value.\n3\nCross-Domain Protocols and MEV\nThis section introduces a list of protocols at the core of cross-domain conver-\nsation. All of these protocols play some role in cross-domain MEV, with some\nprotocols explicitly intended to provide some sort of MEV mitigation. In each\nof the protocols we discuss both the type of value extraction taking place (Sec-\ntion 2.1), and where the extractable value typically originates (Section 2.2).\nWith each protocol we describe the ideal protocol functionality, current im-\nplementations, and open questions that exist towards implementing the ideal\nfunctionality specified. We also describe the state of MEV in each, including,\nbut not limited to, cross-domain MEV.\nIn the analysis that follows, we consider the centralization of power at any\npoint in the transaction/block-building/MEV supply-chain, whether relaying,\nbuilding, running an auction, block proposing, as negative for protocols.\nIf\nprotocols were to centralize power to a truly trusted player/set of players, this\nwould fine. However, the existence of such players ad infinitum is unrealistic,\nand cannot be guaranteed in practice without strict rules and incentives. These\nrules and incentives do not exist as a given, and require careful construction.\nIn-keeping with the core decentralization values of blockchains, we highlight\ncentralizing effects that protocols have. Furthermore, in each such protocol, we\nleave the \u201cCan the protocol protect against such effects?\u201d as an important open\nquestion.\n3.1\nFramework for Analysis\nWe consider blockchains as state machines5. Blockchains can be represented\nas a directed acyclic graph, with each block containing state machine updates.\nThe updates in a block B act on the state achieved by applying all blocks\nin the directed sub-graph reachable from B. Where multiple competing sub-\ngraphs exist, blockchain protocols have tie-breaking rules for deciding which sub-\ngraph to choose (most blocks, greatest height, etc.). Unless otherwise stated,\nblocks contain a sequence of transactions (state machine updates) which must\nbe applied in order6, with this sequence chosen by a single elected block proposer\nfor each block7. Blocks are added to the blockchain at discrete time intervals.\nWhere appropriate, our analysis focuses on the MEV occurring in a an AMM,\nwhere order submission is controlled by the respective protocols. We isolate\n5A state machine consists of set of variables, and sequence of commands/updates on those\nvariables, producing some output.\n6Transactions can also be batch executed as in Section 3.8.\n7Protocols can force proposers to out-source sequencing to a dedicated sequencer (see\nSections 3.2 and 3.3), or incentivize proposers to auction the right to sequence blocks (see\nSection 3.7 and Appendix A).\n7\n\ntotal_pages: 25\nsource: 15\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\n3.6.3\nOpen Questions\nBoth of these solutions effectively require the builder/searchers to arbitrage the\npool against this true price, and then provide liquidity to the user orders. In\nthese solutions, the pool reserves play a limited role beyond providing users\nwith an estimate of how their orders will be executed. On the other hand, the\nend game in PBS also appears to be for builders to implicitly provide liquidity\nto user orders, taking on inventory risk in order to extract all available value\nfrom a block. Importantly, McAMMs and Diamond return the value to where\nit is being extracted, as opposed to the block proposers in PBS, so has clear\nadvantages over existing solutions in this regard.\nPriority auctions likely incentivize private order-flow. By ensuring a signifi-\ncant share of order-flow, an extractor can better price priority auctions, allowing\nsuch an extractor to minimally increment the second best auction bid, guaran-\nteeing priority. This creates a cycle, attracting more private flow. This in turn\nallows the extractor to provide private order-flow extracting arbitrarily high\nrents as the only player in town with access to the protocols offering priority.\nSuch negative centralizing externalities need to be properly addressed.\nAdditionally, in the case where auctions must be carried out by a trusted\nauctioneer, such as in McAMMs, it is not clear if the incentives of the auctioneer\nare aligned with that of protocol. A clear specification and analysis of the role\nof the auctioneer is required.\n3.7\nSlot Auctions\n3.7.1\nIdeal Functionality\nBlock proposers auction off the right to propose a block ahead of time. For a\nplayer winning multiple slot auctions across several chains, this",
                        "previous_answer": "The Miner Extractable Value (MEV) supply chain is a complex process involving several key players, each contributing to the successful inclusion of a user's transaction on the blockchain. The process begins with the user, who expresses an intent to transact. This intent is then converted into a transaction by the user's wallet. Some of these transactions are manipulated or picked up by entities known as searchers, who bundle them, while others are not.\n\nThe next stage involves a builder, who is responsible for selecting the transactions that will be included in the upcoming block. This block is then forwarded to a miner or validator, who provides their attestation. The competition between miners and validators is crucial in this supply chain, as it provides important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to collapse into a single, vertically integrated entity, a scenario referred to as MEV Dystopia. In this situation, one party would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the creation of a monopoly capable of charging users arbitrarily high fees.\n\nHistorically, the roles of the builder and the miner or validator have been distinct. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, providing them with economic weight in the fork choice rule.\n\nThe introduction of MEV has complicated the building process. Builders must now identify MEV transactions, create these transactions, potentially maintain an inventory of coins, and manage associated risks. As we move forward, it is anticipated that all parties will strive to move further up the supply chain, as those higher up wield more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to create a system where it is economically rational for actors to relinquish their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The MEV supply chain is a complex process that begins with users submitting transactions. These transactions are then picked up by searchers who create specific bundles. These bundles are then sent to builders who are responsible for constructing blocks. These blocks are then relayed to validators who propose these blocks to the network. This process separates the specialized task of building blocks, allowing validators to have permissionless access to MEV without the need to be sophisticated actors who perform the more complicated task of building blocks.\n\nThe builder market has diversified significantly since its inception, with many different participants each taking a share of the market. Some builders are able to make a profit, while others operate at a flat rate, passing everything through. The relay market has also seen a lot of diversification, with many different entities now involved in the process of relaying blocks from builders to validators.\n\nHowever, there are concerns about the potential for centralization in this process over time. To address this, efforts are being made to build a fully decentralized block building network with many participants contributing to the building of a single block. This is seen as crucial for maintaining the core decentralization values of blockchains and preventing the concentration of power in the hands of a few entities."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "executed. On the other hand, the\nend game in PBS also appears to be for builders to implicitly provide liquidity\nto user orders, taking on inventory risk in order to extract all available value\nfrom a block. Importantly, McAMMs and Diamond return the value to where\nit is being extracted, as opposed to the block proposers in PBS, so has clear\nadvantages over existing solutions in this regard.\nPriority auctions likely incentivize private order-flow. By ensuring a signifi-\ncant share of order-flow, an extractor can better price priority auctions, allowing\nsuch an extractor to minimally increment the second best auction bid, guaran-\nteeing priority. This creates a cycle, attracting more private flow. This in turn\nallows the extractor to provide private order-flow extracting arbitrarily high\nrents as the only player in town with access to the protocols offering priority.\nSuch negative centralizing externalities need to be properly addressed.\nAdditionally, in the case where auctions must be carried out by a trusted\nauctioneer, such as in McAMMs, it is not clear if the incentives of the auctioneer\nare aligned with that of protocol. A clear specification and analysis of the role\nof the auctioneer is required.\n3.7\nSlot Auctions\n3.7.1\nIdeal Functionality\nBlock proposers auction off the right to propose a block ahead of time. For a\nplayer winning multiple slot auctions across several chains, this gives the ability\nto simultaneously propose a block on these chains. This unlocks the ability to\nprovide users with atomic cross-chain transactions (Appendix B). Atomic order-\ninclusion confirmation has the potential to eliminate time-extractable value if\nthese confirmations are provided immediately, with sequence guaranteed. Re-\ntaining the ability to sequence transactions provides these slot auction winners\nwith some time-extractable value (although less than if the transactions were\nnot confirmed to be included). By simultaneously executing cross-chain trans-\nactions, signal MEV is reduced, while ordering MEV opportunities stand to\nincrease, due to the increase in possible orderings for proposers/searchers con-\ntrolling multiple simultaneous blocks.\n3.7.2\nImplementations and Discussion\nRelated to shared sequencing is the idea of slot auctions, where block proposers\nauction off the right to build the block ahead of time. This idea was discussed\nin a recent blog-post [33], with the Interchain Scheduler [52] an initial proposed\nsolution.\n15\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThere's no way around it. However, yeah, I would say for consumers of liquidity, there really aren't any downsides to trading against RFQ. You're going to get all the advantages of that.\n06:22:06.426 - 06:23:22.370, Speaker A: Consumers don't really need to care about where that liquidity is coming from otherwise. So I would say overall, the trade offs are pretty minimal and there's really good coverage in general. Tom showed a chart in his presentation about how large of a"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "executed. On the other hand, the\nend game in PBS also appears to be for builders to implicitly provide liquidity\nto user orders, taking on inventory risk in order to extract all available value\nfrom a block. Importantly, McAMMs and Diamond return the value to where\nit is being extracted, as opposed to the block proposers in PBS, so has clear\nadvantages over existing solutions in this regard.\nPriority auctions likely incentivize private order-flow. By ensuring a signifi-\ncant share of order-flow, an extractor can better price priority auctions, allowing\nsuch an extractor to minimally increment the second best auction bid, guaran-\nteeing priority. This creates a cycle, attracting more private flow. This in turn\nallows the extractor to provide private order-flow extracting arbitrarily high\nrents as the only player in town with access to the protocols offering priority.\nSuch negative centralizing externalities need to be properly addressed.\nAdditionally, in the case where auctions must be carried out by a trusted\nauctioneer, such as in McAMMs, it is not clear if the incentives of the auctioneer\nare aligned with that of protocol. A clear specification and analysis of the role\nof the auctioneer is required.\n3.7\nSlot Auctions\n3.7.1\nIdeal Functionality\nBlock proposers auction off the right to propose a block ahead of time. For a\nplayer winning multiple slot auctions across several chains, this gives the ability\nto simultaneously propose a block on these chains. This unlocks the ability to\nprovide users with atomic cross-chain transactions (Appendix B). Atomic order-\ninclusion confirmation has the potential to eliminate time-extractable value if\nthese confirmations are provided immediately, with sequence guaranteed. Re-\ntaining the ability to sequence transactions provides these slot auction winners\nwith some time-extractable value (although less than if the transactions were\nnot confirmed to be included). By simultaneously executing cross-chain trans-\nactions, signal MEV is reduced, while ordering MEV opportunities stand to\nincrease, due to the increase in possible orderings for proposers/searchers con-\ntrolling multiple simultaneous blocks.\n3.7.2\nImplementations and Discussion\nRelated to shared sequencing is the idea of slot auctions, where block proposers\nauction off the right to build the block ahead of time. This idea was discussed\nin a recent blog-post [33], with the Interchain Scheduler [52] an initial proposed\nsolution.\n15\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThere's no way around it. However, yeah, I would say for consumers of liquidity, there really aren't any downsides to trading against RFQ. You're going to get all the advantages of that.\n06:22:06.426 - 06:23:22.370, Speaker A: Consumers don't really need to care about where that liquidity is coming from otherwise. So I would say overall, the trade offs are pretty minimal and there's really good coverage in general. Tom showed a chart in his presentation about how large of a",
                        "previous_answer": "The Miner Extractable Value (MEV) supply chain is a complex process involving several key players, each contributing to the successful inclusion of a user's transaction on the blockchain. The process begins with the user, who expresses an intent to transact. This intent is then converted into a transaction by the user's wallet. Some of these transactions are manipulated or picked up by entities known as searchers, who bundle them, while others are not.\n\nThe next stage involves a builder, who is responsible for selecting the transactions that will be included in the upcoming block. This block is then forwarded to a miner or validator, who provides their attestation. The competition between miners and validators is crucial in this supply chain, as it provides important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to collapse into a single, vertically integrated entity, a scenario referred to as MEV Dystopia. In this situation, one party would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the creation of a monopoly capable of charging users arbitrarily high fees.\n\nHistorically, the roles of the builder and the miner or validator have been distinct. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, providing them with economic weight in the fork choice rule.\n\nThe introduction of MEV has complicated the building process. Builders must now identify MEV transactions, create these transactions, potentially maintain an inventory of coins, and manage associated risks. As we move forward, it is anticipated that all parties will strive to move further up the supply chain, as those higher up wield more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to create a system where it is economically rational for actors to relinquish their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The MEV supply chain is a multifaceted process involving several key participants, each playing a role in the successful execution of a user's transaction on the blockchain. It starts with the user expressing a desire to transact, which is then transformed into a transaction by the user's digital wallet. Some transactions are manipulated or gathered by entities known as searchers who bundle them, while others are not.\n\nThe subsequent step involves a builder, who selects the transactions to be included in the forthcoming block. This block is then passed to a miner or validator for their attestation. The competition between miners and validators is a critical aspect of this supply chain, providing essential guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to consolidate into a single, vertically integrated entity, a situation known as MEV Dystopia. In this scenario, one entity would be responsible for creating all blocks on public blockchains, resulting in a loss of censorship resistance and the establishment of a monopoly capable of imposing arbitrarily high fees on users.\n\nHistorically, the roles of the builder and the miner or validator have been separate. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, giving them economic weight in the fork choice rule.\n\nThe introduction of MEV has added complexity to the building process. Builders now need to identify MEV transactions, create these transactions, potentially maintain a coin inventory, and manage associated risks. As we progress, it is expected that all parties will aim to move further up the supply chain, as those higher up have more power. However, this is a complex task, as some interests are already deeply rooted. The ultimate objective is to establish a system where it is economically sensible for actors to give up their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "if\nthese confirmations are provided immediately, with sequence guaranteed. Re-\ntaining the ability to sequence transactions provides these slot auction winners\nwith some time-extractable value (although less than if the transactions were\nnot confirmed to be included). By simultaneously executing cross-chain trans-\nactions, signal MEV is reduced, while ordering MEV opportunities stand to\nincrease, due to the increase in possible orderings for proposers/searchers con-\ntrolling multiple simultaneous blocks.\n3.7.2\nImplementations and Discussion\nRelated to shared sequencing is the idea of slot auctions, where block proposers\nauction off the right to build the block ahead of time. This idea was discussed\nin a recent blog-post [33], with the Interchain Scheduler [52] an initial proposed\nsolution.\n15\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThere's no way around it. However, yeah, I would say for consumers of liquidity, there really aren't any downsides to trading against RFQ. You're going to get all the advantages of that.\n06:22:06.426 - 06:23:22.370, Speaker A: Consumers don't really need to care about where that liquidity is coming from otherwise. So I would say overall, the trade offs are pretty minimal and there's really good coverage in general. Tom showed a chart in his presentation about how large of a percentage RFQ volume makes up in the overall deck space. Yeah, it's probably somewhere around 40% these days, I would think, and probably somewhere between ten and 20 professional market makers who are using these systems, from my experience. So how can you go and actually use this in practice? Well, we have a liquidity aggregation API called Xerox API, where Xerox RFQ is one of the major sources of liquidity taps into several professional market makers behind the scenes. If a trade is routed 100% through zero X, it will be protected from mev and slippage. And you can easily set that in the API parameters or can also just let your trade be aggregated.\n06:23:22.370 - 06:24:23.206, Speaker A: There will be cases where there are mispricings and automated mark makers are offering better prices for whatever reason, you don't really need to worry about it. But if you're doing a very large trade, could be worth considering, even if you are offered a slightly worse price. In some cases, you can think of it as insurance, essentially because it guarantees you're not going to get front run. So yeah, give it a shot at Matcha XYZ, which is our user facing Dex aggregator, or one of several Xerox API. Integrators zapper MetaMask DeFi saver coinbase wallet forgetting a bunch. And if you're using Matcha and you do want that insurance, you are making a really large trade. There is this OTC tab you could check out and that'll just be trading 100% with this RFK system.\n06:24:23.206 - 06:25:10.318, Speaker A: So it's not even going to bother looking at the other liquidity sources in this case. That is all. I guess my closing comment is it's a super simple system which I think is desirable. It's really hard to reason about a lot of these solutions, but with RFQ, I think it's actually like really black and white and hope you enjoy the presentation. Thank you, Amir, for another crystal clear presentation. Up next we have sorry, that was a little out of order, these slides. Up next we have Kai Hua and Li.\n06:25:10.318 - 06:25:48.620, Speaker A: Please feel free to share. Screen. Hi, can everyone see my screen? Yeah. Cool. All right, so thanks for the introduction, Tina. So today we are going to talk about a to Mm, which is an application layer solution we propose to Mitigate mev. This is a joint work with Kai Hua and ASA.\n06:25:48.620 - 06:26:37.304, Speaker A: So to start, I want to discuss why maximizing mev, even in a decentralized way, might be problematic sometimes. So let's consider a simple scenario where we have two miners. One is honest and the other one is malicious. So both of the miners are trying to append to block B one, and they both observe some mev opportunity on the network layer and they are trying to extract them. So let's assume that the honest miner succeeds in mining the mev opportunity. Once the malicious miner receives block B two from the network layer, it will need to decide whether to forfeit the mev opportunity or not. So there are two cases.\n06:26:37.304 - 06:28:00.696, Speaker A: In the first case, the malicious miner forfeits the mev opportunity, so it jumps onto B two, just like following the standard ethereum protocol to mine on the longest block blockchain. However, mining on the longest chain might not be the optimal strategy because the miner can choose to keep mining on block C two in an attempt to fork the blockchain. If the malicious miner is lucky, then the forked chain can catch up to B two. And then this forking behavior is actually quite dangerous because it's going to waste the computational power of the honest miner and it's also going to increase the stable up rate, which makes the game easier for the adversary to do double spending attacks and selfish mining attacks. So how does the malicious miner decide whether to fork or not? So, there are many factors he needs to consider. What's the average block reward, what's the mev reward, what's the hashing power he controls? And how long does it take to switch from mining the old block to the new block. So, in order to quantify this problem, we have modeled the entire thing as a Markov decision process.\n06:28:00.696 - 06:29:05.100, Speaker A: So, based our finding in this paper, we find that if the adverse rail miner owns 10% of the hashing power, the rational behavior is to fork the blockchain if the mev is four times larger than the average block reward, but four times is nothing. We have observed mev opportunities exceeding 874 x of the average block reward. This is actually the BDX attack in the past. So here is the critical takeaway message. When the mev is big, even small miners would fork the blockchain in order to steal the mev opportunity from the other miners. So, although we have not seen substantial efforts indicating such behavior at moment, according to our quantifications, this is actually the optimal strategy to do so. Why the miners are not doing it at the moment? Remember, one year ago, miners are not extracting mev.\n06:29:05.100 - 06:30:13.010, Speaker A: So I believe this is basically because miners do not have the setup at the moment to help them fork the blockchain. I really hope I do not see this kind of forking behavior in the future because this is definitely going to be a disaster. And I hope Flashbots can also be aware of this risk and try to mitigate the problem. Is there any simple way to reduce the mev to kind of mitigate the risk? So, when I think about this problem, I always ask myself the following two questions. So who is the creator of the mev? And if the creator can close the mev itself? So this is very similar to what Tom said about the unionized idea which implemented by the B protocol. So I will give a different example here. So let's think about an arbitrage opportunity."
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "if\nthese confirmations are provided immediately, with sequence guaranteed. Re-\ntaining the ability to sequence transactions provides these slot auction winners\nwith some time-extractable value (although less than if the transactions were\nnot confirmed to be included). By simultaneously executing cross-chain trans-\nactions, signal MEV is reduced, while ordering MEV opportunities stand to\nincrease, due to the increase in possible orderings for proposers/searchers con-\ntrolling multiple simultaneous blocks.\n3.7.2\nImplementations and Discussion\nRelated to shared sequencing is the idea of slot auctions, where block proposers\nauction off the right to build the block ahead of time. This idea was discussed\nin a recent blog-post [33], with the Interchain Scheduler [52] an initial proposed\nsolution.\n15\n\ndocument_type: youtube_video\ntitle: MEV.WTF Summit\nchannel_name: ETHGlobal\nvideo_link: https://www.youtube.com/watch?v=s3nACF7uVZw\nrelease_date: 2021-07-02\n\nThere's no way around it. However, yeah, I would say for consumers of liquidity, there really aren't any downsides to trading against RFQ. You're going to get all the advantages of that.\n06:22:06.426 - 06:23:22.370, Speaker A: Consumers don't really need to care about where that liquidity is coming from otherwise. So I would say overall, the trade offs are pretty minimal and there's really good coverage in general. Tom showed a chart in his presentation about how large of a percentage RFQ volume makes up in the overall deck space. Yeah, it's probably somewhere around 40% these days, I would think, and probably somewhere between ten and 20 professional market makers who are using these systems, from my experience. So how can you go and actually use this in practice? Well, we have a liquidity aggregation API called Xerox API, where Xerox RFQ is one of the major sources of liquidity taps into several professional market makers behind the scenes. If a trade is routed 100% through zero X, it will be protected from mev and slippage. And you can easily set that in the API parameters or can also just let your trade be aggregated.\n06:23:22.370 - 06:24:23.206, Speaker A: There will be cases where there are mispricings and automated mark makers are offering better prices for whatever reason, you don't really need to worry about it. But if you're doing a very large trade, could be worth considering, even if you are offered a slightly worse price. In some cases, you can think of it as insurance, essentially because it guarantees you're not going to get front run. So yeah, give it a shot at Matcha XYZ, which is our user facing Dex aggregator, or one of several Xerox API. Integrators zapper MetaMask DeFi saver coinbase wallet forgetting a bunch. And if you're using Matcha and you do want that insurance, you are making a really large trade. There is this OTC tab you could check out and that'll just be trading 100% with this RFK system.\n06:24:23.206 - 06:25:10.318, Speaker A: So it's not even going to bother looking at the other liquidity sources in this case. That is all. I guess my closing comment is it's a super simple system which I think is desirable. It's really hard to reason about a lot of these solutions, but with RFQ, I think it's actually like really black and white and hope you enjoy the presentation. Thank you, Amir, for another crystal clear presentation. Up next we have sorry, that was a little out of order, these slides. Up next we have Kai Hua and Li.\n06:25:10.318 - 06:25:48.620, Speaker A: Please feel free to share. Screen. Hi, can everyone see my screen? Yeah. Cool. All right, so thanks for the introduction, Tina. So today we are going to talk about a to Mm, which is an application layer solution we propose to Mitigate mev. This is a joint work with Kai Hua and ASA.\n06:25:48.620 - 06:26:37.304, Speaker A: So to start, I want to discuss why maximizing mev, even in a decentralized way, might be problematic sometimes. So let's consider a simple scenario where we have two miners. One is honest and the other one is malicious. So both of the miners are trying to append to block B one, and they both observe some mev opportunity on the network layer and they are trying to extract them. So let's assume that the honest miner succeeds in mining the mev opportunity. Once the malicious miner receives block B two from the network layer, it will need to decide whether to forfeit the mev opportunity or not. So there are two cases.\n06:26:37.304 - 06:28:00.696, Speaker A: In the first case, the malicious miner forfeits the mev opportunity, so it jumps onto B two, just like following the standard ethereum protocol to mine on the longest block blockchain. However, mining on the longest chain might not be the optimal strategy because the miner can choose to keep mining on block C two in an attempt to fork the blockchain. If the malicious miner is lucky, then the forked chain can catch up to B two. And then this forking behavior is actually quite dangerous because it's going to waste the computational power of the honest miner and it's also going to increase the stable up rate, which makes the game easier for the adversary to do double spending attacks and selfish mining attacks. So how does the malicious miner decide whether to fork or not? So, there are many factors he needs to consider. What's the average block reward, what's the mev reward, what's the hashing power he controls? And how long does it take to switch from mining the old block to the new block. So, in order to quantify this problem, we have modeled the entire thing as a Markov decision process.\n06:28:00.696 - 06:29:05.100, Speaker A: So, based our finding in this paper, we find that if the adverse rail miner owns 10% of the hashing power, the rational behavior is to fork the blockchain if the mev is four times larger than the average block reward, but four times is nothing. We have observed mev opportunities exceeding 874 x of the average block reward. This is actually the BDX attack in the past. So here is the critical takeaway message. When the mev is big, even small miners would fork the blockchain in order to steal the mev opportunity from the other miners. So, although we have not seen substantial efforts indicating such behavior at moment, according to our quantifications, this is actually the optimal strategy to do so. Why the miners are not doing it at the moment? Remember, one year ago, miners are not extracting mev.\n06:29:05.100 - 06:30:13.010, Speaker A: So I believe this is basically because miners do not have the setup at the moment to help them fork the blockchain. I really hope I do not see this kind of forking behavior in the future because this is definitely going to be a disaster. And I hope Flashbots can also be aware of this risk and try to mitigate the problem. Is there any simple way to reduce the mev to kind of mitigate the risk? So, when I think about this problem, I always ask myself the following two questions. So who is the creator of the mev? And if the creator can close the mev itself? So this is very similar to what Tom said about the unionized idea which implemented by the B protocol. So I will give a different example here. So let's think about an arbitrage opportunity.",
                        "previous_answer": "The MEV supply chain is a multifaceted process involving several key participants, each playing a role in the successful execution of a user's transaction on the blockchain. It starts with the user expressing a desire to transact, which is then transformed into a transaction by the user's digital wallet. Some transactions are manipulated or gathered by entities known as searchers who bundle them, while others are not.\n\nThe subsequent step involves a builder, who selects the transactions to be included in the forthcoming block. This block is then passed to a miner or validator for their attestation. The competition between miners and validators is a critical aspect of this supply chain, providing essential guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to consolidate into a single, vertically integrated entity, a situation known as MEV Dystopia. In this scenario, one entity would be responsible for creating all blocks on public blockchains, resulting in a loss of censorship resistance and the establishment of a monopoly capable of imposing arbitrarily high fees on users.\n\nHistorically, the roles of the builder and the miner or validator have been separate. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, giving them economic weight in the fork choice rule.\n\nThe introduction of MEV has added complexity to the building process. Builders now need to identify MEV transactions, create these transactions, potentially maintain a coin inventory, and manage associated risks. As we progress, it is expected that all parties will aim to move further up the supply chain, as those higher up have more power. However, this is a complex task, as some interests are already deeply rooted. The ultimate objective is to establish a system where it is economically sensible for actors to give up their order flow and participate in an open auction."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "The MEV supply chain is a complex process that involves several key players, each with a specific role in the successful execution of a blockchain transaction. It begins with a user initiating a transaction, which is then converted into a transaction by the user's digital wallet. Some transactions are collected or manipulated by entities known as searchers who bundle them, while others are not.\n\nThe next step involves a builder, who selects the transactions to be included in the upcoming block. This block is then handed over to a miner or validator for their approval. The competition between miners and validators is a crucial part of this supply chain, providing important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to consolidate into a single, vertically integrated entity, a situation referred to as MEV Dystopia. In this scenario, one entity would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the establishment of a monopoly capable of imposing arbitrarily high fees on users.\n\nHistorically, the roles of the builder and the miner or validator have been separate. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, giving them economic weight in the fork choice rule.\n\nThe introduction of MEV has added complexity to the building process. Builders now need to identify MEV transactions, create these transactions, potentially maintain a coin inventory, and manage associated risks. As we move forward, it is expected that all parties will strive to move further up the supply chain, as those higher up have more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to establish a system where it is economically viable for actors to relinquish their order flow and participate in an open auction."
                    }
                ]
            }
        ]
    },
    {
        "event_type": "function_call end",
        "tool_output": "\nThe MEV supply chain is a complex process that involves several key players, each with a specific role in the successful execution of a blockchain transaction. It begins with a user initiating a transaction, which is then converted into a transaction by the user's digital wallet. Some transactions are collected or manipulated by entities known as searchers who bundle them, while others are not.\n\nThe next step involves a builder, who selects the transactions to be included in the upcoming block. This block is then handed over to a miner or validator for their approval. The competition between miners and validators is a crucial part of this supply chain, providing important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to consolidate into a single, vertically integrated entity, a situation referred to as MEV Dystopia. In this scenario, one entity would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the establishment of a monopoly capable of imposing arbitrarily high fees on users.\n\nHistorically, the roles of the builder and the miner or validator have been separate. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, giving them economic weight in the fork choice rule.\n\nThe introduction of MEV has added complexity to the building process. Builders now need to identify MEV transactions, create these transactions, potentially maintain a coin inventory, and manage associated risks. As we move forward, it is expected that all parties will strive to move further up the supply chain, as those higher up have more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to establish a system where it is economically viable for actors to relinquish their order flow and participate in an open auction.\n\nFetched based on the following sources: \n[Title]: SoK: Cross-Domain MEV, [Authors]: Conor McMenamin, , [Link]: http://arxiv.org/pdf/2308.04159v1, [Release date]: 2023-08-08\n[Title]: It\u2019s My MEV and I Want It Now - Tom Schmidt, [Channel name]: ETHGlobal, [Video Link]: https://www.youtube.com/watch?v=mHsU2hSNNcM, [Published date]: 2023-03-28\n[Title]: State of the MEV Union - Jon Charbonneau, [Channel name]: ETHGlobal, [Video Link]: https://www.youtube.com/watch?v=-STbGjZc_OU, [Published date]: 2023-03-28\n[Title]: Multi-block MEV, [Authors]: Johannes Rude Jensen, Victor von Wachter, Omri Ross, [Link]: http://arxiv.org/pdf/2303.04430v2, [Release date]: 2023-03-08\n[Title]: The threat of MEV centralization: an anatomy of the transaction supply chain - Hasu (Flashbots), [Channel name]: Flashbots, [Video Link]: https://www.youtube.com/watch?v=GmBqoBr6yl4, [Published date]: 2022-05-22\n[Title]: Why your blockchain needs an MEV solution - Hasu (Flashbots), [Channel name]: Flashbots, [Video Link]: https://www.youtube.com/watch?v=sYFuFLe9kp0, [Published date]: 2022-05-22\n[Title]: Flashbots Research Workshop #7: Cross-domain MEV AMA, [Channel name]: Flashbots, [Video Link]: https://www.youtube.com/watch?v=uR0Z0nWXeFU, [Published date]: 2021-12-23\n[Title]: MEV.WTF Summit, [Channel name]: ETHGlobal, [Video Link]: https://www.youtube.com/watch?v=s3nACF7uVZw, [Published date]: 2021-07-02\n"
    },
    {
        "event_type": "llm start",
        "retrieved_context": null,
        "previous_answer": null
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I can answer without using any more tools.\nAnswer: The Maximal Extractable Value (MEV) supply chain is a complex process involving several key players. It starts with a user initiating a transaction, which is then converted into a transaction by the user's digital wallet. Some transactions are collected or manipulated by entities known as searchers who bundle them, while others are not.\n\nThe next step involves a builder, who selects the transactions to be included in the upcoming block. This block is then handed over to a miner or validator for their approval. The competition between miners and validators is a crucial part of this supply chain, providing important guarantees such as censorship resistance and market power for users.\n\nHowever, there are concerns about the potential for the MEV supply chain to consolidate into a single, vertically integrated entity, a situation referred to as MEV Dystopia. In this scenario, one entity would be responsible for creating all blocks on public blockchains, leading to a loss of censorship resistance and the establishment of a monopoly capable of imposing arbitrarily high fees on users.\n\nHistorically, the roles of the builder and the miner or validator have been separate. Mining pools have traditionally acted as builders, selecting transactions and assembling them into blocks. Miners or validators then attest to these blocks, giving them economic weight in the fork choice rule.\n\nThe introduction of MEV has added complexity to the building process. Builders now need to identify MEV transactions, create these transactions, potentially maintain a coin inventory, and manage associated risks. As we move forward, it is expected that all parties will strive to move further up the supply chain, as those higher up have more power. However, this is a complex task, as some interests are already deeply entrenched. The ultimate goal is to establish a system where it is economically viable for actors to relinquish their order flow and participate in an open auction (\"SoK: Cross-Domain MEV\", \"It\u2019s My MEV and I Want It Now - Tom Schmidt\", \"State of the MEV Union - Jon Charbonneau\", \"Multi-block MEV\", \"The threat of MEV centralization: an anatomy of the transaction supply chain - Hasu (Flashbots)\", \"Why your blockchain needs an MEV solution - Hasu (Flashbots)\", \"Flashbots Research Workshop #7: Cross-domain MEV AMA\", \"MEV.WTF Summit\").",
        "subjective grade from 1 to 10": ""
    },
    {
        "event_type": "llm start",
        "model_params": {
            "model": "gpt-4-0613",
            "temperature": 0,
            "max_tokens": null,
            "additional_kwargs": {},
            "max_retries": 10,
            "api_type": "open_ai",
            "api_base": "https://api.openai.com/v1",
            "api_version": "",
            "class_type": "openai",
            "class_name": "openai_llm"
        },
        "user_raw_input": "What is ePBS?",
        "LLM_input": "What is ePBS?\n\nTo determine if you should take the action to use the query engine, use its description detailed below. Use the query engine rather than not and do not rely on your prior knowledge.\nThe query engine tool has access to research papers and \nYouTube videos about the following content: Maximal Extractable Value (MEV); loss-versus-rebalancing (LVR); blockchain intents; \nthe Single Unifying Auction for Value Expression (SUAVE); sequencers; transaction ordering, L1s, L2s, transaction ordering, order flow auctions, auction theory, mechanism design.\n\n"
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I need to use a tool to help me answer the question.\nAction: query_engine_tool\nAction Input: {\"input\": \"ePBS\"}",
        "additional_content": [
            {
                "function_call": [
                    {
                        "event_type": "templating start",
                        "instructions": "system: You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\nuser: Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_str": "document_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd PBS recognizes that not all protocol actors may be able to fulfill all the duties in a comparatively performant way and need external support for that to also keep the decentralization of the network stable.\n00:05:05.034 - 00:05:05.374, Speaker A: Yeah.\n00:05:05.412 - 00:05:55.470, Speaker E: And what I particularly like, and kind of why I picked out this quote is that it really hones in on PBS as philosophy. Right. And I think the PBS, the implementation on Ethereum faces a lot of criticism from different directions, all great arguments and concerns that we will also go into in this episode. But really I think the general idea behind it is one that is extremely sound and I think that all of you laid out here really well. So with this high level overview out of the way, I'd like to go a bit a couple of years back and hear from you PBS, as an idea. Where did it start? What is its history? How do we get from there to where we are today?\n00:05:55.620 - 00:07:04.446, Speaker D: I think historically the PBS marketplace was a little more explicit in the Mevgeth world before we had proof of stake merge. So essentially in that scenario, there was a few large mining pools that controlled a huge portion of the hash rate. Mevgeth was the ability like provided the ability for searchers to send bundles to those miners. The searchers were able to send bundles to the miner kind of without worry about the miner stealing them because since there were so few, the miner's reputation was worth more than stealing the contents of that bundle. So in that regard, the interaction between the searchers and the block producers was simpler because there were so many fewer block producers. And then I guess as the merge kind of approached, a lot of people were talking about PBS as a general approach. And I think even we're considering holding off on shipping the merge until we had some in protocol version of PBS that could be accompanying the merge.\n00:07:04.446 - 00:07:33.950, Speaker D: Hard fork. I think that was discarded in general because the merge was already a huge lift and adding more complexity to the software and to the spec was kind of just going to slow things down more than necessary. And so yeah, maybe I'll pass it over to Chris here as Flashbot stepped in and implemented Mevboost. And that was like the real first PBS instantiation out of protocol that we saw post merge.\n00:07:34.610 - 00:08:43.474, Speaker F: Yeah, I think about one year before the merge, stefan from flashboards posted the OG Math boost specification outlining how proposers could interact with an external blockbuilding network. And then work started in the background in the Dev Connect meeting on Math day in Amsterdam in 22, that was April 22, there was a finalization of all the APIs that were needed and from then on it was clear that everybody is shooting for the merge with PBS, with Mafboost PBS enabled. I think at this point it was fully unclear how permissioned or permissionless this whole thing will be and how this plays out. But it seemed inevitable that some form of this is going to ship. And we worked them through the summer to deliver a permissionless relay on the open source software that also other relay operators can run and had everything ready in time, just in time for the merge that included permissionless builder access.\n00:08:43.672 - 00:09:42.070, Speaker D: Yeah, and it might be worth just kind of running through Mevboost as a software for people who aren't familiar. So the idea of Mevboost is there's a third party actor here that facilitates the auction between the proposer and the builder. And the reason for that is the proposer needs to trust that the block that the builder produces is both valid and accurately pays them the amount that the builder promised. And the builders can't simply send those blocks to the proposer for them to verify that themselves because the proposer could just steal the mev from the block and in that way take away all the earning from the builder themselves. So the relay kind of sits in the middle. It facilitates this auction insofar as the builders send a bunch of blocks to the relay and the proposer commits to the highest paying of those blocks before they actually see the block contents. So that's an important feature here.\n00:09:42.070 - 00:10:18.210, Speaker D: And that kind of comes up as I think it'll probably come up as we think more broadly about EPBs designs, which is that proposers need to commit without seeing the contents of their block in order to protect the builders from the mev being stolen. So the current status quo, I guess, post merge, there was maybe like three or four relays running immediately and now I think we're up to like eight that facilitate most of the mev boost blocks. A bunch of builders are sending blocks to those relays and about 95% of validators are hooked up to one of those relays and using their connection to that relay to source their block production.\n00:10:19.030 - 00:10:22.946, Speaker E: Yeah, I guess I'm really a sucker for proof of work and kind of.\n00:10:22.968 - 00:10:23.794, Speaker A: The history of it.\n00:10:23.832 - 00:11:10.258, Speaker E: So I would add that in some ways there was even a form of proposal builder separation. Before MEB, GEF existed in the division of labor that existed between a mining pool operator and the workers. Because the way that it works is the mining pool operator would construct the block body and then they would hash the block header once and they would send it to the workers to hash it further. And that hash would then have the golden non so or not. Right. So you find a bunch of things here. You find block construction because there was only one party that had to do like all of the peering and the validation and the block construction and so on, and also had to invest into latency infrastructure, right.\n00:11:10.258 - 00:11:36.294, Speaker E: Being like having good propagation to other mining pools and to big exchanges and so on. And then you had the workers who did the actual work on the Encrypted block body. Right. So you also had this idea of the Comet Reveal scheme even back then. So it's funny how far back some of these ideas trace that we established PBS as a design philosophy.\n00:11:36.342 - 00:11:36.506, Speaker A: Right?\n\ndocument_type: youtube_video\ntitle: MEVday Paris: PBS and Beyond\nchannel_name: Frontier Research\nvideo_link: https://www.youtube.com/watch?v=Dk2tQA_Pxfo\nrelease_date: 2023-08-08\n\n00:15:46.608 - 00:16:09.140, Speaker C: Here, we wouldn't have as much transparency into the system. Right. So that's the thing where it's good to have like it gives us again insight into what's happening some and to the extent that exists, it doesn't form a lot of experiments around or like the ways we think about EPBs, just having all this data from relays and things. So it's good and yeah, it's good enough right now.\n00:16:10.710 - 00:16:46.800, Speaker E: Yeah. So I was going to add personally, as much as I don't like mu boost, I thought it was a very required step in the middle because without it we wouldn't understand all these different type of attacks that has happened in a few months. If we just go straight into Enshrine PBS, I think that would have been a bigger problem that we probably have to do some crazy hard work in the middle. So yeah, personally I'm very grateful of Medboos and because it basically allow us to learn so much.\n00:16:47.570 - 00:18:01.350, Speaker B: Yeah. I also wanted to add you asked about what's coming down the pipeline in the next few months. One thing that I've been working a lot on and thinking a lot about is kind of as we do this blue sky research around Enshrined PBS and kind of think about the top down approach of really fixing this at the protocol layer. We also have the advantage of having the relays already running and we have kind of this whole ecosystem in place. So we've been working on this thing called optimistic relaying where we can kind of mold the behavior of the relays to look to kind of both derisk some of the EPBs design aspects but also to make the relays perhaps more sustainable and just better at servicing the builders and the validators. So I guess the overall theme of the optimistic relay roadmap is to remove some of the roles from the relay and replace those roles with a collateral that kind of enforces the builder behavior at the relay level. So I think in the short term that's like the easiest path to both making relays more sustainable and derisking some of the EPBs stuff.\n00:18:01.350 - 00:18:06.630, Speaker B: But longer term I still hope that we go the EPBs route, but I'm biased.\n00:18:07.470 - 00:18:31.840, Speaker A: So there's a few other approaches to designing systems aside from PBS. Right. Like, there's the Pepsi that's sort of discussed quite a bit. And then there's also slot auctions that are sort of another sort of design category. I'd love to hear a bit of a discussion or debate around these different approaches and which ones are the ones that are worth exploring more than others.\n00:18:33.590 - 00:18:36.114, Speaker B: Yeah, I feel like we almost need Barnabay up here.\n00:18:36.232 - 00:18:37.890, Speaker A: Barnabay, you want to participate?\n00:18:41.350 - 00:19:15.440, Speaker B: Okay, well, one thing I would say about the slot versus block auctions, most of the EPBs designs we've been kind of toying around with would probably work with both. Right. So this payload timeliness committee thing, which we published recently, we don't necessarily need to bind the builder to a specific block. We could bind them to a specific slot and kind of have that design space open up. So I guess I don't see it so much as, like, an either or. I see it as they have some synergies, but I'll have Barnaby talk about Pepsi a little bit.\n00:19:23.280 - 00:19:27.250, Speaker D: Well, in the meantime, maybe I can say something. Try again.\n00:19:28.420 - 00:19:28.928, Speaker B: Okay.\n00:19:29.014 - 00:19:29.920, Speaker G: Yeah, it's working.\n00:19:29.990 - 00:19:30.770, Speaker D: Go ahead.\n00:19:31.300 - 00:20:04.664, Speaker G: Yeah. So I've been probably doing the most work budding PBS at the EF because I've had some concerns, notably around the questions that are discussed now, like this idea of enshrining the market structure, of having proposals and builders. That, to me, was pretty scary. I thought the whole block auction was too coarse. And we have results from Quintus, from the special mechanisms groups, guys who tell us that it might be the case. And so while I was thinking about it, actually, funny story. I read this at Home 2.0\n00:20:04.664 - 00:20:41.076, Speaker G: paper that Sam was the lead author on, and it kind of blew open my mind that there was a lot of things, like a much bigger design space than I thought to design this mechanism. And yeah, coming from this discussion, there was this idea of Pepsi which is enshrining this idea that validators can enter into commitments, but not enshrining the specific shape of this commitment. So not enshrining that it has to be the whole block. It has to be the way that the Ethereum protocol dictates. It like you can enter into any kind of contract that you want. I don't know that it's workable. There's a lot of trade offs with it.\n00:20:41.076 - 00:21:07.920, Speaker G: It's a much more opinionated version of the market, let's say. But I feel Cosmos is heading towards and with this idea of it's more adaptable, like, you can really have these activist solutions of designing yourself what your chain looks like. I don't know that it's fitting for Ethereum, but it really came from this place of saying, yeah, maybe the shape of EPC is just too coarse and we need something that's a little more flexible.\n00:21:12.180 - 00:22:09.510, Speaker D: There is a latent structure to ethereum block space that's starting to form. The kind of cr effort is in some ways a kind of modification or retraction of part of PBS and then the whole kind of roll up repricing is another kind of acknowledgement that okay, we have this class of transactions and we want them to behave in a certain way. So if you were to go down a Pepsi route, you might just kind of say that block space is going to be reserved or kind of delineated in a specific way instead of doing everything by market pricing.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAgain, she'll go look at a bunch of Barnabay's writings and presentations on this, of seeing a protocol and what the boundaries are is grace.\n01:26:33.542 - 01:26:39.100, Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n01:26:49.540 - 01:27:36.746, Speaker C: And that is like, the fundamental question that a lot of the researchers at the effort are doing on PBS right now is that question. And it's been interesting. There's definitely been, I feel like, a bit of a change. And this was a lot of what we spoke about at Vienna, particularly after ECC. So a lot of the reason to do enshrine PBS EPBs for a long time was thought of as, okay, we'll do EPBs and then the relays go away. That's kind of the reason to do it. And there's starting to be more, I would say, realization lately is that, okay, even if we do EPBs, relays probably stick around or something very much like them in a reduced role, I would say, from where they are today, where they're significantly less systemically important and less relied upon.\n01:27:36.746 - 01:28:51.686, Speaker C: And they provide less of an advantage, but where there is probably still"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "tool_output": "Context information is below.\n---------------------\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd PBS recognizes that not all protocol actors may be able to fulfill all the duties in a comparatively performant way and need external support for that to also keep the decentralization of the network stable.\n00:05:05.034 - 00:05:05.374, Speaker A: Yeah.\n00:05:05.412 - 00:05:55.470, Speaker E: And what I particularly like, and kind of why I picked out this quote is that it really hones in on PBS as philosophy. Right. And I think the PBS, the implementation on Ethereum faces a lot of criticism from different directions, all great arguments and concerns that we will also go into in this episode. But really I think the general idea behind it is one that is extremely sound and I think that all of you laid out here really well. So with this high level overview out of the way, I'd like to go a bit a couple of years back and hear from you PBS, as an idea. Where did it start? What is its history? How do we get from there to where we are today?\n00:05:55.620 - 00:07:04.446, Speaker D: I think historically the PBS marketplace was a little more explicit in the Mevgeth world before we had proof of stake merge. So essentially in that scenario, there was a few large mining pools that controlled a huge portion of the hash rate. Mevgeth was the ability like provided the ability for searchers to send bundles to those miners. The searchers were able to send bundles to the miner kind of without worry about the miner stealing them because since there were so few, the miner's reputation was worth more than stealing the contents of that bundle. So in that regard, the interaction between the searchers and the block producers was simpler because there were so many fewer block producers. And then I guess as the merge kind of approached, a lot of people were talking about PBS as a general approach. And I think even we're considering holding off on shipping the merge until we had some in protocol version of PBS that could be accompanying the merge.\n00:07:04.446 - 00:07:33.950, Speaker D: Hard fork. I think that was discarded in general because the merge was already a huge lift and adding more complexity to the software and to the spec was kind of just going to slow things down more than necessary. And so yeah, maybe I'll pass it over to Chris here as Flashbot stepped in and implemented Mevboost. And that was like the real first PBS instantiation out of protocol that we saw post merge.\n00:07:34.610 - 00:08:43.474, Speaker F: Yeah, I think about one year before the merge, stefan from flashboards posted the OG Math boost specification outlining how proposers could interact with an external blockbuilding network. And then work started in the background in the Dev Connect meeting on Math day in Amsterdam in 22, that was April 22, there was a finalization of all the APIs that were needed and from then on it was clear that everybody is shooting for the merge with PBS, with Mafboost PBS enabled. I think at this point it was fully unclear how permissioned or permissionless this whole thing will be and how this plays out. But it seemed inevitable that some form of this is going to ship. And we worked them through the summer to deliver a permissionless relay on the open source software that also other relay operators can run and had everything ready in time, just in time for the merge that included permissionless builder access.\n00:08:43.672 - 00:09:42.070, Speaker D: Yeah, and it might be worth just kind of running through Mevboost as a software for people who aren't familiar. So the idea of Mevboost is there's a third party actor here that facilitates the auction between the proposer and the builder. And the reason for that is the proposer needs to trust that the block that the builder produces is both valid and accurately pays them the amount that the builder promised. And the builders can't simply send those blocks to the proposer for them to verify that themselves because the proposer could just steal the mev from the block and in that way take away all the earning from the builder themselves. So the relay kind of sits in the middle. It facilitates this auction insofar as the builders send a bunch of blocks to the relay and the proposer commits to the highest paying of those blocks before they actually see the block contents. So that's an important feature here.\n00:09:42.070 - 00:10:18.210, Speaker D: And that kind of comes up as I think it'll probably come up as we think more broadly about EPBs designs, which is that proposers need to commit without seeing the contents of their block in order to protect the builders from the mev being stolen. So the current status quo, I guess, post merge, there was maybe like three or four relays running immediately and now I think we're up to like eight that facilitate most of the mev boost blocks. A bunch of builders are sending blocks to those relays and about 95% of validators are hooked up to one of those relays and using their connection to that relay to source their block production.\n00:10:19.030 - 00:10:22.946, Speaker E: Yeah, I guess I'm really a sucker for proof of work and kind of.\n00:10:22.968 - 00:10:23.794, Speaker A: The history of it.\n00:10:23.832 - 00:11:10.258, Speaker E: So I would add that in some ways there was even a form of proposal builder separation. Before MEB, GEF existed in the division of labor that existed between a mining pool operator and the workers. Because the way that it works is the mining pool operator would construct the block body and then they would hash the block header once and they would send it to the workers to hash it further. And that hash would then have the golden non so or not. Right. So you find a bunch of things here. You find block construction because there was only one party that had to do like all of the peering and the validation and the block construction and so on, and also had to invest into latency infrastructure, right.\n00:11:10.258 - 00:11:36.294, Speaker E: Being like having good propagation to other mining pools and to big exchanges and so on. And then you had the workers who did the actual work on the Encrypted block body. Right. So you also had this idea of the Comet Reveal scheme even back then. So it's funny how far back some of these ideas trace that we established PBS as a design philosophy.\n00:11:36.342 - 00:11:36.506, Speaker A: Right?\n\ndocument_type: youtube_video\ntitle: MEVday Paris: PBS and Beyond\nchannel_name: Frontier Research\nvideo_link: https://www.youtube.com/watch?v=Dk2tQA_Pxfo\nrelease_date: 2023-08-08\n\n00:15:46.608 - 00:16:09.140, Speaker C: Here, we wouldn't have as much transparency into the system. Right. So that's the thing where it's good to have like it gives us again insight into what's happening some and to the extent that exists, it doesn't form a lot of experiments around or like the ways we think about EPBs, just having all this data from relays and things. So it's good and yeah, it's good enough right now.\n00:16:10.710 - 00:16:46.800, Speaker E: Yeah. So I was going to add personally, as much as I don't like mu boost, I thought it was a very required step in the middle because without it we wouldn't understand all these different type of attacks that has happened in a few months. If we just go straight into Enshrine PBS, I think that would have been a bigger problem that we probably have to do some crazy hard work in the middle. So yeah, personally I'm very grateful of Medboos and because it basically allow us to learn so much.\n00:16:47.570 - 00:18:01.350, Speaker B: Yeah. I also wanted to add you asked about what's coming down the pipeline in the next few months. One thing that I've been working a lot on and thinking a lot about is kind of as we do this blue sky research around Enshrined PBS and kind of think about the top down approach of really fixing this at the protocol layer. We also have the advantage of having the relays already running and we have kind of this whole ecosystem in place. So we've been working on this thing called optimistic relaying where we can kind of mold the behavior of the relays to look to kind of both derisk some of the EPBs design aspects but also to make the relays perhaps more sustainable and just better at servicing the builders and the validators. So I guess the overall theme of the optimistic relay roadmap is to remove some of the roles from the relay and replace those roles with a collateral that kind of enforces the builder behavior at the relay level. So I think in the short term that's like the easiest path to both making relays more sustainable and derisking some of the EPBs stuff.\n00:18:01.350 - 00:18:06.630, Speaker B: But longer term I still hope that we go the EPBs route, but I'm biased.\n00:18:07.470 - 00:18:31.840, Speaker A: So there's a few other approaches to designing systems aside from PBS. Right. Like, there's the Pepsi that's sort of discussed quite a bit. And then there's also slot auctions that are sort of another sort of design category. I'd love to hear a bit of a discussion or debate around these different approaches and which ones are the ones that are worth exploring more than others.\n00:18:33.590 - 00:18:36.114, Speaker B: Yeah, I feel like we almost need Barnabay up here.\n00:18:36.232 - 00:18:37.890, Speaker A: Barnabay, you want to participate?\n00:18:41.350 - 00:19:15.440, Speaker B: Okay, well, one thing I would say about the slot versus block auctions, most of the EPBs designs we've been kind of toying around with would probably work with both. Right. So this payload timeliness committee thing, which we published recently, we don't necessarily need to bind the builder to a specific block. We could bind them to a specific slot and kind of have that design space open up. So I guess I don't see it so much as, like, an either or. I see it as they have some synergies, but I'll have Barnaby talk about Pepsi a little bit.\n00:19:23.280 - 00:19:27.250, Speaker D: Well, in the meantime, maybe I can say something. Try again.\n00:19:28.420 - 00:19:28.928, Speaker B: Okay.\n00:19:29.014 - 00:19:29.920, Speaker G: Yeah, it's working.\n00:19:29.990 - 00:19:30.770, Speaker D: Go ahead.\n00:19:31.300 - 00:20:04.664, Speaker G: Yeah. So I've been probably doing the most work budding PBS at the EF because I've had some concerns, notably around the questions that are discussed now, like this idea of enshrining the market structure, of having proposals and builders. That, to me, was pretty scary. I thought the whole block auction was too coarse. And we have results from Quintus, from the special mechanisms groups, guys who tell us that it might be the case. And so while I was thinking about it, actually, funny story. I read this at Home 2.0\n00:20:04.664 - 00:20:41.076, Speaker G: paper that Sam was the lead author on, and it kind of blew open my mind that there was a lot of things, like a much bigger design space than I thought to design this mechanism. And yeah, coming from this discussion, there was this idea of Pepsi which is enshrining this idea that validators can enter into commitments, but not enshrining the specific shape of this commitment. So not enshrining that it has to be the whole block. It has to be the way that the Ethereum protocol dictates. It like you can enter into any kind of contract that you want. I don't know that it's workable. There's a lot of trade offs with it.\n00:20:41.076 - 00:21:07.920, Speaker G: It's a much more opinionated version of the market, let's say. But I feel Cosmos is heading towards and with this idea of it's more adaptable, like, you can really have these activist solutions of designing yourself what your chain looks like. I don't know that it's fitting for Ethereum, but it really came from this place of saying, yeah, maybe the shape of EPC is just too coarse and we need something that's a little more flexible.\n00:21:12.180 - 00:22:09.510, Speaker D: There is a latent structure to ethereum block space that's starting to form. The kind of cr effort is in some ways a kind of modification or retraction of part of PBS and then the whole kind of roll up repricing is another kind of acknowledgement that okay, we have this class of transactions and we want them to behave in a certain way. So if you were to go down a Pepsi route, you might just kind of say that block space is going to be reserved or kind of delineated in a specific way instead of doing everything by market pricing.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAgain, she'll go look at a bunch of Barnabay's writings and presentations on this, of seeing a protocol and what the boundaries are is grace.\n01:26:33.542 - 01:26:39.100, Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n01:26:49.540 - 01:27:36.746, Speaker C: And that is like, the fundamental question that a lot of the researchers at the effort are doing on PBS right now is that question. And it's been interesting. There's definitely been, I feel like, a bit of a change. And this was a lot of what we spoke about at Vienna, particularly after ECC. So a lot of the reason to do enshrine PBS EPBs for a long time was thought of as, okay, we'll do EPBs and then the relays go away. That's kind of the reason to do it. And there's starting to be more, I would say, realization lately is that, okay, even if we do EPBs, relays probably stick around or something very much like them in a reduced role, I would say, from where they are today, where they're significantly less systemically important and less relied upon.\n01:27:36.746 - 01:28:51.686, Speaker C: And they provide less of an advantage, but where there is probably still\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Always provide an exhaustive answer to the question, unless told otherwise in the question itself.\nDirectly quote the sources of your knowledge in the same sentence in parentheses. If several files are matched across several years of release dates, favor most recent content. Now answer the question: What is ePBS?\nAnswer: "
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation (ePBS) is a concept that is being researched in the context of blockchain protocols. It involves fixing certain aspects of the protocol layer to improve its functionality. The idea behind ePBS is to enshrine the market structure, having proposals and builders, into the protocol itself. This approach is considered to be more opinionated and less flexible compared to other designs, but it allows validators to enter into commitments without dictating the specific shape of these commitments. However, there's a growing realization that even with the implementation of ePBS, relays or similar structures might still be necessary, albeit in a reduced role."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n01:26:49.540 - 01:27:36.746, Speaker C: And that is like, the fundamental question that a lot of the researchers at the effort are doing on PBS right now is that question. And it's been interesting. There's definitely been, I feel like, a bit of a change. And this was a lot of what we spoke about at Vienna, particularly after ECC. So a lot of the reason to do enshrine PBS EPBs for a long time was thought of as, okay, we'll do EPBs and then the relays go away. That's kind of the reason to do it. And there's starting to be more, I would say, realization lately is that, okay, even if we do EPBs, relays probably stick around or something very much like them in a reduced role, I would say, from where they are today, where they're significantly less systemically important and less relied upon.\n01:27:36.746 - 01:28:51.686, Speaker C: And they provide less of an advantage, but where there is probably still an incentive to use some sort of out of protocol solutions that are probably more optimal than using the Enshrined PBS protocol. So some of the simple examples are even if we do this Enshrined PBS, where there is this canonical P to P pool where this is where the bids are and this is where you're supposed to listen to what are some advantages that some sort of out of protocol actor like a relay could still potentially provide you. So a couple of the simple ones that seem to be pretty important are one of the really simple ones is just flexible payments of, like, the way that you would do the payments in this kind of epps world. Would be like the main idea is probably to do something called tob Top of block payments, where I would be able to, as a builder, send you a bid that even if I don't give you the block body, you could take the payment. So that works well in most cases. There are certain times where you would want more flexible payments of, let's say this is like a gigantic mev block where I'm going to get 1000 ETH in the block or whatever and I'm only going to be able to give you the bid for that 1000 ETH after the execution payload. So I can't send you the 1000 ETH in the top of block payment because I actually don't have it yet.\n01:28:51.686 - 01:28:58.470, Speaker C: The only way I can send it to you is you need a check at the end of the block like, hey, I made the money and I can actually send it to you.\n01:28:58.540 - 01:28:59.810, Speaker B: So that's a service that really the.\n01:28:59.820 - 01:29:06.922, Speaker A: Relay is fronting the money but only atomically for the relay. It's trustless. Right? But that is something that the protocol cannot do.\n01:29:06.976 - 01:29:53.466, Speaker C: Yeah, they're effectively guaranteeing to the proposer like, hey, don't worry, the builder is good for this the block, they definitely capture it, we're going to pay it to at the end. And so that is one scenario where it is still potentially useful to have some sort of third party who's mediating this fair exchange between the proposer and the builders. That may be more of like an edge case. One I'd say the more pointed ones are specifically, like through the bidding process of cancellations is one where a lot of these in particular, like the Sex Decks arbitrager builders, they will be continuously updating their bids throughout the sought. And there are times where they will potentially want to cancel their bids at certain times because prices moved off chain and I need to lower my bid. Actually. And so you can't cancel if you broadcast something to a P to P like public mempool, there's no way to do that.\n01:29:53.466 - 01:30:51.174, Speaker C: But a relay can do that. We just have a limitation that like, hey, as a proposer you can only call, get header once, so they'll call it at the end of the slot and I cancel before then. Could also do private auctions which is potentially helpful for some builders who don't want to reveal everything. And then the last thing is just like simple latency of relays are probably going to be like some latency optimization services. Probably going to be able to get a faster connection between if they're absolutely optimized between the builder and the proposer as opposed to sending it just to the main P to PMEM pool. So it's very possible that you would be able to get your bid slightly later towards the end if you're using the relay as opposed to the P to P mempool. So it gives you these on the margin optimizations and that becomes like the fundamental question of is this even the relay that we think of it as it is today? I was going to ask you exactly and it's like is it the relay or is it not? It almost is a different role.\n01:30:51.174 - 01:31:39.082, Speaker C: It is just almost like a latency optimizer, whatever you want to call it. It's not a fundamental role that it's needed anymore to just mediate the fair exchange between the proposers and the builders. And that is the interesting difference is today basically if the relays go down today, the whole PBS thing doesn't work really. There is no interface between the builders and the proposers in this world. If the relays go down, okay, maybe the latency at the end of the slot is slightly suboptimal, and there are times where you can't cancel bids. They're optimizations, but it's not like PBS doesn't fundamentally work well, and it's like, okay, you got to build a box locally now, so it's a very large delta and they're sort of like an optimization service at that point, as opposed to this is like a fundamental role in the middle of this thing and it doesn't work without them. So it is a very different kind of point there.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nRight. PBS really is a design philosophy that is in itself extremely broad.\n01:22:01.822 - 01:22:02.082, Speaker E: Right.\n01:22:02.136 - 01:23:30.590, Speaker A: All it really says is there are incentives for division of labor in the protocol, or, like, framing it differently for protocol actors to outsource part of their duties to external actors who might be more specialized. And then those are explicitly not in the protocol. But what the protocol can do is provide as expressive and as trustless as possible interface as it can to make it so that this outsourcing really becomes as easy, like, as fair and as egalitarian as possible. Because if it doesn't, then what you see is some protocol actors might be better at outsourcing than others. And this is kind of what we saw initially with MEB in kind of pre proposal builder separation days, right. Where there wasn't such a trustless interface and, like, a way for validators or mining pools to really discover, okay, so who are the searchers I should be working with? And now the builders and so on. Just, like, zooming out, basically, and looking at this entire thing as a design philosophy that's really strongly rooted in kind of fairness and decentralization of the protocol that was, for me, I would say, the highlight.\n01:23:31.010 - 01:23:33.810, Speaker C: Yeah, I like that.\n01:23:33.880 - 01:23:34.210, Speaker F: Yeah.\n01:23:34.280 - 01:23:44.798, Speaker C: And it's definitely been really interesting to see. I've noticed this more over the past several months, particularly as Pepsi has gotten a bit more attention.\n01:23:44.974 - 01:23:47.666, Speaker B: Is it's a bit what we talked.\n01:23:47.688 - 01:24:23.760, Speaker C: About in the episode of where a lot of these ideas that are almost thought about as opposites of each other, of the ethereum PBS, and then there's, like, the Cosmos Protocol I'm building or the more opinionated things, you start to realize when you start to do, the more soul searching of, like, okay, fundamentally, what are these things? And you look at things like Pepsi, and you realize how many parallels actually across those different systems there are, and, hey, they actually work really well together. It's not like this one or this. They very much do fit together in these different ways, and they look very different in different ecosystems when you have different goals. But yeah, watching how the pieces actually fit together now and it's like you just approach it from different ends has been very cool.\n01:24:24.130 - 01:24:24.542, Speaker A: Yeah.\n01:24:24.596 - 01:24:25.646, Speaker E: I have to give you a shout.\n01:24:25.678 - 01:24:42.230, Speaker A: Out, I think, especially for that with your efforts around proof of governance, which is really I think what you're doing very effectively is just removing politics and ideology from what should really be kind of a technical subject matter.\n01:24:42.300 - 01:24:42.920, Speaker E: Right.\n01:24:44.170 - 01:25:01.654, Speaker A: Just because it's Ethereum, the Ethereum ecosystem and Ethereum on layer one has PBS doesn't mean that the exact same implementation should also work for or should also be the right one for layer twos, which have totally different kind of needs and goals and constraints.\n01:25:01.702 - 01:25:02.154, Speaker E: Right.\n01:25:02.272 - 01:25:40.858, Speaker A: So it's really about taking the politics out of it and approaching it from first principles and really seeing, well, these are all part of the same kind of design. Family and different implementations work best under different conditions, and they are all fair game. It doesn't matter where they were invented. If something was invented in Cosmos or whether it was invented by the Ethereum Foundation or was invented by Flashpots, we are here to kind of build the best crypto ecosystem that we can. This is something that I see very heavily in your research.\n01:25:41.024 - 01:25:42.442, Speaker C: Yeah. Appreciate it.\n01:25:42.576 - 01:25:57.206, Speaker A: One thing that you pointed out to me that we didn't talk about much in the episode was the question whether to enshrine proposal builder separation or not in Ethereum. How do you think about yeah, yeah, it was weird.\n01:25:57.238 - 01:25:58.446, Speaker B: I felt bad that we didn't cover this.\n01:25:58.468 - 01:26:00.366, Speaker C: I feel like it was the most obvious thing for us to cover.\n01:26:00.388 - 01:26:01.614, Speaker B: And it was also, like, right after.\n01:26:01.652 - 01:26:04.974, Speaker C: Mike wrote the post, too, on a lot of this stuff.\n01:26:05.092 - 01:26:05.374, Speaker D: Yeah.\n01:26:05.412 - 01:26:33.414, Speaker C: So, I mean, like, this is a lot of the interesting kind of like it's really the core question for PBS, but also so many other things tangential to the protocol right now, like PBS, restaking, Pepsi, a lot of them kind of touch different areas where it's like, what is that boundary of the protocol? Again, she'll go look at a bunch of Barnabay's writings and presentations on this, of seeing a protocol and what the boundaries are is grace.\n01:26:33.542 - 01:26:39.100, Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nSo I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n01:26:49.540 - 01:27:36.746, Speaker C: And that is like, the fundamental question that a lot of the researchers at the effort are doing on PBS right now is that question. And it's been interesting. There's definitely been, I feel like, a bit of a change. And this was a lot of what we spoke about at Vienna, particularly after ECC. So a lot of the reason to do enshrine PBS EPBs for a long time was thought of as, okay, we'll do EPBs and then the relays go away. That's kind of the reason to do it. And there's starting to be more, I would say, realization lately is that, okay, even if we do EPBs, relays probably stick around or something very much like them in a reduced role, I would say, from where they are today, where they're significantly less systemically important and less relied upon.\n01:27:36.746 - 01:28:51.686, Speaker C: And they provide less of an advantage, but where there is probably still an incentive to use some sort of out of protocol solutions that are probably more optimal than using the Enshrined PBS protocol. So some of the simple examples are even if we do this Enshrined PBS, where there is this canonical P to P pool where this is where the bids are and this is where you're supposed to listen to what are some advantages that some sort of out of protocol actor like a relay could still potentially provide you. So a couple of the simple ones that seem to be pretty important are one of the really simple ones is just flexible payments of, like, the way that you would do the payments in this kind of epps world. Would be like the main idea is probably to do something called tob Top of block payments, where I would be able to, as a builder, send you a bid that even if I don't give you the block body, you could take the payment. So that works well in most cases. There are certain times where you would want more flexible payments of, let's say this is like a gigantic mev block where I'm going to get 1000 ETH in the block or whatever and I'm only going to be able to give you the bid for that 1000 ETH after the execution payload. So I can't send you the 1000 ETH in the top of block payment because I actually don't have it yet.\n01:28:51.686 - 01:28:58.470, Speaker C: The only way I can send it to you is you need a check at the end of the block like, hey, I made the money and I can actually send it to you.\n01:28:58.540 - 01:28:59.810, Speaker B: So that's a service that really the.\n01:28:59.820 - 01:29:06.922, Speaker A: Relay is fronting the money but only atomically for the relay. It's trustless. Right? But that is something that the protocol cannot do.\n01:29:06.976 - 01:29:53.466, Speaker C: Yeah, they're effectively guaranteeing to the proposer like, hey, don't worry, the builder is good for this the block, they definitely capture it, we're going to pay it to at the end. And so that is one scenario where it is still potentially useful to have some sort of third party who's mediating this fair exchange between the proposer and the builders. That may be more of like an edge case. One I'd say the more pointed ones are specifically, like through the bidding process of cancellations is one where a lot of these in particular, like the Sex Decks arbitrager builders, they will be continuously updating their bids throughout the sought. And there are times where they will potentially want to cancel their bids at certain times because prices moved off chain and I need to lower my bid. Actually. And so you can't cancel if you broadcast something to a P to P like public mempool, there's no way to do that.\n01:29:53.466 - 01:30:51.174, Speaker C: But a relay can do that. We just have a limitation that like, hey, as a proposer you can only call, get header once, so they'll call it at the end of the slot and I cancel before then. Could also do private auctions which is potentially helpful for some builders who don't want to reveal everything. And then the last thing is just like simple latency of relays are probably going to be like some latency optimization services. Probably going to be able to get a faster connection between if they're absolutely optimized between the builder and the proposer as opposed to sending it just to the main P to PMEM pool. So it's very possible that you would be able to get your bid slightly later towards the end if you're using the relay as opposed to the P to P mempool. So it gives you these on the margin optimizations and that becomes like the fundamental question of is this even the relay that we think of it as it is today? I was going to ask you exactly and it's like is it the relay or is it not? It almost is a different role.\n01:30:51.174 - 01:31:39.082, Speaker C: It is just almost like a latency optimizer, whatever you want to call it. It's not a fundamental role that it's needed anymore to just mediate the fair exchange between the proposers and the builders. And that is the interesting difference is today basically if the relays go down today, the whole PBS thing doesn't work really. There is no interface between the builders and the proposers in this world. If the relays go down, okay, maybe the latency at the end of the slot is slightly suboptimal, and there are times where you can't cancel bids. They're optimizations, but it's not like PBS doesn't fundamentally work well, and it's like, okay, you got to build a box locally now, so it's a very large delta and they're sort of like an optimization service at that point, as opposed to this is like a fundamental role in the middle of this thing and it doesn't work without them. So it is a very different kind of point there.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nRight. PBS really is a design philosophy that is in itself extremely broad.\n01:22:01.822 - 01:22:02.082, Speaker E: Right.\n01:22:02.136 - 01:23:30.590, Speaker A: All it really says is there are incentives for division of labor in the protocol, or, like, framing it differently for protocol actors to outsource part of their duties to external actors who might be more specialized. And then those are explicitly not in the protocol. But what the protocol can do is provide as expressive and as trustless as possible interface as it can to make it so that this outsourcing really becomes as easy, like, as fair and as egalitarian as possible. Because if it doesn't, then what you see is some protocol actors might be better at outsourcing than others. And this is kind of what we saw initially with MEB in kind of pre proposal builder separation days, right. Where there wasn't such a trustless interface and, like, a way for validators or mining pools to really discover, okay, so who are the searchers I should be working with? And now the builders and so on. Just, like, zooming out, basically, and looking at this entire thing as a design philosophy that's really strongly rooted in kind of fairness and decentralization of the protocol that was, for me, I would say, the highlight.\n01:23:31.010 - 01:23:33.810, Speaker C: Yeah, I like that.\n01:23:33.880 - 01:23:34.210, Speaker F: Yeah.\n01:23:34.280 - 01:23:44.798, Speaker C: And it's definitely been really interesting to see. I've noticed this more over the past several months, particularly as Pepsi has gotten a bit more attention.\n01:23:44.974 - 01:23:47.666, Speaker B: Is it's a bit what we talked.\n01:23:47.688 - 01:24:23.760, Speaker C: About in the episode of where a lot of these ideas that are almost thought about as opposites of each other, of the ethereum PBS, and then there's, like, the Cosmos Protocol I'm building or the more opinionated things, you start to realize when you start to do, the more soul searching of, like, okay, fundamentally, what are these things? And you look at things like Pepsi, and you realize how many parallels actually across those different systems there are, and, hey, they actually work really well together. It's not like this one or this. They very much do fit together in these different ways, and they look very different in different ecosystems when you have different goals. But yeah, watching how the pieces actually fit together now and it's like you just approach it from different ends has been very cool.\n01:24:24.130 - 01:24:24.542, Speaker A: Yeah.\n01:24:24.596 - 01:24:25.646, Speaker E: I have to give you a shout.\n01:24:25.678 - 01:24:42.230, Speaker A: Out, I think, especially for that with your efforts around proof of governance, which is really I think what you're doing very effectively is just removing politics and ideology from what should really be kind of a technical subject matter.\n01:24:42.300 - 01:24:42.920, Speaker E: Right.\n01:24:44.170 - 01:25:01.654, Speaker A: Just because it's Ethereum, the Ethereum ecosystem and Ethereum on layer one has PBS doesn't mean that the exact same implementation should also work for or should also be the right one for layer twos, which have totally different kind of needs and goals and constraints.\n01:25:01.702 - 01:25:02.154, Speaker E: Right.\n01:25:02.272 - 01:25:40.858, Speaker A: So it's really about taking the politics out of it and approaching it from first principles and really seeing, well, these are all part of the same kind of design. Family and different implementations work best under different conditions, and they are all fair game. It doesn't matter where they were invented. If something was invented in Cosmos or whether it was invented by the Ethereum Foundation or was invented by Flashpots, we are here to kind of build the best crypto ecosystem that we can. This is something that I see very heavily in your research.\n01:25:41.024 - 01:25:42.442, Speaker C: Yeah. Appreciate it.\n01:25:42.576 - 01:25:57.206, Speaker A: One thing that you pointed out to me that we didn't talk about much in the episode was the question whether to enshrine proposal builder separation or not in Ethereum. How do you think about yeah, yeah, it was weird.\n01:25:57.238 - 01:25:58.446, Speaker B: I felt bad that we didn't cover this.\n01:25:58.468 - 01:26:00.366, Speaker C: I feel like it was the most obvious thing for us to cover.\n01:26:00.388 - 01:26:01.614, Speaker B: And it was also, like, right after.\n01:26:01.652 - 01:26:04.974, Speaker C: Mike wrote the post, too, on a lot of this stuff.\n01:26:05.092 - 01:26:05.374, Speaker D: Yeah.\n01:26:05.412 - 01:26:33.414, Speaker C: So, I mean, like, this is a lot of the interesting kind of like it's really the core question for PBS, but also so many other things tangential to the protocol right now, like PBS, restaking, Pepsi, a lot of them kind of touch different areas where it's like, what is that boundary of the protocol? Again, she'll go look at a bunch of Barnabay's writings and presentations on this, of seeing a protocol and what the boundaries are is grace.\n01:26:33.542 - 01:26:39.100, Speaker A: We should just call the episode The Ghost of Barnaby pretty much like the Ghost of Christmas Past or something.\n01:26:39.470 - 01:26:48.346, Speaker C: We're quoting him for half of it. But yeah, that is a lot of what it is, is what is fundamentally kind of the protocol's boundary, what is its role, what should be in protocol.\n01:26:48.378 - 01:26:49.390, Speaker A: What should be out?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nSo I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to",
                        "previous_answer": "Enshrined Proposal Builder Separation (ePBS) is a concept that is being researched in the context of blockchain protocols. It involves fixing certain aspects of the protocol layer to improve its functionality. The idea behind ePBS is to enshrine the market structure, having proposals and builders, into the protocol itself. This approach is considered to be more opinionated and less flexible compared to other designs, but it allows validators to enter into commitments without dictating the specific shape of these commitments. However, there's a growing realization that even with the implementation of ePBS, relays or similar structures might still be necessary, albeit in a reduced role."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation (ePBS) is a concept in blockchain protocols that involves embedding the market structure, specifically the roles of proposals and builders, directly into the protocol layer. This approach is seen as more prescriptive and less adaptable compared to other designs, but it enables validators to make commitments without prescribing the exact form of these commitments. However, there's an increasing understanding that even with the implementation of ePBS, relays or similar entities might still be needed, although in a diminished capacity."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to be outsourcing block production at that point because it's supposed to be at least deterministic of like this is exactly the block that you should be outputting out from this. The reality is you're not going to be able to enshrine something that prescriptive in Ethereum generally. And so if you assume that there are going to be decentralized participants within the validator set and they're going to have some amount of agency to propose a different block, the natural result of that is there are going to be different people in the world who have a better block at different times, and there's going to be an economic incentive for them to kind of outsource that production at"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to be outsourcing block production at that point because it's supposed to be at least deterministic of like this is exactly the block that you should be outputting out from this. The reality is you're not going to be able to enshrine something that prescriptive in Ethereum generally. And so if you assume that there are going to be decentralized participants within the validator set and they're going to have some amount of agency to propose a different block, the natural result of that is there are going to be different people in the world who have a better block at different times, and there's going to be an economic incentive for them to kind of outsource that production at",
                        "previous_answer": "Enshrined Proposal Builder Separation (ePBS) is a concept that is being researched in the context of blockchain protocols. It involves fixing certain aspects of the protocol layer to improve its functionality. The idea behind ePBS is to enshrine the market structure, having proposals and builders, into the protocol itself. This approach is considered to be more opinionated and less flexible compared to other designs, but it allows validators to enter into commitments without dictating the specific shape of these commitments. However, there's a growing realization that even with the implementation of ePBS, relays or similar structures might still be necessary, albeit in a reduced role."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, or ePBS, is a concept under exploration in the realm of blockchain protocols. It aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. The ePBS concept proposes to embed the market structure, consisting of proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less adaptable than other models, it enables validators to make commitments without prescribing the exact form of these commitments. Despite the introduction of ePBS, there's an increasing understanding that structures like relays may still be required, though their role may be diminished."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to be outsourcing block production at that point because it's supposed to be at least deterministic of like this is exactly the block that you should be outputting out from this. The reality is you're not going to be able to enshrine something that prescriptive in Ethereum generally. And so if you assume that there are going to be decentralized participants within the validator set and they're going to have some amount of agency to propose a different block, the natural result of that is there are going to be different people in the world who have a better block at different times, and there's going to be an economic incentive for them to kind of outsource that production at different times. So I don't really think that there's an alternative to PBS to any meaningful extent, like within Ethereum, given a lot of the design constraints that it gives itself for what it's optimizing for.\n00:35:50.832 - 00:35:51.846, Speaker E: And outside of ethereum.\n00:35:51.878 - 00:36:58.194, Speaker B: Outside of Ethereum, I think that you can argue that there are credible alternatives and the credible alternatives are very opinionated and very app specific. And so those you can say that you don't need to outsource to this arbitrary market because we know for our application very specifically, this is the transaction ordering that is going to be welfare maximizing for what we want to achieve. And so we can ingrain very specifically this is the transaction ordering that must result potentially difficult to achieve that. But you can incredibly have a mechanism that works pretty well where I don't think it's just even reasonably viable at all to do something like that on ethereum, which is incredibly opinionated, which is incredibly constrained. I think you can make a credible argument for that in certain app specific use cases. But the thing is, even in the app specific use cases, I think that the reality is it is still a spectrum on how much are you constraining what you're doing. And so one of the things that I feel like is sometimes seen as an alternative to PBS is what's called protocol owned building.\n00:36:58.194 - 00:37:46.462, Speaker B: So this is something that is more popular in the Cosmos context with the Skip guys we're working on where we have these app specific chains and so they have this notion of protocol owned building which is you have certain consensus rules that enforce certain validity conditions upon the blocks. So we have it as part of our consensus in a chain like Osmosis that after these trades we check if there's an arbitrage. If there is an arbitrage, it is baked into consensus that cyclical arbitrage is automatically closed and the funds are distributed how we agreed upon in consensus. There's no way around that. But the thing is, while that is constraining what you are allowed to build as a block, there are still degrees of freedom within that. So there is still flexibility within that. So you can constrain the search space with something like protocol I'm building.\n00:37:46.462 - 00:38:53.058, Speaker B: But depending on how much you constrain the search space, if there are still degrees of freedom, which there very well may very well be, you can still outsource block production, so you can have protocol and building where you have certain validity conditions that are enforced, but the validator can still outsource to some other builder to build according to those rules. And that's kind of what I was getting back to before when I was mentioning Pepsi briefly with Chris is that you don't necessarily have to. Just because you have more constraints on the proposer, that doesn't necessarily mean that there is no more freedom left or that they have to do it themselves. Pepsi is a similar idea of pepsi is a way for proposers to constrain the allowable space of what kind of block it can propose in much the same way that protocol on building does. The difference is more that protocol on building takes the very kind of Cosmos approach know tap specific and we can reasonably know for our given application this is the right way to constrain the search space of allowable blocks. That is relatively welfare optimizing. So like every validator has to go by that commitment.\n00:38:53.058 - 00:39:50.650, Speaker B: Whereas Pepsi is kind of the ethereum variation of that, where we can't say that because ethereum is very general purpose, it is optimizing for very different guarantees. And so you have to allow proposers to be able to locally make those constraints and those commitments which are very analogous to what protocol and building wants to do, but in a very generic context of constraining. What is the block that I'm going to output? Kind of at the end of the day, a lot of things are viewed as alternatives to PBS. And I think that is kind of one of the things that I try to hammer up more is PBS isn't just supposed to refer to this is the concrete implementation that we see on Ethereum today. It is just like the acknowledgment of there is probably going to be a separation between different actors and there is a spectrum of what that separation is and how much we constrain what those different actors can do. And I think that we're starting to see that increasingly across different ecosystems. PBS really is a spectrum of what kind of constraints are you putting on different people and what is the interaction between them.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\n00:55:43.500 - 00:55:48.730, Speaker D: You have to enforce that if the builder reveals their payload it becomes canonical.\n00:55:49.310 - 00:55:53.498, Speaker F: And if it doesn't reveal the payload that the payment is still executed.\n00:55:53.594 - 00:55:54.560, Speaker D: Right, exactly.\n00:55:55.090 - 00:56:38.826, Speaker E: Okay, so EPBs is one way that PBS is going to evolve, as we have heard. Another angle is all of the rollups are looking to decentralize their sequencer in some way. So we'll talk about what that means exactly, because different people can have wildly different opinions but one of the things that they are kind of looking at is PBS. But really it's part of a much broader design spectrum than you have on the layer one. So John, can you kind of walk us through to what degree do we need at all some form of PBS on layer two and how are these different teams thinking about it?\n00:56:39.008 - 00:57:28.038, Speaker B: Yeah, so I would say broadly they have a lot more flexibility in their designs is the very TLDR of it, where Ethereum kind of as I was mentioned before, has this very strict set of constraints where it's like we want to be very generalized, unappinionated, super, permissionless, all of those conditions. It makes it much harder to optimize for. And the reality is roll ups are going to have a lot more degrees of flexibility there. So they don't need to have necessarily a gigantic permissionless set of sequencers. They can have potentially one or a handful or some permission set of them. And that just like it makes it much easier to design the process. Like that interface between the proposers who is kind of like the sequencer more or less here, and some kind of out of protocol builder.\n00:57:28.038 - 00:57:49.618, Speaker B: So it makes it much easier if you kind of know who all the parties are and they're able to have some sort of trust interaction between them for proper execution and fulfilling their commitments. So that makes it a lot easier. And the other part of it is also they can be way more opinionated than Ethereum is going to be. So roll ups can play around with things like threshold encryption with some variations of first come, first serve, with a.\n00:57:49.624 - 00:57:51.726, Speaker C: Batch auction like Shin's proposal.\n00:57:51.918 - 00:57:56.018, Speaker B: There are going to be a lot of these different variations that are going to be more opinionated and people are.\n00:57:56.024 - 00:57:56.754, Speaker C: Going to try different things.\n00:57:56.792 - 00:58:13.046, Speaker B: It's going to be like basically the better analogy for them in large part is Cosmos compared to Ethereum roll ups are the Cosmos app chains of the Ethereum vision. Realistically they are not Ethereum itself. That is the whole point of kind of what I was going back to before of ethereum makes a certain set.\n00:58:13.068 - 00:58:14.678, Speaker C: Of tradeoffs that are very difficult to.\n00:58:14.684 - 00:58:28.974, Speaker B: Deal with, so that roll ups in large part do not have. To deal with those and they can optimize for another kind of end of the trade off us. In large part, though some form of PBS is likely going to rise be necessary in them.\n00:58:29.092 - 00:58:29.486, Speaker A: What that?\n00:58:29.508 - 00:58:31.854, Speaker B: Looks like Will. Look very different, but for those same.\n00:58:31.892 - 00:58:33.886, Speaker C: Reasons before, even when you constrain the.\n00:58:33.908 - 00:58:48.374, Speaker B: Search base of, you do certain things like protocol I'm building or you constrain certain ordering rules. There still are going to potentially be degrees of freedom that you want to outsource to a competitive market such that you are getting the best block that the Sequencers are going to put in there. At the end of the day that.\n00:58:48.412 - 00:58:58.566, Speaker E: Makes sense and another topic that we have touched already on in this call is Pepsi. So what is Pepsi and how does.\n00:58:58.588 - 00:59:00.390, Speaker A: It relate to PVs?\n00:59:00.830 - 01:00:03.854, Speaker D: Cool yeah so Pepsi is a proposal from Barnabay. It stands for Protocol enforced Proposer commitments and the idea here is that it kind of generalizes PBS insofar as expanding the set of commitments that a proposer can make that are enforced at the block validity level. So the idea is in this new design proposers can sign up for different block validity conditions that are applied to their block. And this is kind of often compared to the type of commitments that could be made through Eigen layer. But I think the important distinction is that Eigen layer commitments are only enforceable kind of at the execution layer meaning they're only enforceable by slashing the stake of the validator kind of after the fact if they don't fulfill the commitments that they made. Pepsi is kind of a stronger commitment or in. My mind kind of closer to the medal of Ethereum in that the commitments are actually part of the fork choice rule and part of the state transition function.\n01:00:03.854 - 01:01:31.042, Speaker D: So if a proposer commits to something and their block doesn't satisfy that constraint then it's not even able to be part of the blockchain because of the commitments that they made. So I like to think about the difference between EPBs and Pepsi as the difference between homogeneous and heterogeneous commitments that the proposer can make. So in EPBs we're saying we're going to specifically enshrine a single version of the mechanism that the proposer and builders participate in so that could be a full block auction. So the proposers can commit to a specific block hash the builder has to reveal a payload that corresponds to that block hash.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nIt doesn't even justify the cost and the additional risk of running out of particle software of maintaining this thing. It's just like forget it, the other thing works. 99% is good. I don't care about the last like five milliseconds at the end of this thing.\n01:35:01.434 - 01:35:12.110, Speaker C: And that exact delta does matter and it's unclear exactly what it is, but yeah, it is a very fundamentally different role as opposed to this is the central point that is holding up the whole PBS auction.\n01:35:12.690 - 01:36:05.540, Speaker A: Yeah, I agree. One more thing that I want to touch on is kind of I mean, I was kind of giving Mike a bit of a hard time asking him about different governance entities in Ethereum and like their power distribution, who maintains what what this means for the decentralization of the overall ecosystem. And yeah, Kudos, you gave a good answer. I still want to talk about this a bit more with Mean right now. It's pretty much the case, I think that the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?\n00:34:19.412 - 00:35:50.794, Speaker B: So in Ethereum, I don't know that there really is a meaningful alternative to PBS like in the specific Ethereum context because the way that the kind of broad directional alternative to PBS is just completely constraining what the proposer is allowed to do effectively. And you specify very concrete rules of like this is what you must follow. So like some of the fair ordering type, quote unquote fair ordering proposals where you're trying to say all the consensus participants enforce upon each other, this is the ordering that you must follow within this block. So to the extent that that happens, there really isn't room to be outsourcing block production at that point because it's supposed to be at least deterministic of like this is exactly the block that you should be outputting out from this. The reality is you're not going to be able to enshrine something that prescriptive in Ethereum generally. And so if you assume that there are going to be decentralized participants within the validator set and they're going to have some amount of agency to propose a different block, the natural result of that is there are going to be different people in the world who have a better block at different times, and there's going to be an economic incentive for them to kind of outsource that production at different times. So I don't really think that there's an alternative to PBS to any meaningful extent, like within Ethereum, given a lot of the design constraints that it gives itself for what it's optimizing for.\n00:35:50.832 - 00:35:51.846, Speaker E: And outside of ethereum.\n00:35:51.878 - 00:36:58.194, Speaker B: Outside of Ethereum, I think that you can argue that there are credible alternatives and the credible alternatives are very opinionated and very app specific. And so those you can say that you don't need to outsource to this arbitrary market because we know for our application very specifically, this is the transaction ordering that is going to be welfare maximizing for what we want to achieve. And so we can ingrain very specifically this is the transaction ordering that must result potentially difficult to achieve that. But you can incredibly have a mechanism that works pretty well where I don't think it's just even reasonably viable at all to do something like that on ethereum, which is incredibly opinionated, which is incredibly constrained. I think you can make a credible argument for that in certain app specific use cases. But the thing is, even in the app specific use cases, I think that the reality is it is still a spectrum on how much are you constraining what you're doing. And so one of the things that I feel like is sometimes seen as an alternative to PBS is what's called protocol owned building.\n00:36:58.194 - 00:37:46.462, Speaker B: So this is something that is more popular in the Cosmos context with the Skip guys we're working on where we have these app specific chains and so they have this notion of protocol owned building which is you have certain consensus rules that enforce certain validity conditions upon the blocks. So we have it as part of our consensus in a chain like Osmosis that after these trades we check if there's an arbitrage. If there is an arbitrage, it is baked into consensus that cyclical arbitrage is automatically closed and the funds are distributed how we agreed upon in consensus. There's no way around that. But the thing is, while that is constraining what you are allowed to build as a block, there are still degrees of freedom within that. So there is still flexibility within that. So you can constrain the search space with something like protocol I'm building.\n00:37:46.462 - 00:38:53.058, Speaker B: But depending on how much you constrain the search space, if there are still degrees of freedom, which there very well may very well be, you can still outsource block production, so you can have protocol and building where you have certain validity conditions that are enforced, but the validator can still outsource to some other builder to build according to those rules. And that's kind of what I was getting back to before when I was mentioning Pepsi briefly with Chris is that you don't necessarily have to. Just because you have more constraints on the proposer, that doesn't necessarily mean that there is no more freedom left or that they have to do it themselves. Pepsi is a similar idea of pepsi is a way for proposers to constrain the allowable space of what kind of block it can propose in much the same way that protocol on building does. The difference is more that protocol on building takes the very kind of Cosmos approach know tap specific and we can reasonably know for our given application this is the right way to constrain the search space of allowable blocks. That is relatively welfare optimizing. So like every validator has to go by that commitment.\n00:38:53.058 - 00:39:50.650, Speaker B: Whereas Pepsi is kind of the ethereum variation of that, where we can't say that because ethereum is very general purpose, it is optimizing for very different guarantees. And so you have to allow proposers to be able to locally make those constraints and those commitments which are very analogous to what protocol and building wants to do, but in a very generic context of constraining. What is the block that I'm going to output? Kind of at the end of the day, a lot of things are viewed as alternatives to PBS. And I think that is kind of one of the things that I try to hammer up more is PBS isn't just supposed to refer to this is the concrete implementation that we see on Ethereum today. It is just like the acknowledgment of there is probably going to be a separation between different actors and there is a spectrum of what that separation is and how much we constrain what those different actors can do. And I think that we're starting to see that increasingly across different ecosystems. PBS really is a spectrum of what kind of constraints are you putting on different people and what is the interaction between them.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\n00:55:43.500 - 00:55:48.730, Speaker D: You have to enforce that if the builder reveals their payload it becomes canonical.\n00:55:49.310 - 00:55:53.498, Speaker F: And if it doesn't reveal the payload that the payment is still executed.\n00:55:53.594 - 00:55:54.560, Speaker D: Right, exactly.\n00:55:55.090 - 00:56:38.826, Speaker E: Okay, so EPBs is one way that PBS is going to evolve, as we have heard. Another angle is all of the rollups are looking to decentralize their sequencer in some way. So we'll talk about what that means exactly, because different people can have wildly different opinions but one of the things that they are kind of looking at is PBS. But really it's part of a much broader design spectrum than you have on the layer one. So John, can you kind of walk us through to what degree do we need at all some form of PBS on layer two and how are these different teams thinking about it?\n00:56:39.008 - 00:57:28.038, Speaker B: Yeah, so I would say broadly they have a lot more flexibility in their designs is the very TLDR of it, where Ethereum kind of as I was mentioned before, has this very strict set of constraints where it's like we want to be very generalized, unappinionated, super, permissionless, all of those conditions. It makes it much harder to optimize for. And the reality is roll ups are going to have a lot more degrees of flexibility there. So they don't need to have necessarily a gigantic permissionless set of sequencers. They can have potentially one or a handful or some permission set of them. And that just like it makes it much easier to design the process. Like that interface between the proposers who is kind of like the sequencer more or less here, and some kind of out of protocol builder.\n00:57:28.038 - 00:57:49.618, Speaker B: So it makes it much easier if you kind of know who all the parties are and they're able to have some sort of trust interaction between them for proper execution and fulfilling their commitments. So that makes it a lot easier. And the other part of it is also they can be way more opinionated than Ethereum is going to be. So roll ups can play around with things like threshold encryption with some variations of first come, first serve, with a.\n00:57:49.624 - 00:57:51.726, Speaker C: Batch auction like Shin's proposal.\n00:57:51.918 - 00:57:56.018, Speaker B: There are going to be a lot of these different variations that are going to be more opinionated and people are.\n00:57:56.024 - 00:57:56.754, Speaker C: Going to try different things.\n00:57:56.792 - 00:58:13.046, Speaker B: It's going to be like basically the better analogy for them in large part is Cosmos compared to Ethereum roll ups are the Cosmos app chains of the Ethereum vision. Realistically they are not Ethereum itself. That is the whole point of kind of what I was going back to before of ethereum makes a certain set.\n00:58:13.068 - 00:58:14.678, Speaker C: Of tradeoffs that are very difficult to.\n00:58:14.684 - 00:58:28.974, Speaker B: Deal with, so that roll ups in large part do not have. To deal with those and they can optimize for another kind of end of the trade off us. In large part, though some form of PBS is likely going to rise be necessary in them.\n00:58:29.092 - 00:58:29.486, Speaker A: What that?\n00:58:29.508 - 00:58:31.854, Speaker B: Looks like Will. Look very different, but for those same.\n00:58:31.892 - 00:58:33.886, Speaker C: Reasons before, even when you constrain the.\n00:58:33.908 - 00:58:48.374, Speaker B: Search base of, you do certain things like protocol I'm building or you constrain certain ordering rules. There still are going to potentially be degrees of freedom that you want to outsource to a competitive market such that you are getting the best block that the Sequencers are going to put in there. At the end of the day that.\n00:58:48.412 - 00:58:58.566, Speaker E: Makes sense and another topic that we have touched already on in this call is Pepsi. So what is Pepsi and how does.\n00:58:58.588 - 00:59:00.390, Speaker A: It relate to PVs?\n00:59:00.830 - 01:00:03.854, Speaker D: Cool yeah so Pepsi is a proposal from Barnabay. It stands for Protocol enforced Proposer commitments and the idea here is that it kind of generalizes PBS insofar as expanding the set of commitments that a proposer can make that are enforced at the block validity level. So the idea is in this new design proposers can sign up for different block validity conditions that are applied to their block. And this is kind of often compared to the type of commitments that could be made through Eigen layer. But I think the important distinction is that Eigen layer commitments are only enforceable kind of at the execution layer meaning they're only enforceable by slashing the stake of the validator kind of after the fact if they don't fulfill the commitments that they made. Pepsi is kind of a stronger commitment or in. My mind kind of closer to the medal of Ethereum in that the commitments are actually part of the fork choice rule and part of the state transition function.\n01:00:03.854 - 01:01:31.042, Speaker D: So if a proposer commits to something and their block doesn't satisfy that constraint then it's not even able to be part of the blockchain because of the commitments that they made. So I like to think about the difference between EPBs and Pepsi as the difference between homogeneous and heterogeneous commitments that the proposer can make. So in EPBs we're saying we're going to specifically enshrine a single version of the mechanism that the proposer and builders participate in so that could be a full block auction. So the proposers can commit to a specific block hash the builder has to reveal a payload that corresponds to that block hash.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nIt doesn't even justify the cost and the additional risk of running out of particle software of maintaining this thing. It's just like forget it, the other thing works. 99% is good. I don't care about the last like five milliseconds at the end of this thing.\n01:35:01.434 - 01:35:12.110, Speaker C: And that exact delta does matter and it's unclear exactly what it is, but yeah, it is a very fundamentally different role as opposed to this is the central point that is holding up the whole PBS auction.\n01:35:12.690 - 01:36:05.540, Speaker A: Yeah, I agree. One more thing that I want to touch on is kind of I mean, I was kind of giving Mike a bit of a hard time asking him about different governance entities in Ethereum and like their power distribution, who maintains what what this means for the decentralization of the overall ecosystem. And yeah, Kudos, you gave a good answer. I still want to talk about this a bit more with Mean right now. It's pretty much the case, I think that the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe",
                        "previous_answer": "Enshrined Proposal Builder Separation, or ePBS, is a concept under exploration in the realm of blockchain protocols. It aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. The ePBS concept proposes to embed the market structure, consisting of proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less adaptable than other models, it enables validators to make commitments without prescribing the exact form of these commitments. Despite the introduction of ePBS, there's an increasing understanding that structures like relays may still be required, though their role may be diminished."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, or ePBS, is a proposed enhancement to blockchain protocols. It seeks to integrate the market structure, which includes proposals and builders, directly into the protocol. This approach is more prescriptive and less flexible than other models, but it allows validators to make commitments without specifying the exact nature of these commitments. Despite the introduction of ePBS, there's a growing consensus that structures like relays may still be necessary, although their role might be less significant."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "- 01:36:05.540, Speaker A: Yeah, I agree. One more thing that I want to touch on is kind of I mean, I was kind of giving Mike a bit of a hard time asking him about different governance entities in Ethereum and like their power distribution, who maintains what what this means for the decentralization of the overall ecosystem. And yeah, Kudos, you gave a good answer. I still want to talk about this a bit more with Mean right now. It's pretty much the case, I think that the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe governance around PBS, but outside the protocol.\n01:37:06.882 - 01:37:10.600, Speaker A: So between these two options, what do you think?\n01:37:11.610 - 01:37:55.380, Speaker C: Yeah, I mean, part of it's a time horizon question. I don't think that you need to rush to enshrine something because of this. You definitely want to take your time on it. In an ideal world, yeah, you solve these problems and you enshrine stuff and you don't have to rely on different companies with"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "- 01:36:05.540, Speaker A: Yeah, I agree. One more thing that I want to touch on is kind of I mean, I was kind of giving Mike a bit of a hard time asking him about different governance entities in Ethereum and like their power distribution, who maintains what what this means for the decentralization of the overall ecosystem. And yeah, Kudos, you gave a good answer. I still want to talk about this a bit more with Mean right now. It's pretty much the case, I think that the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe governance around PBS, but outside the protocol.\n01:37:06.882 - 01:37:10.600, Speaker A: So between these two options, what do you think?\n01:37:11.610 - 01:37:55.380, Speaker C: Yeah, I mean, part of it's a time horizon question. I don't think that you need to rush to enshrine something because of this. You definitely want to take your time on it. In an ideal world, yeah, you solve these problems and you enshrine stuff and you don't have to rely on different companies with",
                        "previous_answer": "Enshrined Proposal Builder Separation, or ePBS, is a concept under exploration in the realm of blockchain protocols. It aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. The ePBS concept proposes to embed the market structure, consisting of proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less adaptable than other models, it enables validators to make commitments without prescribing the exact form of these commitments. Despite the introduction of ePBS, there's an increasing understanding that structures like relays may still be required, though their role may be diminished."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, also known as ePBS, is a theoretical approach being considered in the field of blockchain protocols. It seeks to improve the protocol's capabilities by solidifying certain components at the protocol level. The ePBS approach suggests incorporating the market structure, which includes proposals and builders, directly into the protocol. Although this approach is viewed as more directive and less flexible than other models, it allows validators to make commitments without dictating the exact form of these commitments. Despite the emergence of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit their role may be reduced."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe governance around PBS, but outside the protocol.\n01:37:06.882 - 01:37:10.600, Speaker A: So between these two options, what do you think?\n01:37:11.610 - 01:37:55.380, Speaker C: Yeah, I mean, part of it's a time horizon question. I don't think that you need to rush to enshrine something because of this. You definitely want to take your time on it. In an ideal world, yeah, you solve these problems and you enshrine stuff and you don't have to rely on different companies with different interests to be funding this stuff and developing it, et cetera. It is the fundamental recurring trend with ethereum of even execution charge to roll ups is somewhat of the same trend honestly of you start to realize like hey, maybe this actually works really well if we let the free market. Just take this thing and kind of keep innovating it over time. It particularly becomes like that.\n01:37:55.380 - 01:38:39.778, Speaker C: Depending on your view of how much does this thing need to keep being updated over time? That becomes a big part of it, quite frankly. If you start to have more confidence of like, okay, this is a mechanism which is very simple, it is very forward compatible, it's not very opinionated. This is something which works and it can last the next ten years, 20 years, whatever, then you feel pretty good about like, okay, we could just enshrine this thing. It's really simple. Like it works. You don't need to leave people to keep innovating, keep changing this thing over time in the way that roll ups or something else, they're going to keep changing. So part of it changes based on that view, I would say, is like how confident are you that this thing is actually static and can stay there for a long time.\n01:38:39.778 - 01:39:24.266, Speaker C: At that point you want to enshrine it and you want to put it in the protocol if possible. Because just leaving it out to different companies, people have different interests and that leads to potentially worse outcomes over time. So it is suboptimal, I would say in the short to medium term at least, I definitely think it makes sense, like you don't need to rush to do these things. The main pressing result of that though is okay, we do need to figure out funding for a lot of the tangible stuff, particularly for relay funding. That is the main question out of this. PBS Gildon similar ideas is that this is part of the benefit of EPBs in my mind is it gets rid of the relay funding issues. At that point you should not get any funding.\n01:39:24.266 - 01:40:18.098, Speaker C: Like you are a latency optimization service, you're not fundamental to the protocol. But the big question today is that we're not there and the relays are pretty fundamental to holding up the PBS process, at least for the untrusted participants. So in the absence of relays you would have today like the top 90% or whatever number of validators and top 90% of builders, they're fine, they could trust each other. Lido and Beaver build like hey, we know each other, we could trust each other, is fine. But the relays are fundamental to upholding the hey, that last 5%, 10%, whatever that number is of like they would not be trusted to receive something from a builder. So they are fundamental for that and as of right now, they're not a business that's able to monetize that. So the question is how do we try to fund these? Hopefully we have a upvs at some point in the next couple of years, whatever it is.\n01:40:18.098 - 01:40:28.626, Speaker C: But for today, people have to run these relays. It costs some money and it may not be profitable for them to do so. So figuring out that is one of the main directives of something like this.\n01:40:28.808 - 01:41:20.594, Speaker A: Yeah, I would agree. I mean, I think how static you can make it, how close you think you are to something that can be static. I think for me that is a key determinant to whether you want to pull it into the protocol. I think before that point it really makes sense to address kind of the relay sustainability issues. Why are we talking about this?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd you just need to understand in the context of your own protocol what is the right place on that trade off spectrum of does it just look like a very simple, very dumb, like, hey, I sign a commitment, you give me a full block and that's it? Or is it a very opinionated kind of interaction where there's some kind of outsource, but you're giving a lot of constraints and a lot of enforcement over that? It's a different trade off spectrum and different protocols should have a different spot on that. It's not like PBS is good or PBS is bad. It's just like different kind of versions of it make sense in different places.\n01:10:23.190 - 01:10:24.226, Speaker D: Well said.\n01:10:24.408 - 01:10:56.702, Speaker F: Overall, I think what is clear is that entraining PBS is hard. It's a challenge. I think we have been making really good progress as a community towards that. And I think it makes sense to start like we did with Mafboost, with a out of protocol way to experiment and then iterating towards enshrining it and yeah, I think I'm very excited to see where it's going next and working on it with all of you guys.\n01:10:56.836 - 01:11:01.130, Speaker E: Okay, fantastic. So thank you guys so much for the discussion.\n01:11:01.290 - 01:11:02.126, Speaker D: Thanks for having us on.\n01:11:02.148 - 01:11:02.778, Speaker F: Thanks guys.\n01:11:02.884 - 01:11:03.154, Speaker A: Thanks.\n01:11:03.192 - 01:11:04.500, Speaker F: It was nice being here.\n01:11:06.470 - 01:11:08.850, Speaker A: Hey, John, what did you think about this episode?\n01:11:10.150 - 01:11:13.230, Speaker C: Well, it took us like five tries.\n01:11:13.310 - 01:11:18.910, Speaker B: Or something like that over the past month, but it was worth it. It was a lot of fun doing this one.\n01:11:19.080 - 01:11:30.406, Speaker C: So I guess for a background for the listeners, we first tried to do this episode, I think like over a month ago. We did it in Vienna where the.\n01:11:30.428 - 01:11:32.399, Speaker B: Four of us and then Tomas and.\n01:11:32.399 - 01:11:37.766, Speaker C: Tony had spent a week together right after EthCC, which was a ton of fun jamming on all the PBS tamas.\n01:11:37.798 - 01:11:40.970, Speaker A: From flashboards and Tony watched that are from the Ether Foundation.\n01:11:41.310 - 01:11:42.538, Speaker B: Spent like a week jamming on the.\n01:11:42.544 - 01:11:49.034, Speaker C: PBS stuff and then we tried to record it at the end of the week and just absolute awful audio quality on the laptop.\n01:11:49.082 - 01:11:52.174, Speaker B: Took a few tries to do it, finally recorded it a couple of weeks.\n01:11:52.212 - 01:12:02.578, Speaker C: Ago and now we're finally doing the recap currently in the middle of SPC for me. So finally getting to put it together. But it was a lot of fun doing this one.\n01:12:02.744 - 01:12:09.270, Speaker A: Yeah, it's been a long way coming. I'm really glad to put this out. What was for you the highlight of the episode?\n01:12:10.730 - 01:12:16.870, Speaker C: The highlight for me, I'd probably say talking about Pepsi.\n01:12:19.210 - 01:12:22.346, Speaker B: It's at least the most fun thing for me at the moment because I.\n01:12:22.368 - 01:12:31.498, Speaker C: Feel like it's the most probably under talked about thing recently compared to what will be talked about upcoming at least a little bit of like it's an.\n01:12:31.504 - 01:12:33.366, Speaker B: Idea that feels like it's been kicking.\n01:12:33.398 - 01:12:39.374, Speaker C: Around for a while. That Barnaby had brought up last year, and that kind of went away after.\n01:12:39.412 - 01:12:40.334, Speaker B: That for a few months.\n01:12:40.372 - 01:13:42.174, Speaker C: It was kind of this fun thought experiment thing and then especially in the last few months or so seems to be just kind of coming back much more meaningfully. I also am probably biased, like front of my mind because I just came from listening to Barnaby give a presentation on Pepsi like two or 3 hours ago. So it's kind of front of mind for me. But it is very interesting because there's clearly a lot of thought being given on what should really PBS look like to the extent that it's enshrined in the protocol and there's a very wide design space on the types of commitments that it kind of makes sense to potentially have. And potentially even in the shorter term of out of protocol versions of that stuff like Pepsi Boost. And in particular, you had just sent me the link right before this of Mevboost Plus and Mevboost plus plus, which is like the idea for mylayer which touches on a lot of the same ideas and the tougher part with those kinds of constructions. So for brief context we'll link it in the show notes.\n01:13:42.174 - 01:14:54.346, Speaker C: But for the listeners like Mevboost Plus and Mevboost Plus Plus, they're like ideas from Eigen Layer, which are basically partial blocks auctions where you can allow the proposer to opt into restaking commitments, where they can say, like, hey, I agree that I'm going to sell the top half of this block. I'm going to agree to this, and then I'll get the block body, and then after that, I can add in whatever I want at the bottom of the block.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\n00:00:06.090 - 00:00:24.590, Speaker A: Welcome to Uncommon Core where we explore the big ideas in crypto from first principles. This show is hosted by John Chabanot, co founder and general partner of DBA and Me Hasu, strategy lead at Flashbots and advisor to the Lido Dao.\n00:00:26.810 - 00:00:27.906, Speaker B: Today hasu.\n00:00:27.938 - 00:00:36.882, Speaker C: And I sat down with Mike Neuter from the Ethereum Foundation and Chris Hager from Flashbots. We had a great time chatting through PBS, also known as proposal builder separation.\n00:00:37.026 - 00:00:38.562, Speaker B: We talked about the history of PBS.\n00:00:38.626 - 00:01:03.166, Speaker C: On Ethereum, talking about what it looked like on Proof of Work and how that brought us to where we are today with Mevboost on Ethereum right now. We also looked ahead, looking at the future of PBS, asking each other should we ensuring PBS or not and what would that look like? That included talking about really fun new ideas like Pepsi. We also had some fun at the end talking about what should PBS look like on other domains, especially layer two.\n00:01:03.188 - 00:01:04.938, Speaker B: S on Ethereum, like other roll ups.\n00:01:05.034 - 00:01:07.026, Speaker C: Where we chatted through why we think.\n00:01:07.048 - 00:01:10.898, Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "the Ethereum Foundation is working on EPBs with the help of various other researchers. I think Flashbots is contributing, as are various other parties. Meanwhile, Flashbots is primarily maintaining mev boost and that's where you more have the Ethereum foundation and support primarily supporting kind of with research.\n01:36:05.540 - 01:37:06.882, Speaker A: Folks like Tony, for example, have know, do some great monitoring and data analysis and increasingly also like academia is starting to contribute to so. What would you think about the idea of so? On the one hand, you could kind of enshrine it. And I think Mike especially was kind of hinting at that idea, right? So you could resolve this power. It's not a struggle in any sense, this separation. You could address it by just saying specifically, okay, PBS is now part of the protocol. And so the protocol devs basically also have to work on it and make sure that it stays up to date and it stays optimal. But the alternative may be to basically create more sustainability and maybe governance around PBS, but outside the protocol.\n01:37:06.882 - 01:37:10.600, Speaker A: So between these two options, what do you think?\n01:37:11.610 - 01:37:55.380, Speaker C: Yeah, I mean, part of it's a time horizon question. I don't think that you need to rush to enshrine something because of this. You definitely want to take your time on it. In an ideal world, yeah, you solve these problems and you enshrine stuff and you don't have to rely on different companies with different interests to be funding this stuff and developing it, et cetera. It is the fundamental recurring trend with ethereum of even execution charge to roll ups is somewhat of the same trend honestly of you start to realize like hey, maybe this actually works really well if we let the free market. Just take this thing and kind of keep innovating it over time. It particularly becomes like that.\n01:37:55.380 - 01:38:39.778, Speaker C: Depending on your view of how much does this thing need to keep being updated over time? That becomes a big part of it, quite frankly. If you start to have more confidence of like, okay, this is a mechanism which is very simple, it is very forward compatible, it's not very opinionated. This is something which works and it can last the next ten years, 20 years, whatever, then you feel pretty good about like, okay, we could just enshrine this thing. It's really simple. Like it works. You don't need to leave people to keep innovating, keep changing this thing over time in the way that roll ups or something else, they're going to keep changing. So part of it changes based on that view, I would say, is like how confident are you that this thing is actually static and can stay there for a long time.\n01:38:39.778 - 01:39:24.266, Speaker C: At that point you want to enshrine it and you want to put it in the protocol if possible. Because just leaving it out to different companies, people have different interests and that leads to potentially worse outcomes over time. So it is suboptimal, I would say in the short to medium term at least, I definitely think it makes sense, like you don't need to rush to do these things. The main pressing result of that though is okay, we do need to figure out funding for a lot of the tangible stuff, particularly for relay funding. That is the main question out of this. PBS Gildon similar ideas is that this is part of the benefit of EPBs in my mind is it gets rid of the relay funding issues. At that point you should not get any funding.\n01:39:24.266 - 01:40:18.098, Speaker C: Like you are a latency optimization service, you're not fundamental to the protocol. But the big question today is that we're not there and the relays are pretty fundamental to holding up the PBS process, at least for the untrusted participants. So in the absence of relays you would have today like the top 90% or whatever number of validators and top 90% of builders, they're fine, they could trust each other. Lido and Beaver build like hey, we know each other, we could trust each other, is fine. But the relays are fundamental to upholding the hey, that last 5%, 10%, whatever that number is of like they would not be trusted to receive something from a builder. So they are fundamental for that and as of right now, they're not a business that's able to monetize that. So the question is how do we try to fund these? Hopefully we have a upvs at some point in the next couple of years, whatever it is.\n01:40:18.098 - 01:40:28.626, Speaker C: But for today, people have to run these relays. It costs some money and it may not be profitable for them to do so. So figuring out that is one of the main directives of something like this.\n01:40:28.808 - 01:41:20.594, Speaker A: Yeah, I would agree. I mean, I think how static you can make it, how close you think you are to something that can be static. I think for me that is a key determinant to whether you want to pull it into the protocol. I think before that point it really makes sense to address kind of the relay sustainability issues. Why are we talking about this?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd you just need to understand in the context of your own protocol what is the right place on that trade off spectrum of does it just look like a very simple, very dumb, like, hey, I sign a commitment, you give me a full block and that's it? Or is it a very opinionated kind of interaction where there's some kind of outsource, but you're giving a lot of constraints and a lot of enforcement over that? It's a different trade off spectrum and different protocols should have a different spot on that. It's not like PBS is good or PBS is bad. It's just like different kind of versions of it make sense in different places.\n01:10:23.190 - 01:10:24.226, Speaker D: Well said.\n01:10:24.408 - 01:10:56.702, Speaker F: Overall, I think what is clear is that entraining PBS is hard. It's a challenge. I think we have been making really good progress as a community towards that. And I think it makes sense to start like we did with Mafboost, with a out of protocol way to experiment and then iterating towards enshrining it and yeah, I think I'm very excited to see where it's going next and working on it with all of you guys.\n01:10:56.836 - 01:11:01.130, Speaker E: Okay, fantastic. So thank you guys so much for the discussion.\n01:11:01.290 - 01:11:02.126, Speaker D: Thanks for having us on.\n01:11:02.148 - 01:11:02.778, Speaker F: Thanks guys.\n01:11:02.884 - 01:11:03.154, Speaker A: Thanks.\n01:11:03.192 - 01:11:04.500, Speaker F: It was nice being here.\n01:11:06.470 - 01:11:08.850, Speaker A: Hey, John, what did you think about this episode?\n01:11:10.150 - 01:11:13.230, Speaker C: Well, it took us like five tries.\n01:11:13.310 - 01:11:18.910, Speaker B: Or something like that over the past month, but it was worth it. It was a lot of fun doing this one.\n01:11:19.080 - 01:11:30.406, Speaker C: So I guess for a background for the listeners, we first tried to do this episode, I think like over a month ago. We did it in Vienna where the.\n01:11:30.428 - 01:11:32.399, Speaker B: Four of us and then Tomas and.\n01:11:32.399 - 01:11:37.766, Speaker C: Tony had spent a week together right after EthCC, which was a ton of fun jamming on all the PBS tamas.\n01:11:37.798 - 01:11:40.970, Speaker A: From flashboards and Tony watched that are from the Ether Foundation.\n01:11:41.310 - 01:11:42.538, Speaker B: Spent like a week jamming on the.\n01:11:42.544 - 01:11:49.034, Speaker C: PBS stuff and then we tried to record it at the end of the week and just absolute awful audio quality on the laptop.\n01:11:49.082 - 01:11:52.174, Speaker B: Took a few tries to do it, finally recorded it a couple of weeks.\n01:11:52.212 - 01:12:02.578, Speaker C: Ago and now we're finally doing the recap currently in the middle of SPC for me. So finally getting to put it together. But it was a lot of fun doing this one.\n01:12:02.744 - 01:12:09.270, Speaker A: Yeah, it's been a long way coming. I'm really glad to put this out. What was for you the highlight of the episode?\n01:12:10.730 - 01:12:16.870, Speaker C: The highlight for me, I'd probably say talking about Pepsi.\n01:12:19.210 - 01:12:22.346, Speaker B: It's at least the most fun thing for me at the moment because I.\n01:12:22.368 - 01:12:31.498, Speaker C: Feel like it's the most probably under talked about thing recently compared to what will be talked about upcoming at least a little bit of like it's an.\n01:12:31.504 - 01:12:33.366, Speaker B: Idea that feels like it's been kicking.\n01:12:33.398 - 01:12:39.374, Speaker C: Around for a while. That Barnaby had brought up last year, and that kind of went away after.\n01:12:39.412 - 01:12:40.334, Speaker B: That for a few months.\n01:12:40.372 - 01:13:42.174, Speaker C: It was kind of this fun thought experiment thing and then especially in the last few months or so seems to be just kind of coming back much more meaningfully. I also am probably biased, like front of my mind because I just came from listening to Barnaby give a presentation on Pepsi like two or 3 hours ago. So it's kind of front of mind for me. But it is very interesting because there's clearly a lot of thought being given on what should really PBS look like to the extent that it's enshrined in the protocol and there's a very wide design space on the types of commitments that it kind of makes sense to potentially have. And potentially even in the shorter term of out of protocol versions of that stuff like Pepsi Boost. And in particular, you had just sent me the link right before this of Mevboost Plus and Mevboost plus plus, which is like the idea for mylayer which touches on a lot of the same ideas and the tougher part with those kinds of constructions. So for brief context we'll link it in the show notes.\n01:13:42.174 - 01:14:54.346, Speaker C: But for the listeners like Mevboost Plus and Mevboost Plus Plus, they're like ideas from Eigen Layer, which are basically partial blocks auctions where you can allow the proposer to opt into restaking commitments, where they can say, like, hey, I agree that I'm going to sell the top half of this block. I'm going to agree to this, and then I'll get the block body, and then after that, I can add in whatever I want at the bottom of the block.\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\n00:00:06.090 - 00:00:24.590, Speaker A: Welcome to Uncommon Core where we explore the big ideas in crypto from first principles. This show is hosted by John Chabanot, co founder and general partner of DBA and Me Hasu, strategy lead at Flashbots and advisor to the Lido Dao.\n00:00:26.810 - 00:00:27.906, Speaker B: Today hasu.\n00:00:27.938 - 00:00:36.882, Speaker C: And I sat down with Mike Neuter from the Ethereum Foundation and Chris Hager from Flashbots. We had a great time chatting through PBS, also known as proposal builder separation.\n00:00:37.026 - 00:00:38.562, Speaker B: We talked about the history of PBS.\n00:00:38.626 - 00:01:03.166, Speaker C: On Ethereum, talking about what it looked like on Proof of Work and how that brought us to where we are today with Mevboost on Ethereum right now. We also looked ahead, looking at the future of PBS, asking each other should we ensuring PBS or not and what would that look like? That included talking about really fun new ideas like Pepsi. We also had some fun at the end talking about what should PBS look like on other domains, especially layer two.\n00:01:03.188 - 00:01:04.938, Speaker B: S on Ethereum, like other roll ups.\n00:01:05.034 - 00:01:07.026, Speaker C: Where we chatted through why we think.\n00:01:07.048 - 00:01:10.898, Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand",
                        "previous_answer": "Enshrined Proposal Builder Separation, also known as ePBS, is a theoretical approach being considered in the field of blockchain protocols. It seeks to improve the protocol's capabilities by solidifying certain components at the protocol level. The ePBS approach suggests incorporating the market structure, which includes proposals and builders, directly into the protocol. Although this approach is viewed as more directive and less flexible than other models, it allows validators to make commitments without dictating the exact form of these commitments. Despite the emergence of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit their role may be reduced."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, or ePBS, is a concept in blockchain protocols that aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. This approach proposes integrating the market structure, including proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less flexible than other models, it allows validators to make commitments without specifying the exact form of these commitments. Despite the introduction of ePBS, there's a growing agreement that structures like relays may still be needed, although their role might be diminished."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "should we ensuring PBS or not and what would that look like? That included talking about really fun new ideas like Pepsi. We also had some fun at the end talking about what should PBS look like on other domains, especially layer two.\n00:01:03.188 - 00:01:04.938, Speaker B: S on Ethereum, like other roll ups.\n00:01:05.034 - 00:01:07.026, Speaker C: Where we chatted through why we think.\n00:01:07.048 - 00:01:10.898, Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand that we're going to have protocol actors that are responsible for certain things. And then there's going to be an economic incentive and for various other reasons, for them to outsource certain actions to other actors that may not be actually in the protocol. So concretely, the way that we're used to thinking about that is in the Ethereum world where we have validators one of which, who will be the active proposer at a given time to propose a block to the rest of the network.\n00:02:17.426 - 00:03:35.326, Speaker B: And so the reason that we have concretely"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "should we ensuring PBS or not and what would that look like? That included talking about really fun new ideas like Pepsi. We also had some fun at the end talking about what should PBS look like on other domains, especially layer two.\n00:01:03.188 - 00:01:04.938, Speaker B: S on Ethereum, like other roll ups.\n00:01:05.034 - 00:01:07.026, Speaker C: Where we chatted through why we think.\n00:01:07.048 - 00:01:10.898, Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand that we're going to have protocol actors that are responsible for certain things. And then there's going to be an economic incentive and for various other reasons, for them to outsource certain actions to other actors that may not be actually in the protocol. So concretely, the way that we're used to thinking about that is in the Ethereum world where we have validators one of which, who will be the active proposer at a given time to propose a block to the rest of the network.\n00:02:17.426 - 00:03:35.326, Speaker B: And so the reason that we have concretely",
                        "previous_answer": "Enshrined Proposal Builder Separation, also known as ePBS, is a theoretical approach being considered in the field of blockchain protocols. It seeks to improve the protocol's capabilities by solidifying certain components at the protocol level. The ePBS approach suggests incorporating the market structure, which includes proposals and builders, directly into the protocol. Although this approach is viewed as more directive and less flexible than other models, it allows validators to make commitments without dictating the exact form of these commitments. Despite the emergence of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit their role may be reduced."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, often referred to as ePBS, is a concept in the blockchain protocol field. It aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. The ePBS method proposes integrating the market structure, encompassing proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less adaptable than other models, it enables validators to make commitments without specifying the exact form of these commitments. Despite the rise of ePBS, there's an increasing agreement that structures like relays may still be required, although their role might be diminished."
                    },
                    {
                        "event_type": "templating start",
                        "instructions": "user: You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: {context_msg}\nQuery: {query_str}\nOriginal Answer: {existing_answer}\nNew Answer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_msg": "Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand that we're going to have protocol actors that are responsible for certain things. And then there's going to be an economic incentive and for various other reasons, for them to outsource certain actions to other actors that may not be actually in the protocol. So concretely, the way that we're used to thinking about that is in the Ethereum world where we have validators one of which, who will be the active proposer at a given time to propose a block to the rest of the network.\n00:02:17.426 - 00:03:35.326, Speaker B: And so the reason that we have concretely proposer builder separation here is that we want that proposer to be relatively unsophisticated and yet be economically competitive such that we can keep the validator set decentralized, so they can outsource the very specialized task to this network of specialized block builders which sit outside the protocol. And those block builders are responsible for building what is the most optimized block that can extract the most value, such that they can pass the majority of value back. Because otherwise what you would have, conversely, is if we don't have this ability for proposers to kind of interact with this out of protocol market in a relatively trust, minimized way, well, then you would simply have a very clear return to Sophistication, where the only way to be a competitive proposer would be okay, well, now you need to be a builder in house. You need to be super sophisticated and know how to optimize everything. So it's trying to get at the fact that you're going to have these different roles and we need to design what is the right way to have an interface between these kind of in protocol and out of protocol roles. And right now the way that that works with mevboost is kind of a strapped on way of doing that. And a lot of the research right now that Mike has been doing over the F is like, how do we kind of maybe bring that a little bit more in house and what should that look like to try to make that interface between the in protocol and out protocol actors even more trustless?\n00:03:35.518 - 00:04:30.002, Speaker D: Yeah, and I always like to kind of circle back to Vitalik's Endgame post. The last sentence of his post is basically the future of many iterations of these designs will probably end up in a world where there's centralized production, decentralized verification and strong antisensorship properties. And he kind of talks about how some ecosystems might start more centralized in the block production world and evolve into something that has decentralized verification only. And others could take different trade offs in the initial state, but ultimately we might always end up in that state where we need to firewall off the heavy duty kind of actions that the validators need to take from something that can be run on a local machine has like credible decentralization features. So that's kind of how I like to think about it.\n00:04:30.136 - 00:05:04.858, Speaker F: Yeah, you spoke to a lot of things that I'm also thinking. I think in particular, it's also a case of there is either an implicit or an explicit auction and if the auction is implicit, has a lot more negative externalities and incentives to centralization. And PBS recognizes that not all protocol actors may be able to fulfill all the duties in a comparatively performant way and need external support for that to also keep the decentralization of the network stable.\n00:05:05.034 - 00:05:05.374, Speaker A: Yeah.\n00:05:05.412 - 00:05:55.470, Speaker E: And what I particularly like, and kind of why I picked out this quote is that it really hones in on PBS as philosophy. Right. And I think the PBS, the implementation on Ethereum faces a lot of criticism from different directions, all great arguments and concerns that we will also go into in this episode. But really I think the general idea behind it is one that is extremely sound and I think that all of you laid out here really well. So with this high level overview out of the way, I'd like to go a bit a couple of years back and hear from you PBS, as an idea. Where did it start? What is its history?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd one way this actually manifests, not from an economic perspective, but more from just a fragility perspective, has showed up in a few different issues around relay operators and their relationship with consensus clients. So immediately after the CHAPELLA fork, there was a bug in the relationship with how Prism interacted with mevboost and that resulted in huge network instability. Immediately post the hard fork, it took a few epochs for the chain to finalize.\n00:27:45.134 - 00:28:13.862, Speaker D: There was a lot of missed slots, it was full blown like firefighting mode. And that comes from the fact that there's these ten relays and all of the software that is running on the validator machines is kind of decoupled from this mevboost external software. So there is like consensus stability implications around the centralization found in particular in out of protocol PBS systems.\n00:28:14.006 - 00:29:02.570, Speaker F: I would add to that there is the overall technical complexity of entry and PBS. The merge is now basically just a year ago and the whole year we've been thinking about moving PBS more in protocol, how to get rid and move beyond the relays as trusted actors. And it's super hard challenges where you may need a lot of additional responsibilities, you may need to increase the consensus protocol complexity, which is already pretty hard to reason about, and it could introduce new nuanced reorg risks or vulnerabilities. And this is just a thing that is very hard, a very hard problem to get right. So I would say there is a lot of overall technical risk on the path to improrotocol PBS.\n00:29:04.110 - 00:29:52.970, Speaker E: Yeah, I have a bit more kind of arcane point, but clearly we are seeing that proposal builder separation can exist outside the protocol and that's where it does so far most of the time. And not all of this stuff is actually maintained by Ethereum core developers, I guess. As someone who's working for the Ethereum Foundation, Mike, what do you think this does to kind of the power dynamics in the Ethereum ecosystem? Is it on the one hand, more that we have to change the definition of what it means to be a core developer? Or is it that Ethereum should eventually try to pull everything into the protocol? What do you think it does to the invisible kind of power in the ecosystem?\n00:29:53.390 - 00:30:59.050, Speaker D: Yeah, I would say Barnaby has a really nice post on this. We keep calling him out, but he has a post called Seeing like. A protocol and he defines kind of what it could look like to enshrine different things and when to draw the line and say, okay, this is out of protocol versus in protocol. And I think part of EPBs and the work that I've been focusing on is kind of figuring out not only what to enshrine, like what design works for EPBs, whatever, technically speaking, but also on a more meta level, should we actually do the enshrinement? And one of our recent pieces that we wrote with us, four actually, and a few others was kind of talking about the role of PBS and Enshrined PBS in the world in which a relay market exists outside of the protocol still. So we'll probably touch on that later. But I guess in the current meta where mevboost essentially is core protocol software, I think there's a bit of an ownership mismatch. Flashbots.org\n00:30:59.050 - 00:31:53.410, Speaker D: wrote this code and it's been working really well for the year that's been running post merge. But I think everyone would agree that the testing and tooling and specification around that code is not at the same level of the core consensus clients. And part of that is because it's sort of a public good, but it's also originally written by Flashbot. So I'm not sure exactly how the ownership should evolve and the politics there. I will say I guess one of my big reasons why I like Enshrine PBS is because it makes that distinction a lot more clear. Like it draws the line in the sand is like this is the in protocol mechanism that we're going to maintain in terms of the consensus spec and the client teams. If you want to go outside of that, you have to rely on out of protocol software that might inherently be more brittle, more risky, et cetera.\n00:31:53.410 - 00:31:56.580, Speaker D: Hopefully that answered your question.\n00:31:57.270 - 00:32:13.350, Speaker E: Do you think it's more risky for ethereum? That important part of the ethereum stack are maintained by kind of non ethereum foundation teams that may even have a commercial interest? Or do you think it's more risky that it isn't?\n00:32:14.250 - 00:33:31.040, Speaker D: Yeah, I think it feels more risky in the current state, and I'll say especially right now, it feels like the equilibrium we're in is not stable. The relays are kind of fighting for their lives in terms of some of them are third party kind of credibly neutral relays that are trying to get funding from grants and other things. Other relays are parts of companies and commercial entities that are trying to either monetize or trying to figure out if this is part of the core business model. And I think even some of the large relay operators now, it's not clear that if we don't find a viable funding mechanism, we'll be around by the end of the year, for example. So I think insofar as we get to a world where there's only like two or three relays, that is much riskier to the protocol than the current status quo, which seems to be the direction we're headed in. So I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "retrieved_context": "Speaker B: It could actually look very, very different on L two S compared to Ethereum itself.\n00:01:10.984 - 00:01:11.986, Speaker D: Hope you enjoy.\n00:01:12.168 - 00:01:17.010, Speaker E: What is Proposal builder separation or short as we know it, PBS.\n00:01:17.350 - 00:02:17.426, Speaker B: Sure. So the first thing that I'll pick out that we kind of used before it's, the quote from Barnaby is still my favorite kind of one liner description of what it really is. So PBS is first and foremost a design philosophy, recognizing that protocol actors may invoke services from third parties in the course of their consensus duties. So I really like that as just like a kind of high level setting of what it is. Because while we look at it in Ethereum as a very concrete implementation, the reality is it is just kind of a higher level design philosophy of we understand that we're going to have protocol actors that are responsible for certain things. And then there's going to be an economic incentive and for various other reasons, for them to outsource certain actions to other actors that may not be actually in the protocol. So concretely, the way that we're used to thinking about that is in the Ethereum world where we have validators one of which, who will be the active proposer at a given time to propose a block to the rest of the network.\n00:02:17.426 - 00:03:35.326, Speaker B: And so the reason that we have concretely proposer builder separation here is that we want that proposer to be relatively unsophisticated and yet be economically competitive such that we can keep the validator set decentralized, so they can outsource the very specialized task to this network of specialized block builders which sit outside the protocol. And those block builders are responsible for building what is the most optimized block that can extract the most value, such that they can pass the majority of value back. Because otherwise what you would have, conversely, is if we don't have this ability for proposers to kind of interact with this out of protocol market in a relatively trust, minimized way, well, then you would simply have a very clear return to Sophistication, where the only way to be a competitive proposer would be okay, well, now you need to be a builder in house. You need to be super sophisticated and know how to optimize everything. So it's trying to get at the fact that you're going to have these different roles and we need to design what is the right way to have an interface between these kind of in protocol and out of protocol roles. And right now the way that that works with mevboost is kind of a strapped on way of doing that. And a lot of the research right now that Mike has been doing over the F is like, how do we kind of maybe bring that a little bit more in house and what should that look like to try to make that interface between the in protocol and out protocol actors even more trustless?\n00:03:35.518 - 00:04:30.002, Speaker D: Yeah, and I always like to kind of circle back to Vitalik's Endgame post. The last sentence of his post is basically the future of many iterations of these designs will probably end up in a world where there's centralized production, decentralized verification and strong antisensorship properties. And he kind of talks about how some ecosystems might start more centralized in the block production world and evolve into something that has decentralized verification only. And others could take different trade offs in the initial state, but ultimately we might always end up in that state where we need to firewall off the heavy duty kind of actions that the validators need to take from something that can be run on a local machine has like credible decentralization features. So that's kind of how I like to think about it.\n00:04:30.136 - 00:05:04.858, Speaker F: Yeah, you spoke to a lot of things that I'm also thinking. I think in particular, it's also a case of there is either an implicit or an explicit auction and if the auction is implicit, has a lot more negative externalities and incentives to centralization. And PBS recognizes that not all protocol actors may be able to fulfill all the duties in a comparatively performant way and need external support for that to also keep the decentralization of the network stable.\n00:05:05.034 - 00:05:05.374, Speaker A: Yeah.\n00:05:05.412 - 00:05:55.470, Speaker E: And what I particularly like, and kind of why I picked out this quote is that it really hones in on PBS as philosophy. Right. And I think the PBS, the implementation on Ethereum faces a lot of criticism from different directions, all great arguments and concerns that we will also go into in this episode. But really I think the general idea behind it is one that is extremely sound and I think that all of you laid out here really well. So with this high level overview out of the way, I'd like to go a bit a couple of years back and hear from you PBS, as an idea. Where did it start? What is its history?\n\ndocument_type: youtube_video\ntitle: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\nchannel_name: Uncommon Core 2.0\nvideo_link: https://www.youtube.com/watch?v=mEbK9AX7X7o\nrelease_date: 2023-09-02\n\nAnd one way this actually manifests, not from an economic perspective, but more from just a fragility perspective, has showed up in a few different issues around relay operators and their relationship with consensus clients. So immediately after the CHAPELLA fork, there was a bug in the relationship with how Prism interacted with mevboost and that resulted in huge network instability. Immediately post the hard fork, it took a few epochs for the chain to finalize.\n00:27:45.134 - 00:28:13.862, Speaker D: There was a lot of missed slots, it was full blown like firefighting mode. And that comes from the fact that there's these ten relays and all of the software that is running on the validator machines is kind of decoupled from this mevboost external software. So there is like consensus stability implications around the centralization found in particular in out of protocol PBS systems.\n00:28:14.006 - 00:29:02.570, Speaker F: I would add to that there is the overall technical complexity of entry and PBS. The merge is now basically just a year ago and the whole year we've been thinking about moving PBS more in protocol, how to get rid and move beyond the relays as trusted actors. And it's super hard challenges where you may need a lot of additional responsibilities, you may need to increase the consensus protocol complexity, which is already pretty hard to reason about, and it could introduce new nuanced reorg risks or vulnerabilities. And this is just a thing that is very hard, a very hard problem to get right. So I would say there is a lot of overall technical risk on the path to improrotocol PBS.\n00:29:04.110 - 00:29:52.970, Speaker E: Yeah, I have a bit more kind of arcane point, but clearly we are seeing that proposal builder separation can exist outside the protocol and that's where it does so far most of the time. And not all of this stuff is actually maintained by Ethereum core developers, I guess. As someone who's working for the Ethereum Foundation, Mike, what do you think this does to kind of the power dynamics in the Ethereum ecosystem? Is it on the one hand, more that we have to change the definition of what it means to be a core developer? Or is it that Ethereum should eventually try to pull everything into the protocol? What do you think it does to the invisible kind of power in the ecosystem?\n00:29:53.390 - 00:30:59.050, Speaker D: Yeah, I would say Barnaby has a really nice post on this. We keep calling him out, but he has a post called Seeing like. A protocol and he defines kind of what it could look like to enshrine different things and when to draw the line and say, okay, this is out of protocol versus in protocol. And I think part of EPBs and the work that I've been focusing on is kind of figuring out not only what to enshrine, like what design works for EPBs, whatever, technically speaking, but also on a more meta level, should we actually do the enshrinement? And one of our recent pieces that we wrote with us, four actually, and a few others was kind of talking about the role of PBS and Enshrined PBS in the world in which a relay market exists outside of the protocol still. So we'll probably touch on that later. But I guess in the current meta where mevboost essentially is core protocol software, I think there's a bit of an ownership mismatch. Flashbots.org\n00:30:59.050 - 00:31:53.410, Speaker D: wrote this code and it's been working really well for the year that's been running post merge. But I think everyone would agree that the testing and tooling and specification around that code is not at the same level of the core consensus clients. And part of that is because it's sort of a public good, but it's also originally written by Flashbot. So I'm not sure exactly how the ownership should evolve and the politics there. I will say I guess one of my big reasons why I like Enshrine PBS is because it makes that distinction a lot more clear. Like it draws the line in the sand is like this is the in protocol mechanism that we're going to maintain in terms of the consensus spec and the client teams. If you want to go outside of that, you have to rely on out of protocol software that might inherently be more brittle, more risky, et cetera.\n00:31:53.410 - 00:31:56.580, Speaker D: Hopefully that answered your question.\n00:31:57.270 - 00:32:13.350, Speaker E: Do you think it's more risky for ethereum? That important part of the ethereum stack are maintained by kind of non ethereum foundation teams that may even have a commercial interest? Or do you think it's more risky that it isn't?\n00:32:14.250 - 00:33:31.040, Speaker D: Yeah, I think it feels more risky in the current state, and I'll say especially right now, it feels like the equilibrium we're in is not stable. The relays are kind of fighting for their lives in terms of some of them are third party kind of credibly neutral relays that are trying to get funding from grants and other things. Other relays are parts of companies and commercial entities that are trying to either monetize or trying to figure out if this is part of the core business model. And I think even some of the large relay operators now, it's not clear that if we don't find a viable funding mechanism, we'll be around by the end of the year, for example. So I think insofar as we get to a world where there's only like two or three relays, that is much riskier to the protocol than the current status quo, which seems to be the direction we're headed in. So I would say yeah, either enshrining something and clearly delineating between in protocol and out of protocol PBS, or finding a way to ensure that the mevboost ecosystem is more stable into the future and more sustainable, is going to be critical in the coming weeks and months.\n00:33:31.970 - 00:34:19.214, Speaker E: If we see in the protocol that there are some incentives for different actors to specialize or even the same actor to specialize in some way that they can make more money or that they can do additional things for the protocol. I feel like we have established PBS almost as kind of the canonical solution to this problem, but this is not the case at all, right? So I kind of want to place it kind of in contrast to some other things that you could also do. So what would you see as the main kind of schools of thought that are in some way competing with PBS on solving that problem?",
                        "previous_answer": "Enshrined Proposal Builder Separation, often referred to as ePBS, is a concept in the blockchain protocol field. It aims to enhance the protocol's functionality by solidifying certain elements at the protocol level. The ePBS method proposes integrating the market structure, encompassing proposals and builders, directly into the protocol. While this method is seen as more prescriptive and less adaptable than other models, it enables validators to make commitments without specifying the exact form of these commitments. Despite the rise of ePBS, there's an increasing agreement that structures like relays may still be required, although their role might be diminished."
                    },
                    {
                        "event_type": "llm end",
                        "LLM_response": "Enshrined Proposal Builder Separation, or ePBS, is a principle in the blockchain domain that seeks to improve the protocol's capabilities by establishing certain components at the protocol level. This approach suggests incorporating the market structure, which includes proposals and builders, directly into the protocol. While this method is considered more directive and less flexible than other models, it allows validators to make commitments without defining the exact nature of these commitments. Despite the growing popularity of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit in a reduced capacity."
                    }
                ]
            }
        ]
    },
    {
        "event_type": "function_call end",
        "tool_output": "\nEnshrined Proposal Builder Separation, or ePBS, is a principle in the blockchain domain that seeks to improve the protocol's capabilities by establishing certain components at the protocol level. This approach suggests incorporating the market structure, which includes proposals and builders, directly into the protocol. While this method is considered more directive and less flexible than other models, it allows validators to make commitments without defining the exact nature of these commitments. Despite the growing popularity of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit in a reduced capacity.\n\nFetched based on the following sources: \n[Title]: An Incomplete Guide to PBS - with Mike Neuder and Chris Hager, [Channel name]: Uncommon Core 2.0, [Video Link]: https://www.youtube.com/watch?v=mEbK9AX7X7o, [Published date]: 2023-09-02\n[Title]: MEVday Paris: PBS and Beyond, [Channel name]: Frontier Research, [Video Link]: https://www.youtube.com/watch?v=Dk2tQA_Pxfo, [Published date]: 2023-08-08\n"
    },
    {
        "event_type": "llm start",
        "retrieved_context": null,
        "previous_answer": null
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I can answer without using any more tools.\nAnswer: Enshrined Proposal Builder Separation (ePBS) is a principle in the blockchain domain that aims to enhance the protocol's capabilities by integrating certain components at the protocol level. This approach proposes to incorporate the market structure, which includes proposals and builders, directly into the protocol. While this method is seen as more directive and less flexible than other models, it allows validators to make commitments without specifying the exact nature of these commitments. Despite the increasing popularity of ePBS, there's a growing consensus that structures like relays may still be necessary, albeit in a reduced capacity (\"An Incomplete Guide to PBS - with Mike Neuder and Chris Hager\", \"MEVday Paris: PBS and Beyond\").",
        "subjective grade from 1 to 10": ""
    },
    {
        "event_type": "llm start",
        "model_params": {
            "model": "gpt-4-0613",
            "temperature": 0,
            "max_tokens": null,
            "additional_kwargs": {},
            "max_retries": 10,
            "api_type": "open_ai",
            "api_base": "https://api.openai.com/v1",
            "api_version": "",
            "class_type": "openai",
            "class_name": "openai_llm"
        },
        "user_raw_input": "What is SUAVE?",
        "LLM_input": "What is SUAVE?\n\nTo determine if you should take the action to use the query engine, use its description detailed below. Use the query engine rather than not and do not rely on your prior knowledge.\nThe query engine tool has access to research papers and \nYouTube videos about the following content: Maximal Extractable Value (MEV); loss-versus-rebalancing (LVR); blockchain intents; \nthe Single Unifying Auction for Value Expression (SUAVE); sequencers; transaction ordering, L1s, L2s, transaction ordering, order flow auctions, auction theory, mechanism design.\n\n"
    },
    {
        "event_type": "llm end",
        "LLM_response": "Thought: I need to use a tool to help me answer the question.\nAction: query_engine_tool\nAction Input: {\"input\": \"SUAVE\"}",
        "additional_content": [
            {
                "function_call": [
                    {
                        "event_type": "templating start",
                        "instructions": "system: You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\nuser: Context information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query_str}\nAnswer: \nassistant: ",
                        "retrieved_chunk": {
                            "context_str": "total_pages: 25\nsource: 12\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\nthe chain. These sets of bundles represent blocks to be added to participat-\ning blockchains. With enough participation, SUAVE is intended to act as a\none-stop-shop for transaction submission across many blockchains.\nUniswap have announced UniswapX [2], an order-flow auction (Dutch Auc-\ntion) to assist with routing in the proposed Uniswap V4 [1]. Due to the potential\nfor thousands of liquidity pools for the same token pair in Uniswap V4, some\noff-chain routing is likely needed to ensure users receive the best possible pricing.\nEmulating a Dutch Auction among searchers to fill user orders should help in\nthis regard. However, the UniswapX proposal also mentions the ability for auc-\ntion winners to use their own private inventory/other on-chain liquidity, which\nmight have negative implications for on-chain LP providers. A legacy order-flow\nauction protocol that appears to have shutdown in recent months is Rook [50].\n3.4.3\nOpen Questions\nSUAVE depends on a centralized auctioneer [28] (Flashbots or an additional\ntrusted third party). The Flashbots hope is that this auctioneer can eventually\nbe replicated by some form of trusted execution environment, and eventually\na transparent decentralized system.\nThe path to such an end-goal remains\nunsolved and a contentious point. Trust in a trusted auctioneer appears to be\nwidely accepted, at least in the case of Flashbots as the trusted auctioneer,\nevidenced by the amount of blocks produced by MEV-boost [29], which also\ndepends on trusting Flashbots as an auctioneer.\nThe removal of extractable value through revenue-maximizing off-chain auc-\ntions depends on many factors, including searcher competition and auctioneer\ntrust, both of which need further investigation. Without atomicity guarantees,\nmost searchers in these domains must charge fees to off-set the cost and prob-\nability of not executing intended orders on dependent domains. Specifically, if\na searcher agrees to sell some amount of tokens to a user in the order-flow auc-\ntion with the intention of/assuming that they will be buying those tokens back\non Ethereum, the searcher must increase their fee proportionally to the cost of\nnot executing the swap on Ethereum. Sequencers would practically eliminate\nthis cost. With such costs, searcher competition likely diminishes, reintroducing\nextractable value opportunities for dominant searchers.\nThe logic around maximizing extractable value through re-ordering bun-\ndles is intended to be run by trusted execution environments [39], such as\ntrusted off-chain hardware or cloud infrastructure. Centralizing power off-chain\nto trusted execution environments requires significant further study, with guar-\nantees/limitations of such solutions clearly stated for the community. Another\nconcern for order-flow auctions is the ability to auction bundles of orders in a\nrevenue-maximizing way for users generating the orders. The concern here is\nthe creation of incentives to spam the auction and fill bundles with fake orders,\npaying less revenue to the affected users. This is discussed in detail here [34].\nWith respect to UniswapX, questions remain about the effect that improved\nuser execution has for on-chain LPs. Given a particular Uniswap V4 pool is\ntrading with a user, this pool must be offering a better price than any other\n12\n\ndocument_type: youtube_video\ntitle: Are Intents, SUAVE, Account Abstraction, & Cross-Chain Bridging all the same thing? - Uma Roy\nchannel_name: Research Day\nvideo_link: https://www.youtube.com/watch?v=G0nFyq9DDPw\nrelease_date: 2023-05-26\n\nThen the account abstraction bundlers look a lot like this execution market. And then the bundles of user ops look a lot like bundles. I don't have too much. That's very formal to say here, other than I think these ideas of MultiChain account abstraction and Suave and Intents are all very interrelated. So what's my main point here? And bring up Suave.\n00:30:00.648 - 00:31:00.480, Speaker A: So I've noticed, and I think a lot of you would agree that a lot of discourse about Intents and Suave is very academic. It feels like there's a lot of researchers and then they often view Swab and Intense through this lens of the mev searcher and the power user. They're like, okay, there'll be this user, they'll really want to land their transaction across two different roll ups at the exact same time. And they talk about these users quite a bit. And I think I've never really heard Suave and better UX for the normie user mentioned in the same sentence. And I think my thesis with all this stuff is like the normies actually really need Suave too, like cross domain intense and Suave is critical for UX and I actually think it will make a big difference for UX and is really important for the average user, not the searcher. And so my final point is account abstraction has been really excellent at thinking about end users.\n00:31:00.480 - 00:31:46.508, Speaker A: How they made this protocol is kind of like they took really common end user intents, like, okay, my normie user doesn't want to care about gas, my normie user wants rate limiting, my normie user wants whitelist, blacklist. And then they made this kind of complicated, somewhat ugly, like, handcrafted protocol to serve these common end user intents in mind, which is really great. And then Intents and Suave is kind of on the other end of the spectrum. It's this very general thing. It's very flexible. And basically I think a lot of those ideas are actually really useful in making something like account abstraction more general, more powerful and also a better UX. And so my kind of plea to all the flashbox people here and all the people thinking about Intents and Suave is please think about the normie users.\n00:31:46.508 - 00:32:11.370, Speaker A: And I think in this world where Intense and Suave actually do end up thinking about the normie users, cross chain bridging can and will be like 100 x better in the future. And hopefully the normie user doesn't even really need to know what a bridge is. It's all very abstract and thing. All the money gets pulled and routed to where they want it to be and they just take the actions they want to take. Yeah. So that's my talk.\n\ndocument_type: youtube_video\ntitle: SUAVE smart contract programming model: TEE-based smart contracts for block building - Andrew Miller\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=DhsDFKnHPa0\nrelease_date: 2023-07-13\n\n00:00:02.810 - 00:00:56.558, Speaker A: All right, I'm new to Flashbots as past few weeks, past couple of months. But in the past few weeks I've been meeting a lot of my computer security distributed systems colleagues and explaining why the hell I'm working on Flashbots for my Sabbatical project. And so I put a bunch of slide decks together to explain this kind of quickly to some particular friends who have PyCon text with me on technical, you know, easy conversations with them, but who either are unfamiliar with Flashbots or have pre existing strong negative interpretations of Flashbots. And so these are fairly opinionated slides I made, you know, kind of quickly. But Tina, others like the presentation, so encourage me this way. The caveats I would give to them as well are that I'm just going kind of quick on what I've observed or think about Flashbots. So I got to read all the old notion drafts and just simplify stuff from everyone.\n00:00:56.558 - 00:01:32.460, Speaker A: So some bits are like way oversimplified, some are deliberately oversimplified. But I basically have a very simplistic model that's like, everything's just uniswap v two transactions and that's the only thing that really exists. Apparently. The data team says that's not entirely far off, but I leave out in my mental model things like centralized exchange prices so those won't show up. And then this is very much like a vision of slob that I think is an interpretation of what's existed before, but maybe it shapes something a little bit. So take it as like a personal vision. Maybe it'll come true.\n00:01:32.460 - 00:02:25.260, Speaker A: The first thing especially aimed at researchers who have a pre existing negative opinion of what Flashbots does, it normally it's really strange. The first CMU thing that I went to after joining Flashbots and encountering all of these people, and they would say in hush tones like, how are we going to stop Flashbots? Well, I've heard Flashbots is using Teas to extract even more amoebia than they did before. And so after studying myself, I've come into the interpretation that's neither what Flashbots aims to do or what it does. I'm telling you, I think I'm preaching to a choir here, so no one thinks this, but yeah, you'll hear from my way. So I don't even think the extracting mev framing is all that accurate. So to me, what Flashbots is already doing and continues to do is two things preventing front running and making back running better again. In the only world of uniswap v two transactions, those are really only the two things you can do.\n00:02:25.260 - 00:03:05.814, Speaker A: I come from the world of computer security research, and I've done Snarks and NPC crypto things. But in the last year I've been obsessed with hardware based smart contracts like secret Network. I don't have in this thing on things, on disclosures and security bugs in secret network. But that's been like the long story of my research groups, like last year of stuff. So I can mention those on questions, but say forgetting that the basic thing of Secret Network is that it's only got one mem pool and it is a private mem pool because all of the transactions in Secret Network are encrypted. This is like what the encrypted ciphertext looks like. And the only way that Secret network transactions get into a block is by tendermin validators putting them in order.\n00:03:05.814 - 00:03:51.634, Speaker A: But you can't do anything with the ciphertext until the block is already finalized and then you only get to execute them in the next block. That actually wasn't always the case, but that is the case now after our most recent security vulnerability disclosures to them. But the upshot of that is that Secret Network is already a little bit like suave in that they have a uniswap V Two clone. Every blockchain does. Their uniswap V Two clone automatically has front running resistance because of that built in blind ordering applied to every transaction that's sent through Secret Network. So Sienna swap is their uniswap and it eliminates the nearest possibility of front running LDA replay Prevention. That's sort of accurate, but to me, what's taken a long time to adapt to is to realize that that alone doesn't kind of solve the whole problem you want to solve.\n00:03:51.634 - 00:04:28.130, Speaker A: Because if you just have the fair ordering, then you have really messy arbitrage that comes later. This is a new illustration that I've done to convey what this looks like. So messy arbitrage opportunities are the result of shitty user trades. A shitty user trade is one where you only trade on one pool, but most of the liquidity is fragmented on other pools, so the user gets a terrible price. I've skipped to the seeker network version where this is like a private trade so you don't see it. You only see it after the block is finalized. Then you get to see what the result of the trade was in time for the beginning of the process of the next block.\n00:04:28.130 - 00:05:33.750, Speaker A: What happens then? A priority gas auction war breaks out because whoever can get the transaction at the top of block gets to take all the arbitrage opportunity, restoring the balance of the pools. But they get to pocket that arbitrage difference. And so not getting to swab like how this could be done more trust minimize just what Mevshare at Flashbots does today. Is it's like the same thing as in Secret Network in that your transaction is private until it gets settled, but with Mevshare backrunners get to bid on getting to put the transaction immediately after yours, even in the same block. But when bidding, they have to share the larger percentage of the arbitrage that they take back with the original user. So the result is that it's like at a technical level, it is implemented as extract and then redistribute. I prefer to think of it as you are getting a competitive auction to arbitragers for a commission for them to do the backRunning for you automatically upgrading your shitty user transaction into a more sophisticated Arbitrage free transaction that results.\n00:05:33.750 - 00:06:07.090, Speaker A: I think that's pretty realistic. I mean, that's okay with just the uniswap. Things like central exchanges are a little bit different. What's Suave compared to Flashbots as it exists today? It's three things roughly in sequence. So just like T based smart contracts, it's replacing trust in the operator Flashbots to keep the privacy with trusted hardware so that they can't peek at it even if they wanted to.\n\ntotal_pages: 25\nsource: 11\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\nfunctionality given the existence of cross-domain agents. Actions/intents re-\nquiring multiple domains can have conditional execution constraints controlled\nby the user, or delegated cross-domain executors. As users can interact with\ndirectly with these cross-domain agents, the trust required is significantly less\n(pairwise economic/social/legal contracts can be established).\n3.3.3\nOpen Questions\nCurrent implementations of single-domain sequencers are centralized. Decen-\ntralizing the sequencer without degrading performance and usability is an active\narea of research. However, single-domain sequencers have significantly less tech-\nnical barriers than shared sequencers, with connectivity and the alignment of\nincentives simplified in the single domain case. The trade-off here is the ability\nto execute cross-domain transactions atomically. As discussed above, this may\nnot be such a big barrier.\n3.4\nOrder-flow Auctions\n3.4.1\nIdeal Functionality\nUsers are able to auction their orders/order information among a set of searchers.\nThe proceeds of these auctions are returned to the user. An ideal cross-domain\norder-flow auction eliminates both intrinsic and time-extractable value for the\nuser, assuming the order-flow auctions are indeed revenue maximizing.\nThe\nrevenue from such an auction should equal to total extractable value for the\nextractor.\nProtocol-level value extraction (against the LPs) still remains. Multi-domain\norder flow auctions (like the proposed SUAVE protocol) will shift some of the\nsignal MEV to ordering MEV in a similar fashion to shared sequencers.\n3.4.2\nImplementations and Discussion\nThe heavyweight contender in this domain is the Flashbots-proposed SUAVE\n(Single Unified Auction for Value Expression) chain [27]. Here, users"
                        }
                    },
                    {
                        "event_type": "llm start",
                        "tool_output": "Context information is below.\n---------------------\ntotal_pages: 25\nsource: 12\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\nthe chain. These sets of bundles represent blocks to be added to participat-\ning blockchains. With enough participation, SUAVE is intended to act as a\none-stop-shop for transaction submission across many blockchains.\nUniswap have announced UniswapX [2], an order-flow auction (Dutch Auc-\ntion) to assist with routing in the proposed Uniswap V4 [1]. Due to the potential\nfor thousands of liquidity pools for the same token pair in Uniswap V4, some\noff-chain routing is likely needed to ensure users receive the best possible pricing.\nEmulating a Dutch Auction among searchers to fill user orders should help in\nthis regard. However, the UniswapX proposal also mentions the ability for auc-\ntion winners to use their own private inventory/other on-chain liquidity, which\nmight have negative implications for on-chain LP providers. A legacy order-flow\nauction protocol that appears to have shutdown in recent months is Rook [50].\n3.4.3\nOpen Questions\nSUAVE depends on a centralized auctioneer [28] (Flashbots or an additional\ntrusted third party). The Flashbots hope is that this auctioneer can eventually\nbe replicated by some form of trusted execution environment, and eventually\na transparent decentralized system.\nThe path to such an end-goal remains\nunsolved and a contentious point. Trust in a trusted auctioneer appears to be\nwidely accepted, at least in the case of Flashbots as the trusted auctioneer,\nevidenced by the amount of blocks produced by MEV-boost [29], which also\ndepends on trusting Flashbots as an auctioneer.\nThe removal of extractable value through revenue-maximizing off-chain auc-\ntions depends on many factors, including searcher competition and auctioneer\ntrust, both of which need further investigation. Without atomicity guarantees,\nmost searchers in these domains must charge fees to off-set the cost and prob-\nability of not executing intended orders on dependent domains. Specifically, if\na searcher agrees to sell some amount of tokens to a user in the order-flow auc-\ntion with the intention of/assuming that they will be buying those tokens back\non Ethereum, the searcher must increase their fee proportionally to the cost of\nnot executing the swap on Ethereum. Sequencers would practically eliminate\nthis cost. With such costs, searcher competition likely diminishes, reintroducing\nextractable value opportunities for dominant searchers.\nThe logic around maximizing extractable value through re-ordering bun-\ndles is intended to be run by trusted execution environments [39], such as\ntrusted off-chain hardware or cloud infrastructure. Centralizing power off-chain\nto trusted execution environments requires significant further study, with guar-\nantees/limitations of such solutions clearly stated for the community. Another\nconcern for order-flow auctions is the ability to auction bundles of orders in a\nrevenue-maximizing way for users generating the orders. The concern here is\nthe creation of incentives to spam the auction and fill bundles with fake orders,\npaying less revenue to the affected users. This is discussed in detail here [34].\nWith respect to UniswapX, questions remain about the effect that improved\nuser execution has for on-chain LPs. Given a particular Uniswap V4 pool is\ntrading with a user, this pool must be offering a better price than any other\n12\n\ndocument_type: youtube_video\ntitle: Are Intents, SUAVE, Account Abstraction, & Cross-Chain Bridging all the same thing? - Uma Roy\nchannel_name: Research Day\nvideo_link: https://www.youtube.com/watch?v=G0nFyq9DDPw\nrelease_date: 2023-05-26\n\nThen the account abstraction bundlers look a lot like this execution market. And then the bundles of user ops look a lot like bundles. I don't have too much. That's very formal to say here, other than I think these ideas of MultiChain account abstraction and Suave and Intents are all very interrelated. So what's my main point here? And bring up Suave.\n00:30:00.648 - 00:31:00.480, Speaker A: So I've noticed, and I think a lot of you would agree that a lot of discourse about Intents and Suave is very academic. It feels like there's a lot of researchers and then they often view Swab and Intense through this lens of the mev searcher and the power user. They're like, okay, there'll be this user, they'll really want to land their transaction across two different roll ups at the exact same time. And they talk about these users quite a bit. And I think I've never really heard Suave and better UX for the normie user mentioned in the same sentence. And I think my thesis with all this stuff is like the normies actually really need Suave too, like cross domain intense and Suave is critical for UX and I actually think it will make a big difference for UX and is really important for the average user, not the searcher. And so my final point is account abstraction has been really excellent at thinking about end users.\n00:31:00.480 - 00:31:46.508, Speaker A: How they made this protocol is kind of like they took really common end user intents, like, okay, my normie user doesn't want to care about gas, my normie user wants rate limiting, my normie user wants whitelist, blacklist. And then they made this kind of complicated, somewhat ugly, like, handcrafted protocol to serve these common end user intents in mind, which is really great. And then Intents and Suave is kind of on the other end of the spectrum. It's this very general thing. It's very flexible. And basically I think a lot of those ideas are actually really useful in making something like account abstraction more general, more powerful and also a better UX. And so my kind of plea to all the flashbox people here and all the people thinking about Intents and Suave is please think about the normie users.\n00:31:46.508 - 00:32:11.370, Speaker A: And I think in this world where Intense and Suave actually do end up thinking about the normie users, cross chain bridging can and will be like 100 x better in the future. And hopefully the normie user doesn't even really need to know what a bridge is. It's all very abstract and thing. All the money gets pulled and routed to where they want it to be and they just take the actions they want to take. Yeah. So that's my talk.\n\ndocument_type: youtube_video\ntitle: SUAVE smart contract programming model: TEE-based smart contracts for block building - Andrew Miller\nchannel_name: Flashbots\nvideo_link: https://www.youtube.com/watch?v=DhsDFKnHPa0\nrelease_date: 2023-07-13\n\n00:00:02.810 - 00:00:56.558, Speaker A: All right, I'm new to Flashbots as past few weeks, past couple of months. But in the past few weeks I've been meeting a lot of my computer security distributed systems colleagues and explaining why the hell I'm working on Flashbots for my Sabbatical project. And so I put a bunch of slide decks together to explain this kind of quickly to some particular friends who have PyCon text with me on technical, you know, easy conversations with them, but who either are unfamiliar with Flashbots or have pre existing strong negative interpretations of Flashbots. And so these are fairly opinionated slides I made, you know, kind of quickly. But Tina, others like the presentation, so encourage me this way. The caveats I would give to them as well are that I'm just going kind of quick on what I've observed or think about Flashbots. So I got to read all the old notion drafts and just simplify stuff from everyone.\n00:00:56.558 - 00:01:32.460, Speaker A: So some bits are like way oversimplified, some are deliberately oversimplified. But I basically have a very simplistic model that's like, everything's just uniswap v two transactions and that's the only thing that really exists. Apparently. The data team says that's not entirely far off, but I leave out in my mental model things like centralized exchange prices so those won't show up. And then this is very much like a vision of slob that I think is an interpretation of what's existed before, but maybe it shapes something a little bit. So take it as like a personal vision. Maybe it'll come true.\n00:01:32.460 - 00:02:25.260, Speaker A: The first thing especially aimed at researchers who have a pre existing negative opinion of what Flashbots does, it normally it's really strange. The first CMU thing that I went to after joining Flashbots and encountering all of these people, and they would say in hush tones like, how are we going to stop Flashbots? Well, I've heard Flashbots is using Teas to extract even more amoebia than they did before. And so after studying myself, I've come into the interpretation that's neither what Flashbots aims to do or what it does. I'm telling you, I think I'm preaching to a choir here, so no one thinks this, but yeah, you'll hear from my way. So I don't even think the extracting mev framing is all that accurate. So to me, what Flashbots is already doing and continues to do is two things preventing front running and making back running better again. In the only world of uniswap v two transactions, those are really only the two things you can do.\n00:02:25.260 - 00:03:05.814, Speaker A: I come from the world of computer security research, and I've done Snarks and NPC crypto things. But in the last year I've been obsessed with hardware based smart contracts like secret Network. I don't have in this thing on things, on disclosures and security bugs in secret network. But that's been like the long story of my research groups, like last year of stuff. So I can mention those on questions, but say forgetting that the basic thing of Secret Network is that it's only got one mem pool and it is a private mem pool because all of the transactions in Secret Network are encrypted. This is like what the encrypted ciphertext looks like. And the only way that Secret network transactions get into a block is by tendermin validators putting them in order.\n00:03:05.814 - 00:03:51.634, Speaker A: But you can't do anything with the ciphertext until the block is already finalized and then you only get to execute them in the next block. That actually wasn't always the case, but that is the case now after our most recent security vulnerability disclosures to them. But the upshot of that is that Secret Network is already a little bit like suave in that they have a uniswap V Two clone. Every blockchain does. Their uniswap V Two clone automatically has front running resistance because of that built in blind ordering applied to every transaction that's sent through Secret Network. So Sienna swap is their uniswap and it eliminates the nearest possibility of front running LDA replay Prevention. That's sort of accurate, but to me, what's taken a long time to adapt to is to realize that that alone doesn't kind of solve the whole problem you want to solve.\n00:03:51.634 - 00:04:28.130, Speaker A: Because if you just have the fair ordering, then you have really messy arbitrage that comes later. This is a new illustration that I've done to convey what this looks like. So messy arbitrage opportunities are the result of shitty user trades. A shitty user trade is one where you only trade on one pool, but most of the liquidity is fragmented on other pools, so the user gets a terrible price. I've skipped to the seeker network version where this is like a private trade so you don't see it. You only see it after the block is finalized. Then you get to see what the result of the trade was in time for the beginning of the process of the next block.\n00:04:28.130 - 00:05:33.750, Speaker A: What happens then? A priority gas auction war breaks out because whoever can get the transaction at the top of block gets to take all the arbitrage opportunity, restoring the balance of the pools. But they get to pocket that arbitrage difference. And so not getting to swab like how this could be done more trust minimize just what Mevshare at Flashbots does today. Is it's like the same thing as in Secret Network in that your transaction is private until it gets settled, but with Mevshare backrunners get to bid on getting to put the transaction immediately after yours, even in the same block. But when bidding, they have to share the larger percentage of the arbitrage that they take back with the original user. So the result is that it's like at a technical level, it is implemented as extract and then redistribute. I prefer to think of it as you are getting a competitive auction to arbitragers for a commission for them to do the backRunning for you automatically upgrading your shitty user transaction into a more sophisticated Arbitrage free transaction that results.\n00:05:33.750 - 00:06:07.090, Speaker A: I think that's pretty realistic. I mean, that's okay with just the uniswap. Things like central exchanges are a little bit different. What's Suave compared to Flashbots as it exists today? It's three things roughly in sequence. So just like T based smart contracts, it's replacing trust in the operator Flashbots to keep the privacy with trusted hardware so that they can't peek at it even if they wanted to.\n\ntotal_pages: 25\nsource: 11\ndocument_type: research_paper\ntitle: SoK: Cross-Domain MEV\nauthors: Conor McMenamin\npdf_link: http://arxiv.org/pdf/2308.04159v1\nrelease_date: 2023-08-08\n\nfunctionality given the existence of cross-domain agents. Actions/intents re-\nquiring multiple domains can have conditional execution constraints controlled\nby the user, or delegated cross-domain executors. As users can interact with\ndirectly with these cross-domain agents, the trust required is significantly less\n(pairwise economic/social/legal contracts can be established).\n3.3.3\nOpen Questions\nCurrent implementations of single-domain sequencers are centralized. Decen-\ntralizing the sequencer without degrading performance and usability is an active\narea of research. However, single-domain sequencers have significantly less tech-\nnical barriers than shared sequencers, with connectivity and the alignment of\nincentives simplified in the single domain case. The trade-off here is the ability\nto execute cross-domain transactions atomically. As discussed above, this may\nnot be such a big barrier.\n3.4\nOrder-flow Auctions\n3.4.1\nIdeal Functionality\nUsers are able to auction their orders/order information among a set of searchers.\nThe proceeds of these auctions are returned to the user. An ideal cross-domain\norder-flow auction eliminates both intrinsic and time-extractable value for the\nuser, assuming the order-flow auctions are indeed revenue maximizing.\nThe\nrevenue from such an auction should equal to total extractable value for the\nextractor.\nProtocol-level value extraction (against the LPs) still remains. Multi-domain\norder flow auctions (like the proposed SUAVE protocol) will shift some of the\nsignal MEV to ordering MEV in a similar fashion to shared sequencers.\n3.4.2\nImplementations and Discussion\nThe heavyweight contender in this domain is the Flashbots-proposed SUAVE\n(Single Unified Auction for Value Expression) chain [27]. Here, users\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Always provide an exhaustive answer to the question, unless told otherwise in the question itself.\nDirectly quote the sources of your knowledge in the same sentence in parentheses. If several files are matched across several years of release dates, favor most recent content. Now answer the question: What is SUAVE?\nAnswer: "
                    }
                ]
            }
        ]
    }
]